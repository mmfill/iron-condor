{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Notebook for training for stock price with rolling forecast\n",
    "Forecast for last 10% of data. Forecast is done day by day.\n",
    "- Sequential model\n",
    "- LSTM model\n",
    "- not stationary\n",
    "- rolling forecast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from matplotlib import cm\n",
    "import numpy as np\n",
    "import scipy.stats as si\n",
    "from IPython.display import Image\n",
    "import datetime\n",
    "import time\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dense, Activation, Dropout\n",
    "from keras.layers import LSTM\n",
    "from sklearn import preprocessing, metrics\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "backup = pd.HDFStore('backup_v05.h5')\n",
    "#df = backup['df']\n",
    "#unscaled_seq_predictions = backup['unscaled_seq_predictions']\n",
    "#unscaled_data = backup['unscaled_data']\n",
    "#unscaled_lstm_predictions = backup['unscaled_lstm_predictions']\n",
    "#test_data_unscaled = backup['test_data_unscaled']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Iron Condor\n",
    "The iron condor is a investment strategy using four option with the same strike date. More precise, the iron condor uses two vertical spreads, one put spread and a call spread. The put spread consists at of a short put option (P_s) with a strike price below the actual stock price and a long put option (P_l) with a strike price below the short put option P_s. The call spread consists of a short call option (C_s) above the actual stock price and a long call option (C_l) above the short call option C_s. All options have the same strike date and the spread on both legs can vary but is chosen mostly the same.\n",
    "![title](Condor_strategy.png)\n",
    "\"A trader who buys an iron condor speculates that the spot price of the underlying instrument will be between the short strikes when the options expire where the position is the most profitable. Thus, the iron condor is an options strategy considered when the trader has a neutral outlook for the market. Buying iron condors are popular with traders who seek regular income from their trading capital. An iron condor buyer will attempt to construct the trade so that the short strikes are close enough that the position will earn a desirable net credit, but wide enough apart so that it is likely that the spot price of the underlying will remain between the short strikes for the duration of the options contract. The trader would typically play iron condors every month (if possible) thus generating monthly income with the strategy.\" [https://en.wikipedia.org/wiki/Iron_condor]\n",
    "\n",
    "### Goal of the project\n",
    "The goal of the project is to find a strategy to maximise the profit with an iron condor. The ideal short and long spreads should be found at any time based on data of the underlying stock and of the indices S&P500 and Nasdaq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data(var_name,name):\n",
    "    # reads in csv into DataFrame, keeps Adj Close and Volume and calculates rolling averages and \n",
    "    # rolling standard deviation of Adj Close for 4, 9 and 18 days\n",
    "    # var_name: path to csv file\n",
    "    # name: name of the column df\n",
    "    df = pd.read_csv('data/'+var_name+'.csv',index_col='Date', parse_dates=True)\n",
    "    df.rename(columns={\"Adj Close\": name, \"Volume\": name+'_volume', \"Open\": name+'_open'}, inplace = True)\n",
    "    df.rename(columns={\"High\": name+'_high', \"Low\": name+'_low', \"Close\": name+'_close'}, inplace = True)\n",
    "    #df.drop(['Open','High','Low','Close'],axis=1, inplace = True)\n",
    "    #if name == 'google':\n",
    "        #df['google_45d'] = df['google'].shift(-32) # stock price in 45 days (approx. 32 trading days)\n",
    "    df[name+'_ra_04'] = df[name].rolling('4d').mean()\n",
    "    df[name+'_std_04'] = df[name].rolling('4d').std()\n",
    "    df[name+'_ra_09'] = df[name].rolling('9d').mean()\n",
    "    df[name+'_std_09'] = df[name].rolling('9d').std()\n",
    "    df[name+'_ra_18'] = df[name].rolling('18d').mean()\n",
    "    df[name+'_std_18'] = df[name].rolling('18d').std()\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigma_fct(df, name, days = 10):\n",
    "    # For Black-Scholes I need the volatility of return of the underlying assets in the last days\n",
    "    # based on https://www.wikihow.com/Calculate-Historical-Stock-Volatility\n",
    "    # First the ln of daily returns is calculated for each element of names\n",
    "    for name in names:\n",
    "        a=pd.DataFrame()\n",
    "        a['ln_daily_return'] = np.log(df[name] / df[name].shift(1))\n",
    "        # next the rolling average (mean) over certain days is calculated\n",
    "        a['rol_avg'] =  a['ln_daily_return'].rolling(str(days)+'d').mean()\n",
    "        # next deviation from the mean is calculated\n",
    "        a['dev_mean'] = a['ln_daily_return'] - a['rol_avg']\n",
    "        # next variance is calculated for certain days\n",
    "        a['dev_mean_sq'] = a['dev_mean']**2\n",
    "        a['variance'] = a['dev_mean_sq'].rolling(str(days)+'d').sum()/(a['dev_mean_sq'].rolling(str(days)+'d').count()-1)\n",
    "        # volatility is square root of variance\n",
    "        # do you need annual volatility?\n",
    "        df[name + '_daily_vol'] = np.sqrt(a['variance'])*np.sqrt(252) # 252 trading days in a year\n",
    "        #df.drop(['ln_daily_return'])#,'rol_avg','dev_mean','dev_mean_sq','variance'])\n",
    "        #return df[name + '_daily_vol']\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stationary_timeseries(df, damned_list = ['volume','std','short','daily']):\n",
    "    # makes columns in df that are not in damned list stationary. Means it calculates the \n",
    "    # difference row by row\n",
    "    columns = df.columns\n",
    "    df_sta = pd.DataFrame()\n",
    "    for column in columns:\n",
    "        if any(elem in column for elem in damned_list):\n",
    "            df_sta[column] = df[column]\n",
    "        else:\n",
    "            df_sta[column+'_sta'] = df[column].shift(1) - df[column]\n",
    "    df_sta.dropna(inplace = True)  # to make sure we have all the data on all the days\n",
    "    return df_sta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_df(var_name_list,name_list):\n",
    "    # takes list paths of csv files and names and passes it on to get_data.\n",
    "    # Gets df from get_data and merges them on 'Date'\n",
    "    # Calls sigma_fct to get sigma of daily returns \n",
    "    # var_name_list: list of paths to csv files\n",
    "    # name: list of names the columns of df\n",
    "    i=0\n",
    "    for var_name in var_name_list:\n",
    "        if i == 0:\n",
    "            df = get_data(var_name, name_list[i])\n",
    "        else:\n",
    "            df = pd.merge(df,get_data(var_name, name_list[i]),on='Date')\n",
    "        i += 1\n",
    "    df.dropna(inplace = True)  # to make sure we have all the data on all the days\n",
    "    \n",
    "    df = sigma_fct(df,names)\n",
    "    df=df[23:] # drop first entries to get clean results\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_target_data(df, name):\n",
    "    # function to create the target data\n",
    "    # name: name of the column the target data is created of\n",
    "    df[name+'_45d'] = df[name].shift(-32) # stock price in 45 days (approx. 32 trading days)\n",
    "    df.dropna(inplace = True)  # to make sure we have all the data on all the days\n",
    "    # df['short_call'] shows the perfect short call. If you would have shorted a call option with a strike price \n",
    "    # exactly the same as the stock price 45 days later, you made maximum profit. Therefore this is the target \n",
    "    # value for the short call option. The minimum is 0, so we never short a call option with a strike price\n",
    "    # below the stock price right now. The same for the short put option\n",
    "    #df['short_call'] = [x/y-1 if x/y > 1 else 0 for x,y in df[[name+'_45d', name]].values]\n",
    "    #df['short_put'] = [1-x/y if x/y < 1 else 0 for x,y in df[[name+'_45d', name]].values]\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>google_open</th>\n",
       "      <th>google_high</th>\n",
       "      <th>google_low</th>\n",
       "      <th>google_close</th>\n",
       "      <th>google</th>\n",
       "      <th>google_volume</th>\n",
       "      <th>google_ra_04</th>\n",
       "      <th>google_std_04</th>\n",
       "      <th>google_ra_09</th>\n",
       "      <th>google_std_09</th>\n",
       "      <th>...</th>\n",
       "      <th>nasdaq_ra_04</th>\n",
       "      <th>nasdaq_std_04</th>\n",
       "      <th>nasdaq_ra_09</th>\n",
       "      <th>nasdaq_std_09</th>\n",
       "      <th>nasdaq_ra_18</th>\n",
       "      <th>nasdaq_std_18</th>\n",
       "      <th>google_daily_vol</th>\n",
       "      <th>s&amp;p_daily_vol</th>\n",
       "      <th>nasdaq_daily_vol</th>\n",
       "      <th>google_45d</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Date</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2008-11-21</th>\n",
       "      <td>130.764862</td>\n",
       "      <td>134.182053</td>\n",
       "      <td>123.188263</td>\n",
       "      <td>130.725006</td>\n",
       "      <td>130.725006</td>\n",
       "      <td>20565700</td>\n",
       "      <td>136.935478</td>\n",
       "      <td>8.750923</td>\n",
       "      <td>143.875715</td>\n",
       "      <td>10.798013</td>\n",
       "      <td>...</td>\n",
       "      <td>1392.540009</td>\n",
       "      <td>68.742234</td>\n",
       "      <td>1452.251430</td>\n",
       "      <td>95.188063</td>\n",
       "      <td>1541.462141</td>\n",
       "      <td>127.530682</td>\n",
       "      <td>0.716470</td>\n",
       "      <td>0.802275</td>\n",
       "      <td>0.738519</td>\n",
       "      <td>156.946732</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2008-11-24</th>\n",
       "      <td>134.127258</td>\n",
       "      <td>134.470963</td>\n",
       "      <td>124.040070</td>\n",
       "      <td>128.239334</td>\n",
       "      <td>128.239334</td>\n",
       "      <td>20184700</td>\n",
       "      <td>129.482170</td>\n",
       "      <td>1.757636</td>\n",
       "      <td>137.580147</td>\n",
       "      <td>9.599201</td>\n",
       "      <td>...</td>\n",
       "      <td>1428.184998</td>\n",
       "      <td>61.992083</td>\n",
       "      <td>1420.705017</td>\n",
       "      <td>68.914436</td>\n",
       "      <td>1498.502503</td>\n",
       "      <td>101.345711</td>\n",
       "      <td>0.529842</td>\n",
       "      <td>0.864400</td>\n",
       "      <td>0.777819</td>\n",
       "      <td>155.761169</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2008-11-25</th>\n",
       "      <td>133.838348</td>\n",
       "      <td>142.794769</td>\n",
       "      <td>133.160873</td>\n",
       "      <td>140.498383</td>\n",
       "      <td>140.498383</td>\n",
       "      <td>21623100</td>\n",
       "      <td>134.368858</td>\n",
       "      <td>8.668457</td>\n",
       "      <td>137.997038</td>\n",
       "      <td>8.831976</td>\n",
       "      <td>...</td>\n",
       "      <td>1468.375000</td>\n",
       "      <td>5.154837</td>\n",
       "      <td>1426.994298</td>\n",
       "      <td>65.073432</td>\n",
       "      <td>1483.279999</td>\n",
       "      <td>90.035297</td>\n",
       "      <td>0.833833</td>\n",
       "      <td>0.791313</td>\n",
       "      <td>0.710049</td>\n",
       "      <td>156.573120</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2008-11-26</th>\n",
       "      <td>139.616684</td>\n",
       "      <td>147.178345</td>\n",
       "      <td>137.584305</td>\n",
       "      <td>145.499634</td>\n",
       "      <td>145.499634</td>\n",
       "      <td>12760800</td>\n",
       "      <td>138.079117</td>\n",
       "      <td>8.880829</td>\n",
       "      <td>137.425609</td>\n",
       "      <td>8.059325</td>\n",
       "      <td>...</td>\n",
       "      <td>1489.616659</td>\n",
       "      <td>36.971751</td>\n",
       "      <td>1434.144287</td>\n",
       "      <td>74.235572</td>\n",
       "      <td>1487.035382</td>\n",
       "      <td>87.259170</td>\n",
       "      <td>0.812863</td>\n",
       "      <td>0.758392</td>\n",
       "      <td>0.707779</td>\n",
       "      <td>149.923050</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2008-11-28</th>\n",
       "      <td>144.747452</td>\n",
       "      <td>147.671494</td>\n",
       "      <td>143.601746</td>\n",
       "      <td>145.933014</td>\n",
       "      <td>145.933014</td>\n",
       "      <td>5150200</td>\n",
       "      <td>143.977010</td>\n",
       "      <td>3.020363</td>\n",
       "      <td>136.698456</td>\n",
       "      <td>8.236770</td>\n",
       "      <td>...</td>\n",
       "      <td>1510.799967</td>\n",
       "      <td>39.935485</td>\n",
       "      <td>1450.814982</td>\n",
       "      <td>86.016593</td>\n",
       "      <td>1480.791532</td>\n",
       "      <td>79.789023</td>\n",
       "      <td>0.871477</td>\n",
       "      <td>0.804582</td>\n",
       "      <td>0.756923</td>\n",
       "      <td>148.936752</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 40 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            google_open  google_high  google_low  google_close      google  \\\n",
       "Date                                                                         \n",
       "2008-11-21   130.764862   134.182053  123.188263    130.725006  130.725006   \n",
       "2008-11-24   134.127258   134.470963  124.040070    128.239334  128.239334   \n",
       "2008-11-25   133.838348   142.794769  133.160873    140.498383  140.498383   \n",
       "2008-11-26   139.616684   147.178345  137.584305    145.499634  145.499634   \n",
       "2008-11-28   144.747452   147.671494  143.601746    145.933014  145.933014   \n",
       "\n",
       "            google_volume  google_ra_04  google_std_04  google_ra_09  \\\n",
       "Date                                                                   \n",
       "2008-11-21       20565700    136.935478       8.750923    143.875715   \n",
       "2008-11-24       20184700    129.482170       1.757636    137.580147   \n",
       "2008-11-25       21623100    134.368858       8.668457    137.997038   \n",
       "2008-11-26       12760800    138.079117       8.880829    137.425609   \n",
       "2008-11-28        5150200    143.977010       3.020363    136.698456   \n",
       "\n",
       "            google_std_09  ...  nasdaq_ra_04  nasdaq_std_04  nasdaq_ra_09  \\\n",
       "Date                       ...                                              \n",
       "2008-11-21      10.798013  ...   1392.540009      68.742234   1452.251430   \n",
       "2008-11-24       9.599201  ...   1428.184998      61.992083   1420.705017   \n",
       "2008-11-25       8.831976  ...   1468.375000       5.154837   1426.994298   \n",
       "2008-11-26       8.059325  ...   1489.616659      36.971751   1434.144287   \n",
       "2008-11-28       8.236770  ...   1510.799967      39.935485   1450.814982   \n",
       "\n",
       "            nasdaq_std_09  nasdaq_ra_18  nasdaq_std_18  google_daily_vol  \\\n",
       "Date                                                                       \n",
       "2008-11-21      95.188063   1541.462141     127.530682          0.716470   \n",
       "2008-11-24      68.914436   1498.502503     101.345711          0.529842   \n",
       "2008-11-25      65.073432   1483.279999      90.035297          0.833833   \n",
       "2008-11-26      74.235572   1487.035382      87.259170          0.812863   \n",
       "2008-11-28      86.016593   1480.791532      79.789023          0.871477   \n",
       "\n",
       "            s&p_daily_vol  nasdaq_daily_vol  google_45d  \n",
       "Date                                                     \n",
       "2008-11-21       0.802275          0.738519  156.946732  \n",
       "2008-11-24       0.864400          0.777819  155.761169  \n",
       "2008-11-25       0.791313          0.710049  156.573120  \n",
       "2008-11-26       0.758392          0.707779  149.923050  \n",
       "2008-11-28       0.804582          0.756923  148.936752  \n",
       "\n",
       "[5 rows x 40 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "var_names = ['GOOG','S&P','Nasdaq2']\n",
    "names = ['google','s&p','nasdaq']\n",
    "df = get_df(var_names, names)\n",
    "df = get_target_data(df, 'google')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "backup['df'] = df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Price of options\n",
    "It is really hard to get data about historic stock options. Be it as api channel or csv download. I managed only to find sources to pay: 500 USD for every year. Because that is too expensive for this project I decided to calculate the option price with the **Black–Scholes formula**, which gives a theoretical estimate of the price of European-style options.\n",
    "\n",
    "Based on the formula for non-divident paying options I calculate the option prices and add a small random term to create a bit of noise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def black_scholes(S, K, sigma, r=0.03, T=45/365, option = 'call'):\n",
    "    \n",
    "    #S: spot price\n",
    "    #K: strike price\n",
    "    #T: time to maturity in years, 45 days as default\n",
    "    #r: risk-free interest rate, assumed to be constant between t and T\n",
    "    #sigma: standard deviation (volatility) of RETURNS of underlying asset\n",
    "    \n",
    "    d1 = (np.log(S / K) + (r + 0.5 * sigma ** 2) * T) / (sigma * np.sqrt(T))\n",
    "    #d2 = (np.log(S / K) + (r - 0.5 * sigma ** 2) * T) / (sigma * np.sqrt(T))\n",
    "    d2 = d1 - sigma * np.sqrt(T)\n",
    "    \n",
    "    if option == 'call':\n",
    "        result = (S * si.norm.cdf(d1, 0.0, 1.0) - K * np.exp(-r * T) * si.norm.cdf(d2, 0.0, 1.0))\n",
    "    if option == 'put':\n",
    "        result = (K * np.exp(-r * T) * si.norm.cdf(-d2, 0.0, 1.0) - S * si.norm.cdf(-d1, 0.0, 1.0))\n",
    "        \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def option_prices(date, short_spread, long_spread, strike_days = 45):\n",
    "    # calculates prices of options and calculates initial return (net premium)\n",
    "    date = date_fct(date)\n",
    "    P_s_strike_price, P_l_strike_price, C_s_strike_price, C_l_strike_price = strike_prices(date, short_spread, long_spread)\n",
    "    sigma = get_sigma(date)\n",
    "    stock_price = stock_price_fct(date)\n",
    "    P_s_price = black_scholes(stock_price, P_s_strike_price, sigma, T=strike_days/365, option = 'put')\n",
    "    P_l_price = black_scholes(stock_price, P_l_strike_price, sigma, T=strike_days/365, option = 'put')\n",
    "    C_s_price = black_scholes(stock_price, C_s_strike_price, sigma, T=strike_days/365, option = 'call')\n",
    "    C_l_price = black_scholes(stock_price, C_l_strike_price, sigma, T=strike_days/365, option = 'call')\n",
    "    initial_return = P_s_price + C_s_price - P_l_price - C_l_price\n",
    "    \n",
    "    return P_s_price, P_l_price, C_s_price, C_l_price, initial_return"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now it is time to calculate the strike prices and option prices of all 4 options and the initial return by setting up the iron condor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate return of options\n",
    "There are five different regimes for the return. R: return, P(C) put(call) option, s(l): short(long), ir: initial return, stock_price: stock price\n",
    "- stock price is below long put option: R = ir + P_l_strike_price - P_s_strike_price\n",
    "- stock price is between long and short put option: R = ir + stock_price - P_s_strike_price\n",
    "- stock price is between short call and short put option: R = ir\n",
    "- stock price is between short and long call option: R = ir - stock_price + C_s_strike price\n",
    "- stock price is above long call option: R = ir - C_l_strike_price + C_s_strike_price"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define return function of iron condor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ic_return(trading_date, short_spread, long_spread, strike_days = 45):\n",
    "    P_s_strike_price, P_l_strike_price, C_s_strike_price, C_l_strike_price = strike_prices(trading_date, short_spread, long_spread)\n",
    "    P_s_price, P_l_price, C_s_price, C_l_price, initial_return = option_prices(trading_date,short_spread, long_spread, strike_days)\n",
    "    strike_date = date_fct(trading_date, 45)\n",
    "    final_stock_price = stock_price_fct(strike_date)\n",
    "    if final_stock_price < P_l_strike_price:\n",
    "        final_result = initial_return + P_l_strike_price - P_s_strike_price\n",
    "    elif final_stock_price >= P_l_strike_price and final_stock_price < P_s_strike_price:\n",
    "        final_result = initial_return + stock_price - P_s_strike_price\n",
    "    elif final_stock_price >= P_s_strike_price and final_stock_price < C_s_strike_price:\n",
    "        final_result = initial_return\n",
    "    elif final_stock_price >= C_s_strike_price and final_stock_price < C_l_strike_price:\n",
    "        final_result = initial_return - final_stock_price + C_s_strike_price\n",
    "    elif final_stock_price >= C_l_strike_price:\n",
    "        final_result = initial_return - C_l_strike_price + C_s_strike_price\n",
    "    maximum_result = initial_return\n",
    "    return final_result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Scale data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/matthias/anaconda3/lib/python3.7/site-packages/sklearn/preprocessing/data.py:334: DataConversionWarning: Data with input dtype int64, float64 were all converted to float64 by MinMaxScaler.\n",
      "  return self.partial_fit(X, y)\n"
     ]
    }
   ],
   "source": [
    "scaler = preprocessing.MinMaxScaler(feature_range=(-1, 1))\n",
    "df_scaled = pd.DataFrame(scaler.fit_transform(df), columns=df.columns, index = df.index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Split into train and test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_train_test_data(data, train_test_split, batch_size):\n",
    "    # data is dataframe to get train test data out of\n",
    "    train_size_raw = data.shape[0]*train_test_split\n",
    "    train_size = int(train_size_raw - train_size_raw % batch_size) # make train_size multiple of batch_size for \"statefull = True\" in LSTM\n",
    "    train_data = data.iloc[0:train_size]\n",
    "    test_size_raw = data.shape[0] - train_size\n",
    "    test_size = int(test_size_raw - test_size_raw%batch_size) # multiple of batch_size\n",
    "    test_data = data.iloc[train_size : train_size + test_size]\n",
    "    x_train = train_data.drop(columns=['google_45d']) # google_45d is not for training, just for reversing stationarity\n",
    "    y_train = train_data['google_45d']\n",
    "    x_test = test_data.drop(columns=['google_45d'])\n",
    "    y_test = test_data['google_45d']\n",
    "    train_index=x_train.index\n",
    "    x_columns = x_train.columns\n",
    "    test_index = x_test.index\n",
    "    return x_train, x_test, y_train, y_test, test_data, test_index, x_columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 10\n",
    "train_test_split = 0.9\n",
    "x_train, x_test, y_train, y_test, test_data, test_index, x_columns = get_train_test_data(df_scaled, train_test_split, batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sequential model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/matthias/anaconda3/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:74: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/matthias/anaconda3/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:517: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/matthias/anaconda3/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:4138: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/matthias/anaconda3/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:133: The name tf.placeholder_with_default is deprecated. Please use tf.compat.v1.placeholder_with_default instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/matthias/anaconda3/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "WARNING:tensorflow:From /home/matthias/anaconda3/lib/python3.7/site-packages/keras/optimizers.py:790: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_1 (Dense)              (None, 128)               5120      \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 16)                2064      \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 16)                0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 4)                 68        \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 4)                 0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 1)                 5         \n",
      "=================================================================\n",
      "Total params: 7,257\n",
      "Trainable params: 7,257\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Build the model architecture\n",
    "seq_model = Sequential()\n",
    "seq_model.add(Dense(128, activation='relu', input_shape=(x_train.shape[1],)))\n",
    "seq_model.add(Dropout(0.2))\n",
    "seq_model.add(Dense(16, activation='relu'))\n",
    "seq_model.add(Dropout(0.2))\n",
    "seq_model.add(Dense(4, activation='relu'))\n",
    "seq_model.add(Dropout(0.2))\n",
    "seq_model.add(Dense(1))\n",
    "\n",
    "# Compile the model using a loss function and an optimizer.\n",
    "seq_model.compile(optimizer='adam',loss='mean_squared_error')\n",
    "seq_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/matthias/anaconda3/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:986: The name tf.assign_add is deprecated. Please use tf.compat.v1.assign_add instead.\n",
      "\n",
      "Train on 2370 samples, validate on 270 samples\n",
      "Epoch 1/3000\n",
      "2370/2370 [==============================] - 1s 229us/step - loss: 0.2206 - val_loss: 0.0661\n",
      "Epoch 2/3000\n",
      "2370/2370 [==============================] - 0s 67us/step - loss: 0.1033 - val_loss: 0.0683\n",
      "Epoch 3/3000\n",
      "2370/2370 [==============================] - 0s 94us/step - loss: 0.0941 - val_loss: 0.0791\n",
      "Epoch 4/3000\n",
      "2370/2370 [==============================] - 0s 65us/step - loss: 0.0849 - val_loss: 0.0730\n",
      "Epoch 5/3000\n",
      "2370/2370 [==============================] - 0s 105us/step - loss: 0.0771 - val_loss: 0.1048\n",
      "Epoch 6/3000\n",
      "2370/2370 [==============================] - 0s 103us/step - loss: 0.0801 - val_loss: 0.0626\n",
      "Epoch 7/3000\n",
      "2370/2370 [==============================] - 0s 113us/step - loss: 0.0704 - val_loss: 0.0856\n",
      "Epoch 8/3000\n",
      "2370/2370 [==============================] - 0s 81us/step - loss: 0.0711 - val_loss: 0.0709\n",
      "Epoch 9/3000\n",
      "2370/2370 [==============================] - 0s 90us/step - loss: 0.0686 - val_loss: 0.0744\n",
      "Epoch 10/3000\n",
      "2370/2370 [==============================] - 0s 122us/step - loss: 0.0674 - val_loss: 0.0865\n",
      "Epoch 11/3000\n",
      "2370/2370 [==============================] - 0s 122us/step - loss: 0.0611 - val_loss: 0.1195\n",
      "Epoch 12/3000\n",
      "2370/2370 [==============================] - 0s 101us/step - loss: 0.0599 - val_loss: 0.0684\n",
      "Epoch 13/3000\n",
      "2370/2370 [==============================] - 0s 110us/step - loss: 0.0608 - val_loss: 0.1048\n",
      "Epoch 14/3000\n",
      "2370/2370 [==============================] - 0s 96us/step - loss: 0.0545 - val_loss: 0.0866\n",
      "Epoch 15/3000\n",
      "2370/2370 [==============================] - 0s 100us/step - loss: 0.0565 - val_loss: 0.0688\n",
      "Epoch 16/3000\n",
      "2370/2370 [==============================] - 0s 93us/step - loss: 0.0630 - val_loss: 0.0989\n",
      "Epoch 17/3000\n",
      "2370/2370 [==============================] - 0s 60us/step - loss: 0.0601 - val_loss: 0.0856\n",
      "Epoch 18/3000\n",
      "2370/2370 [==============================] - 0s 107us/step - loss: 0.0601 - val_loss: 0.0743\n",
      "Epoch 19/3000\n",
      "2370/2370 [==============================] - 0s 113us/step - loss: 0.0543 - val_loss: 0.0889\n",
      "Epoch 20/3000\n",
      "2370/2370 [==============================] - 0s 114us/step - loss: 0.0561 - val_loss: 0.0909\n",
      "Epoch 21/3000\n",
      "2370/2370 [==============================] - 0s 90us/step - loss: 0.0561 - val_loss: 0.0981\n",
      "Epoch 22/3000\n",
      "2370/2370 [==============================] - 0s 88us/step - loss: 0.0520 - val_loss: 0.0724\n",
      "Epoch 23/3000\n",
      "2370/2370 [==============================] - 0s 111us/step - loss: 0.0541 - val_loss: 0.0828\n",
      "Epoch 24/3000\n",
      "2370/2370 [==============================] - 0s 111us/step - loss: 0.0600 - val_loss: 0.1038\n",
      "Epoch 25/3000\n",
      "2370/2370 [==============================] - 0s 123us/step - loss: 0.0550 - val_loss: 0.0831\n",
      "Epoch 26/3000\n",
      "2370/2370 [==============================] - 0s 121us/step - loss: 0.0564 - val_loss: 0.1295\n",
      "Epoch 27/3000\n",
      "2370/2370 [==============================] - 0s 87us/step - loss: 0.0527 - val_loss: 0.0906\n",
      "Epoch 28/3000\n",
      "2370/2370 [==============================] - 0s 108us/step - loss: 0.0581 - val_loss: 0.1138\n",
      "Epoch 29/3000\n",
      "2370/2370 [==============================] - 0s 115us/step - loss: 0.0541 - val_loss: 0.0964\n",
      "Epoch 30/3000\n",
      "2370/2370 [==============================] - 0s 123us/step - loss: 0.0521 - val_loss: 0.0875\n",
      "Epoch 31/3000\n",
      "2370/2370 [==============================] - 0s 104us/step - loss: 0.0536 - val_loss: 0.1061\n",
      "Epoch 32/3000\n",
      "2370/2370 [==============================] - 0s 103us/step - loss: 0.0536 - val_loss: 0.1136\n",
      "Epoch 33/3000\n",
      "2370/2370 [==============================] - 0s 103us/step - loss: 0.0487 - val_loss: 0.1003\n",
      "Epoch 34/3000\n",
      "2370/2370 [==============================] - 0s 107us/step - loss: 0.0554 - val_loss: 0.1278\n",
      "Epoch 35/3000\n",
      "2370/2370 [==============================] - 0s 115us/step - loss: 0.0563 - val_loss: 0.0981\n",
      "Epoch 36/3000\n",
      "2370/2370 [==============================] - 0s 93us/step - loss: 0.0503 - val_loss: 0.1312\n",
      "Epoch 37/3000\n",
      "2370/2370 [==============================] - 0s 106us/step - loss: 0.0528 - val_loss: 0.0403\n",
      "Epoch 38/3000\n",
      "2370/2370 [==============================] - 0s 91us/step - loss: 0.0450 - val_loss: 0.0234\n",
      "Epoch 39/3000\n",
      "2370/2370 [==============================] - 0s 110us/step - loss: 0.0470 - val_loss: 0.0341\n",
      "Epoch 40/3000\n",
      "2370/2370 [==============================] - 0s 95us/step - loss: 0.0453 - val_loss: 0.0379\n",
      "Epoch 41/3000\n",
      "2370/2370 [==============================] - 0s 92us/step - loss: 0.0462 - val_loss: 0.0297\n",
      "Epoch 42/3000\n",
      "2370/2370 [==============================] - 0s 109us/step - loss: 0.0448 - val_loss: 0.0513\n",
      "Epoch 43/3000\n",
      "2370/2370 [==============================] - 0s 109us/step - loss: 0.0490 - val_loss: 0.0276\n",
      "Epoch 44/3000\n",
      "2370/2370 [==============================] - 0s 118us/step - loss: 0.0481 - val_loss: 0.0383\n",
      "Epoch 45/3000\n",
      "2370/2370 [==============================] - 0s 103us/step - loss: 0.0459 - val_loss: 0.0407\n",
      "Epoch 46/3000\n",
      "2370/2370 [==============================] - 0s 90us/step - loss: 0.0445 - val_loss: 0.0642\n",
      "Epoch 47/3000\n",
      "2370/2370 [==============================] - 0s 100us/step - loss: 0.0461 - val_loss: 0.0646\n",
      "Epoch 48/3000\n",
      "2370/2370 [==============================] - 0s 119us/step - loss: 0.0481 - val_loss: 0.0396\n",
      "Epoch 49/3000\n",
      "2370/2370 [==============================] - 0s 98us/step - loss: 0.0464 - val_loss: 0.0591\n",
      "Epoch 50/3000\n",
      "2370/2370 [==============================] - 0s 123us/step - loss: 0.0445 - val_loss: 0.0388\n",
      "Epoch 51/3000\n",
      "2370/2370 [==============================] - 0s 87us/step - loss: 0.0449 - val_loss: 0.0329\n",
      "Epoch 52/3000\n",
      "2370/2370 [==============================] - 0s 63us/step - loss: 0.0451 - val_loss: 0.0232\n",
      "Epoch 53/3000\n",
      "2370/2370 [==============================] - 0s 125us/step - loss: 0.0435 - val_loss: 0.0297\n",
      "Epoch 54/3000\n",
      "2370/2370 [==============================] - 0s 138us/step - loss: 0.0409 - val_loss: 0.0648\n",
      "Epoch 55/3000\n",
      "2370/2370 [==============================] - 0s 111us/step - loss: 0.0469 - val_loss: 0.0323\n",
      "Epoch 56/3000\n",
      "2370/2370 [==============================] - 0s 136us/step - loss: 0.0459 - val_loss: 0.0242\n",
      "Epoch 57/3000\n",
      "2370/2370 [==============================] - 0s 132us/step - loss: 0.0451 - val_loss: 0.0238\n",
      "Epoch 58/3000\n",
      "2370/2370 [==============================] - 0s 123us/step - loss: 0.0458 - val_loss: 0.0349\n",
      "Epoch 59/3000\n",
      "2370/2370 [==============================] - 0s 147us/step - loss: 0.0442 - val_loss: 0.0231\n",
      "Epoch 60/3000\n",
      "2370/2370 [==============================] - 0s 143us/step - loss: 0.0400 - val_loss: 0.0253\n",
      "Epoch 61/3000\n",
      "2370/2370 [==============================] - 0s 126us/step - loss: 0.0450 - val_loss: 0.0268\n",
      "Epoch 62/3000\n",
      "2370/2370 [==============================] - 0s 140us/step - loss: 0.0401 - val_loss: 0.0302\n",
      "Epoch 63/3000\n",
      "2370/2370 [==============================] - 0s 121us/step - loss: 0.0454 - val_loss: 0.0372\n",
      "Epoch 64/3000\n",
      "2370/2370 [==============================] - 0s 107us/step - loss: 0.0380 - val_loss: 0.0275\n",
      "Epoch 65/3000\n",
      "2370/2370 [==============================] - 0s 120us/step - loss: 0.0412 - val_loss: 0.0293\n",
      "Epoch 66/3000\n",
      "2370/2370 [==============================] - 0s 115us/step - loss: 0.0432 - val_loss: 0.0279\n",
      "Epoch 67/3000\n",
      "2370/2370 [==============================] - 0s 109us/step - loss: 0.0443 - val_loss: 0.0358\n",
      "Epoch 68/3000\n",
      "2370/2370 [==============================] - 0s 111us/step - loss: 0.0412 - val_loss: 0.0237\n",
      "Epoch 69/3000\n",
      "2370/2370 [==============================] - 0s 131us/step - loss: 0.0391 - val_loss: 0.0458\n",
      "Epoch 70/3000\n",
      "2370/2370 [==============================] - 0s 113us/step - loss: 0.0427 - val_loss: 0.0415\n",
      "Epoch 71/3000\n",
      "2370/2370 [==============================] - 0s 78us/step - loss: 0.0387 - val_loss: 0.0340\n",
      "Epoch 72/3000\n",
      "2370/2370 [==============================] - 0s 127us/step - loss: 0.0394 - val_loss: 0.0401\n",
      "Epoch 73/3000\n",
      "2370/2370 [==============================] - 0s 114us/step - loss: 0.0435 - val_loss: 0.0273\n",
      "Epoch 74/3000\n",
      "2370/2370 [==============================] - 0s 140us/step - loss: 0.0430 - val_loss: 0.0434\n",
      "Epoch 75/3000\n",
      "2370/2370 [==============================] - 0s 125us/step - loss: 0.0389 - val_loss: 0.0271\n",
      "Epoch 76/3000\n",
      "2370/2370 [==============================] - 0s 146us/step - loss: 0.0402 - val_loss: 0.0280\n",
      "Epoch 77/3000\n",
      "2370/2370 [==============================] - 0s 151us/step - loss: 0.0416 - val_loss: 0.0364\n",
      "Epoch 78/3000\n",
      "2370/2370 [==============================] - 0s 141us/step - loss: 0.0403 - val_loss: 0.0399\n",
      "Epoch 79/3000\n",
      "2370/2370 [==============================] - 0s 114us/step - loss: 0.0373 - val_loss: 0.0315\n",
      "Epoch 80/3000\n",
      "2370/2370 [==============================] - 0s 118us/step - loss: 0.0405 - val_loss: 0.0657\n",
      "Epoch 81/3000\n",
      "2370/2370 [==============================] - 0s 135us/step - loss: 0.0386 - val_loss: 0.0369\n",
      "Epoch 82/3000\n",
      "2370/2370 [==============================] - 0s 141us/step - loss: 0.0400 - val_loss: 0.0584\n",
      "Epoch 83/3000\n",
      "2370/2370 [==============================] - 0s 138us/step - loss: 0.0439 - val_loss: 0.0370\n",
      "Epoch 84/3000\n",
      "2370/2370 [==============================] - 0s 136us/step - loss: 0.0350 - val_loss: 0.0465\n",
      "Epoch 85/3000\n",
      "2370/2370 [==============================] - 0s 127us/step - loss: 0.0415 - val_loss: 0.0246\n",
      "Epoch 86/3000\n",
      "2370/2370 [==============================] - 0s 128us/step - loss: 0.0388 - val_loss: 0.0232\n",
      "Epoch 87/3000\n",
      "2370/2370 [==============================] - 0s 131us/step - loss: 0.0408 - val_loss: 0.0317\n",
      "Epoch 88/3000\n",
      "2370/2370 [==============================] - 0s 129us/step - loss: 0.0383 - val_loss: 0.0278\n",
      "Epoch 89/3000\n",
      "2370/2370 [==============================] - 0s 134us/step - loss: 0.0379 - val_loss: 0.0234\n",
      "Epoch 90/3000\n",
      "2370/2370 [==============================] - 0s 135us/step - loss: 0.0405 - val_loss: 0.0462\n",
      "Epoch 91/3000\n",
      "2370/2370 [==============================] - 0s 149us/step - loss: 0.0409 - val_loss: 0.0339\n",
      "Epoch 92/3000\n",
      "2370/2370 [==============================] - 0s 137us/step - loss: 0.0436 - val_loss: 0.0466\n",
      "Epoch 93/3000\n",
      "2370/2370 [==============================] - 0s 124us/step - loss: 0.0405 - val_loss: 0.0471\n",
      "Epoch 94/3000\n",
      "2370/2370 [==============================] - 0s 140us/step - loss: 0.0428 - val_loss: 0.0262\n",
      "Epoch 95/3000\n",
      "2370/2370 [==============================] - 0s 127us/step - loss: 0.0413 - val_loss: 0.0436\n",
      "Epoch 96/3000\n",
      "2370/2370 [==============================] - 0s 135us/step - loss: 0.0429 - val_loss: 0.0257\n",
      "Epoch 97/3000\n",
      "2370/2370 [==============================] - 0s 109us/step - loss: 0.0423 - val_loss: 0.0421\n",
      "Epoch 98/3000\n",
      "2370/2370 [==============================] - 0s 137us/step - loss: 0.0410 - val_loss: 0.0324\n",
      "Epoch 99/3000\n",
      "2370/2370 [==============================] - 0s 129us/step - loss: 0.0397 - val_loss: 0.0560\n",
      "Epoch 100/3000\n",
      "2370/2370 [==============================] - 0s 89us/step - loss: 0.0386 - val_loss: 0.0531\n",
      "Epoch 101/3000\n",
      "2370/2370 [==============================] - 0s 109us/step - loss: 0.0391 - val_loss: 0.0274\n",
      "Epoch 102/3000\n",
      "2370/2370 [==============================] - 0s 142us/step - loss: 0.0402 - val_loss: 0.0312\n",
      "Epoch 103/3000\n",
      "2370/2370 [==============================] - 0s 140us/step - loss: 0.0388 - val_loss: 0.0272\n",
      "Epoch 104/3000\n",
      "2370/2370 [==============================] - 0s 92us/step - loss: 0.0386 - val_loss: 0.0286\n",
      "Epoch 105/3000\n",
      "2370/2370 [==============================] - 0s 145us/step - loss: 0.0389 - val_loss: 0.0311\n",
      "Epoch 106/3000\n",
      "2370/2370 [==============================] - 0s 133us/step - loss: 0.0390 - val_loss: 0.0302\n",
      "Epoch 107/3000\n",
      "2370/2370 [==============================] - 0s 114us/step - loss: 0.0427 - val_loss: 0.0323\n",
      "Epoch 108/3000\n",
      "2370/2370 [==============================] - 0s 135us/step - loss: 0.0393 - val_loss: 0.0248\n",
      "Epoch 109/3000\n",
      "2370/2370 [==============================] - 0s 137us/step - loss: 0.0418 - val_loss: 0.0327\n",
      "Epoch 110/3000\n",
      "2370/2370 [==============================] - 0s 149us/step - loss: 0.0395 - val_loss: 0.0435\n",
      "Epoch 111/3000\n",
      "2370/2370 [==============================] - 0s 145us/step - loss: 0.0381 - val_loss: 0.0447\n",
      "Epoch 112/3000\n",
      "2370/2370 [==============================] - 0s 116us/step - loss: 0.0397 - val_loss: 0.0434\n",
      "Epoch 113/3000\n",
      "2370/2370 [==============================] - 0s 126us/step - loss: 0.0396 - val_loss: 0.0341\n",
      "Epoch 114/3000\n",
      "2370/2370 [==============================] - 0s 115us/step - loss: 0.0374 - val_loss: 0.0494\n",
      "Epoch 115/3000\n",
      "2370/2370 [==============================] - 0s 116us/step - loss: 0.0371 - val_loss: 0.0472\n",
      "Epoch 116/3000\n",
      "2370/2370 [==============================] - 0s 84us/step - loss: 0.0446 - val_loss: 0.0270\n",
      "Epoch 117/3000\n",
      "2370/2370 [==============================] - 0s 93us/step - loss: 0.0400 - val_loss: 0.0506\n",
      "Epoch 118/3000\n",
      "2370/2370 [==============================] - 0s 101us/step - loss: 0.0359 - val_loss: 0.0472\n",
      "Epoch 119/3000\n",
      "2370/2370 [==============================] - 0s 127us/step - loss: 0.0373 - val_loss: 0.0364\n",
      "Epoch 120/3000\n",
      "2370/2370 [==============================] - 0s 132us/step - loss: 0.0389 - val_loss: 0.0267\n",
      "Epoch 121/3000\n",
      "2370/2370 [==============================] - 0s 132us/step - loss: 0.0414 - val_loss: 0.0248\n",
      "Epoch 122/3000\n",
      "2370/2370 [==============================] - 0s 127us/step - loss: 0.0377 - val_loss: 0.0426\n",
      "Epoch 123/3000\n",
      "2370/2370 [==============================] - 0s 130us/step - loss: 0.0408 - val_loss: 0.0262\n",
      "Epoch 124/3000\n",
      "2370/2370 [==============================] - 0s 134us/step - loss: 0.0363 - val_loss: 0.0472\n",
      "Epoch 125/3000\n",
      "2370/2370 [==============================] - 0s 136us/step - loss: 0.0385 - val_loss: 0.0264\n",
      "Epoch 126/3000\n",
      "2370/2370 [==============================] - 0s 145us/step - loss: 0.0418 - val_loss: 0.0352\n",
      "Epoch 127/3000\n",
      "2370/2370 [==============================] - 0s 113us/step - loss: 0.0404 - val_loss: 0.0432\n",
      "Epoch 128/3000\n",
      "2370/2370 [==============================] - 0s 143us/step - loss: 0.0422 - val_loss: 0.0286\n",
      "Epoch 129/3000\n",
      "2370/2370 [==============================] - 0s 128us/step - loss: 0.0398 - val_loss: 0.0470\n",
      "Epoch 130/3000\n",
      "2370/2370 [==============================] - 0s 115us/step - loss: 0.0374 - val_loss: 0.0419\n",
      "Epoch 131/3000\n",
      "2370/2370 [==============================] - 0s 108us/step - loss: 0.0391 - val_loss: 0.0281\n",
      "Epoch 132/3000\n",
      "2370/2370 [==============================] - 0s 114us/step - loss: 0.0430 - val_loss: 0.0335\n",
      "Epoch 133/3000\n",
      "2370/2370 [==============================] - 0s 115us/step - loss: 0.0403 - val_loss: 0.0372\n",
      "Epoch 134/3000\n",
      "2370/2370 [==============================] - 0s 88us/step - loss: 0.0415 - val_loss: 0.0288\n",
      "Epoch 135/3000\n",
      "2370/2370 [==============================] - 0s 133us/step - loss: 0.0392 - val_loss: 0.0404\n",
      "Epoch 136/3000\n",
      "2370/2370 [==============================] - 0s 118us/step - loss: 0.0396 - val_loss: 0.0337\n",
      "Epoch 137/3000\n",
      "2370/2370 [==============================] - 0s 111us/step - loss: 0.0415 - val_loss: 0.0291\n",
      "Epoch 138/3000\n",
      "2370/2370 [==============================] - 0s 135us/step - loss: 0.0430 - val_loss: 0.0480\n",
      "Epoch 139/3000\n",
      "2370/2370 [==============================] - 0s 110us/step - loss: 0.0409 - val_loss: 0.0289\n",
      "Epoch 140/3000\n",
      "2370/2370 [==============================] - 0s 126us/step - loss: 0.0403 - val_loss: 0.0497\n",
      "Epoch 141/3000\n",
      "2370/2370 [==============================] - 0s 114us/step - loss: 0.0392 - val_loss: 0.0265\n",
      "Epoch 142/3000\n",
      "2370/2370 [==============================] - 0s 117us/step - loss: 0.0399 - val_loss: 0.0375\n",
      "Epoch 143/3000\n",
      "2370/2370 [==============================] - 0s 119us/step - loss: 0.0417 - val_loss: 0.0491\n",
      "Epoch 144/3000\n",
      "2370/2370 [==============================] - 0s 140us/step - loss: 0.0364 - val_loss: 0.0289\n",
      "Epoch 145/3000\n",
      "2370/2370 [==============================] - 0s 92us/step - loss: 0.0395 - val_loss: 0.0563\n",
      "Epoch 146/3000\n",
      "2370/2370 [==============================] - 0s 95us/step - loss: 0.0393 - val_loss: 0.0368\n",
      "Epoch 147/3000\n",
      "2370/2370 [==============================] - 0s 85us/step - loss: 0.0353 - val_loss: 0.0482\n",
      "Epoch 148/3000\n",
      "2370/2370 [==============================] - 0s 77us/step - loss: 0.0413 - val_loss: 0.0356\n",
      "Epoch 149/3000\n",
      "2370/2370 [==============================] - 0s 91us/step - loss: 0.0370 - val_loss: 0.0285\n",
      "Epoch 150/3000\n",
      "2370/2370 [==============================] - 0s 133us/step - loss: 0.0425 - val_loss: 0.0283\n",
      "Epoch 151/3000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2370/2370 [==============================] - 0s 116us/step - loss: 0.0390 - val_loss: 0.0331\n",
      "Epoch 152/3000\n",
      "2370/2370 [==============================] - 0s 151us/step - loss: 0.0426 - val_loss: 0.0286\n",
      "Epoch 153/3000\n",
      "2370/2370 [==============================] - 0s 128us/step - loss: 0.0358 - val_loss: 0.0301\n",
      "Epoch 154/3000\n",
      "2370/2370 [==============================] - 0s 117us/step - loss: 0.0424 - val_loss: 0.0272\n",
      "Epoch 155/3000\n",
      "2370/2370 [==============================] - 0s 139us/step - loss: 0.0442 - val_loss: 0.0442\n",
      "Epoch 156/3000\n",
      "2370/2370 [==============================] - 0s 137us/step - loss: 0.0378 - val_loss: 0.0431\n",
      "Epoch 157/3000\n",
      "2370/2370 [==============================] - 0s 125us/step - loss: 0.0382 - val_loss: 0.0288\n",
      "Epoch 158/3000\n",
      "2370/2370 [==============================] - 0s 155us/step - loss: 0.0430 - val_loss: 0.0321\n",
      "Epoch 159/3000\n",
      "2370/2370 [==============================] - 0s 111us/step - loss: 0.0379 - val_loss: 0.0322\n",
      "Epoch 160/3000\n",
      "2370/2370 [==============================] - 0s 140us/step - loss: 0.0390 - val_loss: 0.0336\n",
      "Epoch 161/3000\n",
      "2370/2370 [==============================] - 0s 126us/step - loss: 0.0391 - val_loss: 0.0315\n",
      "Epoch 162/3000\n",
      "2370/2370 [==============================] - 0s 120us/step - loss: 0.0407 - val_loss: 0.0328\n",
      "Epoch 163/3000\n",
      "2370/2370 [==============================] - 0s 136us/step - loss: 0.0381 - val_loss: 0.0403\n",
      "Epoch 164/3000\n",
      "2370/2370 [==============================] - 0s 139us/step - loss: 0.0362 - val_loss: 0.0285\n",
      "Epoch 165/3000\n",
      "2370/2370 [==============================] - 0s 135us/step - loss: 0.0368 - val_loss: 0.0372\n",
      "Epoch 166/3000\n",
      "2370/2370 [==============================] - 0s 118us/step - loss: 0.0382 - val_loss: 0.0294\n",
      "Epoch 167/3000\n",
      "2370/2370 [==============================] - 0s 136us/step - loss: 0.0399 - val_loss: 0.0290\n",
      "Epoch 168/3000\n",
      "2370/2370 [==============================] - 0s 143us/step - loss: 0.0412 - val_loss: 0.0539\n",
      "Epoch 169/3000\n",
      "2370/2370 [==============================] - 0s 141us/step - loss: 0.0383 - val_loss: 0.0541\n",
      "Epoch 170/3000\n",
      "2370/2370 [==============================] - 0s 142us/step - loss: 0.0414 - val_loss: 0.0411\n",
      "Epoch 171/3000\n",
      "2370/2370 [==============================] - 0s 138us/step - loss: 0.0444 - val_loss: 0.0296\n",
      "Epoch 172/3000\n",
      "2370/2370 [==============================] - 0s 148us/step - loss: 0.0395 - val_loss: 0.0398\n",
      "Epoch 173/3000\n",
      "2370/2370 [==============================] - 0s 140us/step - loss: 0.0400 - val_loss: 0.0347\n",
      "Epoch 174/3000\n",
      "2370/2370 [==============================] - 0s 141us/step - loss: 0.0382 - val_loss: 0.0352\n",
      "Epoch 175/3000\n",
      "2370/2370 [==============================] - 0s 150us/step - loss: 0.0408 - val_loss: 0.0376\n",
      "Epoch 176/3000\n",
      "2370/2370 [==============================] - 0s 132us/step - loss: 0.0391 - val_loss: 0.0302\n",
      "Epoch 177/3000\n",
      "2370/2370 [==============================] - 0s 137us/step - loss: 0.0413 - val_loss: 0.0389\n",
      "Epoch 178/3000\n",
      "2370/2370 [==============================] - 0s 95us/step - loss: 0.0371 - val_loss: 0.0488\n",
      "Epoch 179/3000\n",
      "2370/2370 [==============================] - 0s 56us/step - loss: 0.0384 - val_loss: 0.0334\n",
      "Epoch 180/3000\n",
      "2370/2370 [==============================] - 0s 86us/step - loss: 0.0358 - val_loss: 0.0526\n",
      "Epoch 181/3000\n",
      "2370/2370 [==============================] - 0s 145us/step - loss: 0.0362 - val_loss: 0.0327\n",
      "Epoch 182/3000\n",
      "2370/2370 [==============================] - 0s 143us/step - loss: 0.0402 - val_loss: 0.0557\n",
      "Epoch 183/3000\n",
      "2370/2370 [==============================] - 0s 139us/step - loss: 0.0414 - val_loss: 0.0383\n",
      "Epoch 184/3000\n",
      "2370/2370 [==============================] - 0s 142us/step - loss: 0.0383 - val_loss: 0.0316\n",
      "Epoch 185/3000\n",
      "2370/2370 [==============================] - 0s 100us/step - loss: 0.0406 - val_loss: 0.0305\n",
      "Epoch 186/3000\n",
      "2370/2370 [==============================] - 0s 107us/step - loss: 0.0377 - val_loss: 0.0316\n",
      "Epoch 187/3000\n",
      "2370/2370 [==============================] - 0s 119us/step - loss: 0.0428 - val_loss: 0.0367\n",
      "Epoch 188/3000\n",
      "2370/2370 [==============================] - 0s 123us/step - loss: 0.0410 - val_loss: 0.0370\n",
      "Epoch 189/3000\n",
      "2370/2370 [==============================] - 0s 46us/step - loss: 0.0431 - val_loss: 0.0591\n",
      "Epoch 190/3000\n",
      "2370/2370 [==============================] - 0s 119us/step - loss: 0.0418 - val_loss: 0.0378\n",
      "Epoch 191/3000\n",
      "2370/2370 [==============================] - 0s 135us/step - loss: 0.0365 - val_loss: 0.0292\n",
      "Epoch 192/3000\n",
      "2370/2370 [==============================] - 0s 126us/step - loss: 0.0425 - val_loss: 0.0468\n",
      "Epoch 193/3000\n",
      "2370/2370 [==============================] - 0s 129us/step - loss: 0.0382 - val_loss: 0.0399\n",
      "Epoch 194/3000\n",
      "2370/2370 [==============================] - 0s 108us/step - loss: 0.0350 - val_loss: 0.0308\n",
      "Epoch 195/3000\n",
      "2370/2370 [==============================] - 0s 66us/step - loss: 0.0409 - val_loss: 0.0517\n",
      "Epoch 196/3000\n",
      "2370/2370 [==============================] - 0s 130us/step - loss: 0.0379 - val_loss: 0.0333\n",
      "Epoch 197/3000\n",
      "2370/2370 [==============================] - 0s 129us/step - loss: 0.0384 - val_loss: 0.0278\n",
      "Epoch 198/3000\n",
      "2370/2370 [==============================] - 0s 146us/step - loss: 0.0357 - val_loss: 0.0444\n",
      "Epoch 199/3000\n",
      "2370/2370 [==============================] - 0s 137us/step - loss: 0.0372 - val_loss: 0.0432\n",
      "Epoch 200/3000\n",
      "2370/2370 [==============================] - 0s 85us/step - loss: 0.0415 - val_loss: 0.0632\n",
      "Epoch 201/3000\n",
      "2370/2370 [==============================] - 0s 45us/step - loss: 0.0366 - val_loss: 0.0324\n",
      "Epoch 202/3000\n",
      "2370/2370 [==============================] - 0s 78us/step - loss: 0.0393 - val_loss: 0.0366\n",
      "Epoch 203/3000\n",
      "2370/2370 [==============================] - 0s 134us/step - loss: 0.0408 - val_loss: 0.0495\n",
      "Epoch 204/3000\n",
      "2370/2370 [==============================] - 0s 133us/step - loss: 0.0412 - val_loss: 0.0331\n",
      "Epoch 205/3000\n",
      "2370/2370 [==============================] - 0s 75us/step - loss: 0.0392 - val_loss: 0.0311\n",
      "Epoch 206/3000\n",
      "2370/2370 [==============================] - 0s 44us/step - loss: 0.0370 - val_loss: 0.0301\n",
      "Epoch 207/3000\n",
      "2370/2370 [==============================] - 0s 35us/step - loss: 0.0356 - val_loss: 0.0477\n",
      "Epoch 208/3000\n",
      "2370/2370 [==============================] - 0s 34us/step - loss: 0.0423 - val_loss: 0.0258\n",
      "Epoch 209/3000\n",
      "2370/2370 [==============================] - 0s 34us/step - loss: 0.0384 - val_loss: 0.0355\n",
      "Epoch 210/3000\n",
      "2370/2370 [==============================] - 0s 33us/step - loss: 0.0374 - val_loss: 0.0547\n",
      "Epoch 211/3000\n",
      "2370/2370 [==============================] - 0s 37us/step - loss: 0.0386 - val_loss: 0.0474\n",
      "Epoch 212/3000\n",
      "2370/2370 [==============================] - 0s 33us/step - loss: 0.0408 - val_loss: 0.0291\n",
      "Epoch 213/3000\n",
      "2370/2370 [==============================] - 0s 28us/step - loss: 0.0420 - val_loss: 0.0305\n",
      "Epoch 214/3000\n",
      "2370/2370 [==============================] - 0s 28us/step - loss: 0.0374 - val_loss: 0.0440\n",
      "Epoch 215/3000\n",
      "2370/2370 [==============================] - 0s 28us/step - loss: 0.0366 - val_loss: 0.0297\n",
      "Epoch 216/3000\n",
      "2370/2370 [==============================] - 0s 30us/step - loss: 0.0383 - val_loss: 0.0315\n",
      "Epoch 217/3000\n",
      "2370/2370 [==============================] - 0s 34us/step - loss: 0.0391 - val_loss: 0.0388\n",
      "Epoch 218/3000\n",
      "2370/2370 [==============================] - 0s 32us/step - loss: 0.0398 - val_loss: 0.0314\n",
      "Epoch 219/3000\n",
      "2370/2370 [==============================] - 0s 30us/step - loss: 0.0392 - val_loss: 0.0314\n",
      "Epoch 220/3000\n",
      "2370/2370 [==============================] - 0s 28us/step - loss: 0.0405 - val_loss: 0.0419\n",
      "Epoch 221/3000\n",
      "2370/2370 [==============================] - 0s 28us/step - loss: 0.0379 - val_loss: 0.0351\n",
      "Epoch 222/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0394 - val_loss: 0.0308\n",
      "Epoch 223/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0404 - val_loss: 0.0534\n",
      "Epoch 224/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0385 - val_loss: 0.0282\n",
      "Epoch 225/3000\n",
      "2370/2370 [==============================] - 0s 31us/step - loss: 0.0365 - val_loss: 0.0467\n",
      "Epoch 226/3000\n",
      "2370/2370 [==============================] - 0s 35us/step - loss: 0.0411 - val_loss: 0.0405\n",
      "Epoch 227/3000\n",
      "2370/2370 [==============================] - 0s 30us/step - loss: 0.0363 - val_loss: 0.0331\n",
      "Epoch 228/3000\n",
      "2370/2370 [==============================] - 0s 31us/step - loss: 0.0421 - val_loss: 0.0304\n",
      "Epoch 229/3000\n",
      "2370/2370 [==============================] - 0s 32us/step - loss: 0.0379 - val_loss: 0.0290\n",
      "Epoch 230/3000\n",
      "2370/2370 [==============================] - 0s 30us/step - loss: 0.0407 - val_loss: 0.0674\n",
      "Epoch 231/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0405 - val_loss: 0.0309\n",
      "Epoch 232/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0440 - val_loss: 0.0274\n",
      "Epoch 233/3000\n",
      "2370/2370 [==============================] - 0s 33us/step - loss: 0.0366 - val_loss: 0.0491\n",
      "Epoch 234/3000\n",
      "2370/2370 [==============================] - 0s 30us/step - loss: 0.0357 - val_loss: 0.0330\n",
      "Epoch 235/3000\n",
      "2370/2370 [==============================] - 0s 30us/step - loss: 0.0399 - val_loss: 0.0325\n",
      "Epoch 236/3000\n",
      "2370/2370 [==============================] - 0s 30us/step - loss: 0.0394 - val_loss: 0.0372\n",
      "Epoch 237/3000\n",
      "2370/2370 [==============================] - 0s 36us/step - loss: 0.0374 - val_loss: 0.0467\n",
      "Epoch 238/3000\n",
      "2370/2370 [==============================] - 0s 39us/step - loss: 0.0424 - val_loss: 0.0263\n",
      "Epoch 239/3000\n",
      "2370/2370 [==============================] - 0s 113us/step - loss: 0.0373 - val_loss: 0.0288\n",
      "Epoch 240/3000\n",
      "2370/2370 [==============================] - 0s 112us/step - loss: 0.0380 - val_loss: 0.0410\n",
      "Epoch 241/3000\n",
      "2370/2370 [==============================] - 0s 123us/step - loss: 0.0358 - val_loss: 0.0396\n",
      "Epoch 242/3000\n",
      "2370/2370 [==============================] - 0s 87us/step - loss: 0.0411 - val_loss: 0.0295\n",
      "Epoch 243/3000\n",
      "2370/2370 [==============================] - 0s 41us/step - loss: 0.0362 - val_loss: 0.0339\n",
      "Epoch 244/3000\n",
      "2370/2370 [==============================] - 0s 35us/step - loss: 0.0386 - val_loss: 0.0280\n",
      "Epoch 245/3000\n",
      "2370/2370 [==============================] - 0s 35us/step - loss: 0.0414 - val_loss: 0.0274\n",
      "Epoch 246/3000\n",
      "2370/2370 [==============================] - 0s 36us/step - loss: 0.0355 - val_loss: 0.0305\n",
      "Epoch 247/3000\n",
      "2370/2370 [==============================] - 0s 35us/step - loss: 0.0391 - val_loss: 0.0299\n",
      "Epoch 248/3000\n",
      "2370/2370 [==============================] - 0s 32us/step - loss: 0.0391 - val_loss: 0.0318\n",
      "Epoch 249/3000\n",
      "2370/2370 [==============================] - 0s 32us/step - loss: 0.0410 - val_loss: 0.0604\n",
      "Epoch 250/3000\n",
      "2370/2370 [==============================] - 0s 70us/step - loss: 0.0362 - val_loss: 0.0321\n",
      "Epoch 251/3000\n",
      "2370/2370 [==============================] - 0s 163us/step - loss: 0.0399 - val_loss: 0.0505\n",
      "Epoch 252/3000\n",
      "2370/2370 [==============================] - 0s 114us/step - loss: 0.0459 - val_loss: 0.0351\n",
      "Epoch 253/3000\n",
      "2370/2370 [==============================] - 0s 130us/step - loss: 0.0365 - val_loss: 0.0507\n",
      "Epoch 254/3000\n",
      "2370/2370 [==============================] - 0s 86us/step - loss: 0.0375 - val_loss: 0.0521\n",
      "Epoch 255/3000\n",
      "2370/2370 [==============================] - 0s 135us/step - loss: 0.0411 - val_loss: 0.0310\n",
      "Epoch 256/3000\n",
      "2370/2370 [==============================] - 0s 160us/step - loss: 0.0395 - val_loss: 0.0374\n",
      "Epoch 257/3000\n",
      "2370/2370 [==============================] - 0s 140us/step - loss: 0.0379 - val_loss: 0.0309\n",
      "Epoch 258/3000\n",
      "2370/2370 [==============================] - 0s 92us/step - loss: 0.0341 - val_loss: 0.0443\n",
      "Epoch 259/3000\n",
      "2370/2370 [==============================] - 0s 88us/step - loss: 0.0388 - val_loss: 0.0334\n",
      "Epoch 260/3000\n",
      "2370/2370 [==============================] - 0s 47us/step - loss: 0.0399 - val_loss: 0.0374\n",
      "Epoch 261/3000\n",
      "2370/2370 [==============================] - 0s 38us/step - loss: 0.0394 - val_loss: 0.0336\n",
      "Epoch 262/3000\n",
      "2370/2370 [==============================] - 0s 33us/step - loss: 0.0374 - val_loss: 0.0323\n",
      "Epoch 263/3000\n",
      "2370/2370 [==============================] - 0s 34us/step - loss: 0.0430 - val_loss: 0.0382\n",
      "Epoch 264/3000\n",
      "2370/2370 [==============================] - 0s 32us/step - loss: 0.0397 - val_loss: 0.0350\n",
      "Epoch 265/3000\n",
      "2370/2370 [==============================] - 0s 32us/step - loss: 0.0333 - val_loss: 0.0488\n",
      "Epoch 266/3000\n",
      "2370/2370 [==============================] - 0s 28us/step - loss: 0.0417 - val_loss: 0.0339\n",
      "Epoch 267/3000\n",
      "2370/2370 [==============================] - 0s 33us/step - loss: 0.0382 - val_loss: 0.0566\n",
      "Epoch 268/3000\n",
      "2370/2370 [==============================] - 0s 32us/step - loss: 0.0416 - val_loss: 0.0301\n",
      "Epoch 269/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0369 - val_loss: 0.0523\n",
      "Epoch 270/3000\n",
      "2370/2370 [==============================] - 0s 31us/step - loss: 0.0392 - val_loss: 0.0295\n",
      "Epoch 271/3000\n",
      "2370/2370 [==============================] - 0s 34us/step - loss: 0.0371 - val_loss: 0.0581\n",
      "Epoch 272/3000\n",
      "2370/2370 [==============================] - 0s 31us/step - loss: 0.0376 - val_loss: 0.0361\n",
      "Epoch 273/3000\n",
      "2370/2370 [==============================] - 0s 30us/step - loss: 0.0402 - val_loss: 0.0326\n",
      "Epoch 274/3000\n",
      "2370/2370 [==============================] - 0s 32us/step - loss: 0.0432 - val_loss: 0.0421\n",
      "Epoch 275/3000\n",
      "2370/2370 [==============================] - 0s 40us/step - loss: 0.0370 - val_loss: 0.0421\n",
      "Epoch 276/3000\n",
      "2370/2370 [==============================] - 0s 35us/step - loss: 0.0409 - val_loss: 0.0379\n",
      "Epoch 277/3000\n",
      "2370/2370 [==============================] - 0s 32us/step - loss: 0.0370 - val_loss: 0.0382\n",
      "Epoch 278/3000\n",
      "2370/2370 [==============================] - 0s 32us/step - loss: 0.0400 - val_loss: 0.0334\n",
      "Epoch 279/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0385 - val_loss: 0.0376\n",
      "Epoch 280/3000\n",
      "2370/2370 [==============================] - 0s 28us/step - loss: 0.0358 - val_loss: 0.0395\n",
      "Epoch 281/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0380 - val_loss: 0.0387\n",
      "Epoch 282/3000\n",
      "2370/2370 [==============================] - 0s 37us/step - loss: 0.0395 - val_loss: 0.0334\n",
      "Epoch 283/3000\n",
      "2370/2370 [==============================] - 0s 78us/step - loss: 0.0367 - val_loss: 0.0362\n",
      "Epoch 284/3000\n",
      "2370/2370 [==============================] - 0s 66us/step - loss: 0.0361 - val_loss: 0.0387\n",
      "Epoch 285/3000\n",
      "2370/2370 [==============================] - 0s 32us/step - loss: 0.0347 - val_loss: 0.0289\n",
      "Epoch 286/3000\n",
      "2370/2370 [==============================] - 0s 28us/step - loss: 0.0379 - val_loss: 0.0437\n",
      "Epoch 287/3000\n",
      "2370/2370 [==============================] - 0s 28us/step - loss: 0.0420 - val_loss: 0.0315\n",
      "Epoch 288/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0363 - val_loss: 0.0362\n",
      "Epoch 289/3000\n",
      "2370/2370 [==============================] - 0s 28us/step - loss: 0.0388 - val_loss: 0.0350\n",
      "Epoch 290/3000\n",
      "2370/2370 [==============================] - 0s 28us/step - loss: 0.0375 - val_loss: 0.0376\n",
      "Epoch 291/3000\n",
      "2370/2370 [==============================] - 0s 28us/step - loss: 0.0406 - val_loss: 0.0352\n",
      "Epoch 292/3000\n",
      "2370/2370 [==============================] - 0s 33us/step - loss: 0.0394 - val_loss: 0.0345\n",
      "Epoch 293/3000\n",
      "2370/2370 [==============================] - 0s 28us/step - loss: 0.0381 - val_loss: 0.0463\n",
      "Epoch 294/3000\n",
      "2370/2370 [==============================] - 0s 28us/step - loss: 0.0391 - val_loss: 0.0337\n",
      "Epoch 295/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0366 - val_loss: 0.0312\n",
      "Epoch 296/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0386 - val_loss: 0.0425\n",
      "Epoch 297/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0390 - val_loss: 0.0368\n",
      "Epoch 298/3000\n",
      "2370/2370 [==============================] - 0s 28us/step - loss: 0.0387 - val_loss: 0.0360\n",
      "Epoch 299/3000\n",
      "2370/2370 [==============================] - 0s 30us/step - loss: 0.0361 - val_loss: 0.0412\n",
      "Epoch 300/3000\n",
      "2370/2370 [==============================] - 0s 33us/step - loss: 0.0365 - val_loss: 0.0320\n",
      "Epoch 301/3000\n",
      "2370/2370 [==============================] - 0s 28us/step - loss: 0.0397 - val_loss: 0.0490\n",
      "Epoch 302/3000\n",
      "2370/2370 [==============================] - 0s 28us/step - loss: 0.0378 - val_loss: 0.0346\n",
      "Epoch 303/3000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2370/2370 [==============================] - 0s 28us/step - loss: 0.0368 - val_loss: 0.0457\n",
      "Epoch 304/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0389 - val_loss: 0.0320\n",
      "Epoch 305/3000\n",
      "2370/2370 [==============================] - 0s 34us/step - loss: 0.0390 - val_loss: 0.0373\n",
      "Epoch 306/3000\n",
      "2370/2370 [==============================] - 0s 54us/step - loss: 0.0384 - val_loss: 0.0455\n",
      "Epoch 307/3000\n",
      "2370/2370 [==============================] - 0s 89us/step - loss: 0.0428 - val_loss: 0.0381\n",
      "Epoch 308/3000\n",
      "2370/2370 [==============================] - 0s 126us/step - loss: 0.0347 - val_loss: 0.0382\n",
      "Epoch 309/3000\n",
      "2370/2370 [==============================] - 0s 126us/step - loss: 0.0380 - val_loss: 0.0580\n",
      "Epoch 310/3000\n",
      "2370/2370 [==============================] - 0s 135us/step - loss: 0.0419 - val_loss: 0.0338\n",
      "Epoch 311/3000\n",
      "2370/2370 [==============================] - 0s 81us/step - loss: 0.0397 - val_loss: 0.0461\n",
      "Epoch 312/3000\n",
      "2370/2370 [==============================] - 0s 53us/step - loss: 0.0383 - val_loss: 0.0513\n",
      "Epoch 313/3000\n",
      "2370/2370 [==============================] - 0s 72us/step - loss: 0.0398 - val_loss: 0.0383\n",
      "Epoch 314/3000\n",
      "2370/2370 [==============================] - 0s 65us/step - loss: 0.0372 - val_loss: 0.0392\n",
      "Epoch 315/3000\n",
      "2370/2370 [==============================] - 0s 91us/step - loss: 0.0379 - val_loss: 0.0346\n",
      "Epoch 316/3000\n",
      "2370/2370 [==============================] - 0s 141us/step - loss: 0.0343 - val_loss: 0.0395\n",
      "Epoch 317/3000\n",
      "2370/2370 [==============================] - 0s 138us/step - loss: 0.0386 - val_loss: 0.0393\n",
      "Epoch 318/3000\n",
      "2370/2370 [==============================] - 0s 135us/step - loss: 0.0363 - val_loss: 0.0360\n",
      "Epoch 319/3000\n",
      "2370/2370 [==============================] - 0s 131us/step - loss: 0.0379 - val_loss: 0.0302\n",
      "Epoch 320/3000\n",
      "2370/2370 [==============================] - 0s 135us/step - loss: 0.0372 - val_loss: 0.0352\n",
      "Epoch 321/3000\n",
      "2370/2370 [==============================] - 0s 141us/step - loss: 0.0399 - val_loss: 0.0349\n",
      "Epoch 322/3000\n",
      "2370/2370 [==============================] - 0s 136us/step - loss: 0.0398 - val_loss: 0.0405\n",
      "Epoch 323/3000\n",
      "2370/2370 [==============================] - 0s 144us/step - loss: 0.0375 - val_loss: 0.0391\n",
      "Epoch 324/3000\n",
      "2370/2370 [==============================] - 0s 113us/step - loss: 0.0391 - val_loss: 0.0363\n",
      "Epoch 325/3000\n",
      "2370/2370 [==============================] - 0s 106us/step - loss: 0.0383 - val_loss: 0.0368\n",
      "Epoch 326/3000\n",
      "2370/2370 [==============================] - 0s 106us/step - loss: 0.0390 - val_loss: 0.0324\n",
      "Epoch 327/3000\n",
      "2370/2370 [==============================] - 0s 101us/step - loss: 0.0345 - val_loss: 0.0301\n",
      "Epoch 328/3000\n",
      "2370/2370 [==============================] - 0s 123us/step - loss: 0.0380 - val_loss: 0.0359\n",
      "Epoch 329/3000\n",
      "2370/2370 [==============================] - 0s 137us/step - loss: 0.0378 - val_loss: 0.0410\n",
      "Epoch 330/3000\n",
      "2370/2370 [==============================] - 0s 100us/step - loss: 0.0373 - val_loss: 0.0327\n",
      "Epoch 331/3000\n",
      "2370/2370 [==============================] - 0s 60us/step - loss: 0.0362 - val_loss: 0.0324\n",
      "Epoch 332/3000\n",
      "2370/2370 [==============================] - 0s 79us/step - loss: 0.0436 - val_loss: 0.0424\n",
      "Epoch 333/3000\n",
      "2370/2370 [==============================] - 0s 113us/step - loss: 0.0438 - val_loss: 0.0334\n",
      "Epoch 334/3000\n",
      "2370/2370 [==============================] - 0s 101us/step - loss: 0.0377 - val_loss: 0.0333\n",
      "Epoch 335/3000\n",
      "2370/2370 [==============================] - 0s 61us/step - loss: 0.0459 - val_loss: 0.0410\n",
      "Epoch 336/3000\n",
      "2370/2370 [==============================] - 0s 133us/step - loss: 0.0383 - val_loss: 0.0412\n",
      "Epoch 337/3000\n",
      "2370/2370 [==============================] - 0s 133us/step - loss: 0.0388 - val_loss: 0.0569\n",
      "Epoch 338/3000\n",
      "2370/2370 [==============================] - 0s 51us/step - loss: 0.0392 - val_loss: 0.0469\n",
      "Epoch 339/3000\n",
      "2370/2370 [==============================] - 0s 125us/step - loss: 0.0392 - val_loss: 0.0319\n",
      "Epoch 340/3000\n",
      "2370/2370 [==============================] - 0s 121us/step - loss: 0.0366 - val_loss: 0.0535\n",
      "Epoch 341/3000\n",
      "2370/2370 [==============================] - 0s 132us/step - loss: 0.0355 - val_loss: 0.0343\n",
      "Epoch 342/3000\n",
      "2370/2370 [==============================] - 0s 127us/step - loss: 0.0408 - val_loss: 0.0376\n",
      "Epoch 343/3000\n",
      "2370/2370 [==============================] - 0s 127us/step - loss: 0.0362 - val_loss: 0.0343\n",
      "Epoch 344/3000\n",
      "2370/2370 [==============================] - 0s 134us/step - loss: 0.0375 - val_loss: 0.0321\n",
      "Epoch 345/3000\n",
      "2370/2370 [==============================] - 0s 141us/step - loss: 0.0348 - val_loss: 0.0322\n",
      "Epoch 346/3000\n",
      "2370/2370 [==============================] - 0s 116us/step - loss: 0.0376 - val_loss: 0.0376\n",
      "Epoch 347/3000\n",
      "2370/2370 [==============================] - 0s 105us/step - loss: 0.0372 - val_loss: 0.0418\n",
      "Epoch 348/3000\n",
      "2370/2370 [==============================] - 0s 103us/step - loss: 0.0393 - val_loss: 0.0312\n",
      "Epoch 349/3000\n",
      "2370/2370 [==============================] - 0s 98us/step - loss: 0.0415 - val_loss: 0.0453\n",
      "Epoch 350/3000\n",
      "2370/2370 [==============================] - 0s 63us/step - loss: 0.0367 - val_loss: 0.0329\n",
      "Epoch 351/3000\n",
      "2370/2370 [==============================] - 0s 94us/step - loss: 0.0338 - val_loss: 0.0288\n",
      "Epoch 352/3000\n",
      "2370/2370 [==============================] - 0s 118us/step - loss: 0.0424 - val_loss: 0.0415\n",
      "Epoch 353/3000\n",
      "2370/2370 [==============================] - 0s 82us/step - loss: 0.0395 - val_loss: 0.0355\n",
      "Epoch 354/3000\n",
      "2370/2370 [==============================] - 0s 91us/step - loss: 0.0383 - val_loss: 0.0282\n",
      "Epoch 355/3000\n",
      "2370/2370 [==============================] - 0s 133us/step - loss: 0.0414 - val_loss: 0.0268\n",
      "Epoch 356/3000\n",
      "2370/2370 [==============================] - 0s 102us/step - loss: 0.0389 - val_loss: 0.0318\n",
      "Epoch 357/3000\n",
      "2370/2370 [==============================] - 0s 130us/step - loss: 0.0380 - val_loss: 0.0459\n",
      "Epoch 358/3000\n",
      "2370/2370 [==============================] - 0s 131us/step - loss: 0.0384 - val_loss: 0.0348\n",
      "Epoch 359/3000\n",
      "2370/2370 [==============================] - 0s 139us/step - loss: 0.0379 - val_loss: 0.0409\n",
      "Epoch 360/3000\n",
      "2370/2370 [==============================] - 0s 141us/step - loss: 0.0360 - val_loss: 0.0459\n",
      "Epoch 361/3000\n",
      "2370/2370 [==============================] - 0s 114us/step - loss: 0.0377 - val_loss: 0.0326\n",
      "Epoch 362/3000\n",
      "2370/2370 [==============================] - 0s 80us/step - loss: 0.0380 - val_loss: 0.0455\n",
      "Epoch 363/3000\n",
      "2370/2370 [==============================] - 0s 127us/step - loss: 0.0395 - val_loss: 0.0449\n",
      "Epoch 364/3000\n",
      "2370/2370 [==============================] - 0s 120us/step - loss: 0.0385 - val_loss: 0.0321\n",
      "Epoch 365/3000\n",
      "2370/2370 [==============================] - 0s 137us/step - loss: 0.0403 - val_loss: 0.0296\n",
      "Epoch 366/3000\n",
      "2370/2370 [==============================] - 0s 123us/step - loss: 0.0348 - val_loss: 0.0383\n",
      "Epoch 367/3000\n",
      "2370/2370 [==============================] - 0s 130us/step - loss: 0.0343 - val_loss: 0.0485\n",
      "Epoch 368/3000\n",
      "2370/2370 [==============================] - 0s 111us/step - loss: 0.0386 - val_loss: 0.0385\n",
      "Epoch 369/3000\n",
      "2370/2370 [==============================] - 0s 97us/step - loss: 0.0359 - val_loss: 0.0396\n",
      "Epoch 370/3000\n",
      "2370/2370 [==============================] - 0s 118us/step - loss: 0.0383 - val_loss: 0.0273\n",
      "Epoch 371/3000\n",
      "2370/2370 [==============================] - 0s 126us/step - loss: 0.0387 - val_loss: 0.0369\n",
      "Epoch 372/3000\n",
      "2370/2370 [==============================] - 0s 114us/step - loss: 0.0394 - val_loss: 0.0611\n",
      "Epoch 373/3000\n",
      "2370/2370 [==============================] - 0s 95us/step - loss: 0.0412 - val_loss: 0.0356\n",
      "Epoch 374/3000\n",
      "2370/2370 [==============================] - 0s 115us/step - loss: 0.0388 - val_loss: 0.0420\n",
      "Epoch 375/3000\n",
      "2370/2370 [==============================] - 0s 137us/step - loss: 0.0368 - val_loss: 0.0399\n",
      "Epoch 376/3000\n",
      "2370/2370 [==============================] - 0s 110us/step - loss: 0.0375 - val_loss: 0.0319\n",
      "Epoch 377/3000\n",
      "2370/2370 [==============================] - 0s 102us/step - loss: 0.0393 - val_loss: 0.0303\n",
      "Epoch 378/3000\n",
      "2370/2370 [==============================] - 0s 111us/step - loss: 0.0350 - val_loss: 0.0344\n",
      "Epoch 379/3000\n",
      "2370/2370 [==============================] - 0s 111us/step - loss: 0.0389 - val_loss: 0.0320\n",
      "Epoch 380/3000\n",
      "2370/2370 [==============================] - 0s 118us/step - loss: 0.0405 - val_loss: 0.0368\n",
      "Epoch 381/3000\n",
      "2370/2370 [==============================] - 0s 94us/step - loss: 0.0351 - val_loss: 0.0325\n",
      "Epoch 382/3000\n",
      "2370/2370 [==============================] - 0s 140us/step - loss: 0.0385 - val_loss: 0.0377\n",
      "Epoch 383/3000\n",
      "2370/2370 [==============================] - 0s 139us/step - loss: 0.0354 - val_loss: 0.0497\n",
      "Epoch 384/3000\n",
      "2370/2370 [==============================] - 0s 133us/step - loss: 0.0374 - val_loss: 0.0383\n",
      "Epoch 385/3000\n",
      "2370/2370 [==============================] - 0s 122us/step - loss: 0.0390 - val_loss: 0.0475\n",
      "Epoch 386/3000\n",
      "2370/2370 [==============================] - 0s 101us/step - loss: 0.0407 - val_loss: 0.0493\n",
      "Epoch 387/3000\n",
      "2370/2370 [==============================] - 0s 107us/step - loss: 0.0386 - val_loss: 0.0365\n",
      "Epoch 388/3000\n",
      "2370/2370 [==============================] - 0s 121us/step - loss: 0.0391 - val_loss: 0.0439\n",
      "Epoch 389/3000\n",
      "2370/2370 [==============================] - 0s 104us/step - loss: 0.0398 - val_loss: 0.0373\n",
      "Epoch 390/3000\n",
      "2370/2370 [==============================] - 0s 97us/step - loss: 0.0364 - val_loss: 0.0424\n",
      "Epoch 391/3000\n",
      "2370/2370 [==============================] - 0s 130us/step - loss: 0.0382 - val_loss: 0.0314\n",
      "Epoch 392/3000\n",
      "2370/2370 [==============================] - 0s 127us/step - loss: 0.0387 - val_loss: 0.0425\n",
      "Epoch 393/3000\n",
      "2370/2370 [==============================] - 0s 119us/step - loss: 0.0365 - val_loss: 0.0327\n",
      "Epoch 394/3000\n",
      "2370/2370 [==============================] - 0s 111us/step - loss: 0.0393 - val_loss: 0.0412\n",
      "Epoch 395/3000\n",
      "2370/2370 [==============================] - 0s 82us/step - loss: 0.0412 - val_loss: 0.0629\n",
      "Epoch 396/3000\n",
      "2370/2370 [==============================] - 0s 116us/step - loss: 0.0401 - val_loss: 0.0319\n",
      "Epoch 397/3000\n",
      "2370/2370 [==============================] - 0s 133us/step - loss: 0.0401 - val_loss: 0.0333\n",
      "Epoch 398/3000\n",
      "2370/2370 [==============================] - 0s 131us/step - loss: 0.0406 - val_loss: 0.0456\n",
      "Epoch 399/3000\n",
      "2370/2370 [==============================] - 0s 138us/step - loss: 0.0381 - val_loss: 0.0434\n",
      "Epoch 400/3000\n",
      "2370/2370 [==============================] - 0s 134us/step - loss: 0.0422 - val_loss: 0.0314\n",
      "Epoch 401/3000\n",
      "2370/2370 [==============================] - 0s 139us/step - loss: 0.0365 - val_loss: 0.0516\n",
      "Epoch 402/3000\n",
      "2370/2370 [==============================] - 0s 122us/step - loss: 0.0441 - val_loss: 0.0429\n",
      "Epoch 403/3000\n",
      "2370/2370 [==============================] - 0s 107us/step - loss: 0.0390 - val_loss: 0.0408\n",
      "Epoch 404/3000\n",
      "2370/2370 [==============================] - 0s 129us/step - loss: 0.0367 - val_loss: 0.0325\n",
      "Epoch 405/3000\n",
      "2370/2370 [==============================] - 0s 138us/step - loss: 0.0378 - val_loss: 0.0382\n",
      "Epoch 406/3000\n",
      "2370/2370 [==============================] - 0s 117us/step - loss: 0.0395 - val_loss: 0.0363\n",
      "Epoch 407/3000\n",
      "2370/2370 [==============================] - 0s 133us/step - loss: 0.0379 - val_loss: 0.0459\n",
      "Epoch 408/3000\n",
      "2370/2370 [==============================] - 0s 119us/step - loss: 0.0469 - val_loss: 0.0362\n",
      "Epoch 409/3000\n",
      "2370/2370 [==============================] - 0s 112us/step - loss: 0.0407 - val_loss: 0.0360\n",
      "Epoch 410/3000\n",
      "2370/2370 [==============================] - 0s 116us/step - loss: 0.0392 - val_loss: 0.0337\n",
      "Epoch 411/3000\n",
      "2370/2370 [==============================] - 0s 126us/step - loss: 0.0364 - val_loss: 0.0340\n",
      "Epoch 412/3000\n",
      "2370/2370 [==============================] - 0s 109us/step - loss: 0.0362 - val_loss: 0.0369\n",
      "Epoch 413/3000\n",
      "2370/2370 [==============================] - 0s 119us/step - loss: 0.0366 - val_loss: 0.0321\n",
      "Epoch 414/3000\n",
      "2370/2370 [==============================] - 0s 122us/step - loss: 0.0378 - val_loss: 0.0330\n",
      "Epoch 415/3000\n",
      "2370/2370 [==============================] - 0s 129us/step - loss: 0.0409 - val_loss: 0.0555\n",
      "Epoch 416/3000\n",
      "2370/2370 [==============================] - 0s 129us/step - loss: 0.0420 - val_loss: 0.0366\n",
      "Epoch 417/3000\n",
      "2370/2370 [==============================] - 0s 105us/step - loss: 0.0362 - val_loss: 0.0401\n",
      "Epoch 418/3000\n",
      "2370/2370 [==============================] - 0s 118us/step - loss: 0.0399 - val_loss: 0.0332\n",
      "Epoch 419/3000\n",
      "2370/2370 [==============================] - 0s 134us/step - loss: 0.0391 - val_loss: 0.0458\n",
      "Epoch 420/3000\n",
      "2370/2370 [==============================] - 0s 109us/step - loss: 0.0381 - val_loss: 0.0366\n",
      "Epoch 421/3000\n",
      "2370/2370 [==============================] - 0s 135us/step - loss: 0.0387 - val_loss: 0.0401\n",
      "Epoch 422/3000\n",
      "2370/2370 [==============================] - 0s 91us/step - loss: 0.0380 - val_loss: 0.0414\n",
      "Epoch 423/3000\n",
      "2370/2370 [==============================] - 0s 80us/step - loss: 0.0375 - val_loss: 0.0430\n",
      "Epoch 424/3000\n",
      "2370/2370 [==============================] - 0s 89us/step - loss: 0.0412 - val_loss: 0.0371\n",
      "Epoch 425/3000\n",
      "2370/2370 [==============================] - 0s 95us/step - loss: 0.0349 - val_loss: 0.0331\n",
      "Epoch 426/3000\n",
      "2370/2370 [==============================] - 0s 107us/step - loss: 0.0409 - val_loss: 0.0457\n",
      "Epoch 427/3000\n",
      "2370/2370 [==============================] - 0s 112us/step - loss: 0.0373 - val_loss: 0.0363\n",
      "Epoch 428/3000\n",
      "2370/2370 [==============================] - 0s 133us/step - loss: 0.0375 - val_loss: 0.0396\n",
      "Epoch 429/3000\n",
      "2370/2370 [==============================] - 0s 134us/step - loss: 0.0379 - val_loss: 0.0460\n",
      "Epoch 430/3000\n",
      "2370/2370 [==============================] - 0s 111us/step - loss: 0.0367 - val_loss: 0.0378\n",
      "Epoch 431/3000\n",
      "2370/2370 [==============================] - 0s 124us/step - loss: 0.0391 - val_loss: 0.0310\n",
      "Epoch 432/3000\n",
      "2370/2370 [==============================] - 0s 116us/step - loss: 0.0376 - val_loss: 0.0348\n",
      "Epoch 433/3000\n",
      "2370/2370 [==============================] - 0s 100us/step - loss: 0.0377 - val_loss: 0.0340\n",
      "Epoch 434/3000\n",
      "2370/2370 [==============================] - 0s 115us/step - loss: 0.0380 - val_loss: 0.0340\n",
      "Epoch 435/3000\n",
      "2370/2370 [==============================] - 0s 117us/step - loss: 0.0403 - val_loss: 0.0397\n",
      "Epoch 436/3000\n",
      "2370/2370 [==============================] - 0s 123us/step - loss: 0.0397 - val_loss: 0.0505\n",
      "Epoch 437/3000\n",
      "2370/2370 [==============================] - 0s 130us/step - loss: 0.0379 - val_loss: 0.0362\n",
      "Epoch 438/3000\n",
      "2370/2370 [==============================] - 0s 126us/step - loss: 0.0390 - val_loss: 0.0522\n",
      "Epoch 439/3000\n",
      "2370/2370 [==============================] - 0s 118us/step - loss: 0.0362 - val_loss: 0.0415\n",
      "Epoch 440/3000\n",
      "2370/2370 [==============================] - 0s 140us/step - loss: 0.0407 - val_loss: 0.0345\n",
      "Epoch 441/3000\n",
      "2370/2370 [==============================] - 0s 106us/step - loss: 0.0436 - val_loss: 0.0560\n",
      "Epoch 442/3000\n",
      "2370/2370 [==============================] - 0s 135us/step - loss: 0.0362 - val_loss: 0.0317\n",
      "Epoch 443/3000\n",
      "2370/2370 [==============================] - 0s 119us/step - loss: 0.0411 - val_loss: 0.0386\n",
      "Epoch 444/3000\n",
      "2370/2370 [==============================] - 0s 125us/step - loss: 0.0427 - val_loss: 0.0332\n",
      "Epoch 445/3000\n",
      "2370/2370 [==============================] - 0s 91us/step - loss: 0.0389 - val_loss: 0.0327\n",
      "Epoch 446/3000\n",
      "2370/2370 [==============================] - 0s 80us/step - loss: 0.0411 - val_loss: 0.0373\n",
      "Epoch 447/3000\n",
      "2370/2370 [==============================] - 0s 123us/step - loss: 0.0373 - val_loss: 0.0366\n",
      "Epoch 448/3000\n",
      "2370/2370 [==============================] - 0s 128us/step - loss: 0.0400 - val_loss: 0.0329\n",
      "Epoch 449/3000\n",
      "2370/2370 [==============================] - 0s 130us/step - loss: 0.0361 - val_loss: 0.0357\n",
      "Epoch 450/3000\n",
      "2370/2370 [==============================] - 0s 107us/step - loss: 0.0404 - val_loss: 0.0359\n",
      "Epoch 451/3000\n",
      "2370/2370 [==============================] - 0s 83us/step - loss: 0.0377 - val_loss: 0.0315\n",
      "Epoch 452/3000\n",
      "2370/2370 [==============================] - 0s 128us/step - loss: 0.0340 - val_loss: 0.0425\n",
      "Epoch 453/3000\n",
      "2370/2370 [==============================] - 0s 103us/step - loss: 0.0376 - val_loss: 0.0350\n",
      "Epoch 454/3000\n",
      "2370/2370 [==============================] - 0s 130us/step - loss: 0.0399 - val_loss: 0.0444\n",
      "Epoch 455/3000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2370/2370 [==============================] - 0s 107us/step - loss: 0.0368 - val_loss: 0.0658\n",
      "Epoch 456/3000\n",
      "2370/2370 [==============================] - 0s 99us/step - loss: 0.0335 - val_loss: 0.0317\n",
      "Epoch 457/3000\n",
      "2370/2370 [==============================] - 0s 110us/step - loss: 0.0361 - val_loss: 0.0448\n",
      "Epoch 458/3000\n",
      "2370/2370 [==============================] - 0s 121us/step - loss: 0.0366 - val_loss: 0.0358\n",
      "Epoch 459/3000\n",
      "2370/2370 [==============================] - 0s 128us/step - loss: 0.0369 - val_loss: 0.0363\n",
      "Epoch 460/3000\n",
      "2370/2370 [==============================] - 0s 130us/step - loss: 0.0413 - val_loss: 0.0314\n",
      "Epoch 461/3000\n",
      "2370/2370 [==============================] - 0s 148us/step - loss: 0.0380 - val_loss: 0.0425\n",
      "Epoch 462/3000\n",
      "2370/2370 [==============================] - 0s 135us/step - loss: 0.0397 - val_loss: 0.0308\n",
      "Epoch 463/3000\n",
      "2370/2370 [==============================] - 0s 132us/step - loss: 0.0376 - val_loss: 0.0349\n",
      "Epoch 464/3000\n",
      "2370/2370 [==============================] - 0s 132us/step - loss: 0.0384 - val_loss: 0.0474\n",
      "Epoch 465/3000\n",
      "2370/2370 [==============================] - 0s 94us/step - loss: 0.0347 - val_loss: 0.0320\n",
      "Epoch 466/3000\n",
      "2370/2370 [==============================] - 0s 146us/step - loss: 0.0374 - val_loss: 0.0325\n",
      "Epoch 467/3000\n",
      "2370/2370 [==============================] - 0s 124us/step - loss: 0.0412 - val_loss: 0.0392\n",
      "Epoch 468/3000\n",
      "2370/2370 [==============================] - 0s 98us/step - loss: 0.0353 - val_loss: 0.0406\n",
      "Epoch 469/3000\n",
      "2370/2370 [==============================] - 0s 132us/step - loss: 0.0386 - val_loss: 0.0429\n",
      "Epoch 470/3000\n",
      "2370/2370 [==============================] - 0s 143us/step - loss: 0.0341 - val_loss: 0.0361\n",
      "Epoch 471/3000\n",
      "2370/2370 [==============================] - 0s 117us/step - loss: 0.0377 - val_loss: 0.0307\n",
      "Epoch 472/3000\n",
      "2370/2370 [==============================] - 0s 136us/step - loss: 0.0387 - val_loss: 0.0310\n",
      "Epoch 473/3000\n",
      "2370/2370 [==============================] - 0s 98us/step - loss: 0.0339 - val_loss: 0.0332\n",
      "Epoch 474/3000\n",
      "2370/2370 [==============================] - 0s 89us/step - loss: 0.0366 - val_loss: 0.0383\n",
      "Epoch 475/3000\n",
      "2370/2370 [==============================] - 0s 131us/step - loss: 0.0373 - val_loss: 0.0323\n",
      "Epoch 476/3000\n",
      "2370/2370 [==============================] - 0s 131us/step - loss: 0.0401 - val_loss: 0.0460\n",
      "Epoch 477/3000\n",
      "2370/2370 [==============================] - 0s 125us/step - loss: 0.0369 - val_loss: 0.0293\n",
      "Epoch 478/3000\n",
      "2370/2370 [==============================] - 0s 106us/step - loss: 0.0349 - val_loss: 0.0325\n",
      "Epoch 479/3000\n",
      "2370/2370 [==============================] - 0s 137us/step - loss: 0.0379 - val_loss: 0.0305\n",
      "Epoch 480/3000\n",
      "2370/2370 [==============================] - 0s 105us/step - loss: 0.0418 - val_loss: 0.0576\n",
      "Epoch 481/3000\n",
      "2370/2370 [==============================] - 0s 100us/step - loss: 0.0377 - val_loss: 0.0363\n",
      "Epoch 482/3000\n",
      "2370/2370 [==============================] - 0s 49us/step - loss: 0.0363 - val_loss: 0.0373\n",
      "Epoch 483/3000\n",
      "2370/2370 [==============================] - 0s 94us/step - loss: 0.0376 - val_loss: 0.0304\n",
      "Epoch 484/3000\n",
      "2370/2370 [==============================] - 0s 132us/step - loss: 0.0343 - val_loss: 0.0308\n",
      "Epoch 485/3000\n",
      "2370/2370 [==============================] - 0s 133us/step - loss: 0.0386 - val_loss: 0.0429\n",
      "Epoch 486/3000\n",
      "2370/2370 [==============================] - 0s 109us/step - loss: 0.0400 - val_loss: 0.0475\n",
      "Epoch 487/3000\n",
      "2370/2370 [==============================] - 0s 117us/step - loss: 0.0378 - val_loss: 0.0544\n",
      "Epoch 488/3000\n",
      "2370/2370 [==============================] - 0s 109us/step - loss: 0.0373 - val_loss: 0.0400\n",
      "Epoch 489/3000\n",
      "2370/2370 [==============================] - 0s 62us/step - loss: 0.0376 - val_loss: 0.0322\n",
      "Epoch 490/3000\n",
      "2370/2370 [==============================] - 0s 101us/step - loss: 0.0410 - val_loss: 0.0446\n",
      "Epoch 491/3000\n",
      "2370/2370 [==============================] - 0s 71us/step - loss: 0.0367 - val_loss: 0.0372\n",
      "Epoch 492/3000\n",
      "2370/2370 [==============================] - 0s 125us/step - loss: 0.0410 - val_loss: 0.0431\n",
      "Epoch 493/3000\n",
      "2370/2370 [==============================] - 0s 122us/step - loss: 0.0393 - val_loss: 0.0348\n",
      "Epoch 494/3000\n",
      "2370/2370 [==============================] - 0s 100us/step - loss: 0.0358 - val_loss: 0.0349\n",
      "Epoch 495/3000\n",
      "2370/2370 [==============================] - 0s 106us/step - loss: 0.0391 - val_loss: 0.0389\n",
      "Epoch 496/3000\n",
      "2370/2370 [==============================] - 0s 119us/step - loss: 0.0412 - val_loss: 0.0386\n",
      "Epoch 497/3000\n",
      "2370/2370 [==============================] - 0s 102us/step - loss: 0.0380 - val_loss: 0.0307\n",
      "Epoch 498/3000\n",
      "2370/2370 [==============================] - 0s 118us/step - loss: 0.0416 - val_loss: 0.0409\n",
      "Epoch 499/3000\n",
      "2370/2370 [==============================] - 0s 130us/step - loss: 0.0393 - val_loss: 0.0385\n",
      "Epoch 500/3000\n",
      "2370/2370 [==============================] - 0s 129us/step - loss: 0.0349 - val_loss: 0.0625\n",
      "Epoch 501/3000\n",
      "2370/2370 [==============================] - 0s 137us/step - loss: 0.0398 - val_loss: 0.0391\n",
      "Epoch 502/3000\n",
      "2370/2370 [==============================] - 0s 135us/step - loss: 0.0381 - val_loss: 0.0354\n",
      "Epoch 503/3000\n",
      "2370/2370 [==============================] - 0s 112us/step - loss: 0.0424 - val_loss: 0.0322\n",
      "Epoch 504/3000\n",
      "2370/2370 [==============================] - 0s 53us/step - loss: 0.0393 - val_loss: 0.0342\n",
      "Epoch 505/3000\n",
      "2370/2370 [==============================] - 0s 85us/step - loss: 0.0408 - val_loss: 0.0330\n",
      "Epoch 506/3000\n",
      "2370/2370 [==============================] - 0s 136us/step - loss: 0.0370 - val_loss: 0.0322\n",
      "Epoch 507/3000\n",
      "2370/2370 [==============================] - 0s 134us/step - loss: 0.0333 - val_loss: 0.0481\n",
      "Epoch 508/3000\n",
      "2370/2370 [==============================] - 0s 134us/step - loss: 0.0415 - val_loss: 0.0416\n",
      "Epoch 509/3000\n",
      "2370/2370 [==============================] - 0s 119us/step - loss: 0.0409 - val_loss: 0.0378\n",
      "Epoch 510/3000\n",
      "2370/2370 [==============================] - 0s 126us/step - loss: 0.0375 - val_loss: 0.0367\n",
      "Epoch 511/3000\n",
      "2370/2370 [==============================] - 0s 133us/step - loss: 0.0404 - val_loss: 0.0380\n",
      "Epoch 512/3000\n",
      "2370/2370 [==============================] - 0s 124us/step - loss: 0.0433 - val_loss: 0.0614\n",
      "Epoch 513/3000\n",
      "2370/2370 [==============================] - 0s 130us/step - loss: 0.0374 - val_loss: 0.0373\n",
      "Epoch 514/3000\n",
      "2370/2370 [==============================] - 0s 123us/step - loss: 0.0398 - val_loss: 0.0443\n",
      "Epoch 515/3000\n",
      "2370/2370 [==============================] - 0s 130us/step - loss: 0.0374 - val_loss: 0.0433\n",
      "Epoch 516/3000\n",
      "2370/2370 [==============================] - 0s 75us/step - loss: 0.0349 - val_loss: 0.0515\n",
      "Epoch 517/3000\n",
      "2370/2370 [==============================] - 0s 51us/step - loss: 0.0410 - val_loss: 0.0386\n",
      "Epoch 518/3000\n",
      "2370/2370 [==============================] - 0s 104us/step - loss: 0.0356 - val_loss: 0.0320\n",
      "Epoch 519/3000\n",
      "2370/2370 [==============================] - 0s 104us/step - loss: 0.0430 - val_loss: 0.0340\n",
      "Epoch 520/3000\n",
      "2370/2370 [==============================] - 0s 134us/step - loss: 0.0349 - val_loss: 0.0488\n",
      "Epoch 521/3000\n",
      "2370/2370 [==============================] - 0s 131us/step - loss: 0.0393 - val_loss: 0.0549\n",
      "Epoch 522/3000\n",
      "2370/2370 [==============================] - 0s 122us/step - loss: 0.0366 - val_loss: 0.0355\n",
      "Epoch 523/3000\n",
      "2370/2370 [==============================] - 0s 126us/step - loss: 0.0387 - val_loss: 0.0312\n",
      "Epoch 524/3000\n",
      "2370/2370 [==============================] - 0s 120us/step - loss: 0.0383 - val_loss: 0.0347\n",
      "Epoch 525/3000\n",
      "2370/2370 [==============================] - 0s 100us/step - loss: 0.0377 - val_loss: 0.0374\n",
      "Epoch 526/3000\n",
      "2370/2370 [==============================] - 0s 133us/step - loss: 0.0376 - val_loss: 0.0349\n",
      "Epoch 527/3000\n",
      "2370/2370 [==============================] - 0s 130us/step - loss: 0.0361 - val_loss: 0.0492\n",
      "Epoch 528/3000\n",
      "2370/2370 [==============================] - 0s 126us/step - loss: 0.0378 - val_loss: 0.0516\n",
      "Epoch 529/3000\n",
      "2370/2370 [==============================] - 0s 137us/step - loss: 0.0387 - val_loss: 0.0320\n",
      "Epoch 530/3000\n",
      "2370/2370 [==============================] - 0s 116us/step - loss: 0.0419 - val_loss: 0.0432\n",
      "Epoch 531/3000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2370/2370 [==============================] - 0s 116us/step - loss: 0.0404 - val_loss: 0.0333\n",
      "Epoch 532/3000\n",
      "2370/2370 [==============================] - 0s 131us/step - loss: 0.0384 - val_loss: 0.0364\n",
      "Epoch 533/3000\n",
      "2370/2370 [==============================] - 0s 125us/step - loss: 0.0389 - val_loss: 0.0359\n",
      "Epoch 534/3000\n",
      "2370/2370 [==============================] - 0s 137us/step - loss: 0.0381 - val_loss: 0.0413\n",
      "Epoch 535/3000\n",
      "2370/2370 [==============================] - 0s 154us/step - loss: 0.0425 - val_loss: 0.0393\n",
      "Epoch 536/3000\n",
      "2370/2370 [==============================] - 0s 146us/step - loss: 0.0354 - val_loss: 0.0298\n",
      "Epoch 537/3000\n",
      "2370/2370 [==============================] - 0s 145us/step - loss: 0.0383 - val_loss: 0.0379\n",
      "Epoch 538/3000\n",
      "2370/2370 [==============================] - 0s 150us/step - loss: 0.0376 - val_loss: 0.0382\n",
      "Epoch 539/3000\n",
      "2370/2370 [==============================] - 0s 135us/step - loss: 0.0362 - val_loss: 0.0304\n",
      "Epoch 540/3000\n",
      "2370/2370 [==============================] - 0s 138us/step - loss: 0.0373 - val_loss: 0.0330\n",
      "Epoch 541/3000\n",
      "2370/2370 [==============================] - 0s 138us/step - loss: 0.0414 - val_loss: 0.0365\n",
      "Epoch 542/3000\n",
      "2370/2370 [==============================] - 0s 141us/step - loss: 0.0410 - val_loss: 0.0352\n",
      "Epoch 543/3000\n",
      "2370/2370 [==============================] - 0s 141us/step - loss: 0.0396 - val_loss: 0.0358\n",
      "Epoch 544/3000\n",
      "2370/2370 [==============================] - 0s 134us/step - loss: 0.0404 - val_loss: 0.0343\n",
      "Epoch 545/3000\n",
      "2370/2370 [==============================] - 0s 121us/step - loss: 0.0394 - val_loss: 0.0615\n",
      "Epoch 546/3000\n",
      "2370/2370 [==============================] - 0s 134us/step - loss: 0.0366 - val_loss: 0.0376\n",
      "Epoch 547/3000\n",
      "2370/2370 [==============================] - 0s 112us/step - loss: 0.0354 - val_loss: 0.0341\n",
      "Epoch 548/3000\n",
      "2370/2370 [==============================] - 0s 131us/step - loss: 0.0378 - val_loss: 0.0377\n",
      "Epoch 549/3000\n",
      "2370/2370 [==============================] - 0s 136us/step - loss: 0.0389 - val_loss: 0.0376\n",
      "Epoch 550/3000\n",
      "2370/2370 [==============================] - 0s 106us/step - loss: 0.0405 - val_loss: 0.0712\n",
      "Epoch 551/3000\n",
      "2370/2370 [==============================] - 0s 62us/step - loss: 0.0344 - val_loss: 0.0381\n",
      "Epoch 552/3000\n",
      "2370/2370 [==============================] - 0s 121us/step - loss: 0.0414 - val_loss: 0.0457\n",
      "Epoch 553/3000\n",
      "2370/2370 [==============================] - 0s 136us/step - loss: 0.0377 - val_loss: 0.0389\n",
      "Epoch 554/3000\n",
      "2370/2370 [==============================] - 0s 138us/step - loss: 0.0362 - val_loss: 0.0536\n",
      "Epoch 555/3000\n",
      "2370/2370 [==============================] - 0s 134us/step - loss: 0.0408 - val_loss: 0.0374\n",
      "Epoch 556/3000\n",
      "2370/2370 [==============================] - 0s 111us/step - loss: 0.0355 - val_loss: 0.0352\n",
      "Epoch 557/3000\n",
      "2370/2370 [==============================] - 0s 81us/step - loss: 0.0354 - val_loss: 0.0490\n",
      "Epoch 558/3000\n",
      "2370/2370 [==============================] - 0s 51us/step - loss: 0.0356 - val_loss: 0.0333\n",
      "Epoch 559/3000\n",
      "2370/2370 [==============================] - 0s 114us/step - loss: 0.0385 - val_loss: 0.0318\n",
      "Epoch 560/3000\n",
      "2370/2370 [==============================] - 0s 130us/step - loss: 0.0354 - val_loss: 0.0449\n",
      "Epoch 561/3000\n",
      "2370/2370 [==============================] - 0s 137us/step - loss: 0.0372 - val_loss: 0.0571\n",
      "Epoch 562/3000\n",
      "2370/2370 [==============================] - 0s 73us/step - loss: 0.0409 - val_loss: 0.0412\n",
      "Epoch 563/3000\n",
      "2370/2370 [==============================] - 0s 136us/step - loss: 0.0361 - val_loss: 0.0340\n",
      "Epoch 564/3000\n",
      "2370/2370 [==============================] - 0s 124us/step - loss: 0.0393 - val_loss: 0.0346\n",
      "Epoch 565/3000\n",
      "2370/2370 [==============================] - 0s 124us/step - loss: 0.0380 - val_loss: 0.0565\n",
      "Epoch 566/3000\n",
      "2370/2370 [==============================] - 0s 139us/step - loss: 0.0375 - val_loss: 0.0533\n",
      "Epoch 567/3000\n",
      "2370/2370 [==============================] - 0s 102us/step - loss: 0.0422 - val_loss: 0.0342\n",
      "Epoch 568/3000\n",
      "2370/2370 [==============================] - 0s 79us/step - loss: 0.0410 - val_loss: 0.0388\n",
      "Epoch 569/3000\n",
      "2370/2370 [==============================] - 0s 78us/step - loss: 0.0376 - val_loss: 0.0370\n",
      "Epoch 570/3000\n",
      "2370/2370 [==============================] - 0s 81us/step - loss: 0.0383 - val_loss: 0.0300\n",
      "Epoch 571/3000\n",
      "2370/2370 [==============================] - 0s 75us/step - loss: 0.0385 - val_loss: 0.0307\n",
      "Epoch 572/3000\n",
      "2370/2370 [==============================] - 0s 81us/step - loss: 0.0395 - val_loss: 0.0427\n",
      "Epoch 573/3000\n",
      "2370/2370 [==============================] - 0s 77us/step - loss: 0.0350 - val_loss: 0.0364\n",
      "Epoch 574/3000\n",
      "2370/2370 [==============================] - 0s 72us/step - loss: 0.0384 - val_loss: 0.0371\n",
      "Epoch 575/3000\n",
      "2370/2370 [==============================] - 0s 61us/step - loss: 0.0402 - val_loss: 0.0337\n",
      "Epoch 576/3000\n",
      "2370/2370 [==============================] - 0s 69us/step - loss: 0.0359 - val_loss: 0.0303\n",
      "Epoch 577/3000\n",
      "2370/2370 [==============================] - 0s 61us/step - loss: 0.0408 - val_loss: 0.0345\n",
      "Epoch 578/3000\n",
      "2370/2370 [==============================] - 0s 63us/step - loss: 0.0382 - val_loss: 0.0329\n",
      "Epoch 579/3000\n",
      "2370/2370 [==============================] - 0s 65us/step - loss: 0.0373 - val_loss: 0.0453\n",
      "Epoch 580/3000\n",
      "2370/2370 [==============================] - 0s 71us/step - loss: 0.0371 - val_loss: 0.0440\n",
      "Epoch 581/3000\n",
      "2370/2370 [==============================] - 0s 62us/step - loss: 0.0371 - val_loss: 0.0545\n",
      "Epoch 582/3000\n",
      "2370/2370 [==============================] - 0s 59us/step - loss: 0.0438 - val_loss: 0.0365\n",
      "Epoch 583/3000\n",
      "2370/2370 [==============================] - 0s 67us/step - loss: 0.0368 - val_loss: 0.0417\n",
      "Epoch 584/3000\n",
      "2370/2370 [==============================] - 0s 64us/step - loss: 0.0385 - val_loss: 0.0462\n",
      "Epoch 585/3000\n",
      "2370/2370 [==============================] - 0s 63us/step - loss: 0.0419 - val_loss: 0.0332\n",
      "Epoch 586/3000\n",
      "2370/2370 [==============================] - 0s 62us/step - loss: 0.0383 - val_loss: 0.0456\n",
      "Epoch 587/3000\n",
      "2370/2370 [==============================] - 0s 66us/step - loss: 0.0398 - val_loss: 0.0376\n",
      "Epoch 588/3000\n",
      "2370/2370 [==============================] - 0s 62us/step - loss: 0.0355 - val_loss: 0.0318\n",
      "Epoch 589/3000\n",
      "2370/2370 [==============================] - 0s 66us/step - loss: 0.0371 - val_loss: 0.0451\n",
      "Epoch 590/3000\n",
      "2370/2370 [==============================] - 0s 67us/step - loss: 0.0381 - val_loss: 0.0402\n",
      "Epoch 591/3000\n",
      "2370/2370 [==============================] - 0s 61us/step - loss: 0.0393 - val_loss: 0.0390\n",
      "Epoch 592/3000\n",
      "2370/2370 [==============================] - 0s 64us/step - loss: 0.0348 - val_loss: 0.0311\n",
      "Epoch 593/3000\n",
      "2370/2370 [==============================] - 0s 59us/step - loss: 0.0428 - val_loss: 0.0325\n",
      "Epoch 594/3000\n",
      "2370/2370 [==============================] - 0s 64us/step - loss: 0.0394 - val_loss: 0.0355\n",
      "Epoch 595/3000\n",
      "2370/2370 [==============================] - 0s 60us/step - loss: 0.0404 - val_loss: 0.0404\n",
      "Epoch 596/3000\n",
      "2370/2370 [==============================] - 0s 63us/step - loss: 0.0371 - val_loss: 0.0342\n",
      "Epoch 597/3000\n",
      "2370/2370 [==============================] - 0s 63us/step - loss: 0.0409 - val_loss: 0.0324\n",
      "Epoch 598/3000\n",
      "2370/2370 [==============================] - 0s 64us/step - loss: 0.0362 - val_loss: 0.0368\n",
      "Epoch 599/3000\n",
      "2370/2370 [==============================] - 0s 63us/step - loss: 0.0371 - val_loss: 0.0373\n",
      "Epoch 600/3000\n",
      "2370/2370 [==============================] - 0s 62us/step - loss: 0.0380 - val_loss: 0.0346\n",
      "Epoch 601/3000\n",
      "2370/2370 [==============================] - 0s 63us/step - loss: 0.0384 - val_loss: 0.0336\n",
      "Epoch 602/3000\n",
      "2370/2370 [==============================] - 0s 62us/step - loss: 0.0361 - val_loss: 0.0379\n",
      "Epoch 603/3000\n",
      "2370/2370 [==============================] - 0s 58us/step - loss: 0.0385 - val_loss: 0.0351\n",
      "Epoch 604/3000\n",
      "2370/2370 [==============================] - 0s 63us/step - loss: 0.0358 - val_loss: 0.0467\n",
      "Epoch 605/3000\n",
      "2370/2370 [==============================] - 0s 63us/step - loss: 0.0399 - val_loss: 0.0426\n",
      "Epoch 606/3000\n",
      "2370/2370 [==============================] - 0s 66us/step - loss: 0.0357 - val_loss: 0.0405\n",
      "Epoch 607/3000\n",
      "2370/2370 [==============================] - 0s 63us/step - loss: 0.0391 - val_loss: 0.0409\n",
      "Epoch 608/3000\n",
      "2370/2370 [==============================] - 0s 65us/step - loss: 0.0357 - val_loss: 0.0356\n",
      "Epoch 609/3000\n",
      "2370/2370 [==============================] - 0s 66us/step - loss: 0.0382 - val_loss: 0.0351\n",
      "Epoch 610/3000\n",
      "2370/2370 [==============================] - 0s 57us/step - loss: 0.0391 - val_loss: 0.0367\n",
      "Epoch 611/3000\n",
      "2370/2370 [==============================] - 0s 65us/step - loss: 0.0411 - val_loss: 0.0409\n",
      "Epoch 612/3000\n",
      "2370/2370 [==============================] - 0s 60us/step - loss: 0.0379 - val_loss: 0.0377\n",
      "Epoch 613/3000\n",
      "2370/2370 [==============================] - 0s 68us/step - loss: 0.0359 - val_loss: 0.0456\n",
      "Epoch 614/3000\n",
      "2370/2370 [==============================] - 0s 57us/step - loss: 0.0380 - val_loss: 0.0340\n",
      "Epoch 615/3000\n",
      "2370/2370 [==============================] - 0s 64us/step - loss: 0.0386 - val_loss: 0.0536\n",
      "Epoch 616/3000\n",
      "2370/2370 [==============================] - 0s 62us/step - loss: 0.0400 - val_loss: 0.0334\n",
      "Epoch 617/3000\n",
      "2370/2370 [==============================] - 0s 59us/step - loss: 0.0391 - val_loss: 0.0450\n",
      "Epoch 618/3000\n",
      "2370/2370 [==============================] - 0s 64us/step - loss: 0.0351 - val_loss: 0.0333\n",
      "Epoch 619/3000\n",
      "2370/2370 [==============================] - 0s 67us/step - loss: 0.0393 - val_loss: 0.0425\n",
      "Epoch 620/3000\n",
      "2370/2370 [==============================] - 0s 66us/step - loss: 0.0391 - val_loss: 0.0400\n",
      "Epoch 621/3000\n",
      "2370/2370 [==============================] - 0s 65us/step - loss: 0.0381 - val_loss: 0.0376\n",
      "Epoch 622/3000\n",
      "2370/2370 [==============================] - 0s 59us/step - loss: 0.0353 - val_loss: 0.0556\n",
      "Epoch 623/3000\n",
      "2370/2370 [==============================] - 0s 66us/step - loss: 0.0366 - val_loss: 0.0369\n",
      "Epoch 624/3000\n",
      "2370/2370 [==============================] - 0s 61us/step - loss: 0.0381 - val_loss: 0.0340\n",
      "Epoch 625/3000\n",
      "2370/2370 [==============================] - 0s 65us/step - loss: 0.0411 - val_loss: 0.0428\n",
      "Epoch 626/3000\n",
      "2370/2370 [==============================] - 0s 65us/step - loss: 0.0386 - val_loss: 0.0345\n",
      "Epoch 627/3000\n",
      "2370/2370 [==============================] - 0s 72us/step - loss: 0.0378 - val_loss: 0.0528\n",
      "Epoch 628/3000\n",
      "2370/2370 [==============================] - 0s 57us/step - loss: 0.0381 - val_loss: 0.0418\n",
      "Epoch 629/3000\n",
      "2370/2370 [==============================] - 0s 65us/step - loss: 0.0396 - val_loss: 0.0387\n",
      "Epoch 630/3000\n",
      "2370/2370 [==============================] - 0s 58us/step - loss: 0.0371 - val_loss: 0.0402\n",
      "Epoch 631/3000\n",
      "2370/2370 [==============================] - 0s 62us/step - loss: 0.0413 - val_loss: 0.0444\n",
      "Epoch 632/3000\n",
      "2370/2370 [==============================] - 0s 60us/step - loss: 0.0382 - val_loss: 0.0363\n",
      "Epoch 633/3000\n",
      "2370/2370 [==============================] - 0s 57us/step - loss: 0.0423 - val_loss: 0.0343\n",
      "Epoch 634/3000\n",
      "2370/2370 [==============================] - 0s 55us/step - loss: 0.0387 - val_loss: 0.0352\n",
      "Epoch 635/3000\n",
      "2370/2370 [==============================] - 0s 65us/step - loss: 0.0413 - val_loss: 0.0532\n",
      "Epoch 636/3000\n",
      "2370/2370 [==============================] - 0s 56us/step - loss: 0.0385 - val_loss: 0.0321\n",
      "Epoch 637/3000\n",
      "2370/2370 [==============================] - 0s 60us/step - loss: 0.0410 - val_loss: 0.0330\n",
      "Epoch 638/3000\n",
      "2370/2370 [==============================] - 0s 64us/step - loss: 0.0338 - val_loss: 0.0425\n",
      "Epoch 639/3000\n",
      "2370/2370 [==============================] - 0s 62us/step - loss: 0.0375 - val_loss: 0.0363\n",
      "Epoch 640/3000\n",
      "2370/2370 [==============================] - 0s 72us/step - loss: 0.0377 - val_loss: 0.0433\n",
      "Epoch 641/3000\n",
      "2370/2370 [==============================] - 0s 66us/step - loss: 0.0396 - val_loss: 0.0358\n",
      "Epoch 642/3000\n",
      "2370/2370 [==============================] - 0s 64us/step - loss: 0.0358 - val_loss: 0.0429\n",
      "Epoch 643/3000\n",
      "2370/2370 [==============================] - 0s 72us/step - loss: 0.0372 - val_loss: 0.0462\n",
      "Epoch 644/3000\n",
      "2370/2370 [==============================] - 0s 69us/step - loss: 0.0399 - val_loss: 0.0371\n",
      "Epoch 645/3000\n",
      "2370/2370 [==============================] - 0s 58us/step - loss: 0.0372 - val_loss: 0.0559\n",
      "Epoch 646/3000\n",
      "2370/2370 [==============================] - 0s 67us/step - loss: 0.0406 - val_loss: 0.0360\n",
      "Epoch 647/3000\n",
      "2370/2370 [==============================] - 0s 65us/step - loss: 0.0360 - val_loss: 0.0363\n",
      "Epoch 648/3000\n",
      "2370/2370 [==============================] - 0s 63us/step - loss: 0.0390 - val_loss: 0.0412\n",
      "Epoch 649/3000\n",
      "2370/2370 [==============================] - 0s 66us/step - loss: 0.0394 - val_loss: 0.0445\n",
      "Epoch 650/3000\n",
      "2370/2370 [==============================] - 0s 57us/step - loss: 0.0398 - val_loss: 0.0402\n",
      "Epoch 651/3000\n",
      "2370/2370 [==============================] - 0s 64us/step - loss: 0.0389 - val_loss: 0.0458\n",
      "Epoch 652/3000\n",
      "2370/2370 [==============================] - 0s 63us/step - loss: 0.0361 - val_loss: 0.0451\n",
      "Epoch 653/3000\n",
      "2370/2370 [==============================] - 0s 62us/step - loss: 0.0384 - val_loss: 0.0359\n",
      "Epoch 654/3000\n",
      "2370/2370 [==============================] - 0s 60us/step - loss: 0.0356 - val_loss: 0.0380\n",
      "Epoch 655/3000\n",
      "2370/2370 [==============================] - 0s 66us/step - loss: 0.0387 - val_loss: 0.0368\n",
      "Epoch 656/3000\n",
      "2370/2370 [==============================] - 0s 69us/step - loss: 0.0367 - val_loss: 0.0318\n",
      "Epoch 657/3000\n",
      "2370/2370 [==============================] - 0s 66us/step - loss: 0.0361 - val_loss: 0.0333\n",
      "Epoch 658/3000\n",
      "2370/2370 [==============================] - 0s 69us/step - loss: 0.0396 - val_loss: 0.0654\n",
      "Epoch 659/3000\n",
      "2370/2370 [==============================] - 0s 65us/step - loss: 0.0348 - val_loss: 0.0568\n",
      "Epoch 660/3000\n",
      "2370/2370 [==============================] - 0s 65us/step - loss: 0.0385 - val_loss: 0.0420\n",
      "Epoch 661/3000\n",
      "2370/2370 [==============================] - 0s 70us/step - loss: 0.0376 - val_loss: 0.0440\n",
      "Epoch 662/3000\n",
      "2370/2370 [==============================] - 0s 66us/step - loss: 0.0378 - val_loss: 0.0376\n",
      "Epoch 663/3000\n",
      "2370/2370 [==============================] - 0s 57us/step - loss: 0.0404 - val_loss: 0.0433\n",
      "Epoch 664/3000\n",
      "2370/2370 [==============================] - 0s 67us/step - loss: 0.0390 - val_loss: 0.0456\n",
      "Epoch 665/3000\n",
      "2370/2370 [==============================] - 0s 64us/step - loss: 0.0357 - val_loss: 0.0411\n",
      "Epoch 666/3000\n",
      "2370/2370 [==============================] - 0s 62us/step - loss: 0.0386 - val_loss: 0.0449\n",
      "Epoch 667/3000\n",
      "2370/2370 [==============================] - 0s 59us/step - loss: 0.0386 - val_loss: 0.0526\n",
      "Epoch 668/3000\n",
      "2370/2370 [==============================] - 0s 60us/step - loss: 0.0404 - val_loss: 0.0369\n",
      "Epoch 669/3000\n",
      "2370/2370 [==============================] - 0s 66us/step - loss: 0.0356 - val_loss: 0.0422\n",
      "Epoch 670/3000\n",
      "2370/2370 [==============================] - 0s 60us/step - loss: 0.0370 - val_loss: 0.0400\n",
      "Epoch 671/3000\n",
      "2370/2370 [==============================] - 0s 64us/step - loss: 0.0396 - val_loss: 0.0382\n",
      "Epoch 672/3000\n",
      "2370/2370 [==============================] - 0s 61us/step - loss: 0.0392 - val_loss: 0.0348\n",
      "Epoch 673/3000\n",
      "2370/2370 [==============================] - 0s 67us/step - loss: 0.0408 - val_loss: 0.0321\n",
      "Epoch 674/3000\n",
      "2370/2370 [==============================] - 0s 62us/step - loss: 0.0394 - val_loss: 0.0392\n",
      "Epoch 675/3000\n",
      "2370/2370 [==============================] - 0s 66us/step - loss: 0.0400 - val_loss: 0.0499\n",
      "Epoch 676/3000\n",
      "2370/2370 [==============================] - 0s 65us/step - loss: 0.0410 - val_loss: 0.0351\n",
      "Epoch 677/3000\n",
      "2370/2370 [==============================] - 0s 65us/step - loss: 0.0404 - val_loss: 0.0385\n",
      "Epoch 678/3000\n",
      "2370/2370 [==============================] - 0s 66us/step - loss: 0.0362 - val_loss: 0.0517\n",
      "Epoch 679/3000\n",
      "2370/2370 [==============================] - 0s 58us/step - loss: 0.0365 - val_loss: 0.0401\n",
      "Epoch 680/3000\n",
      "2370/2370 [==============================] - 0s 64us/step - loss: 0.0420 - val_loss: 0.0436\n",
      "Epoch 681/3000\n",
      "2370/2370 [==============================] - 0s 63us/step - loss: 0.0391 - val_loss: 0.0333\n",
      "Epoch 682/3000\n",
      "2370/2370 [==============================] - 0s 60us/step - loss: 0.0368 - val_loss: 0.0320\n",
      "Epoch 683/3000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2370/2370 [==============================] - 0s 63us/step - loss: 0.0345 - val_loss: 0.0484\n",
      "Epoch 684/3000\n",
      "2370/2370 [==============================] - 0s 55us/step - loss: 0.0361 - val_loss: 0.0315\n",
      "Epoch 685/3000\n",
      "2370/2370 [==============================] - 0s 58us/step - loss: 0.0388 - val_loss: 0.0375\n",
      "Epoch 686/3000\n",
      "2370/2370 [==============================] - 0s 65us/step - loss: 0.0388 - val_loss: 0.0325\n",
      "Epoch 687/3000\n",
      "2370/2370 [==============================] - 0s 65us/step - loss: 0.0380 - val_loss: 0.0321\n",
      "Epoch 688/3000\n",
      "2370/2370 [==============================] - 0s 52us/step - loss: 0.0392 - val_loss: 0.0418\n",
      "Epoch 689/3000\n",
      "2370/2370 [==============================] - 0s 65us/step - loss: 0.0392 - val_loss: 0.0322\n",
      "Epoch 690/3000\n",
      "2370/2370 [==============================] - 0s 63us/step - loss: 0.0376 - val_loss: 0.0422\n",
      "Epoch 691/3000\n",
      "2370/2370 [==============================] - 0s 62us/step - loss: 0.0391 - val_loss: 0.0369\n",
      "Epoch 692/3000\n",
      "2370/2370 [==============================] - 0s 56us/step - loss: 0.0372 - val_loss: 0.0366\n",
      "Epoch 693/3000\n",
      "2370/2370 [==============================] - 0s 57us/step - loss: 0.0386 - val_loss: 0.0470\n",
      "Epoch 694/3000\n",
      "2370/2370 [==============================] - 0s 65us/step - loss: 0.0375 - val_loss: 0.0428\n",
      "Epoch 695/3000\n",
      "2370/2370 [==============================] - 0s 66us/step - loss: 0.0385 - val_loss: 0.0527\n",
      "Epoch 696/3000\n",
      "2370/2370 [==============================] - 0s 71us/step - loss: 0.0386 - val_loss: 0.0422\n",
      "Epoch 697/3000\n",
      "2370/2370 [==============================] - 0s 74us/step - loss: 0.0386 - val_loss: 0.0393\n",
      "Epoch 698/3000\n",
      "2370/2370 [==============================] - 0s 70us/step - loss: 0.0374 - val_loss: 0.0366\n",
      "Epoch 699/3000\n",
      "2370/2370 [==============================] - 0s 63us/step - loss: 0.0380 - val_loss: 0.0339\n",
      "Epoch 700/3000\n",
      "2370/2370 [==============================] - 0s 69us/step - loss: 0.0391 - val_loss: 0.0353\n",
      "Epoch 701/3000\n",
      "2370/2370 [==============================] - 0s 74us/step - loss: 0.0366 - val_loss: 0.0335\n",
      "Epoch 702/3000\n",
      "2370/2370 [==============================] - 0s 68us/step - loss: 0.0356 - val_loss: 0.0469\n",
      "Epoch 703/3000\n",
      "2370/2370 [==============================] - 0s 59us/step - loss: 0.0377 - val_loss: 0.0373\n",
      "Epoch 704/3000\n",
      "2370/2370 [==============================] - 0s 69us/step - loss: 0.0387 - val_loss: 0.0335\n",
      "Epoch 705/3000\n",
      "2370/2370 [==============================] - 0s 73us/step - loss: 0.0385 - val_loss: 0.0337\n",
      "Epoch 706/3000\n",
      "2370/2370 [==============================] - 0s 68us/step - loss: 0.0417 - val_loss: 0.0392\n",
      "Epoch 707/3000\n",
      "2370/2370 [==============================] - 0s 66us/step - loss: 0.0343 - val_loss: 0.0407\n",
      "Epoch 708/3000\n",
      "2370/2370 [==============================] - 0s 69us/step - loss: 0.0401 - val_loss: 0.0428\n",
      "Epoch 709/3000\n",
      "2370/2370 [==============================] - 0s 56us/step - loss: 0.0403 - val_loss: 0.0424\n",
      "Epoch 710/3000\n",
      "2370/2370 [==============================] - 0s 66us/step - loss: 0.0390 - val_loss: 0.0547\n",
      "Epoch 711/3000\n",
      "2370/2370 [==============================] - 0s 63us/step - loss: 0.0399 - val_loss: 0.0403\n",
      "Epoch 712/3000\n",
      "2370/2370 [==============================] - 0s 64us/step - loss: 0.0383 - val_loss: 0.0406\n",
      "Epoch 713/3000\n",
      "2370/2370 [==============================] - 0s 61us/step - loss: 0.0381 - val_loss: 0.0354\n",
      "Epoch 714/3000\n",
      "2370/2370 [==============================] - 0s 72us/step - loss: 0.0372 - val_loss: 0.0471\n",
      "Epoch 715/3000\n",
      "2370/2370 [==============================] - 0s 58us/step - loss: 0.0369 - val_loss: 0.0311\n",
      "Epoch 716/3000\n",
      "2370/2370 [==============================] - 0s 63us/step - loss: 0.0361 - val_loss: 0.0515\n",
      "Epoch 717/3000\n",
      "2370/2370 [==============================] - 0s 61us/step - loss: 0.0396 - val_loss: 0.0605\n",
      "Epoch 718/3000\n",
      "2370/2370 [==============================] - 0s 67us/step - loss: 0.0391 - val_loss: 0.0390\n",
      "Epoch 719/3000\n",
      "2370/2370 [==============================] - 0s 64us/step - loss: 0.0374 - val_loss: 0.0471\n",
      "Epoch 720/3000\n",
      "2370/2370 [==============================] - 0s 63us/step - loss: 0.0402 - val_loss: 0.0491\n",
      "Epoch 721/3000\n",
      "2370/2370 [==============================] - 0s 65us/step - loss: 0.0367 - val_loss: 0.0346\n",
      "Epoch 722/3000\n",
      "2370/2370 [==============================] - 0s 65us/step - loss: 0.0382 - val_loss: 0.0343\n",
      "Epoch 723/3000\n",
      "2370/2370 [==============================] - 0s 69us/step - loss: 0.0365 - val_loss: 0.0415\n",
      "Epoch 724/3000\n",
      "2370/2370 [==============================] - 0s 55us/step - loss: 0.0378 - val_loss: 0.0340\n",
      "Epoch 725/3000\n",
      "2370/2370 [==============================] - 0s 64us/step - loss: 0.0384 - val_loss: 0.0515\n",
      "Epoch 726/3000\n",
      "2370/2370 [==============================] - 0s 57us/step - loss: 0.0389 - val_loss: 0.0499\n",
      "Epoch 727/3000\n",
      "2370/2370 [==============================] - 0s 68us/step - loss: 0.0402 - val_loss: 0.0450\n",
      "Epoch 728/3000\n",
      "2370/2370 [==============================] - 0s 57us/step - loss: 0.0408 - val_loss: 0.0447\n",
      "Epoch 729/3000\n",
      "2370/2370 [==============================] - 0s 66us/step - loss: 0.0340 - val_loss: 0.0564\n",
      "Epoch 730/3000\n",
      "2370/2370 [==============================] - 0s 62us/step - loss: 0.0362 - val_loss: 0.0349\n",
      "Epoch 731/3000\n",
      "2370/2370 [==============================] - 0s 67us/step - loss: 0.0420 - val_loss: 0.0416\n",
      "Epoch 732/3000\n",
      "2370/2370 [==============================] - 0s 60us/step - loss: 0.0358 - val_loss: 0.0518\n",
      "Epoch 733/3000\n",
      "2370/2370 [==============================] - 0s 68us/step - loss: 0.0377 - val_loss: 0.0433\n",
      "Epoch 734/3000\n",
      "2370/2370 [==============================] - 0s 65us/step - loss: 0.0394 - val_loss: 0.0451\n",
      "Epoch 735/3000\n",
      "2370/2370 [==============================] - 0s 53us/step - loss: 0.0379 - val_loss: 0.0415\n",
      "Epoch 736/3000\n",
      "2370/2370 [==============================] - 0s 62us/step - loss: 0.0380 - val_loss: 0.0429\n",
      "Epoch 737/3000\n",
      "2370/2370 [==============================] - 0s 65us/step - loss: 0.0388 - val_loss: 0.0399\n",
      "Epoch 738/3000\n",
      "2370/2370 [==============================] - 0s 59us/step - loss: 0.0371 - val_loss: 0.0364\n",
      "Epoch 739/3000\n",
      "2370/2370 [==============================] - 0s 55us/step - loss: 0.0382 - val_loss: 0.0559\n",
      "Epoch 740/3000\n",
      "2370/2370 [==============================] - 0s 64us/step - loss: 0.0381 - val_loss: 0.0623\n",
      "Epoch 741/3000\n",
      "2370/2370 [==============================] - 0s 62us/step - loss: 0.0380 - val_loss: 0.0472\n",
      "Epoch 742/3000\n",
      "2370/2370 [==============================] - 0s 63us/step - loss: 0.0370 - val_loss: 0.0373\n",
      "Epoch 743/3000\n",
      "2370/2370 [==============================] - 0s 62us/step - loss: 0.0372 - val_loss: 0.0397\n",
      "Epoch 744/3000\n",
      "2370/2370 [==============================] - 0s 61us/step - loss: 0.0388 - val_loss: 0.0515\n",
      "Epoch 745/3000\n",
      "2370/2370 [==============================] - 0s 57us/step - loss: 0.0366 - val_loss: 0.0400\n",
      "Epoch 746/3000\n",
      "2370/2370 [==============================] - 0s 59us/step - loss: 0.0385 - val_loss: 0.0338\n",
      "Epoch 747/3000\n",
      "2370/2370 [==============================] - 0s 64us/step - loss: 0.0408 - val_loss: 0.0391\n",
      "Epoch 748/3000\n",
      "2370/2370 [==============================] - 0s 62us/step - loss: 0.0374 - val_loss: 0.0386\n",
      "Epoch 749/3000\n",
      "2370/2370 [==============================] - 0s 69us/step - loss: 0.0444 - val_loss: 0.0528\n",
      "Epoch 750/3000\n",
      "2370/2370 [==============================] - 0s 63us/step - loss: 0.0367 - val_loss: 0.0419\n",
      "Epoch 751/3000\n",
      "2370/2370 [==============================] - 0s 65us/step - loss: 0.0347 - val_loss: 0.0385\n",
      "Epoch 752/3000\n",
      "2370/2370 [==============================] - 0s 60us/step - loss: 0.0393 - val_loss: 0.0469\n",
      "Epoch 753/3000\n",
      "2370/2370 [==============================] - 0s 62us/step - loss: 0.0372 - val_loss: 0.0403\n",
      "Epoch 754/3000\n",
      "2370/2370 [==============================] - 0s 62us/step - loss: 0.0368 - val_loss: 0.0434\n",
      "Epoch 755/3000\n",
      "2370/2370 [==============================] - 0s 60us/step - loss: 0.0360 - val_loss: 0.0435\n",
      "Epoch 756/3000\n",
      "2370/2370 [==============================] - 0s 58us/step - loss: 0.0377 - val_loss: 0.0353\n",
      "Epoch 757/3000\n",
      "2370/2370 [==============================] - 0s 65us/step - loss: 0.0368 - val_loss: 0.0476\n",
      "Epoch 758/3000\n",
      "2370/2370 [==============================] - 0s 64us/step - loss: 0.0388 - val_loss: 0.0505\n",
      "Epoch 759/3000\n",
      "2370/2370 [==============================] - 0s 61us/step - loss: 0.0386 - val_loss: 0.0390\n",
      "Epoch 760/3000\n",
      "2370/2370 [==============================] - 0s 68us/step - loss: 0.0390 - val_loss: 0.0327\n",
      "Epoch 761/3000\n",
      "2370/2370 [==============================] - 0s 63us/step - loss: 0.0408 - val_loss: 0.0374\n",
      "Epoch 762/3000\n",
      "2370/2370 [==============================] - 0s 72us/step - loss: 0.0385 - val_loss: 0.0334\n",
      "Epoch 763/3000\n",
      "2370/2370 [==============================] - 0s 74us/step - loss: 0.0352 - val_loss: 0.0341\n",
      "Epoch 764/3000\n",
      "2370/2370 [==============================] - 0s 77us/step - loss: 0.0410 - val_loss: 0.0484\n",
      "Epoch 765/3000\n",
      "2370/2370 [==============================] - 0s 69us/step - loss: 0.0392 - val_loss: 0.0445\n",
      "Epoch 766/3000\n",
      "2370/2370 [==============================] - 0s 55us/step - loss: 0.0352 - val_loss: 0.0338\n",
      "Epoch 767/3000\n",
      "2370/2370 [==============================] - 0s 71us/step - loss: 0.0385 - val_loss: 0.0333\n",
      "Epoch 768/3000\n",
      "2370/2370 [==============================] - 0s 66us/step - loss: 0.0379 - val_loss: 0.0471\n",
      "Epoch 769/3000\n",
      "2370/2370 [==============================] - 0s 63us/step - loss: 0.0354 - val_loss: 0.0383\n",
      "Epoch 770/3000\n",
      "2370/2370 [==============================] - 0s 65us/step - loss: 0.0400 - val_loss: 0.0337\n",
      "Epoch 771/3000\n",
      "2370/2370 [==============================] - 0s 62us/step - loss: 0.0361 - val_loss: 0.0347\n",
      "Epoch 772/3000\n",
      "2370/2370 [==============================] - 0s 60us/step - loss: 0.0374 - val_loss: 0.0435\n",
      "Epoch 773/3000\n",
      "2370/2370 [==============================] - 0s 71us/step - loss: 0.0377 - val_loss: 0.0401\n",
      "Epoch 774/3000\n",
      "2370/2370 [==============================] - 0s 74us/step - loss: 0.0409 - val_loss: 0.0340\n",
      "Epoch 775/3000\n",
      "2370/2370 [==============================] - 0s 58us/step - loss: 0.0380 - val_loss: 0.0346\n",
      "Epoch 776/3000\n",
      "2370/2370 [==============================] - 0s 62us/step - loss: 0.0398 - val_loss: 0.0392\n",
      "Epoch 777/3000\n",
      "2370/2370 [==============================] - 0s 62us/step - loss: 0.0351 - val_loss: 0.0388\n",
      "Epoch 778/3000\n",
      "2370/2370 [==============================] - 0s 66us/step - loss: 0.0375 - val_loss: 0.0471\n",
      "Epoch 779/3000\n",
      "2370/2370 [==============================] - 0s 58us/step - loss: 0.0401 - val_loss: 0.0505\n",
      "Epoch 780/3000\n",
      "2370/2370 [==============================] - 0s 63us/step - loss: 0.0351 - val_loss: 0.0508\n",
      "Epoch 781/3000\n",
      "2370/2370 [==============================] - 0s 62us/step - loss: 0.0400 - val_loss: 0.0422\n",
      "Epoch 782/3000\n",
      "2370/2370 [==============================] - 0s 59us/step - loss: 0.0366 - val_loss: 0.0382\n",
      "Epoch 783/3000\n",
      "2370/2370 [==============================] - 0s 56us/step - loss: 0.0388 - val_loss: 0.0520\n",
      "Epoch 784/3000\n",
      "2370/2370 [==============================] - 0s 62us/step - loss: 0.0387 - val_loss: 0.0356\n",
      "Epoch 785/3000\n",
      "2370/2370 [==============================] - 0s 61us/step - loss: 0.0376 - val_loss: 0.0420\n",
      "Epoch 786/3000\n",
      "2370/2370 [==============================] - 0s 65us/step - loss: 0.0401 - val_loss: 0.0413\n",
      "Epoch 787/3000\n",
      "2370/2370 [==============================] - 0s 63us/step - loss: 0.0373 - val_loss: 0.0453\n",
      "Epoch 788/3000\n",
      "2370/2370 [==============================] - 0s 62us/step - loss: 0.0349 - val_loss: 0.0404\n",
      "Epoch 789/3000\n",
      "2370/2370 [==============================] - 0s 61us/step - loss: 0.0401 - val_loss: 0.0354\n",
      "Epoch 790/3000\n",
      "2370/2370 [==============================] - 0s 62us/step - loss: 0.0392 - val_loss: 0.0356\n",
      "Epoch 791/3000\n",
      "2370/2370 [==============================] - 0s 61us/step - loss: 0.0356 - val_loss: 0.0333\n",
      "Epoch 792/3000\n",
      "2370/2370 [==============================] - 0s 63us/step - loss: 0.0355 - val_loss: 0.0413\n",
      "Epoch 793/3000\n",
      "2370/2370 [==============================] - 0s 60us/step - loss: 0.0394 - val_loss: 0.0330\n",
      "Epoch 794/3000\n",
      "2370/2370 [==============================] - 0s 66us/step - loss: 0.0356 - val_loss: 0.0371\n",
      "Epoch 795/3000\n",
      "2370/2370 [==============================] - 0s 60us/step - loss: 0.0411 - val_loss: 0.0352\n",
      "Epoch 796/3000\n",
      "2370/2370 [==============================] - 0s 70us/step - loss: 0.0367 - val_loss: 0.0344\n",
      "Epoch 797/3000\n",
      "2370/2370 [==============================] - 0s 56us/step - loss: 0.0396 - val_loss: 0.0341\n",
      "Epoch 798/3000\n",
      "2370/2370 [==============================] - 0s 61us/step - loss: 0.0384 - val_loss: 0.0377\n",
      "Epoch 799/3000\n",
      "2370/2370 [==============================] - 0s 64us/step - loss: 0.0371 - val_loss: 0.0397\n",
      "Epoch 800/3000\n",
      "2370/2370 [==============================] - 0s 62us/step - loss: 0.0361 - val_loss: 0.0547\n",
      "Epoch 801/3000\n",
      "2370/2370 [==============================] - 0s 59us/step - loss: 0.0414 - val_loss: 0.0445\n",
      "Epoch 802/3000\n",
      "2370/2370 [==============================] - 0s 54us/step - loss: 0.0364 - val_loss: 0.0491\n",
      "Epoch 803/3000\n",
      "2370/2370 [==============================] - 0s 56us/step - loss: 0.0384 - val_loss: 0.0482\n",
      "Epoch 804/3000\n",
      "2370/2370 [==============================] - 0s 63us/step - loss: 0.0396 - val_loss: 0.0415\n",
      "Epoch 805/3000\n",
      "2370/2370 [==============================] - 0s 63us/step - loss: 0.0415 - val_loss: 0.0357\n",
      "Epoch 806/3000\n",
      "2370/2370 [==============================] - 0s 62us/step - loss: 0.0360 - val_loss: 0.0451\n",
      "Epoch 807/3000\n",
      "2370/2370 [==============================] - 0s 64us/step - loss: 0.0354 - val_loss: 0.0558\n",
      "Epoch 808/3000\n",
      "2370/2370 [==============================] - 0s 62us/step - loss: 0.0376 - val_loss: 0.0361\n",
      "Epoch 809/3000\n",
      "2370/2370 [==============================] - 0s 63us/step - loss: 0.0360 - val_loss: 0.0372\n",
      "Epoch 810/3000\n",
      "2370/2370 [==============================] - 0s 68us/step - loss: 0.0351 - val_loss: 0.0471\n",
      "Epoch 811/3000\n",
      "2370/2370 [==============================] - 0s 57us/step - loss: 0.0350 - val_loss: 0.0408\n",
      "Epoch 812/3000\n",
      "2370/2370 [==============================] - 0s 59us/step - loss: 0.0346 - val_loss: 0.0446\n",
      "Epoch 813/3000\n",
      "2370/2370 [==============================] - 0s 65us/step - loss: 0.0349 - val_loss: 0.0401\n",
      "Epoch 814/3000\n",
      "2370/2370 [==============================] - 0s 60us/step - loss: 0.0393 - val_loss: 0.0501\n",
      "Epoch 815/3000\n",
      "2370/2370 [==============================] - 0s 58us/step - loss: 0.0368 - val_loss: 0.0449\n",
      "Epoch 816/3000\n",
      "2370/2370 [==============================] - 0s 64us/step - loss: 0.0385 - val_loss: 0.0422\n",
      "Epoch 817/3000\n",
      "2370/2370 [==============================] - 0s 60us/step - loss: 0.0371 - val_loss: 0.0411\n",
      "Epoch 818/3000\n",
      "2370/2370 [==============================] - 0s 68us/step - loss: 0.0423 - val_loss: 0.0424\n",
      "Epoch 819/3000\n",
      "2370/2370 [==============================] - 0s 57us/step - loss: 0.0390 - val_loss: 0.0346\n",
      "Epoch 820/3000\n",
      "2370/2370 [==============================] - 0s 67us/step - loss: 0.0407 - val_loss: 0.0449\n",
      "Epoch 821/3000\n",
      "2370/2370 [==============================] - 0s 59us/step - loss: 0.0396 - val_loss: 0.0332\n",
      "Epoch 822/3000\n",
      "2370/2370 [==============================] - 0s 67us/step - loss: 0.0400 - val_loss: 0.0363\n",
      "Epoch 823/3000\n",
      "2370/2370 [==============================] - 0s 58us/step - loss: 0.0391 - val_loss: 0.0415\n",
      "Epoch 824/3000\n",
      "2370/2370 [==============================] - 0s 67us/step - loss: 0.0372 - val_loss: 0.0372\n",
      "Epoch 825/3000\n",
      "2370/2370 [==============================] - 0s 58us/step - loss: 0.0387 - val_loss: 0.0335\n",
      "Epoch 826/3000\n",
      "2370/2370 [==============================] - 0s 57us/step - loss: 0.0389 - val_loss: 0.0319\n",
      "Epoch 827/3000\n",
      "2370/2370 [==============================] - 0s 63us/step - loss: 0.0378 - val_loss: 0.0458\n",
      "Epoch 828/3000\n",
      "2370/2370 [==============================] - 0s 67us/step - loss: 0.0361 - val_loss: 0.0480\n",
      "Epoch 829/3000\n",
      "2370/2370 [==============================] - 0s 58us/step - loss: 0.0364 - val_loss: 0.0345\n",
      "Epoch 830/3000\n",
      "2370/2370 [==============================] - 0s 64us/step - loss: 0.0374 - val_loss: 0.0351\n",
      "Epoch 831/3000\n",
      "2370/2370 [==============================] - 0s 63us/step - loss: 0.0367 - val_loss: 0.0409\n",
      "Epoch 832/3000\n",
      "2370/2370 [==============================] - 0s 64us/step - loss: 0.0355 - val_loss: 0.0422\n",
      "Epoch 833/3000\n",
      "2370/2370 [==============================] - 0s 56us/step - loss: 0.0382 - val_loss: 0.0333\n",
      "Epoch 834/3000\n",
      "2370/2370 [==============================] - 0s 63us/step - loss: 0.0366 - val_loss: 0.0411\n",
      "Epoch 835/3000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2370/2370 [==============================] - 0s 65us/step - loss: 0.0366 - val_loss: 0.0367\n",
      "Epoch 836/3000\n",
      "2370/2370 [==============================] - 0s 61us/step - loss: 0.0368 - val_loss: 0.0337\n",
      "Epoch 837/3000\n",
      "2370/2370 [==============================] - 0s 60us/step - loss: 0.0389 - val_loss: 0.0474\n",
      "Epoch 838/3000\n",
      "2370/2370 [==============================] - 0s 63us/step - loss: 0.0401 - val_loss: 0.0438\n",
      "Epoch 839/3000\n",
      "2370/2370 [==============================] - 0s 63us/step - loss: 0.0385 - val_loss: 0.0418\n",
      "Epoch 840/3000\n",
      "2370/2370 [==============================] - 0s 60us/step - loss: 0.0392 - val_loss: 0.0344\n",
      "Epoch 841/3000\n",
      "2370/2370 [==============================] - 0s 63us/step - loss: 0.0383 - val_loss: 0.0618\n",
      "Epoch 842/3000\n",
      "2370/2370 [==============================] - 0s 59us/step - loss: 0.0379 - val_loss: 0.0377\n",
      "Epoch 843/3000\n",
      "2370/2370 [==============================] - 0s 57us/step - loss: 0.0398 - val_loss: 0.0407\n",
      "Epoch 844/3000\n",
      "2370/2370 [==============================] - 0s 60us/step - loss: 0.0356 - val_loss: 0.0471\n",
      "Epoch 845/3000\n",
      "2370/2370 [==============================] - 0s 62us/step - loss: 0.0363 - val_loss: 0.0362\n",
      "Epoch 846/3000\n",
      "2370/2370 [==============================] - 0s 63us/step - loss: 0.0396 - val_loss: 0.0327\n",
      "Epoch 847/3000\n",
      "2370/2370 [==============================] - 0s 62us/step - loss: 0.0395 - val_loss: 0.0370\n",
      "Epoch 848/3000\n",
      "2370/2370 [==============================] - 0s 62us/step - loss: 0.0375 - val_loss: 0.0390\n",
      "Epoch 849/3000\n",
      "2370/2370 [==============================] - 0s 62us/step - loss: 0.0348 - val_loss: 0.0367\n",
      "Epoch 850/3000\n",
      "2370/2370 [==============================] - 0s 62us/step - loss: 0.0376 - val_loss: 0.0365\n",
      "Epoch 851/3000\n",
      "2370/2370 [==============================] - 0s 61us/step - loss: 0.0375 - val_loss: 0.0478\n",
      "Epoch 852/3000\n",
      "2370/2370 [==============================] - 0s 52us/step - loss: 0.0389 - val_loss: 0.0364\n",
      "Epoch 853/3000\n",
      "2370/2370 [==============================] - 0s 60us/step - loss: 0.0378 - val_loss: 0.0486\n",
      "Epoch 854/3000\n",
      "2370/2370 [==============================] - 0s 59us/step - loss: 0.0399 - val_loss: 0.0327\n",
      "Epoch 855/3000\n",
      "2370/2370 [==============================] - 0s 62us/step - loss: 0.0371 - val_loss: 0.0576\n",
      "Epoch 856/3000\n",
      "2370/2370 [==============================] - 0s 61us/step - loss: 0.0342 - val_loss: 0.0540\n",
      "Epoch 857/3000\n",
      "2370/2370 [==============================] - 0s 66us/step - loss: 0.0383 - val_loss: 0.0328\n",
      "Epoch 858/3000\n",
      "2370/2370 [==============================] - 0s 54us/step - loss: 0.0360 - val_loss: 0.0584\n",
      "Epoch 859/3000\n",
      "2370/2370 [==============================] - 0s 62us/step - loss: 0.0369 - val_loss: 0.0420\n",
      "Epoch 860/3000\n",
      "2370/2370 [==============================] - 0s 55us/step - loss: 0.0408 - val_loss: 0.0511\n",
      "Epoch 861/3000\n",
      "2370/2370 [==============================] - 0s 64us/step - loss: 0.0372 - val_loss: 0.0331\n",
      "Epoch 862/3000\n",
      "2370/2370 [==============================] - 0s 63us/step - loss: 0.0404 - val_loss: 0.0443\n",
      "Epoch 863/3000\n",
      "2370/2370 [==============================] - 0s 59us/step - loss: 0.0381 - val_loss: 0.0466\n",
      "Epoch 864/3000\n",
      "2370/2370 [==============================] - 0s 61us/step - loss: 0.0380 - val_loss: 0.0592\n",
      "Epoch 865/3000\n",
      "2370/2370 [==============================] - 0s 56us/step - loss: 0.0392 - val_loss: 0.0353\n",
      "Epoch 866/3000\n",
      "2370/2370 [==============================] - 0s 63us/step - loss: 0.0378 - val_loss: 0.0360\n",
      "Epoch 867/3000\n",
      "2370/2370 [==============================] - 0s 61us/step - loss: 0.0365 - val_loss: 0.0396\n",
      "Epoch 868/3000\n",
      "2370/2370 [==============================] - 0s 58us/step - loss: 0.0373 - val_loss: 0.0399\n",
      "Epoch 869/3000\n",
      "2370/2370 [==============================] - 0s 58us/step - loss: 0.0383 - val_loss: 0.0438\n",
      "Epoch 870/3000\n",
      "2370/2370 [==============================] - 0s 56us/step - loss: 0.0380 - val_loss: 0.0386\n",
      "Epoch 871/3000\n",
      "2370/2370 [==============================] - 0s 63us/step - loss: 0.0382 - val_loss: 0.0463\n",
      "Epoch 872/3000\n",
      "2370/2370 [==============================] - 0s 61us/step - loss: 0.0354 - val_loss: 0.0327\n",
      "Epoch 873/3000\n",
      "2370/2370 [==============================] - 0s 63us/step - loss: 0.0371 - val_loss: 0.0524\n",
      "Epoch 874/3000\n",
      "2370/2370 [==============================] - 0s 63us/step - loss: 0.0346 - val_loss: 0.0352\n",
      "Epoch 875/3000\n",
      "2370/2370 [==============================] - 0s 63us/step - loss: 0.0399 - val_loss: 0.0465\n",
      "Epoch 876/3000\n",
      "2370/2370 [==============================] - 0s 63us/step - loss: 0.0383 - val_loss: 0.0324\n",
      "Epoch 877/3000\n",
      "2370/2370 [==============================] - 0s 56us/step - loss: 0.0370 - val_loss: 0.0353\n",
      "Epoch 878/3000\n",
      "2370/2370 [==============================] - 0s 59us/step - loss: 0.0388 - val_loss: 0.0328\n",
      "Epoch 879/3000\n",
      "2370/2370 [==============================] - 0s 59us/step - loss: 0.0390 - val_loss: 0.0377\n",
      "Epoch 880/3000\n",
      "2370/2370 [==============================] - 0s 61us/step - loss: 0.0443 - val_loss: 0.0367\n",
      "Epoch 881/3000\n",
      "2370/2370 [==============================] - 0s 65us/step - loss: 0.0386 - val_loss: 0.0366\n",
      "Epoch 882/3000\n",
      "2370/2370 [==============================] - 0s 63us/step - loss: 0.0402 - val_loss: 0.0414\n",
      "Epoch 883/3000\n",
      "2370/2370 [==============================] - 0s 65us/step - loss: 0.0331 - val_loss: 0.0429\n",
      "Epoch 884/3000\n",
      "2370/2370 [==============================] - 0s 67us/step - loss: 0.0364 - val_loss: 0.0330\n",
      "Epoch 885/3000\n",
      "2370/2370 [==============================] - 0s 63us/step - loss: 0.0397 - val_loss: 0.0359\n",
      "Epoch 886/3000\n",
      "2370/2370 [==============================] - 0s 61us/step - loss: 0.0392 - val_loss: 0.0598\n",
      "Epoch 887/3000\n",
      "2370/2370 [==============================] - 0s 62us/step - loss: 0.0389 - val_loss: 0.0352\n",
      "Epoch 888/3000\n",
      "2370/2370 [==============================] - 0s 63us/step - loss: 0.0339 - val_loss: 0.0359\n",
      "Epoch 889/3000\n",
      "2370/2370 [==============================] - 0s 52us/step - loss: 0.0423 - val_loss: 0.0360\n",
      "Epoch 890/3000\n",
      "2370/2370 [==============================] - 0s 63us/step - loss: 0.0352 - val_loss: 0.0532\n",
      "Epoch 891/3000\n",
      "2370/2370 [==============================] - 0s 65us/step - loss: 0.0353 - val_loss: 0.0494\n",
      "Epoch 892/3000\n",
      "2370/2370 [==============================] - 0s 55us/step - loss: 0.0415 - val_loss: 0.0396\n",
      "Epoch 893/3000\n",
      "2370/2370 [==============================] - 0s 62us/step - loss: 0.0391 - val_loss: 0.0388\n",
      "Epoch 894/3000\n",
      "2370/2370 [==============================] - 0s 63us/step - loss: 0.0364 - val_loss: 0.0395\n",
      "Epoch 895/3000\n",
      "2370/2370 [==============================] - 0s 63us/step - loss: 0.0347 - val_loss: 0.0400\n",
      "Epoch 896/3000\n",
      "2370/2370 [==============================] - 0s 61us/step - loss: 0.0346 - val_loss: 0.0567\n",
      "Epoch 897/3000\n",
      "2370/2370 [==============================] - 0s 59us/step - loss: 0.0366 - val_loss: 0.0431\n",
      "Epoch 898/3000\n",
      "2370/2370 [==============================] - 0s 64us/step - loss: 0.0391 - val_loss: 0.0422\n",
      "Epoch 899/3000\n",
      "2370/2370 [==============================] - 0s 64us/step - loss: 0.0387 - val_loss: 0.0328\n",
      "Epoch 900/3000\n",
      "2370/2370 [==============================] - 0s 64us/step - loss: 0.0405 - val_loss: 0.0362\n",
      "Epoch 901/3000\n",
      "2370/2370 [==============================] - 0s 63us/step - loss: 0.0402 - val_loss: 0.0433\n",
      "Epoch 902/3000\n",
      "2370/2370 [==============================] - 0s 66us/step - loss: 0.0373 - val_loss: 0.0379\n",
      "Epoch 903/3000\n",
      "2370/2370 [==============================] - 0s 69us/step - loss: 0.0341 - val_loss: 0.0352\n",
      "Epoch 904/3000\n",
      "2370/2370 [==============================] - 0s 55us/step - loss: 0.0360 - val_loss: 0.0493\n",
      "Epoch 905/3000\n",
      "2370/2370 [==============================] - 0s 62us/step - loss: 0.0357 - val_loss: 0.0404\n",
      "Epoch 906/3000\n",
      "2370/2370 [==============================] - 0s 56us/step - loss: 0.0362 - val_loss: 0.0355\n",
      "Epoch 907/3000\n",
      "2370/2370 [==============================] - 0s 63us/step - loss: 0.0392 - val_loss: 0.0440\n",
      "Epoch 908/3000\n",
      "2370/2370 [==============================] - 0s 63us/step - loss: 0.0406 - val_loss: 0.0453\n",
      "Epoch 909/3000\n",
      "2370/2370 [==============================] - 0s 56us/step - loss: 0.0366 - val_loss: 0.0410\n",
      "Epoch 910/3000\n",
      "2370/2370 [==============================] - 0s 62us/step - loss: 0.0385 - val_loss: 0.0334\n",
      "Epoch 911/3000\n",
      "2370/2370 [==============================] - 0s 58us/step - loss: 0.0361 - val_loss: 0.0380\n",
      "Epoch 912/3000\n",
      "2370/2370 [==============================] - 0s 63us/step - loss: 0.0353 - val_loss: 0.0412\n",
      "Epoch 913/3000\n",
      "2370/2370 [==============================] - 0s 60us/step - loss: 0.0397 - val_loss: 0.0564\n",
      "Epoch 914/3000\n",
      "2370/2370 [==============================] - 0s 56us/step - loss: 0.0374 - val_loss: 0.0527\n",
      "Epoch 915/3000\n",
      "2370/2370 [==============================] - 0s 55us/step - loss: 0.0389 - val_loss: 0.0330\n",
      "Epoch 916/3000\n",
      "2370/2370 [==============================] - 0s 68us/step - loss: 0.0334 - val_loss: 0.0411\n",
      "Epoch 917/3000\n",
      "2370/2370 [==============================] - 0s 58us/step - loss: 0.0398 - val_loss: 0.0353\n",
      "Epoch 918/3000\n",
      "2370/2370 [==============================] - 0s 68us/step - loss: 0.0389 - val_loss: 0.0435\n",
      "Epoch 919/3000\n",
      "2370/2370 [==============================] - 0s 52us/step - loss: 0.0397 - val_loss: 0.0360\n",
      "Epoch 920/3000\n",
      "2370/2370 [==============================] - 0s 64us/step - loss: 0.0381 - val_loss: 0.0408\n",
      "Epoch 921/3000\n",
      "2370/2370 [==============================] - 0s 63us/step - loss: 0.0379 - val_loss: 0.0351\n",
      "Epoch 922/3000\n",
      "2370/2370 [==============================] - 0s 65us/step - loss: 0.0407 - val_loss: 0.0400\n",
      "Epoch 923/3000\n",
      "2370/2370 [==============================] - 0s 73us/step - loss: 0.0392 - val_loss: 0.0418\n",
      "Epoch 924/3000\n",
      "2370/2370 [==============================] - 0s 55us/step - loss: 0.0382 - val_loss: 0.0566\n",
      "Epoch 925/3000\n",
      "2370/2370 [==============================] - 0s 65us/step - loss: 0.0348 - val_loss: 0.0702\n",
      "Epoch 926/3000\n",
      "2370/2370 [==============================] - 0s 61us/step - loss: 0.0372 - val_loss: 0.0547\n",
      "Epoch 927/3000\n",
      "2370/2370 [==============================] - 0s 55us/step - loss: 0.0384 - val_loss: 0.0352\n",
      "Epoch 928/3000\n",
      "2370/2370 [==============================] - 0s 64us/step - loss: 0.0390 - val_loss: 0.0358\n",
      "Epoch 929/3000\n",
      "2370/2370 [==============================] - 0s 66us/step - loss: 0.0372 - val_loss: 0.0384\n",
      "Epoch 930/3000\n",
      "2370/2370 [==============================] - 0s 47us/step - loss: 0.0429 - val_loss: 0.0335\n",
      "Epoch 931/3000\n",
      "2370/2370 [==============================] - 0s 60us/step - loss: 0.0387 - val_loss: 0.0336\n",
      "Epoch 932/3000\n",
      "2370/2370 [==============================] - 0s 70us/step - loss: 0.0358 - val_loss: 0.0447\n",
      "Epoch 933/3000\n",
      "2370/2370 [==============================] - 0s 57us/step - loss: 0.0368 - val_loss: 0.0453\n",
      "Epoch 934/3000\n",
      "2370/2370 [==============================] - 0s 67us/step - loss: 0.0380 - val_loss: 0.0462\n",
      "Epoch 935/3000\n",
      "2370/2370 [==============================] - 0s 57us/step - loss: 0.0375 - val_loss: 0.0365\n",
      "Epoch 936/3000\n",
      "2370/2370 [==============================] - 0s 63us/step - loss: 0.0331 - val_loss: 0.0509\n",
      "Epoch 937/3000\n",
      "2370/2370 [==============================] - 0s 63us/step - loss: 0.0379 - val_loss: 0.0403\n",
      "Epoch 938/3000\n",
      "2370/2370 [==============================] - 0s 63us/step - loss: 0.0385 - val_loss: 0.0338\n",
      "Epoch 939/3000\n",
      "2370/2370 [==============================] - 0s 65us/step - loss: 0.0413 - val_loss: 0.0360\n",
      "Epoch 940/3000\n",
      "2370/2370 [==============================] - 0s 70us/step - loss: 0.0364 - val_loss: 0.0411\n",
      "Epoch 941/3000\n",
      "2370/2370 [==============================] - 0s 71us/step - loss: 0.0342 - val_loss: 0.0377\n",
      "Epoch 942/3000\n",
      "2370/2370 [==============================] - 0s 42us/step - loss: 0.0370 - val_loss: 0.0330\n",
      "Epoch 943/3000\n",
      "2370/2370 [==============================] - 0s 43us/step - loss: 0.0411 - val_loss: 0.0339\n",
      "Epoch 944/3000\n",
      "2370/2370 [==============================] - 0s 45us/step - loss: 0.0378 - val_loss: 0.0699\n",
      "Epoch 945/3000\n",
      "2370/2370 [==============================] - 0s 36us/step - loss: 0.0360 - val_loss: 0.0529\n",
      "Epoch 946/3000\n",
      "2370/2370 [==============================] - 0s 33us/step - loss: 0.0351 - val_loss: 0.0400\n",
      "Epoch 947/3000\n",
      "2370/2370 [==============================] - 0s 33us/step - loss: 0.0366 - val_loss: 0.0330\n",
      "Epoch 948/3000\n",
      "2370/2370 [==============================] - 0s 33us/step - loss: 0.0373 - val_loss: 0.0319\n",
      "Epoch 949/3000\n",
      "2370/2370 [==============================] - 0s 32us/step - loss: 0.0380 - val_loss: 0.0332\n",
      "Epoch 950/3000\n",
      "2370/2370 [==============================] - 0s 34us/step - loss: 0.0335 - val_loss: 0.0369\n",
      "Epoch 951/3000\n",
      "2370/2370 [==============================] - 0s 35us/step - loss: 0.0379 - val_loss: 0.0337\n",
      "Epoch 952/3000\n",
      "2370/2370 [==============================] - 0s 40us/step - loss: 0.0403 - val_loss: 0.0338\n",
      "Epoch 953/3000\n",
      "2370/2370 [==============================] - 0s 33us/step - loss: 0.0374 - val_loss: 0.0422\n",
      "Epoch 954/3000\n",
      "2370/2370 [==============================] - 0s 32us/step - loss: 0.0395 - val_loss: 0.0588\n",
      "Epoch 955/3000\n",
      "2370/2370 [==============================] - 0s 39us/step - loss: 0.0396 - val_loss: 0.0434\n",
      "Epoch 956/3000\n",
      "2370/2370 [==============================] - 0s 91us/step - loss: 0.0341 - val_loss: 0.0405\n",
      "Epoch 957/3000\n",
      "2370/2370 [==============================] - 0s 40us/step - loss: 0.0379 - val_loss: 0.0361\n",
      "Epoch 958/3000\n",
      "2370/2370 [==============================] - 0s 37us/step - loss: 0.0366 - val_loss: 0.0443\n",
      "Epoch 959/3000\n",
      "2370/2370 [==============================] - 0s 38us/step - loss: 0.0412 - val_loss: 0.0386\n",
      "Epoch 960/3000\n",
      "2370/2370 [==============================] - 0s 38us/step - loss: 0.0363 - val_loss: 0.0595\n",
      "Epoch 961/3000\n",
      "2370/2370 [==============================] - 0s 36us/step - loss: 0.0369 - val_loss: 0.0317\n",
      "Epoch 962/3000\n",
      "2370/2370 [==============================] - 0s 88us/step - loss: 0.0373 - val_loss: 0.0393\n",
      "Epoch 963/3000\n",
      "2370/2370 [==============================] - 0s 134us/step - loss: 0.0395 - val_loss: 0.0601\n",
      "Epoch 964/3000\n",
      "2370/2370 [==============================] - 0s 141us/step - loss: 0.0401 - val_loss: 0.0320\n",
      "Epoch 965/3000\n",
      "2370/2370 [==============================] - 0s 152us/step - loss: 0.0416 - val_loss: 0.0437\n",
      "Epoch 966/3000\n",
      "2370/2370 [==============================] - 0s 113us/step - loss: 0.0374 - val_loss: 0.0350\n",
      "Epoch 967/3000\n",
      "2370/2370 [==============================] - 0s 106us/step - loss: 0.0356 - val_loss: 0.0319\n",
      "Epoch 968/3000\n",
      "2370/2370 [==============================] - 0s 93us/step - loss: 0.0376 - val_loss: 0.0353\n",
      "Epoch 969/3000\n",
      "2370/2370 [==============================] - 0s 103us/step - loss: 0.0387 - val_loss: 0.0390\n",
      "Epoch 970/3000\n",
      "2370/2370 [==============================] - 0s 146us/step - loss: 0.0397 - val_loss: 0.0361\n",
      "Epoch 971/3000\n",
      "2370/2370 [==============================] - 0s 155us/step - loss: 0.0380 - val_loss: 0.0487\n",
      "Epoch 972/3000\n",
      "2370/2370 [==============================] - 0s 155us/step - loss: 0.0362 - val_loss: 0.0326\n",
      "Epoch 973/3000\n",
      "2370/2370 [==============================] - 0s 113us/step - loss: 0.0380 - val_loss: 0.0374\n",
      "Epoch 974/3000\n",
      "2370/2370 [==============================] - 0s 109us/step - loss: 0.0354 - val_loss: 0.0390\n",
      "Epoch 975/3000\n",
      "2370/2370 [==============================] - 0s 167us/step - loss: 0.0390 - val_loss: 0.0379\n",
      "Epoch 976/3000\n",
      "2370/2370 [==============================] - 0s 99us/step - loss: 0.0404 - val_loss: 0.0340\n",
      "Epoch 977/3000\n",
      "2370/2370 [==============================] - 0s 85us/step - loss: 0.0420 - val_loss: 0.0595\n",
      "Epoch 978/3000\n",
      "2370/2370 [==============================] - 0s 55us/step - loss: 0.0399 - val_loss: 0.0344\n",
      "Epoch 979/3000\n",
      "2370/2370 [==============================] - 0s 33us/step - loss: 0.0365 - val_loss: 0.0393\n",
      "Epoch 980/3000\n",
      "2370/2370 [==============================] - 0s 28us/step - loss: 0.0394 - val_loss: 0.0419\n",
      "Epoch 981/3000\n",
      "2370/2370 [==============================] - 0s 31us/step - loss: 0.0418 - val_loss: 0.0481\n",
      "Epoch 982/3000\n",
      "2370/2370 [==============================] - 0s 67us/step - loss: 0.0392 - val_loss: 0.0457\n",
      "Epoch 983/3000\n",
      "2370/2370 [==============================] - 0s 118us/step - loss: 0.0359 - val_loss: 0.0453\n",
      "Epoch 984/3000\n",
      "2370/2370 [==============================] - 0s 103us/step - loss: 0.0393 - val_loss: 0.0385\n",
      "Epoch 985/3000\n",
      "2370/2370 [==============================] - 0s 95us/step - loss: 0.0354 - val_loss: 0.0472\n",
      "Epoch 986/3000\n",
      "2370/2370 [==============================] - 0s 92us/step - loss: 0.0407 - val_loss: 0.0386\n",
      "Epoch 987/3000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2370/2370 [==============================] - 0s 104us/step - loss: 0.0431 - val_loss: 0.0437\n",
      "Epoch 988/3000\n",
      "2370/2370 [==============================] - 0s 80us/step - loss: 0.0359 - val_loss: 0.0415\n",
      "Epoch 989/3000\n",
      "2370/2370 [==============================] - 0s 89us/step - loss: 0.0386 - val_loss: 0.0422\n",
      "Epoch 990/3000\n",
      "2370/2370 [==============================] - 0s 49us/step - loss: 0.0368 - val_loss: 0.0379\n",
      "Epoch 991/3000\n",
      "2370/2370 [==============================] - 0s 34us/step - loss: 0.0392 - val_loss: 0.0404\n",
      "Epoch 992/3000\n",
      "2370/2370 [==============================] - 0s 32us/step - loss: 0.0390 - val_loss: 0.0484\n",
      "Epoch 993/3000\n",
      "2370/2370 [==============================] - 0s 31us/step - loss: 0.0359 - val_loss: 0.0340\n",
      "Epoch 994/3000\n",
      "2370/2370 [==============================] - 0s 33us/step - loss: 0.0401 - val_loss: 0.0416\n",
      "Epoch 995/3000\n",
      "2370/2370 [==============================] - 0s 33us/step - loss: 0.0347 - val_loss: 0.0359\n",
      "Epoch 996/3000\n",
      "2370/2370 [==============================] - 0s 31us/step - loss: 0.0424 - val_loss: 0.0333\n",
      "Epoch 997/3000\n",
      "2370/2370 [==============================] - 0s 42us/step - loss: 0.0348 - val_loss: 0.0448\n",
      "Epoch 998/3000\n",
      "2370/2370 [==============================] - 0s 33us/step - loss: 0.0393 - val_loss: 0.0344\n",
      "Epoch 999/3000\n",
      "2370/2370 [==============================] - 0s 34us/step - loss: 0.0405 - val_loss: 0.0357\n",
      "Epoch 1000/3000\n",
      "2370/2370 [==============================] - 0s 34us/step - loss: 0.0351 - val_loss: 0.0513\n",
      "Epoch 1001/3000\n",
      "2370/2370 [==============================] - 0s 34us/step - loss: 0.0387 - val_loss: 0.0354\n",
      "Epoch 1002/3000\n",
      "2370/2370 [==============================] - 0s 33us/step - loss: 0.0395 - val_loss: 0.0386\n",
      "Epoch 1003/3000\n",
      "2370/2370 [==============================] - 0s 32us/step - loss: 0.0378 - val_loss: 0.0566\n",
      "Epoch 1004/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0390 - val_loss: 0.0392\n",
      "Epoch 1005/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0371 - val_loss: 0.0451\n",
      "Epoch 1006/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0389 - val_loss: 0.0379\n",
      "Epoch 1007/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0363 - val_loss: 0.0473\n",
      "Epoch 1008/3000\n",
      "2370/2370 [==============================] - 0s 30us/step - loss: 0.0405 - val_loss: 0.0356\n",
      "Epoch 1009/3000\n",
      "2370/2370 [==============================] - 0s 39us/step - loss: 0.0374 - val_loss: 0.0522\n",
      "Epoch 1010/3000\n",
      "2370/2370 [==============================] - 0s 37us/step - loss: 0.0374 - val_loss: 0.0418\n",
      "Epoch 1011/3000\n",
      "2370/2370 [==============================] - 0s 33us/step - loss: 0.0388 - val_loss: 0.0606\n",
      "Epoch 1012/3000\n",
      "2370/2370 [==============================] - 0s 33us/step - loss: 0.0387 - val_loss: 0.0348\n",
      "Epoch 1013/3000\n",
      "2370/2370 [==============================] - 0s 32us/step - loss: 0.0393 - val_loss: 0.0509\n",
      "Epoch 1014/3000\n",
      "2370/2370 [==============================] - 0s 32us/step - loss: 0.0346 - val_loss: 0.0370\n",
      "Epoch 1015/3000\n",
      "2370/2370 [==============================] - 0s 33us/step - loss: 0.0370 - val_loss: 0.0459\n",
      "Epoch 1016/3000\n",
      "2370/2370 [==============================] - 0s 36us/step - loss: 0.0363 - val_loss: 0.0398\n",
      "Epoch 1017/3000\n",
      "2370/2370 [==============================] - 0s 34us/step - loss: 0.0398 - val_loss: 0.0333\n",
      "Epoch 1018/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0353 - val_loss: 0.0326\n",
      "Epoch 1019/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0353 - val_loss: 0.0443\n",
      "Epoch 1020/3000\n",
      "2370/2370 [==============================] - 0s 31us/step - loss: 0.0437 - val_loss: 0.0547\n",
      "Epoch 1021/3000\n",
      "2370/2370 [==============================] - 0s 34us/step - loss: 0.0364 - val_loss: 0.0432\n",
      "Epoch 1022/3000\n",
      "2370/2370 [==============================] - 0s 28us/step - loss: 0.0355 - val_loss: 0.0390\n",
      "Epoch 1023/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0369 - val_loss: 0.0415\n",
      "Epoch 1024/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0381 - val_loss: 0.0388\n",
      "Epoch 1025/3000\n",
      "2370/2370 [==============================] - 0s 28us/step - loss: 0.0354 - val_loss: 0.0429\n",
      "Epoch 1026/3000\n",
      "2370/2370 [==============================] - 0s 28us/step - loss: 0.0381 - val_loss: 0.0453\n",
      "Epoch 1027/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0366 - val_loss: 0.0466\n",
      "Epoch 1028/3000\n",
      "2370/2370 [==============================] - 0s 33us/step - loss: 0.0383 - val_loss: 0.0321\n",
      "Epoch 1029/3000\n",
      "2370/2370 [==============================] - 0s 30us/step - loss: 0.0352 - val_loss: 0.0396\n",
      "Epoch 1030/3000\n",
      "2370/2370 [==============================] - 0s 39us/step - loss: 0.0391 - val_loss: 0.0382\n",
      "Epoch 1031/3000\n",
      "2370/2370 [==============================] - 0s 30us/step - loss: 0.0373 - val_loss: 0.0463\n",
      "Epoch 1032/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0370 - val_loss: 0.0407\n",
      "Epoch 1033/3000\n",
      "2370/2370 [==============================] - 0s 31us/step - loss: 0.0367 - val_loss: 0.0397\n",
      "Epoch 1034/3000\n",
      "2370/2370 [==============================] - 0s 30us/step - loss: 0.0353 - val_loss: 0.0355\n",
      "Epoch 1035/3000\n",
      "2370/2370 [==============================] - 0s 34us/step - loss: 0.0378 - val_loss: 0.0667\n",
      "Epoch 1036/3000\n",
      "2370/2370 [==============================] - 0s 30us/step - loss: 0.0382 - val_loss: 0.0543\n",
      "Epoch 1037/3000\n",
      "2370/2370 [==============================] - 0s 30us/step - loss: 0.0365 - val_loss: 0.0495\n",
      "Epoch 1038/3000\n",
      "2370/2370 [==============================] - 0s 30us/step - loss: 0.0383 - val_loss: 0.0457\n",
      "Epoch 1039/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0339 - val_loss: 0.0493\n",
      "Epoch 1040/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0379 - val_loss: 0.0531\n",
      "Epoch 1041/3000\n",
      "2370/2370 [==============================] - 0s 36us/step - loss: 0.0394 - val_loss: 0.0407\n",
      "Epoch 1042/3000\n",
      "2370/2370 [==============================] - 0s 73us/step - loss: 0.0379 - val_loss: 0.0396\n",
      "Epoch 1043/3000\n",
      "2370/2370 [==============================] - 0s 106us/step - loss: 0.0398 - val_loss: 0.0558\n",
      "Epoch 1044/3000\n",
      "2370/2370 [==============================] - 0s 117us/step - loss: 0.0394 - val_loss: 0.0377\n",
      "Epoch 1045/3000\n",
      "2370/2370 [==============================] - 0s 108us/step - loss: 0.0365 - val_loss: 0.0507\n",
      "Epoch 1046/3000\n",
      "2370/2370 [==============================] - 0s 72us/step - loss: 0.0399 - val_loss: 0.0344\n",
      "Epoch 1047/3000\n",
      "2370/2370 [==============================] - 0s 79us/step - loss: 0.0381 - val_loss: 0.0414\n",
      "Epoch 1048/3000\n",
      "2370/2370 [==============================] - 0s 76us/step - loss: 0.0380 - val_loss: 0.0528\n",
      "Epoch 1049/3000\n",
      "2370/2370 [==============================] - 0s 71us/step - loss: 0.0378 - val_loss: 0.0520\n",
      "Epoch 1050/3000\n",
      "2370/2370 [==============================] - 0s 86us/step - loss: 0.0385 - val_loss: 0.0348\n",
      "Epoch 1051/3000\n",
      "2370/2370 [==============================] - 0s 129us/step - loss: 0.0363 - val_loss: 0.0371\n",
      "Epoch 1052/3000\n",
      "2370/2370 [==============================] - 0s 83us/step - loss: 0.0393 - val_loss: 0.0357\n",
      "Epoch 1053/3000\n",
      "2370/2370 [==============================] - 0s 110us/step - loss: 0.0440 - val_loss: 0.0442\n",
      "Epoch 1054/3000\n",
      "2370/2370 [==============================] - 0s 57us/step - loss: 0.0357 - val_loss: 0.0617\n",
      "Epoch 1055/3000\n",
      "2370/2370 [==============================] - 0s 96us/step - loss: 0.0375 - val_loss: 0.0359\n",
      "Epoch 1056/3000\n",
      "2370/2370 [==============================] - 0s 119us/step - loss: 0.0410 - val_loss: 0.0399\n",
      "Epoch 1057/3000\n",
      "2370/2370 [==============================] - 0s 120us/step - loss: 0.0382 - val_loss: 0.0524\n",
      "Epoch 1058/3000\n",
      "2370/2370 [==============================] - 0s 126us/step - loss: 0.0381 - val_loss: 0.0478\n",
      "Epoch 1059/3000\n",
      "2370/2370 [==============================] - 0s 136us/step - loss: 0.0380 - val_loss: 0.0485\n",
      "Epoch 1060/3000\n",
      "2370/2370 [==============================] - 0s 135us/step - loss: 0.0358 - val_loss: 0.0371\n",
      "Epoch 1061/3000\n",
      "2370/2370 [==============================] - 0s 130us/step - loss: 0.0376 - val_loss: 0.0352\n",
      "Epoch 1062/3000\n",
      "2370/2370 [==============================] - 0s 128us/step - loss: 0.0365 - val_loss: 0.0570\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1063/3000\n",
      "2370/2370 [==============================] - 0s 135us/step - loss: 0.0368 - val_loss: 0.0472\n",
      "Epoch 1064/3000\n",
      "2370/2370 [==============================] - 0s 133us/step - loss: 0.0370 - val_loss: 0.0403\n",
      "Epoch 1065/3000\n",
      "2370/2370 [==============================] - 0s 107us/step - loss: 0.0385 - val_loss: 0.0385\n",
      "Epoch 1066/3000\n",
      "2370/2370 [==============================] - 0s 118us/step - loss: 0.0383 - val_loss: 0.0368\n",
      "Epoch 1067/3000\n",
      "2370/2370 [==============================] - 0s 119us/step - loss: 0.0418 - val_loss: 0.0483\n",
      "Epoch 1068/3000\n",
      "2370/2370 [==============================] - 0s 108us/step - loss: 0.0335 - val_loss: 0.0394\n",
      "Epoch 1069/3000\n",
      "2370/2370 [==============================] - 0s 100us/step - loss: 0.0348 - val_loss: 0.0566\n",
      "Epoch 1070/3000\n",
      "2370/2370 [==============================] - 0s 108us/step - loss: 0.0379 - val_loss: 0.0353\n",
      "Epoch 1071/3000\n",
      "2370/2370 [==============================] - 0s 115us/step - loss: 0.0408 - val_loss: 0.0357\n",
      "Epoch 1072/3000\n",
      "2370/2370 [==============================] - 0s 135us/step - loss: 0.0385 - val_loss: 0.0433\n",
      "Epoch 1073/3000\n",
      "2370/2370 [==============================] - 0s 134us/step - loss: 0.0419 - val_loss: 0.0463\n",
      "Epoch 1074/3000\n",
      "2370/2370 [==============================] - 0s 135us/step - loss: 0.0386 - val_loss: 0.0424\n",
      "Epoch 1075/3000\n",
      "2370/2370 [==============================] - 0s 131us/step - loss: 0.0363 - val_loss: 0.0494\n",
      "Epoch 1076/3000\n",
      "2370/2370 [==============================] - 0s 114us/step - loss: 0.0398 - val_loss: 0.0392\n",
      "Epoch 1077/3000\n",
      "2370/2370 [==============================] - 0s 67us/step - loss: 0.0358 - val_loss: 0.0506\n",
      "Epoch 1078/3000\n",
      "2370/2370 [==============================] - 0s 124us/step - loss: 0.0387 - val_loss: 0.0422\n",
      "Epoch 1079/3000\n",
      "2370/2370 [==============================] - 0s 129us/step - loss: 0.0379 - val_loss: 0.0397\n",
      "Epoch 1080/3000\n",
      "2370/2370 [==============================] - 0s 132us/step - loss: 0.0367 - val_loss: 0.0407\n",
      "Epoch 1081/3000\n",
      "2370/2370 [==============================] - 0s 136us/step - loss: 0.0379 - val_loss: 0.0465\n",
      "Epoch 1082/3000\n",
      "2370/2370 [==============================] - 0s 109us/step - loss: 0.0416 - val_loss: 0.0557\n",
      "Epoch 1083/3000\n",
      "2370/2370 [==============================] - 0s 89us/step - loss: 0.0364 - val_loss: 0.0457\n",
      "Epoch 1084/3000\n",
      "2370/2370 [==============================] - 0s 124us/step - loss: 0.0368 - val_loss: 0.0370\n",
      "Epoch 1085/3000\n",
      "2370/2370 [==============================] - 0s 125us/step - loss: 0.0358 - val_loss: 0.0522\n",
      "Epoch 1086/3000\n",
      "2370/2370 [==============================] - 0s 112us/step - loss: 0.0363 - val_loss: 0.0641\n",
      "Epoch 1087/3000\n",
      "2370/2370 [==============================] - 0s 106us/step - loss: 0.0379 - val_loss: 0.0382\n",
      "Epoch 1088/3000\n",
      "2370/2370 [==============================] - 0s 107us/step - loss: 0.0389 - val_loss: 0.0407\n",
      "Epoch 1089/3000\n",
      "2370/2370 [==============================] - 0s 143us/step - loss: 0.0377 - val_loss: 0.0501\n",
      "Epoch 1090/3000\n",
      "2370/2370 [==============================] - 0s 136us/step - loss: 0.0379 - val_loss: 0.0359\n",
      "Epoch 1091/3000\n",
      "2370/2370 [==============================] - 0s 135us/step - loss: 0.0383 - val_loss: 0.0392\n",
      "Epoch 1092/3000\n",
      "2370/2370 [==============================] - 0s 105us/step - loss: 0.0383 - val_loss: 0.0354\n",
      "Epoch 1093/3000\n",
      "2370/2370 [==============================] - 0s 134us/step - loss: 0.0383 - val_loss: 0.0550\n",
      "Epoch 1094/3000\n",
      "2370/2370 [==============================] - 0s 122us/step - loss: 0.0374 - val_loss: 0.0392\n",
      "Epoch 1095/3000\n",
      "2370/2370 [==============================] - 0s 136us/step - loss: 0.0369 - val_loss: 0.0389\n",
      "Epoch 1096/3000\n",
      "2370/2370 [==============================] - 0s 133us/step - loss: 0.0367 - val_loss: 0.0362\n",
      "Epoch 1097/3000\n",
      "2370/2370 [==============================] - 0s 137us/step - loss: 0.0385 - val_loss: 0.0352\n",
      "Epoch 1098/3000\n",
      "2370/2370 [==============================] - 0s 116us/step - loss: 0.0398 - val_loss: 0.0491\n",
      "Epoch 1099/3000\n",
      "2370/2370 [==============================] - 0s 135us/step - loss: 0.0360 - val_loss: 0.0387\n",
      "Epoch 1100/3000\n",
      "2370/2370 [==============================] - 0s 113us/step - loss: 0.0398 - val_loss: 0.0402\n",
      "Epoch 1101/3000\n",
      "2370/2370 [==============================] - 0s 119us/step - loss: 0.0368 - val_loss: 0.0638\n",
      "Epoch 1102/3000\n",
      "2370/2370 [==============================] - 0s 113us/step - loss: 0.0356 - val_loss: 0.0462\n",
      "Epoch 1103/3000\n",
      "2370/2370 [==============================] - 0s 74us/step - loss: 0.0383 - val_loss: 0.0354\n",
      "Epoch 1104/3000\n",
      "2370/2370 [==============================] - 0s 100us/step - loss: 0.0351 - val_loss: 0.0358\n",
      "Epoch 1105/3000\n",
      "2370/2370 [==============================] - 0s 136us/step - loss: 0.0390 - val_loss: 0.0369\n",
      "Epoch 1106/3000\n",
      "2370/2370 [==============================] - 0s 121us/step - loss: 0.0370 - val_loss: 0.0472\n",
      "Epoch 1107/3000\n",
      "2370/2370 [==============================] - 0s 110us/step - loss: 0.0348 - val_loss: 0.0485\n",
      "Epoch 1108/3000\n",
      "2370/2370 [==============================] - 0s 101us/step - loss: 0.0343 - val_loss: 0.0631\n",
      "Epoch 1109/3000\n",
      "2370/2370 [==============================] - 0s 115us/step - loss: 0.0394 - val_loss: 0.0431\n",
      "Epoch 1110/3000\n",
      "2370/2370 [==============================] - 0s 83us/step - loss: 0.0372 - val_loss: 0.0488\n",
      "Epoch 1111/3000\n",
      "2370/2370 [==============================] - 0s 112us/step - loss: 0.0374 - val_loss: 0.0372\n",
      "Epoch 1112/3000\n",
      "2370/2370 [==============================] - 0s 116us/step - loss: 0.0342 - val_loss: 0.0429\n",
      "Epoch 1113/3000\n",
      "2370/2370 [==============================] - 0s 109us/step - loss: 0.0423 - val_loss: 0.0372\n",
      "Epoch 1114/3000\n",
      "2370/2370 [==============================] - 0s 139us/step - loss: 0.0353 - val_loss: 0.0428\n",
      "Epoch 1115/3000\n",
      "2370/2370 [==============================] - 0s 118us/step - loss: 0.0397 - val_loss: 0.0621\n",
      "Epoch 1116/3000\n",
      "2370/2370 [==============================] - 0s 121us/step - loss: 0.0396 - val_loss: 0.0405\n",
      "Epoch 1117/3000\n",
      "2370/2370 [==============================] - 0s 112us/step - loss: 0.0384 - val_loss: 0.0362\n",
      "Epoch 1118/3000\n",
      "2370/2370 [==============================] - 0s 113us/step - loss: 0.0390 - val_loss: 0.0405\n",
      "Epoch 1119/3000\n",
      "2370/2370 [==============================] - 0s 119us/step - loss: 0.0427 - val_loss: 0.0460\n",
      "Epoch 1120/3000\n",
      "2370/2370 [==============================] - 0s 134us/step - loss: 0.0362 - val_loss: 0.0430\n",
      "Epoch 1121/3000\n",
      "2370/2370 [==============================] - 0s 116us/step - loss: 0.0373 - val_loss: 0.0745\n",
      "Epoch 1122/3000\n",
      "2370/2370 [==============================] - 0s 129us/step - loss: 0.0391 - val_loss: 0.0392\n",
      "Epoch 1123/3000\n",
      "2370/2370 [==============================] - 0s 111us/step - loss: 0.0360 - val_loss: 0.0444\n",
      "Epoch 1124/3000\n",
      "2370/2370 [==============================] - 0s 123us/step - loss: 0.0394 - val_loss: 0.0374\n",
      "Epoch 1125/3000\n",
      "2370/2370 [==============================] - 0s 131us/step - loss: 0.0403 - val_loss: 0.0375\n",
      "Epoch 1126/3000\n",
      "2370/2370 [==============================] - 0s 98us/step - loss: 0.0401 - val_loss: 0.0520\n",
      "Epoch 1127/3000\n",
      "2370/2370 [==============================] - 0s 99us/step - loss: 0.0369 - val_loss: 0.0353\n",
      "Epoch 1128/3000\n",
      "2370/2370 [==============================] - 0s 124us/step - loss: 0.0391 - val_loss: 0.0363\n",
      "Epoch 1129/3000\n",
      "2370/2370 [==============================] - 0s 115us/step - loss: 0.0363 - val_loss: 0.0376\n",
      "Epoch 1130/3000\n",
      "2370/2370 [==============================] - 0s 134us/step - loss: 0.0367 - val_loss: 0.0353\n",
      "Epoch 1131/3000\n",
      "2370/2370 [==============================] - 0s 108us/step - loss: 0.0372 - val_loss: 0.0400\n",
      "Epoch 1132/3000\n",
      "2370/2370 [==============================] - 0s 107us/step - loss: 0.0357 - val_loss: 0.0368\n",
      "Epoch 1133/3000\n",
      "2370/2370 [==============================] - 0s 110us/step - loss: 0.0377 - val_loss: 0.0595\n",
      "Epoch 1134/3000\n",
      "2370/2370 [==============================] - 0s 116us/step - loss: 0.0342 - val_loss: 0.0486\n",
      "Epoch 1135/3000\n",
      "2370/2370 [==============================] - 0s 123us/step - loss: 0.0380 - val_loss: 0.0408\n",
      "Epoch 1136/3000\n",
      "2370/2370 [==============================] - 0s 105us/step - loss: 0.0366 - val_loss: 0.0450\n",
      "Epoch 1137/3000\n",
      "2370/2370 [==============================] - 0s 123us/step - loss: 0.0351 - val_loss: 0.0437\n",
      "Epoch 1138/3000\n",
      "2370/2370 [==============================] - 0s 140us/step - loss: 0.0356 - val_loss: 0.0348\n",
      "Epoch 1139/3000\n",
      "2370/2370 [==============================] - 0s 114us/step - loss: 0.0395 - val_loss: 0.0336\n",
      "Epoch 1140/3000\n",
      "2370/2370 [==============================] - 0s 112us/step - loss: 0.0366 - val_loss: 0.0477\n",
      "Epoch 1141/3000\n",
      "2370/2370 [==============================] - 0s 128us/step - loss: 0.0353 - val_loss: 0.0511\n",
      "Epoch 1142/3000\n",
      "2370/2370 [==============================] - 0s 130us/step - loss: 0.0389 - val_loss: 0.0343\n",
      "Epoch 1143/3000\n",
      "2370/2370 [==============================] - 0s 114us/step - loss: 0.0361 - val_loss: 0.0468\n",
      "Epoch 1144/3000\n",
      "2370/2370 [==============================] - 0s 137us/step - loss: 0.0393 - val_loss: 0.0346\n",
      "Epoch 1145/3000\n",
      "2370/2370 [==============================] - 0s 139us/step - loss: 0.0402 - val_loss: 0.0345\n",
      "Epoch 1146/3000\n",
      "2370/2370 [==============================] - 0s 131us/step - loss: 0.0390 - val_loss: 0.0451\n",
      "Epoch 1147/3000\n",
      "2370/2370 [==============================] - 0s 117us/step - loss: 0.0429 - val_loss: 0.0402\n",
      "Epoch 1148/3000\n",
      "2370/2370 [==============================] - 0s 135us/step - loss: 0.0379 - val_loss: 0.0405\n",
      "Epoch 1149/3000\n",
      "2370/2370 [==============================] - 0s 150us/step - loss: 0.0367 - val_loss: 0.0522\n",
      "Epoch 1150/3000\n",
      "2370/2370 [==============================] - 0s 152us/step - loss: 0.0417 - val_loss: 0.0413\n",
      "Epoch 1151/3000\n",
      "2370/2370 [==============================] - 0s 110us/step - loss: 0.0414 - val_loss: 0.0390\n",
      "Epoch 1152/3000\n",
      "2370/2370 [==============================] - 0s 130us/step - loss: 0.0370 - val_loss: 0.0383\n",
      "Epoch 1153/3000\n",
      "2370/2370 [==============================] - 0s 123us/step - loss: 0.0386 - val_loss: 0.0439\n",
      "Epoch 1154/3000\n",
      "2370/2370 [==============================] - 0s 135us/step - loss: 0.0363 - val_loss: 0.0373\n",
      "Epoch 1155/3000\n",
      "2370/2370 [==============================] - 0s 95us/step - loss: 0.0392 - val_loss: 0.0452\n",
      "Epoch 1156/3000\n",
      "2370/2370 [==============================] - 0s 97us/step - loss: 0.0373 - val_loss: 0.0362\n",
      "Epoch 1157/3000\n",
      "2370/2370 [==============================] - 0s 90us/step - loss: 0.0379 - val_loss: 0.0426\n",
      "Epoch 1158/3000\n",
      "2370/2370 [==============================] - 0s 91us/step - loss: 0.0368 - val_loss: 0.0406\n",
      "Epoch 1159/3000\n",
      "2370/2370 [==============================] - 0s 84us/step - loss: 0.0342 - val_loss: 0.0424\n",
      "Epoch 1160/3000\n",
      "2370/2370 [==============================] - 0s 129us/step - loss: 0.0386 - val_loss: 0.0551\n",
      "Epoch 1161/3000\n",
      "2370/2370 [==============================] - 0s 135us/step - loss: 0.0374 - val_loss: 0.0476\n",
      "Epoch 1162/3000\n",
      "2370/2370 [==============================] - 0s 132us/step - loss: 0.0362 - val_loss: 0.0478\n",
      "Epoch 1163/3000\n",
      "2370/2370 [==============================] - 0s 142us/step - loss: 0.0425 - val_loss: 0.0388\n",
      "Epoch 1164/3000\n",
      "2370/2370 [==============================] - 0s 121us/step - loss: 0.0424 - val_loss: 0.0428\n",
      "Epoch 1165/3000\n",
      "2370/2370 [==============================] - 0s 119us/step - loss: 0.0373 - val_loss: 0.0560\n",
      "Epoch 1166/3000\n",
      "2370/2370 [==============================] - 0s 136us/step - loss: 0.0386 - val_loss: 0.0384\n",
      "Epoch 1167/3000\n",
      "2370/2370 [==============================] - 0s 134us/step - loss: 0.0332 - val_loss: 0.0405\n",
      "Epoch 1168/3000\n",
      "2370/2370 [==============================] - 0s 135us/step - loss: 0.0373 - val_loss: 0.0362\n",
      "Epoch 1169/3000\n",
      "2370/2370 [==============================] - 0s 129us/step - loss: 0.0394 - val_loss: 0.0370\n",
      "Epoch 1170/3000\n",
      "2370/2370 [==============================] - 0s 51us/step - loss: 0.0423 - val_loss: 0.0480\n",
      "Epoch 1171/3000\n",
      "2370/2370 [==============================] - 0s 66us/step - loss: 0.0390 - val_loss: 0.0354\n",
      "Epoch 1172/3000\n",
      "2370/2370 [==============================] - 0s 135us/step - loss: 0.0402 - val_loss: 0.0438\n",
      "Epoch 1173/3000\n",
      "2370/2370 [==============================] - 0s 135us/step - loss: 0.0366 - val_loss: 0.0500\n",
      "Epoch 1174/3000\n",
      "2370/2370 [==============================] - 0s 138us/step - loss: 0.0392 - val_loss: 0.0423\n",
      "Epoch 1175/3000\n",
      "2370/2370 [==============================] - 0s 134us/step - loss: 0.0414 - val_loss: 0.0423\n",
      "Epoch 1176/3000\n",
      "2370/2370 [==============================] - 0s 130us/step - loss: 0.0389 - val_loss: 0.0411\n",
      "Epoch 1177/3000\n",
      "2370/2370 [==============================] - 0s 61us/step - loss: 0.0365 - val_loss: 0.0412\n",
      "Epoch 1178/3000\n",
      "2370/2370 [==============================] - 0s 88us/step - loss: 0.0404 - val_loss: 0.0529\n",
      "Epoch 1179/3000\n",
      "2370/2370 [==============================] - 0s 132us/step - loss: 0.0375 - val_loss: 0.0446\n",
      "Epoch 1180/3000\n",
      "2370/2370 [==============================] - 0s 97us/step - loss: 0.0364 - val_loss: 0.0490\n",
      "Epoch 1181/3000\n",
      "2370/2370 [==============================] - 0s 104us/step - loss: 0.0384 - val_loss: 0.0398\n",
      "Epoch 1182/3000\n",
      "2370/2370 [==============================] - 0s 120us/step - loss: 0.0387 - val_loss: 0.0380\n",
      "Epoch 1183/3000\n",
      "2370/2370 [==============================] - 0s 75us/step - loss: 0.0383 - val_loss: 0.0387\n",
      "Epoch 1184/3000\n",
      "2370/2370 [==============================] - 0s 76us/step - loss: 0.0367 - val_loss: 0.0394\n",
      "Epoch 1185/3000\n",
      "2370/2370 [==============================] - 0s 84us/step - loss: 0.0369 - val_loss: 0.0627\n",
      "Epoch 1186/3000\n",
      "2370/2370 [==============================] - 0s 50us/step - loss: 0.0361 - val_loss: 0.0373\n",
      "Epoch 1187/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0383 - val_loss: 0.0585\n",
      "Epoch 1188/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0377 - val_loss: 0.0368\n",
      "Epoch 1189/3000\n",
      "2370/2370 [==============================] - 0s 30us/step - loss: 0.0365 - val_loss: 0.0371\n",
      "Epoch 1190/3000\n",
      "2370/2370 [==============================] - 0s 79us/step - loss: 0.0346 - val_loss: 0.0457\n",
      "Epoch 1191/3000\n",
      "2370/2370 [==============================] - 0s 96us/step - loss: 0.0339 - val_loss: 0.0403\n",
      "Epoch 1192/3000\n",
      "2370/2370 [==============================] - 0s 89us/step - loss: 0.0342 - val_loss: 0.0580\n",
      "Epoch 1193/3000\n",
      "2370/2370 [==============================] - 0s 78us/step - loss: 0.0393 - val_loss: 0.0362\n",
      "Epoch 1194/3000\n",
      "2370/2370 [==============================] - 0s 46us/step - loss: 0.0356 - val_loss: 0.0418\n",
      "Epoch 1195/3000\n",
      "2370/2370 [==============================] - 0s 37us/step - loss: 0.0406 - val_loss: 0.0362\n",
      "Epoch 1196/3000\n",
      "2370/2370 [==============================] - 0s 30us/step - loss: 0.0363 - val_loss: 0.0464\n",
      "Epoch 1197/3000\n",
      "2370/2370 [==============================] - 0s 28us/step - loss: 0.0399 - val_loss: 0.0469\n",
      "Epoch 1198/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0347 - val_loss: 0.0541\n",
      "Epoch 1199/3000\n",
      "2370/2370 [==============================] - 0s 30us/step - loss: 0.0387 - val_loss: 0.0369\n",
      "Epoch 1200/3000\n",
      "2370/2370 [==============================] - 0s 34us/step - loss: 0.0371 - val_loss: 0.0340\n",
      "Epoch 1201/3000\n",
      "2370/2370 [==============================] - 0s 30us/step - loss: 0.0382 - val_loss: 0.0466\n",
      "Epoch 1202/3000\n",
      "2370/2370 [==============================] - 0s 75us/step - loss: 0.0372 - val_loss: 0.0360\n",
      "Epoch 1203/3000\n",
      "2370/2370 [==============================] - 0s 129us/step - loss: 0.0386 - val_loss: 0.0413\n",
      "Epoch 1204/3000\n",
      "2370/2370 [==============================] - 0s 127us/step - loss: 0.0385 - val_loss: 0.0348\n",
      "Epoch 1205/3000\n",
      "2370/2370 [==============================] - 0s 128us/step - loss: 0.0399 - val_loss: 0.0401\n",
      "Epoch 1206/3000\n",
      "2370/2370 [==============================] - 0s 90us/step - loss: 0.0381 - val_loss: 0.0404\n",
      "Epoch 1207/3000\n",
      "2370/2370 [==============================] - 0s 98us/step - loss: 0.0372 - val_loss: 0.0416\n",
      "Epoch 1208/3000\n",
      "2370/2370 [==============================] - 0s 91us/step - loss: 0.0354 - val_loss: 0.0392\n",
      "Epoch 1209/3000\n",
      "2370/2370 [==============================] - 0s 49us/step - loss: 0.0418 - val_loss: 0.0524\n",
      "Epoch 1210/3000\n",
      "2370/2370 [==============================] - 0s 41us/step - loss: 0.0382 - val_loss: 0.0423\n",
      "Epoch 1211/3000\n",
      "2370/2370 [==============================] - 0s 31us/step - loss: 0.0361 - val_loss: 0.0396\n",
      "Epoch 1212/3000\n",
      "2370/2370 [==============================] - 0s 28us/step - loss: 0.0373 - val_loss: 0.0457\n",
      "Epoch 1213/3000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2370/2370 [==============================] - 0s 28us/step - loss: 0.0381 - val_loss: 0.0395\n",
      "Epoch 1214/3000\n",
      "2370/2370 [==============================] - 0s 28us/step - loss: 0.0363 - val_loss: 0.0485\n",
      "Epoch 1215/3000\n",
      "2370/2370 [==============================] - 0s 28us/step - loss: 0.0349 - val_loss: 0.0449\n",
      "Epoch 1216/3000\n",
      "2370/2370 [==============================] - 0s 28us/step - loss: 0.0410 - val_loss: 0.0435\n",
      "Epoch 1217/3000\n",
      "2370/2370 [==============================] - 0s 28us/step - loss: 0.0340 - val_loss: 0.0450\n",
      "Epoch 1218/3000\n",
      "2370/2370 [==============================] - 0s 35us/step - loss: 0.0376 - val_loss: 0.0559\n",
      "Epoch 1219/3000\n",
      "2370/2370 [==============================] - 0s 38us/step - loss: 0.0401 - val_loss: 0.0428\n",
      "Epoch 1220/3000\n",
      "2370/2370 [==============================] - 0s 47us/step - loss: 0.0372 - val_loss: 0.0383\n",
      "Epoch 1221/3000\n",
      "2370/2370 [==============================] - 0s 73us/step - loss: 0.0364 - val_loss: 0.0367\n",
      "Epoch 1222/3000\n",
      "2370/2370 [==============================] - 0s 107us/step - loss: 0.0372 - val_loss: 0.0550\n",
      "Epoch 1223/3000\n",
      "2370/2370 [==============================] - 0s 138us/step - loss: 0.0373 - val_loss: 0.0420\n",
      "Epoch 1224/3000\n",
      "2370/2370 [==============================] - 0s 139us/step - loss: 0.0400 - val_loss: 0.0485\n",
      "Epoch 1225/3000\n",
      "2370/2370 [==============================] - 0s 116us/step - loss: 0.0376 - val_loss: 0.0395\n",
      "Epoch 1226/3000\n",
      "2370/2370 [==============================] - 0s 82us/step - loss: 0.0349 - val_loss: 0.0752\n",
      "Epoch 1227/3000\n",
      "2370/2370 [==============================] - 0s 80us/step - loss: 0.0340 - val_loss: 0.0461\n",
      "Epoch 1228/3000\n",
      "2370/2370 [==============================] - 0s 86us/step - loss: 0.0373 - val_loss: 0.0358\n",
      "Epoch 1229/3000\n",
      "2370/2370 [==============================] - 0s 66us/step - loss: 0.0370 - val_loss: 0.0791\n",
      "Epoch 1230/3000\n",
      "2370/2370 [==============================] - 0s 46us/step - loss: 0.0369 - val_loss: 0.0376\n",
      "Epoch 1231/3000\n",
      "2370/2370 [==============================] - 0s 31us/step - loss: 0.0372 - val_loss: 0.0366\n",
      "Epoch 1232/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0367 - val_loss: 0.0354\n",
      "Epoch 1233/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0393 - val_loss: 0.0381\n",
      "Epoch 1234/3000\n",
      "2370/2370 [==============================] - 0s 31us/step - loss: 0.0363 - val_loss: 0.0611\n",
      "Epoch 1235/3000\n",
      "2370/2370 [==============================] - 0s 33us/step - loss: 0.0398 - val_loss: 0.0365\n",
      "Epoch 1236/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0376 - val_loss: 0.0376\n",
      "Epoch 1237/3000\n",
      "2370/2370 [==============================] - 0s 45us/step - loss: 0.0400 - val_loss: 0.0340\n",
      "Epoch 1238/3000\n",
      "2370/2370 [==============================] - 0s 157us/step - loss: 0.0379 - val_loss: 0.0370\n",
      "Epoch 1239/3000\n",
      "2370/2370 [==============================] - 0s 102us/step - loss: 0.0372 - val_loss: 0.0498\n",
      "Epoch 1240/3000\n",
      "2370/2370 [==============================] - 0s 77us/step - loss: 0.0384 - val_loss: 0.0445\n",
      "Epoch 1241/3000\n",
      "2370/2370 [==============================] - 0s 32us/step - loss: 0.0375 - val_loss: 0.0352\n",
      "Epoch 1242/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0374 - val_loss: 0.0399\n",
      "Epoch 1243/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0373 - val_loss: 0.0565\n",
      "Epoch 1244/3000\n",
      "2370/2370 [==============================] - 0s 31us/step - loss: 0.0361 - val_loss: 0.0377\n",
      "Epoch 1245/3000\n",
      "2370/2370 [==============================] - 0s 31us/step - loss: 0.0383 - val_loss: 0.0470\n",
      "Epoch 1246/3000\n",
      "2370/2370 [==============================] - 0s 33us/step - loss: 0.0402 - val_loss: 0.0412\n",
      "Epoch 1247/3000\n",
      "2370/2370 [==============================] - 0s 36us/step - loss: 0.0346 - val_loss: 0.0477\n",
      "Epoch 1248/3000\n",
      "2370/2370 [==============================] - 0s 39us/step - loss: 0.0352 - val_loss: 0.0378\n",
      "Epoch 1249/3000\n",
      "2370/2370 [==============================] - 0s 72us/step - loss: 0.0372 - val_loss: 0.0460\n",
      "Epoch 1250/3000\n",
      "2370/2370 [==============================] - 0s 72us/step - loss: 0.0393 - val_loss: 0.0478\n",
      "Epoch 1251/3000\n",
      "2370/2370 [==============================] - 0s 39us/step - loss: 0.0374 - val_loss: 0.0430\n",
      "Epoch 1252/3000\n",
      "2370/2370 [==============================] - 0s 39us/step - loss: 0.0367 - val_loss: 0.0495\n",
      "Epoch 1253/3000\n",
      "2370/2370 [==============================] - 0s 28us/step - loss: 0.0388 - val_loss: 0.0384\n",
      "Epoch 1254/3000\n",
      "2370/2370 [==============================] - 0s 28us/step - loss: 0.0358 - val_loss: 0.0547\n",
      "Epoch 1255/3000\n",
      "2370/2370 [==============================] - 0s 37us/step - loss: 0.0376 - val_loss: 0.0396\n",
      "Epoch 1256/3000\n",
      "2370/2370 [==============================] - 0s 37us/step - loss: 0.0396 - val_loss: 0.0509\n",
      "Epoch 1257/3000\n",
      "2370/2370 [==============================] - 0s 34us/step - loss: 0.0372 - val_loss: 0.0486\n",
      "Epoch 1258/3000\n",
      "2370/2370 [==============================] - 0s 37us/step - loss: 0.0346 - val_loss: 0.0450\n",
      "Epoch 1259/3000\n",
      "2370/2370 [==============================] - 0s 30us/step - loss: 0.0347 - val_loss: 0.0502\n",
      "Epoch 1260/3000\n",
      "2370/2370 [==============================] - 0s 28us/step - loss: 0.0377 - val_loss: 0.0515\n",
      "Epoch 1261/3000\n",
      "2370/2370 [==============================] - 0s 28us/step - loss: 0.0388 - val_loss: 0.0728\n",
      "Epoch 1262/3000\n",
      "2370/2370 [==============================] - 0s 28us/step - loss: 0.0346 - val_loss: 0.0397\n",
      "Epoch 1263/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0390 - val_loss: 0.0471\n",
      "Epoch 1264/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0368 - val_loss: 0.0377\n",
      "Epoch 1265/3000\n",
      "2370/2370 [==============================] - 0s 28us/step - loss: 0.0349 - val_loss: 0.0580\n",
      "Epoch 1266/3000\n",
      "2370/2370 [==============================] - 0s 32us/step - loss: 0.0438 - val_loss: 0.0375\n",
      "Epoch 1267/3000\n",
      "2370/2370 [==============================] - 0s 28us/step - loss: 0.0367 - val_loss: 0.0393\n",
      "Epoch 1268/3000\n",
      "2370/2370 [==============================] - 0s 28us/step - loss: 0.0388 - val_loss: 0.0357\n",
      "Epoch 1269/3000\n",
      "2370/2370 [==============================] - 0s 28us/step - loss: 0.0382 - val_loss: 0.0452\n",
      "Epoch 1270/3000\n",
      "2370/2370 [==============================] - 0s 28us/step - loss: 0.0381 - val_loss: 0.0354\n",
      "Epoch 1271/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0406 - val_loss: 0.0466\n",
      "Epoch 1272/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0365 - val_loss: 0.0403\n",
      "Epoch 1273/3000\n",
      "2370/2370 [==============================] - 0s 32us/step - loss: 0.0325 - val_loss: 0.0359\n",
      "Epoch 1274/3000\n",
      "2370/2370 [==============================] - 0s 32us/step - loss: 0.0391 - val_loss: 0.0564\n",
      "Epoch 1275/3000\n",
      "2370/2370 [==============================] - 0s 30us/step - loss: 0.0379 - val_loss: 0.0354\n",
      "Epoch 1276/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0375 - val_loss: 0.0390\n",
      "Epoch 1277/3000\n",
      "2370/2370 [==============================] - 0s 30us/step - loss: 0.0403 - val_loss: 0.0532\n",
      "Epoch 1278/3000\n",
      "2370/2370 [==============================] - 0s 49us/step - loss: 0.0381 - val_loss: 0.0495\n",
      "Epoch 1279/3000\n",
      "2370/2370 [==============================] - 0s 85us/step - loss: 0.0378 - val_loss: 0.0358\n",
      "Epoch 1280/3000\n",
      "2370/2370 [==============================] - 0s 100us/step - loss: 0.0340 - val_loss: 0.0393\n",
      "Epoch 1281/3000\n",
      "2370/2370 [==============================] - 0s 95us/step - loss: 0.0360 - val_loss: 0.0486\n",
      "Epoch 1282/3000\n",
      "2370/2370 [==============================] - 0s 120us/step - loss: 0.0362 - val_loss: 0.0415\n",
      "Epoch 1283/3000\n",
      "2370/2370 [==============================] - 0s 101us/step - loss: 0.0360 - val_loss: 0.0649\n",
      "Epoch 1284/3000\n",
      "2370/2370 [==============================] - 0s 100us/step - loss: 0.0400 - val_loss: 0.0384\n",
      "Epoch 1285/3000\n",
      "2370/2370 [==============================] - 0s 131us/step - loss: 0.0387 - val_loss: 0.0391\n",
      "Epoch 1286/3000\n",
      "2370/2370 [==============================] - 0s 77us/step - loss: 0.0373 - val_loss: 0.0460\n",
      "Epoch 1287/3000\n",
      "2370/2370 [==============================] - 0s 44us/step - loss: 0.0373 - val_loss: 0.0383\n",
      "Epoch 1288/3000\n",
      "2370/2370 [==============================] - 0s 80us/step - loss: 0.0400 - val_loss: 0.0376\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1289/3000\n",
      "2370/2370 [==============================] - 0s 104us/step - loss: 0.0379 - val_loss: 0.0366\n",
      "Epoch 1290/3000\n",
      "2370/2370 [==============================] - 0s 101us/step - loss: 0.0390 - val_loss: 0.0386\n",
      "Epoch 1291/3000\n",
      "2370/2370 [==============================] - 0s 112us/step - loss: 0.0353 - val_loss: 0.0390\n",
      "Epoch 1292/3000\n",
      "2370/2370 [==============================] - 0s 108us/step - loss: 0.0386 - val_loss: 0.0532\n",
      "Epoch 1293/3000\n",
      "2370/2370 [==============================] - 0s 134us/step - loss: 0.0386 - val_loss: 0.0448\n",
      "Epoch 1294/3000\n",
      "2370/2370 [==============================] - 0s 128us/step - loss: 0.0380 - val_loss: 0.0359\n",
      "Epoch 1295/3000\n",
      "2370/2370 [==============================] - 0s 145us/step - loss: 0.0397 - val_loss: 0.0692\n",
      "Epoch 1296/3000\n",
      "2370/2370 [==============================] - 0s 141us/step - loss: 0.0342 - val_loss: 0.0488\n",
      "Epoch 1297/3000\n",
      "2370/2370 [==============================] - 0s 128us/step - loss: 0.0366 - val_loss: 0.0425\n",
      "Epoch 1298/3000\n",
      "2370/2370 [==============================] - 0s 137us/step - loss: 0.0368 - val_loss: 0.0401\n",
      "Epoch 1299/3000\n",
      "2370/2370 [==============================] - 0s 143us/step - loss: 0.0391 - val_loss: 0.0395\n",
      "Epoch 1300/3000\n",
      "2370/2370 [==============================] - 0s 132us/step - loss: 0.0355 - val_loss: 0.0356\n",
      "Epoch 1301/3000\n",
      "2370/2370 [==============================] - 0s 140us/step - loss: 0.0436 - val_loss: 0.0377\n",
      "Epoch 1302/3000\n",
      "2370/2370 [==============================] - 0s 121us/step - loss: 0.0386 - val_loss: 0.0440\n",
      "Epoch 1303/3000\n",
      "2370/2370 [==============================] - 0s 137us/step - loss: 0.0402 - val_loss: 0.0496\n",
      "Epoch 1304/3000\n",
      "2370/2370 [==============================] - 0s 130us/step - loss: 0.0369 - val_loss: 0.0415\n",
      "Epoch 1305/3000\n",
      "2370/2370 [==============================] - 0s 124us/step - loss: 0.0376 - val_loss: 0.0609\n",
      "Epoch 1306/3000\n",
      "2370/2370 [==============================] - 0s 130us/step - loss: 0.0373 - val_loss: 0.0412\n",
      "Epoch 1307/3000\n",
      "2370/2370 [==============================] - 0s 74us/step - loss: 0.0373 - val_loss: 0.0585\n",
      "Epoch 1308/3000\n",
      "2370/2370 [==============================] - 0s 62us/step - loss: 0.0394 - val_loss: 0.0497\n",
      "Epoch 1309/3000\n",
      "2370/2370 [==============================] - 0s 91us/step - loss: 0.0389 - val_loss: 0.0387\n",
      "Epoch 1310/3000\n",
      "2370/2370 [==============================] - 0s 129us/step - loss: 0.0362 - val_loss: 0.0563\n",
      "Epoch 1311/3000\n",
      "2370/2370 [==============================] - 0s 118us/step - loss: 0.0398 - val_loss: 0.0382\n",
      "Epoch 1312/3000\n",
      "2370/2370 [==============================] - 0s 113us/step - loss: 0.0416 - val_loss: 0.0403\n",
      "Epoch 1313/3000\n",
      "2370/2370 [==============================] - 0s 112us/step - loss: 0.0419 - val_loss: 0.0344\n",
      "Epoch 1314/3000\n",
      "2370/2370 [==============================] - 0s 147us/step - loss: 0.0345 - val_loss: 0.0606\n",
      "Epoch 1315/3000\n",
      "2370/2370 [==============================] - 0s 136us/step - loss: 0.0369 - val_loss: 0.0403\n",
      "Epoch 1316/3000\n",
      "2370/2370 [==============================] - 0s 143us/step - loss: 0.0353 - val_loss: 0.0495\n",
      "Epoch 1317/3000\n",
      "2370/2370 [==============================] - 0s 167us/step - loss: 0.0370 - val_loss: 0.0577\n",
      "Epoch 1318/3000\n",
      "2370/2370 [==============================] - 0s 157us/step - loss: 0.0397 - val_loss: 0.0341\n",
      "Epoch 1319/3000\n",
      "2370/2370 [==============================] - 0s 127us/step - loss: 0.0388 - val_loss: 0.0373\n",
      "Epoch 1320/3000\n",
      "2370/2370 [==============================] - 0s 143us/step - loss: 0.0389 - val_loss: 0.0457\n",
      "Epoch 1321/3000\n",
      "2370/2370 [==============================] - 0s 131us/step - loss: 0.0385 - val_loss: 0.0479\n",
      "Epoch 1322/3000\n",
      "2370/2370 [==============================] - 0s 118us/step - loss: 0.0363 - val_loss: 0.0532\n",
      "Epoch 1323/3000\n",
      "2370/2370 [==============================] - 0s 108us/step - loss: 0.0385 - val_loss: 0.0471\n",
      "Epoch 1324/3000\n",
      "2370/2370 [==============================] - 0s 109us/step - loss: 0.0380 - val_loss: 0.0429\n",
      "Epoch 1325/3000\n",
      "2370/2370 [==============================] - 0s 127us/step - loss: 0.0359 - val_loss: 0.0518\n",
      "Epoch 1326/3000\n",
      "2370/2370 [==============================] - 0s 128us/step - loss: 0.0376 - val_loss: 0.0585\n",
      "Epoch 1327/3000\n",
      "2370/2370 [==============================] - 0s 140us/step - loss: 0.0352 - val_loss: 0.0490\n",
      "Epoch 1328/3000\n",
      "2370/2370 [==============================] - 0s 148us/step - loss: 0.0343 - val_loss: 0.0454\n",
      "Epoch 1329/3000\n",
      "2370/2370 [==============================] - 0s 138us/step - loss: 0.0389 - val_loss: 0.0542\n",
      "Epoch 1330/3000\n",
      "2370/2370 [==============================] - 0s 140us/step - loss: 0.0397 - val_loss: 0.0379\n",
      "Epoch 1331/3000\n",
      "2370/2370 [==============================] - 0s 136us/step - loss: 0.0367 - val_loss: 0.0366\n",
      "Epoch 1332/3000\n",
      "2370/2370 [==============================] - 0s 128us/step - loss: 0.0388 - val_loss: 0.0440\n",
      "Epoch 1333/3000\n",
      "2370/2370 [==============================] - 0s 137us/step - loss: 0.0386 - val_loss: 0.0368\n",
      "Epoch 1334/3000\n",
      "2370/2370 [==============================] - 0s 133us/step - loss: 0.0365 - val_loss: 0.0482\n",
      "Epoch 1335/3000\n",
      "2370/2370 [==============================] - 0s 147us/step - loss: 0.0326 - val_loss: 0.0549\n",
      "Epoch 1336/3000\n",
      "2370/2370 [==============================] - 0s 120us/step - loss: 0.0389 - val_loss: 0.0601\n",
      "Epoch 1337/3000\n",
      "2370/2370 [==============================] - 0s 132us/step - loss: 0.0353 - val_loss: 0.0385\n",
      "Epoch 1338/3000\n",
      "2370/2370 [==============================] - 0s 126us/step - loss: 0.0384 - val_loss: 0.0501\n",
      "Epoch 1339/3000\n",
      "2370/2370 [==============================] - 0s 143us/step - loss: 0.0376 - val_loss: 0.0550\n",
      "Epoch 1340/3000\n",
      "2370/2370 [==============================] - 0s 144us/step - loss: 0.0357 - val_loss: 0.0471\n",
      "Epoch 1341/3000\n",
      "2370/2370 [==============================] - 0s 119us/step - loss: 0.0384 - val_loss: 0.0425\n",
      "Epoch 1342/3000\n",
      "2370/2370 [==============================] - 0s 121us/step - loss: 0.0383 - val_loss: 0.0465\n",
      "Epoch 1343/3000\n",
      "2370/2370 [==============================] - 0s 122us/step - loss: 0.0385 - val_loss: 0.0450\n",
      "Epoch 1344/3000\n",
      "2370/2370 [==============================] - 0s 118us/step - loss: 0.0373 - val_loss: 0.0426\n",
      "Epoch 1345/3000\n",
      "2370/2370 [==============================] - 0s 135us/step - loss: 0.0382 - val_loss: 0.0515\n",
      "Epoch 1346/3000\n",
      "2370/2370 [==============================] - 0s 132us/step - loss: 0.0406 - val_loss: 0.0628\n",
      "Epoch 1347/3000\n",
      "2370/2370 [==============================] - 0s 140us/step - loss: 0.0382 - val_loss: 0.0441\n",
      "Epoch 1348/3000\n",
      "2370/2370 [==============================] - 0s 139us/step - loss: 0.0377 - val_loss: 0.0565\n",
      "Epoch 1349/3000\n",
      "2370/2370 [==============================] - 0s 137us/step - loss: 0.0396 - val_loss: 0.0590\n",
      "Epoch 1350/3000\n",
      "2370/2370 [==============================] - 0s 82us/step - loss: 0.0377 - val_loss: 0.0424\n",
      "Epoch 1351/3000\n",
      "2370/2370 [==============================] - 0s 106us/step - loss: 0.0371 - val_loss: 0.0509\n",
      "Epoch 1352/3000\n",
      "2370/2370 [==============================] - 0s 119us/step - loss: 0.0348 - val_loss: 0.0393\n",
      "Epoch 1353/3000\n",
      "2370/2370 [==============================] - 0s 73us/step - loss: 0.0381 - val_loss: 0.0552\n",
      "Epoch 1354/3000\n",
      "2370/2370 [==============================] - 0s 53us/step - loss: 0.0397 - val_loss: 0.0405\n",
      "Epoch 1355/3000\n",
      "2370/2370 [==============================] - 0s 55us/step - loss: 0.0373 - val_loss: 0.0522\n",
      "Epoch 1356/3000\n",
      "2370/2370 [==============================] - 0s 63us/step - loss: 0.0356 - val_loss: 0.0389\n",
      "Epoch 1357/3000\n",
      "2370/2370 [==============================] - 0s 134us/step - loss: 0.0372 - val_loss: 0.0545\n",
      "Epoch 1358/3000\n",
      "2370/2370 [==============================] - 0s 129us/step - loss: 0.0367 - val_loss: 0.0460\n",
      "Epoch 1359/3000\n",
      "2370/2370 [==============================] - 0s 127us/step - loss: 0.0368 - val_loss: 0.0445\n",
      "Epoch 1360/3000\n",
      "2370/2370 [==============================] - 0s 131us/step - loss: 0.0381 - val_loss: 0.0413\n",
      "Epoch 1361/3000\n",
      "2370/2370 [==============================] - 0s 129us/step - loss: 0.0349 - val_loss: 0.0387\n",
      "Epoch 1362/3000\n",
      "2370/2370 [==============================] - 0s 127us/step - loss: 0.0365 - val_loss: 0.0596\n",
      "Epoch 1363/3000\n",
      "2370/2370 [==============================] - 0s 137us/step - loss: 0.0404 - val_loss: 0.0398\n",
      "Epoch 1364/3000\n",
      "2370/2370 [==============================] - 0s 54us/step - loss: 0.0350 - val_loss: 0.0390\n",
      "Epoch 1365/3000\n",
      "2370/2370 [==============================] - 0s 53us/step - loss: 0.0352 - val_loss: 0.0441\n",
      "Epoch 1366/3000\n",
      "2370/2370 [==============================] - 0s 135us/step - loss: 0.0368 - val_loss: 0.0470\n",
      "Epoch 1367/3000\n",
      "2370/2370 [==============================] - 0s 136us/step - loss: 0.0387 - val_loss: 0.0510\n",
      "Epoch 1368/3000\n",
      "2370/2370 [==============================] - 0s 133us/step - loss: 0.0392 - val_loss: 0.0517\n",
      "Epoch 1369/3000\n",
      "2370/2370 [==============================] - 0s 148us/step - loss: 0.0378 - val_loss: 0.0420\n",
      "Epoch 1370/3000\n",
      "2370/2370 [==============================] - 0s 146us/step - loss: 0.0401 - val_loss: 0.0473\n",
      "Epoch 1371/3000\n",
      "2370/2370 [==============================] - 0s 131us/step - loss: 0.0375 - val_loss: 0.0398\n",
      "Epoch 1372/3000\n",
      "2370/2370 [==============================] - 0s 130us/step - loss: 0.0396 - val_loss: 0.0419\n",
      "Epoch 1373/3000\n",
      "2370/2370 [==============================] - 0s 131us/step - loss: 0.0355 - val_loss: 0.0427\n",
      "Epoch 1374/3000\n",
      "2370/2370 [==============================] - 0s 108us/step - loss: 0.0398 - val_loss: 0.0374\n",
      "Epoch 1375/3000\n",
      "2370/2370 [==============================] - 0s 100us/step - loss: 0.0376 - val_loss: 0.0388\n",
      "Epoch 1376/3000\n",
      "2370/2370 [==============================] - 0s 112us/step - loss: 0.0373 - val_loss: 0.0420\n",
      "Epoch 1377/3000\n",
      "2370/2370 [==============================] - 0s 65us/step - loss: 0.0384 - val_loss: 0.0389\n",
      "Epoch 1378/3000\n",
      "2370/2370 [==============================] - 0s 60us/step - loss: 0.0399 - val_loss: 0.0455\n",
      "Epoch 1379/3000\n",
      "2370/2370 [==============================] - 0s 59us/step - loss: 0.0389 - val_loss: 0.0419\n",
      "Epoch 1380/3000\n",
      "2370/2370 [==============================] - 0s 50us/step - loss: 0.0338 - val_loss: 0.0382\n",
      "Epoch 1381/3000\n",
      "2370/2370 [==============================] - 0s 28us/step - loss: 0.0380 - val_loss: 0.0447\n",
      "Epoch 1382/3000\n",
      "2370/2370 [==============================] - 0s 28us/step - loss: 0.0400 - val_loss: 0.0394\n",
      "Epoch 1383/3000\n",
      "2370/2370 [==============================] - 0s 28us/step - loss: 0.0358 - val_loss: 0.0446\n",
      "Epoch 1384/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0362 - val_loss: 0.0364\n",
      "Epoch 1385/3000\n",
      "2370/2370 [==============================] - 0s 31us/step - loss: 0.0378 - val_loss: 0.0445\n",
      "Epoch 1386/3000\n",
      "2370/2370 [==============================] - 0s 28us/step - loss: 0.0433 - val_loss: 0.0430\n",
      "Epoch 1387/3000\n",
      "2370/2370 [==============================] - 0s 28us/step - loss: 0.0350 - val_loss: 0.0360\n",
      "Epoch 1388/3000\n",
      "2370/2370 [==============================] - 0s 28us/step - loss: 0.0371 - val_loss: 0.0366\n",
      "Epoch 1389/3000\n",
      "2370/2370 [==============================] - 0s 28us/step - loss: 0.0400 - val_loss: 0.0354\n",
      "Epoch 1390/3000\n",
      "2370/2370 [==============================] - 0s 28us/step - loss: 0.0388 - val_loss: 0.0355\n",
      "Epoch 1391/3000\n",
      "2370/2370 [==============================] - 0s 28us/step - loss: 0.0359 - val_loss: 0.0434\n",
      "Epoch 1392/3000\n",
      "2370/2370 [==============================] - 0s 28us/step - loss: 0.0342 - val_loss: 0.0467\n",
      "Epoch 1393/3000\n",
      "2370/2370 [==============================] - 0s 32us/step - loss: 0.0350 - val_loss: 0.0379\n",
      "Epoch 1394/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0384 - val_loss: 0.0392\n",
      "Epoch 1395/3000\n",
      "2370/2370 [==============================] - 0s 31us/step - loss: 0.0382 - val_loss: 0.0513\n",
      "Epoch 1396/3000\n",
      "2370/2370 [==============================] - 0s 28us/step - loss: 0.0341 - val_loss: 0.0391\n",
      "Epoch 1397/3000\n",
      "2370/2370 [==============================] - 0s 28us/step - loss: 0.0373 - val_loss: 0.0384\n",
      "Epoch 1398/3000\n",
      "2370/2370 [==============================] - 0s 28us/step - loss: 0.0364 - val_loss: 0.0523\n",
      "Epoch 1399/3000\n",
      "2370/2370 [==============================] - 0s 28us/step - loss: 0.0361 - val_loss: 0.0378\n",
      "Epoch 1400/3000\n",
      "2370/2370 [==============================] - 0s 28us/step - loss: 0.0378 - val_loss: 0.0433\n",
      "Epoch 1401/3000\n",
      "2370/2370 [==============================] - 0s 28us/step - loss: 0.0399 - val_loss: 0.0450\n",
      "Epoch 1402/3000\n",
      "2370/2370 [==============================] - 0s 33us/step - loss: 0.0376 - val_loss: 0.0449\n",
      "Epoch 1403/3000\n",
      "2370/2370 [==============================] - 0s 28us/step - loss: 0.0369 - val_loss: 0.0439\n",
      "Epoch 1404/3000\n",
      "2370/2370 [==============================] - 0s 28us/step - loss: 0.0406 - val_loss: 0.0383\n",
      "Epoch 1405/3000\n",
      "2370/2370 [==============================] - 0s 28us/step - loss: 0.0363 - val_loss: 0.0351\n",
      "Epoch 1406/3000\n",
      "2370/2370 [==============================] - 0s 28us/step - loss: 0.0360 - val_loss: 0.0482\n",
      "Epoch 1407/3000\n",
      "2370/2370 [==============================] - 0s 28us/step - loss: 0.0408 - val_loss: 0.0351\n",
      "Epoch 1408/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0358 - val_loss: 0.0372\n",
      "Epoch 1409/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0355 - val_loss: 0.0487\n",
      "Epoch 1410/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0366 - val_loss: 0.0364\n",
      "Epoch 1411/3000\n",
      "2370/2370 [==============================] - 0s 31us/step - loss: 0.0346 - val_loss: 0.0429\n",
      "Epoch 1412/3000\n",
      "2370/2370 [==============================] - 0s 28us/step - loss: 0.0350 - val_loss: 0.0443\n",
      "Epoch 1413/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0391 - val_loss: 0.0389\n",
      "Epoch 1414/3000\n",
      "2370/2370 [==============================] - 0s 52us/step - loss: 0.0377 - val_loss: 0.0385\n",
      "Epoch 1415/3000\n",
      "2370/2370 [==============================] - 0s 90us/step - loss: 0.0350 - val_loss: 0.0642\n",
      "Epoch 1416/3000\n",
      "2370/2370 [==============================] - 0s 124us/step - loss: 0.0425 - val_loss: 0.0359\n",
      "Epoch 1417/3000\n",
      "2370/2370 [==============================] - 0s 125us/step - loss: 0.0398 - val_loss: 0.0402\n",
      "Epoch 1418/3000\n",
      "2370/2370 [==============================] - 0s 136us/step - loss: 0.0364 - val_loss: 0.0403\n",
      "Epoch 1419/3000\n",
      "2370/2370 [==============================] - 0s 142us/step - loss: 0.0392 - val_loss: 0.0571\n",
      "Epoch 1420/3000\n",
      "2370/2370 [==============================] - 0s 121us/step - loss: 0.0379 - val_loss: 0.0400\n",
      "Epoch 1421/3000\n",
      "2370/2370 [==============================] - 0s 92us/step - loss: 0.0393 - val_loss: 0.0366\n",
      "Epoch 1422/3000\n",
      "2370/2370 [==============================] - 0s 56us/step - loss: 0.0357 - val_loss: 0.0427\n",
      "Epoch 1423/3000\n",
      "2370/2370 [==============================] - 0s 81us/step - loss: 0.0400 - val_loss: 0.0474\n",
      "Epoch 1424/3000\n",
      "2370/2370 [==============================] - 0s 103us/step - loss: 0.0375 - val_loss: 0.0493\n",
      "Epoch 1425/3000\n",
      "2370/2370 [==============================] - 0s 75us/step - loss: 0.0368 - val_loss: 0.0461\n",
      "Epoch 1426/3000\n",
      "2370/2370 [==============================] - 0s 34us/step - loss: 0.0401 - val_loss: 0.0390\n",
      "Epoch 1427/3000\n",
      "2370/2370 [==============================] - 0s 33us/step - loss: 0.0352 - val_loss: 0.0429\n",
      "Epoch 1428/3000\n",
      "2370/2370 [==============================] - 0s 32us/step - loss: 0.0356 - val_loss: 0.0375\n",
      "Epoch 1429/3000\n",
      "2370/2370 [==============================] - 0s 32us/step - loss: 0.0380 - val_loss: 0.0399\n",
      "Epoch 1430/3000\n",
      "2370/2370 [==============================] - 0s 33us/step - loss: 0.0363 - val_loss: 0.0406\n",
      "Epoch 1431/3000\n",
      "2370/2370 [==============================] - 0s 32us/step - loss: 0.0334 - val_loss: 0.0424\n",
      "Epoch 1432/3000\n",
      "2370/2370 [==============================] - 0s 33us/step - loss: 0.0358 - val_loss: 0.0375\n",
      "Epoch 1433/3000\n",
      "2370/2370 [==============================] - 0s 32us/step - loss: 0.0356 - val_loss: 0.0430\n",
      "Epoch 1434/3000\n",
      "2370/2370 [==============================] - 0s 36us/step - loss: 0.0393 - val_loss: 0.0517\n",
      "Epoch 1435/3000\n",
      "2370/2370 [==============================] - 0s 34us/step - loss: 0.0326 - val_loss: 0.0531\n",
      "Epoch 1436/3000\n",
      "2370/2370 [==============================] - 0s 33us/step - loss: 0.0368 - val_loss: 0.0401\n",
      "Epoch 1437/3000\n",
      "2370/2370 [==============================] - 0s 32us/step - loss: 0.0392 - val_loss: 0.0624\n",
      "Epoch 1438/3000\n",
      "2370/2370 [==============================] - 0s 33us/step - loss: 0.0409 - val_loss: 0.0443\n",
      "Epoch 1439/3000\n",
      "2370/2370 [==============================] - 0s 32us/step - loss: 0.0366 - val_loss: 0.0393\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1440/3000\n",
      "2370/2370 [==============================] - 0s 32us/step - loss: 0.0385 - val_loss: 0.0403\n",
      "Epoch 1441/3000\n",
      "2370/2370 [==============================] - 0s 34us/step - loss: 0.0393 - val_loss: 0.0523\n",
      "Epoch 1442/3000\n",
      "2370/2370 [==============================] - 0s 34us/step - loss: 0.0391 - val_loss: 0.0367\n",
      "Epoch 1443/3000\n",
      "2370/2370 [==============================] - 0s 31us/step - loss: 0.0375 - val_loss: 0.0416\n",
      "Epoch 1444/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0411 - val_loss: 0.0370\n",
      "Epoch 1445/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0379 - val_loss: 0.0372\n",
      "Epoch 1446/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0369 - val_loss: 0.0664\n",
      "Epoch 1447/3000\n",
      "2370/2370 [==============================] - 0s 31us/step - loss: 0.0383 - val_loss: 0.0483\n",
      "Epoch 1448/3000\n",
      "2370/2370 [==============================] - 0s 31us/step - loss: 0.0380 - val_loss: 0.0414\n",
      "Epoch 1449/3000\n",
      "2370/2370 [==============================] - 0s 37us/step - loss: 0.0360 - val_loss: 0.0571\n",
      "Epoch 1450/3000\n",
      "2370/2370 [==============================] - 0s 33us/step - loss: 0.0391 - val_loss: 0.0476\n",
      "Epoch 1451/3000\n",
      "2370/2370 [==============================] - 0s 31us/step - loss: 0.0368 - val_loss: 0.0396\n",
      "Epoch 1452/3000\n",
      "2370/2370 [==============================] - 0s 31us/step - loss: 0.0359 - val_loss: 0.0417\n",
      "Epoch 1453/3000\n",
      "2370/2370 [==============================] - 0s 31us/step - loss: 0.0381 - val_loss: 0.0557\n",
      "Epoch 1454/3000\n",
      "2370/2370 [==============================] - 0s 31us/step - loss: 0.0351 - val_loss: 0.0378\n",
      "Epoch 1455/3000\n",
      "2370/2370 [==============================] - 0s 30us/step - loss: 0.0366 - val_loss: 0.0617\n",
      "Epoch 1456/3000\n",
      "2370/2370 [==============================] - 0s 31us/step - loss: 0.0376 - val_loss: 0.0511\n",
      "Epoch 1457/3000\n",
      "2370/2370 [==============================] - 0s 32us/step - loss: 0.0361 - val_loss: 0.0400\n",
      "Epoch 1458/3000\n",
      "2370/2370 [==============================] - 0s 32us/step - loss: 0.0373 - val_loss: 0.0380\n",
      "Epoch 1459/3000\n",
      "2370/2370 [==============================] - 0s 31us/step - loss: 0.0376 - val_loss: 0.0462\n",
      "Epoch 1460/3000\n",
      "2370/2370 [==============================] - 0s 31us/step - loss: 0.0361 - val_loss: 0.0528\n",
      "Epoch 1461/3000\n",
      "2370/2370 [==============================] - 0s 31us/step - loss: 0.0340 - val_loss: 0.0452\n",
      "Epoch 1462/3000\n",
      "2370/2370 [==============================] - 0s 31us/step - loss: 0.0373 - val_loss: 0.0445\n",
      "Epoch 1463/3000\n",
      "2370/2370 [==============================] - 0s 31us/step - loss: 0.0381 - val_loss: 0.0655\n",
      "Epoch 1464/3000\n",
      "2370/2370 [==============================] - 0s 31us/step - loss: 0.0335 - val_loss: 0.0508\n",
      "Epoch 1465/3000\n",
      "2370/2370 [==============================] - 0s 31us/step - loss: 0.0349 - val_loss: 0.0455\n",
      "Epoch 1466/3000\n",
      "2370/2370 [==============================] - 0s 33us/step - loss: 0.0412 - val_loss: 0.0395\n",
      "Epoch 1467/3000\n",
      "2370/2370 [==============================] - 0s 31us/step - loss: 0.0394 - val_loss: 0.0377\n",
      "Epoch 1468/3000\n",
      "2370/2370 [==============================] - 0s 30us/step - loss: 0.0393 - val_loss: 0.0512\n",
      "Epoch 1469/3000\n",
      "2370/2370 [==============================] - 0s 30us/step - loss: 0.0375 - val_loss: 0.0378\n",
      "Epoch 1470/3000\n",
      "2370/2370 [==============================] - 0s 33us/step - loss: 0.0380 - val_loss: 0.0506\n",
      "Epoch 1471/3000\n",
      "2370/2370 [==============================] - 0s 28us/step - loss: 0.0359 - val_loss: 0.0404\n",
      "Epoch 1472/3000\n",
      "2370/2370 [==============================] - 0s 28us/step - loss: 0.0385 - val_loss: 0.0356\n",
      "Epoch 1473/3000\n",
      "2370/2370 [==============================] - 0s 28us/step - loss: 0.0356 - val_loss: 0.0426\n",
      "Epoch 1474/3000\n",
      "2370/2370 [==============================] - 0s 28us/step - loss: 0.0418 - val_loss: 0.0394\n",
      "Epoch 1475/3000\n",
      "2370/2370 [==============================] - 0s 28us/step - loss: 0.0422 - val_loss: 0.0469\n",
      "Epoch 1476/3000\n",
      "2370/2370 [==============================] - 0s 28us/step - loss: 0.0386 - val_loss: 0.0570\n",
      "Epoch 1477/3000\n",
      "2370/2370 [==============================] - 0s 28us/step - loss: 0.0340 - val_loss: 0.0548\n",
      "Epoch 1478/3000\n",
      "2370/2370 [==============================] - 0s 28us/step - loss: 0.0407 - val_loss: 0.0488\n",
      "Epoch 1479/3000\n",
      "2370/2370 [==============================] - 0s 30us/step - loss: 0.0389 - val_loss: 0.0438\n",
      "Epoch 1480/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0368 - val_loss: 0.0448\n",
      "Epoch 1481/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0363 - val_loss: 0.0620\n",
      "Epoch 1482/3000\n",
      "2370/2370 [==============================] - 0s 28us/step - loss: 0.0375 - val_loss: 0.0462\n",
      "Epoch 1483/3000\n",
      "2370/2370 [==============================] - 0s 28us/step - loss: 0.0374 - val_loss: 0.0434\n",
      "Epoch 1484/3000\n",
      "2370/2370 [==============================] - 0s 27us/step - loss: 0.0338 - val_loss: 0.0420\n",
      "Epoch 1485/3000\n",
      "2370/2370 [==============================] - 0s 28us/step - loss: 0.0382 - val_loss: 0.0532\n",
      "Epoch 1486/3000\n",
      "2370/2370 [==============================] - 0s 28us/step - loss: 0.0354 - val_loss: 0.0543\n",
      "Epoch 1487/3000\n",
      "2370/2370 [==============================] - 0s 28us/step - loss: 0.0376 - val_loss: 0.0387\n",
      "Epoch 1488/3000\n",
      "2370/2370 [==============================] - 0s 27us/step - loss: 0.0383 - val_loss: 0.0458\n",
      "Epoch 1489/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0370 - val_loss: 0.0779\n",
      "Epoch 1490/3000\n",
      "2370/2370 [==============================] - 0s 28us/step - loss: 0.0388 - val_loss: 0.0429\n",
      "Epoch 1491/3000\n",
      "2370/2370 [==============================] - 0s 35us/step - loss: 0.0422 - val_loss: 0.0514\n",
      "Epoch 1492/3000\n",
      "2370/2370 [==============================] - 0s 28us/step - loss: 0.0353 - val_loss: 0.0532\n",
      "Epoch 1493/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0358 - val_loss: 0.0404\n",
      "Epoch 1494/3000\n",
      "2370/2370 [==============================] - 0s 28us/step - loss: 0.0380 - val_loss: 0.0370\n",
      "Epoch 1495/3000\n",
      "2370/2370 [==============================] - 0s 28us/step - loss: 0.0400 - val_loss: 0.0464\n",
      "Epoch 1496/3000\n",
      "2370/2370 [==============================] - 0s 28us/step - loss: 0.0347 - val_loss: 0.0670\n",
      "Epoch 1497/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0334 - val_loss: 0.0514\n",
      "Epoch 1498/3000\n",
      "2370/2370 [==============================] - 0s 28us/step - loss: 0.0393 - val_loss: 0.0576\n",
      "Epoch 1499/3000\n",
      "2370/2370 [==============================] - 0s 28us/step - loss: 0.0378 - val_loss: 0.0452\n",
      "Epoch 1500/3000\n",
      "2370/2370 [==============================] - 0s 30us/step - loss: 0.0368 - val_loss: 0.0413\n",
      "Epoch 1501/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0397 - val_loss: 0.0386\n",
      "Epoch 1502/3000\n",
      "2370/2370 [==============================] - 0s 28us/step - loss: 0.0370 - val_loss: 0.0375\n",
      "Epoch 1503/3000\n",
      "2370/2370 [==============================] - 0s 28us/step - loss: 0.0324 - val_loss: 0.0400\n",
      "Epoch 1504/3000\n",
      "2370/2370 [==============================] - 0s 28us/step - loss: 0.0371 - val_loss: 0.0401\n",
      "Epoch 1505/3000\n",
      "2370/2370 [==============================] - 0s 28us/step - loss: 0.0376 - val_loss: 0.0478\n",
      "Epoch 1506/3000\n",
      "2370/2370 [==============================] - 0s 28us/step - loss: 0.0387 - val_loss: 0.0375\n",
      "Epoch 1507/3000\n",
      "2370/2370 [==============================] - 0s 28us/step - loss: 0.0396 - val_loss: 0.0480\n",
      "Epoch 1508/3000\n",
      "2370/2370 [==============================] - 0s 28us/step - loss: 0.0367 - val_loss: 0.0580\n",
      "Epoch 1509/3000\n",
      "2370/2370 [==============================] - 0s 31us/step - loss: 0.0368 - val_loss: 0.0410\n",
      "Epoch 1510/3000\n",
      "2370/2370 [==============================] - 0s 28us/step - loss: 0.0365 - val_loss: 0.0378\n",
      "Epoch 1511/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0376 - val_loss: 0.0400\n",
      "Epoch 1512/3000\n",
      "2370/2370 [==============================] - 0s 28us/step - loss: 0.0389 - val_loss: 0.0398\n",
      "Epoch 1513/3000\n",
      "2370/2370 [==============================] - 0s 28us/step - loss: 0.0393 - val_loss: 0.0414\n",
      "Epoch 1514/3000\n",
      "2370/2370 [==============================] - 0s 28us/step - loss: 0.0368 - val_loss: 0.0376\n",
      "Epoch 1515/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0378 - val_loss: 0.0399\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1516/3000\n",
      "2370/2370 [==============================] - 0s 28us/step - loss: 0.0381 - val_loss: 0.0395\n",
      "Epoch 1517/3000\n",
      "2370/2370 [==============================] - 0s 28us/step - loss: 0.0418 - val_loss: 0.0369\n",
      "Epoch 1518/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0377 - val_loss: 0.0547\n",
      "Epoch 1519/3000\n",
      "2370/2370 [==============================] - 0s 32us/step - loss: 0.0357 - val_loss: 0.0404\n",
      "Epoch 1520/3000\n",
      "2370/2370 [==============================] - 0s 33us/step - loss: 0.0351 - val_loss: 0.0537\n",
      "Epoch 1521/3000\n",
      "2370/2370 [==============================] - 0s 28us/step - loss: 0.0389 - val_loss: 0.0486\n",
      "Epoch 1522/3000\n",
      "2370/2370 [==============================] - 0s 28us/step - loss: 0.0427 - val_loss: 0.0389\n",
      "Epoch 1523/3000\n",
      "2370/2370 [==============================] - 0s 28us/step - loss: 0.0377 - val_loss: 0.0408\n",
      "Epoch 1524/3000\n",
      "2370/2370 [==============================] - 0s 27us/step - loss: 0.0397 - val_loss: 0.0417\n",
      "Epoch 1525/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0377 - val_loss: 0.0707\n",
      "Epoch 1526/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0390 - val_loss: 0.0564\n",
      "Epoch 1527/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0378 - val_loss: 0.0411\n",
      "Epoch 1528/3000\n",
      "2370/2370 [==============================] - 0s 28us/step - loss: 0.0388 - val_loss: 0.0481\n",
      "Epoch 1529/3000\n",
      "2370/2370 [==============================] - 0s 28us/step - loss: 0.0364 - val_loss: 0.0469\n",
      "Epoch 1530/3000\n",
      "2370/2370 [==============================] - 0s 28us/step - loss: 0.0405 - val_loss: 0.0479\n",
      "Epoch 1531/3000\n",
      "2370/2370 [==============================] - 0s 28us/step - loss: 0.0404 - val_loss: 0.0391\n",
      "Epoch 1532/3000\n",
      "2370/2370 [==============================] - 0s 28us/step - loss: 0.0409 - val_loss: 0.0376\n",
      "Epoch 1533/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0367 - val_loss: 0.0371\n",
      "Epoch 1534/3000\n",
      "2370/2370 [==============================] - 0s 28us/step - loss: 0.0379 - val_loss: 0.0399\n",
      "Epoch 1535/3000\n",
      "2370/2370 [==============================] - 0s 30us/step - loss: 0.0366 - val_loss: 0.0491\n",
      "Epoch 1536/3000\n",
      "2370/2370 [==============================] - 0s 28us/step - loss: 0.0367 - val_loss: 0.0377\n",
      "Epoch 1537/3000\n",
      "2370/2370 [==============================] - 0s 28us/step - loss: 0.0379 - val_loss: 0.0357\n",
      "Epoch 1538/3000\n",
      "2370/2370 [==============================] - 0s 28us/step - loss: 0.0375 - val_loss: 0.0439\n",
      "Epoch 1539/3000\n",
      "2370/2370 [==============================] - 0s 28us/step - loss: 0.0436 - val_loss: 0.0440\n",
      "Epoch 1540/3000\n",
      "2370/2370 [==============================] - 0s 28us/step - loss: 0.0348 - val_loss: 0.0496\n",
      "Epoch 1541/3000\n",
      "2370/2370 [==============================] - 0s 28us/step - loss: 0.0378 - val_loss: 0.0439\n",
      "Epoch 1542/3000\n",
      "2370/2370 [==============================] - 0s 28us/step - loss: 0.0373 - val_loss: 0.0434\n",
      "Epoch 1543/3000\n",
      "2370/2370 [==============================] - 0s 28us/step - loss: 0.0391 - val_loss: 0.0399\n",
      "Epoch 1544/3000\n",
      "2370/2370 [==============================] - 0s 30us/step - loss: 0.0384 - val_loss: 0.0525\n",
      "Epoch 1545/3000\n",
      "2370/2370 [==============================] - 0s 28us/step - loss: 0.0363 - val_loss: 0.0388\n",
      "Epoch 1546/3000\n",
      "2370/2370 [==============================] - 0s 28us/step - loss: 0.0385 - val_loss: 0.0375\n",
      "Epoch 1547/3000\n",
      "2370/2370 [==============================] - 0s 28us/step - loss: 0.0346 - val_loss: 0.0434\n",
      "Epoch 1548/3000\n",
      "2370/2370 [==============================] - 0s 31us/step - loss: 0.0358 - val_loss: 0.0495\n",
      "Epoch 1549/3000\n",
      "2370/2370 [==============================] - 0s 28us/step - loss: 0.0350 - val_loss: 0.0575\n",
      "Epoch 1550/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0373 - val_loss: 0.0470\n",
      "Epoch 1551/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0399 - val_loss: 0.0375\n",
      "Epoch 1552/3000\n",
      "2370/2370 [==============================] - 0s 28us/step - loss: 0.0403 - val_loss: 0.0382\n",
      "Epoch 1553/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0365 - val_loss: 0.0457\n",
      "Epoch 1554/3000\n",
      "2370/2370 [==============================] - 0s 28us/step - loss: 0.0404 - val_loss: 0.0524\n",
      "Epoch 1555/3000\n",
      "2370/2370 [==============================] - 0s 28us/step - loss: 0.0398 - val_loss: 0.0424\n",
      "Epoch 1556/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0358 - val_loss: 0.0422\n",
      "Epoch 1557/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0386 - val_loss: 0.0587\n",
      "Epoch 1558/3000\n",
      "2370/2370 [==============================] - 0s 28us/step - loss: 0.0381 - val_loss: 0.0397\n",
      "Epoch 1559/3000\n",
      "2370/2370 [==============================] - 0s 28us/step - loss: 0.0355 - val_loss: 0.0401\n",
      "Epoch 1560/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0384 - val_loss: 0.0418\n",
      "Epoch 1561/3000\n",
      "2370/2370 [==============================] - 0s 28us/step - loss: 0.0381 - val_loss: 0.0450\n",
      "Epoch 1562/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0360 - val_loss: 0.0401\n",
      "Epoch 1563/3000\n",
      "2370/2370 [==============================] - 0s 30us/step - loss: 0.0369 - val_loss: 0.0487\n",
      "Epoch 1564/3000\n",
      "2370/2370 [==============================] - 0s 31us/step - loss: 0.0406 - val_loss: 0.0679\n",
      "Epoch 1565/3000\n",
      "2370/2370 [==============================] - 0s 28us/step - loss: 0.0385 - val_loss: 0.0511\n",
      "Epoch 1566/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0395 - val_loss: 0.0487\n",
      "Epoch 1567/3000\n",
      "2370/2370 [==============================] - 0s 28us/step - loss: 0.0356 - val_loss: 0.0486\n",
      "Epoch 1568/3000\n",
      "2370/2370 [==============================] - 0s 28us/step - loss: 0.0359 - val_loss: 0.0489\n",
      "Epoch 1569/3000\n",
      "2370/2370 [==============================] - 0s 28us/step - loss: 0.0365 - val_loss: 0.0445\n",
      "Epoch 1570/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0359 - val_loss: 0.0366\n",
      "Epoch 1571/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0388 - val_loss: 0.0403\n",
      "Epoch 1572/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0386 - val_loss: 0.0356\n",
      "Epoch 1573/3000\n",
      "2370/2370 [==============================] - 0s 28us/step - loss: 0.0360 - val_loss: 0.0509\n",
      "Epoch 1574/3000\n",
      "2370/2370 [==============================] - 0s 28us/step - loss: 0.0377 - val_loss: 0.0399\n",
      "Epoch 1575/3000\n",
      "2370/2370 [==============================] - 0s 28us/step - loss: 0.0408 - val_loss: 0.0448\n",
      "Epoch 1576/3000\n",
      "2370/2370 [==============================] - 0s 28us/step - loss: 0.0408 - val_loss: 0.0365\n",
      "Epoch 1577/3000\n",
      "2370/2370 [==============================] - 0s 28us/step - loss: 0.0365 - val_loss: 0.0473\n",
      "Epoch 1578/3000\n",
      "2370/2370 [==============================] - 0s 28us/step - loss: 0.0378 - val_loss: 0.0397\n",
      "Epoch 1579/3000\n",
      "2370/2370 [==============================] - 0s 31us/step - loss: 0.0395 - val_loss: 0.0372\n",
      "Epoch 1580/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0378 - val_loss: 0.0441\n",
      "Epoch 1581/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0335 - val_loss: 0.0394\n",
      "Epoch 1582/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0406 - val_loss: 0.0490\n",
      "Epoch 1583/3000\n",
      "2370/2370 [==============================] - 0s 28us/step - loss: 0.0383 - val_loss: 0.0359\n",
      "Epoch 1584/3000\n",
      "2370/2370 [==============================] - 0s 28us/step - loss: 0.0358 - val_loss: 0.0567\n",
      "Epoch 1585/3000\n",
      "2370/2370 [==============================] - 0s 28us/step - loss: 0.0356 - val_loss: 0.0404\n",
      "Epoch 1586/3000\n",
      "2370/2370 [==============================] - 0s 28us/step - loss: 0.0352 - val_loss: 0.0397\n",
      "Epoch 1587/3000\n",
      "2370/2370 [==============================] - 0s 28us/step - loss: 0.0387 - val_loss: 0.0439\n",
      "Epoch 1588/3000\n",
      "2370/2370 [==============================] - 0s 31us/step - loss: 0.0348 - val_loss: 0.0740\n",
      "Epoch 1589/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0405 - val_loss: 0.0381\n",
      "Epoch 1590/3000\n",
      "2370/2370 [==============================] - 0s 28us/step - loss: 0.0379 - val_loss: 0.0433\n",
      "Epoch 1591/3000\n",
      "2370/2370 [==============================] - 0s 28us/step - loss: 0.0384 - val_loss: 0.0400\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1592/3000\n",
      "2370/2370 [==============================] - 0s 28us/step - loss: 0.0387 - val_loss: 0.0406\n",
      "Epoch 1593/3000\n",
      "2370/2370 [==============================] - 0s 28us/step - loss: 0.0356 - val_loss: 0.0384\n",
      "Epoch 1594/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0370 - val_loss: 0.0384\n",
      "Epoch 1595/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0358 - val_loss: 0.0365\n",
      "Epoch 1596/3000\n",
      "2370/2370 [==============================] - 0s 28us/step - loss: 0.0426 - val_loss: 0.0390\n",
      "Epoch 1597/3000\n",
      "2370/2370 [==============================] - 0s 28us/step - loss: 0.0381 - val_loss: 0.0540\n",
      "Epoch 1598/3000\n",
      "2370/2370 [==============================] - 0s 28us/step - loss: 0.0384 - val_loss: 0.0383\n",
      "Epoch 1599/3000\n",
      "2370/2370 [==============================] - 0s 28us/step - loss: 0.0378 - val_loss: 0.0373\n",
      "Epoch 1600/3000\n",
      "2370/2370 [==============================] - 0s 28us/step - loss: 0.0363 - val_loss: 0.0611\n",
      "Epoch 1601/3000\n",
      "2370/2370 [==============================] - 0s 28us/step - loss: 0.0372 - val_loss: 0.0442\n",
      "Epoch 1602/3000\n",
      "2370/2370 [==============================] - 0s 28us/step - loss: 0.0381 - val_loss: 0.0479\n",
      "Epoch 1603/3000\n",
      "2370/2370 [==============================] - 0s 28us/step - loss: 0.0368 - val_loss: 0.0390\n",
      "Epoch 1604/3000\n",
      "2370/2370 [==============================] - 0s 28us/step - loss: 0.0371 - val_loss: 0.0633\n",
      "Epoch 1605/3000\n",
      "2370/2370 [==============================] - 0s 28us/step - loss: 0.0389 - val_loss: 0.0386\n",
      "Epoch 1606/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0384 - val_loss: 0.0476\n",
      "Epoch 1607/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0382 - val_loss: 0.0419\n",
      "Epoch 1608/3000\n",
      "2370/2370 [==============================] - 0s 32us/step - loss: 0.0396 - val_loss: 0.0414\n",
      "Epoch 1609/3000\n",
      "2370/2370 [==============================] - 0s 28us/step - loss: 0.0370 - val_loss: 0.0465\n",
      "Epoch 1610/3000\n",
      "2370/2370 [==============================] - 0s 28us/step - loss: 0.0348 - val_loss: 0.0382\n",
      "Epoch 1611/3000\n",
      "2370/2370 [==============================] - 0s 28us/step - loss: 0.0364 - val_loss: 0.0379\n",
      "Epoch 1612/3000\n",
      "2370/2370 [==============================] - 0s 28us/step - loss: 0.0354 - val_loss: 0.0386\n",
      "Epoch 1613/3000\n",
      "2370/2370 [==============================] - 0s 28us/step - loss: 0.0373 - val_loss: 0.0424\n",
      "Epoch 1614/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0390 - val_loss: 0.0395\n",
      "Epoch 1615/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0367 - val_loss: 0.0528\n",
      "Epoch 1616/3000\n",
      "2370/2370 [==============================] - 0s 28us/step - loss: 0.0409 - val_loss: 0.0388\n",
      "Epoch 1617/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0362 - val_loss: 0.0400\n",
      "Epoch 1618/3000\n",
      "2370/2370 [==============================] - 0s 28us/step - loss: 0.0370 - val_loss: 0.0377\n",
      "Epoch 1619/3000\n",
      "2370/2370 [==============================] - 0s 28us/step - loss: 0.0368 - val_loss: 0.0387\n",
      "Epoch 1620/3000\n",
      "2370/2370 [==============================] - 0s 28us/step - loss: 0.0351 - val_loss: 0.0404\n",
      "Epoch 1621/3000\n",
      "2370/2370 [==============================] - 0s 28us/step - loss: 0.0362 - val_loss: 0.0453\n",
      "Epoch 1622/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0404 - val_loss: 0.0440\n",
      "Epoch 1623/3000\n",
      "2370/2370 [==============================] - 0s 30us/step - loss: 0.0397 - val_loss: 0.0459\n",
      "Epoch 1624/3000\n",
      "2370/2370 [==============================] - 0s 28us/step - loss: 0.0388 - val_loss: 0.0414\n",
      "Epoch 1625/3000\n",
      "2370/2370 [==============================] - 0s 28us/step - loss: 0.0374 - val_loss: 0.0422\n",
      "Epoch 1626/3000\n",
      "2370/2370 [==============================] - 0s 41us/step - loss: 0.0368 - val_loss: 0.0471\n",
      "Epoch 1627/3000\n",
      "2370/2370 [==============================] - 0s 76us/step - loss: 0.0359 - val_loss: 0.0460\n",
      "Epoch 1628/3000\n",
      "2370/2370 [==============================] - 0s 115us/step - loss: 0.0366 - val_loss: 0.0391\n",
      "Epoch 1629/3000\n",
      "2370/2370 [==============================] - 0s 103us/step - loss: 0.0393 - val_loss: 0.0472\n",
      "Epoch 1630/3000\n",
      "2370/2370 [==============================] - 0s 99us/step - loss: 0.0364 - val_loss: 0.0380\n",
      "Epoch 1631/3000\n",
      "2370/2370 [==============================] - 0s 78us/step - loss: 0.0392 - val_loss: 0.0446\n",
      "Epoch 1632/3000\n",
      "2370/2370 [==============================] - 0s 93us/step - loss: 0.0368 - val_loss: 0.0394\n",
      "Epoch 1633/3000\n",
      "2370/2370 [==============================] - 0s 153us/step - loss: 0.0384 - val_loss: 0.0369\n",
      "Epoch 1634/3000\n",
      "2370/2370 [==============================] - 0s 141us/step - loss: 0.0373 - val_loss: 0.0459\n",
      "Epoch 1635/3000\n",
      "2370/2370 [==============================] - 0s 136us/step - loss: 0.0367 - val_loss: 0.0523\n",
      "Epoch 1636/3000\n",
      "2370/2370 [==============================] - 0s 89us/step - loss: 0.0394 - val_loss: 0.0429\n",
      "Epoch 1637/3000\n",
      "2370/2370 [==============================] - 0s 61us/step - loss: 0.0371 - val_loss: 0.0394\n",
      "Epoch 1638/3000\n",
      "2370/2370 [==============================] - 0s 36us/step - loss: 0.0344 - val_loss: 0.0396\n",
      "Epoch 1639/3000\n",
      "2370/2370 [==============================] - 0s 30us/step - loss: 0.0379 - val_loss: 0.0407\n",
      "Epoch 1640/3000\n",
      "2370/2370 [==============================] - 0s 31us/step - loss: 0.0382 - val_loss: 0.0412\n",
      "Epoch 1641/3000\n",
      "2370/2370 [==============================] - 0s 58us/step - loss: 0.0374 - val_loss: 0.0423\n",
      "Epoch 1642/3000\n",
      "2370/2370 [==============================] - 0s 119us/step - loss: 0.0351 - val_loss: 0.0405\n",
      "Epoch 1643/3000\n",
      "2370/2370 [==============================] - 0s 150us/step - loss: 0.0382 - val_loss: 0.0537\n",
      "Epoch 1644/3000\n",
      "2370/2370 [==============================] - 0s 134us/step - loss: 0.0362 - val_loss: 0.0388\n",
      "Epoch 1645/3000\n",
      "2370/2370 [==============================] - 0s 69us/step - loss: 0.0379 - val_loss: 0.0442\n",
      "Epoch 1646/3000\n",
      "2370/2370 [==============================] - 0s 43us/step - loss: 0.0414 - val_loss: 0.0417\n",
      "Epoch 1647/3000\n",
      "2370/2370 [==============================] - 0s 42us/step - loss: 0.0367 - val_loss: 0.0408\n",
      "Epoch 1648/3000\n",
      "2370/2370 [==============================] - 0s 41us/step - loss: 0.0392 - val_loss: 0.0406\n",
      "Epoch 1649/3000\n",
      "2370/2370 [==============================] - 0s 41us/step - loss: 0.0370 - val_loss: 0.0363\n",
      "Epoch 1650/3000\n",
      "2370/2370 [==============================] - 0s 35us/step - loss: 0.0378 - val_loss: 0.0363\n",
      "Epoch 1651/3000\n",
      "2370/2370 [==============================] - 0s 27us/step - loss: 0.0376 - val_loss: 0.0350\n",
      "Epoch 1652/3000\n",
      "2370/2370 [==============================] - 0s 28us/step - loss: 0.0377 - val_loss: 0.0359\n",
      "Epoch 1653/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0370 - val_loss: 0.0377\n",
      "Epoch 1654/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0360 - val_loss: 0.0437\n",
      "Epoch 1655/3000\n",
      "2370/2370 [==============================] - 0s 28us/step - loss: 0.0377 - val_loss: 0.0386\n",
      "Epoch 1656/3000\n",
      "2370/2370 [==============================] - 0s 28us/step - loss: 0.0368 - val_loss: 0.0479\n",
      "Epoch 1657/3000\n",
      "2370/2370 [==============================] - 0s 28us/step - loss: 0.0402 - val_loss: 0.0355\n",
      "Epoch 1658/3000\n",
      "2370/2370 [==============================] - 0s 28us/step - loss: 0.0361 - val_loss: 0.0381\n",
      "Epoch 1659/3000\n",
      "2370/2370 [==============================] - 0s 28us/step - loss: 0.0397 - val_loss: 0.0406\n",
      "Epoch 1660/3000\n",
      "2370/2370 [==============================] - 0s 28us/step - loss: 0.0372 - val_loss: 0.0384\n",
      "Epoch 1661/3000\n",
      "2370/2370 [==============================] - 0s 28us/step - loss: 0.0361 - val_loss: 0.0382\n",
      "Epoch 1662/3000\n",
      "2370/2370 [==============================] - 0s 28us/step - loss: 0.0395 - val_loss: 0.0428\n",
      "Epoch 1663/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0358 - val_loss: 0.0356\n",
      "Epoch 1664/3000\n",
      "2370/2370 [==============================] - 0s 28us/step - loss: 0.0361 - val_loss: 0.0346\n",
      "Epoch 1665/3000\n",
      "2370/2370 [==============================] - 0s 28us/step - loss: 0.0410 - val_loss: 0.0356\n",
      "Epoch 1666/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0382 - val_loss: 0.0444\n",
      "Epoch 1667/3000\n",
      "2370/2370 [==============================] - 0s 28us/step - loss: 0.0386 - val_loss: 0.0355\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1668/3000\n",
      "2370/2370 [==============================] - 0s 28us/step - loss: 0.0360 - val_loss: 0.0414\n",
      "Epoch 1669/3000\n",
      "2370/2370 [==============================] - 0s 28us/step - loss: 0.0407 - val_loss: 0.0358\n",
      "Epoch 1670/3000\n",
      "2370/2370 [==============================] - 0s 28us/step - loss: 0.0345 - val_loss: 0.0460\n",
      "Epoch 1671/3000\n",
      "2370/2370 [==============================] - 0s 28us/step - loss: 0.0380 - val_loss: 0.0585\n",
      "Epoch 1672/3000\n",
      "2370/2370 [==============================] - 0s 28us/step - loss: 0.0365 - val_loss: 0.0491\n",
      "Epoch 1673/3000\n",
      "2370/2370 [==============================] - 0s 28us/step - loss: 0.0376 - val_loss: 0.0476\n",
      "Epoch 1674/3000\n",
      "2370/2370 [==============================] - 0s 28us/step - loss: 0.0383 - val_loss: 0.0368\n",
      "Epoch 1675/3000\n",
      "2370/2370 [==============================] - 0s 28us/step - loss: 0.0405 - val_loss: 0.0362\n",
      "Epoch 1676/3000\n",
      "2370/2370 [==============================] - 0s 28us/step - loss: 0.0360 - val_loss: 0.0421\n",
      "Epoch 1677/3000\n",
      "2370/2370 [==============================] - 0s 28us/step - loss: 0.0364 - val_loss: 0.0446\n",
      "Epoch 1678/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0398 - val_loss: 0.0393\n",
      "Epoch 1679/3000\n",
      "2370/2370 [==============================] - 0s 28us/step - loss: 0.0365 - val_loss: 0.0373\n",
      "Epoch 1680/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0386 - val_loss: 0.0367\n",
      "Epoch 1681/3000\n",
      "2370/2370 [==============================] - 0s 27us/step - loss: 0.0423 - val_loss: 0.0469\n",
      "Epoch 1682/3000\n",
      "2370/2370 [==============================] - 0s 28us/step - loss: 0.0387 - val_loss: 0.0419\n",
      "Epoch 1683/3000\n",
      "2370/2370 [==============================] - 0s 28us/step - loss: 0.0373 - val_loss: 0.0430\n",
      "Epoch 1684/3000\n",
      "2370/2370 [==============================] - 0s 28us/step - loss: 0.0404 - val_loss: 0.0363\n",
      "Epoch 1685/3000\n",
      "2370/2370 [==============================] - 0s 27us/step - loss: 0.0383 - val_loss: 0.0576\n",
      "Epoch 1686/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0392 - val_loss: 0.0482\n",
      "Epoch 1687/3000\n",
      "2370/2370 [==============================] - 0s 28us/step - loss: 0.0323 - val_loss: 0.0453\n",
      "Epoch 1688/3000\n",
      "2370/2370 [==============================] - 0s 28us/step - loss: 0.0345 - val_loss: 0.0360\n",
      "Epoch 1689/3000\n",
      "2370/2370 [==============================] - 0s 28us/step - loss: 0.0404 - val_loss: 0.0435\n",
      "Epoch 1690/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0349 - val_loss: 0.0368\n",
      "Epoch 1691/3000\n",
      "2370/2370 [==============================] - 0s 28us/step - loss: 0.0343 - val_loss: 0.0434\n",
      "Epoch 1692/3000\n",
      "2370/2370 [==============================] - 0s 30us/step - loss: 0.0410 - val_loss: 0.0404\n",
      "Epoch 1693/3000\n",
      "2370/2370 [==============================] - 0s 28us/step - loss: 0.0354 - val_loss: 0.0367\n",
      "Epoch 1694/3000\n",
      "2370/2370 [==============================] - 0s 34us/step - loss: 0.0370 - val_loss: 0.0372\n",
      "Epoch 1695/3000\n",
      "2370/2370 [==============================] - 0s 31us/step - loss: 0.0381 - val_loss: 0.0415\n",
      "Epoch 1696/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0338 - val_loss: 0.0405\n",
      "Epoch 1697/3000\n",
      "2370/2370 [==============================] - 0s 28us/step - loss: 0.0414 - val_loss: 0.0389\n",
      "Epoch 1698/3000\n",
      "2370/2370 [==============================] - 0s 28us/step - loss: 0.0361 - val_loss: 0.0489\n",
      "Epoch 1699/3000\n",
      "2370/2370 [==============================] - 0s 28us/step - loss: 0.0337 - val_loss: 0.0440\n",
      "Epoch 1700/3000\n",
      "2370/2370 [==============================] - 0s 28us/step - loss: 0.0385 - val_loss: 0.0419\n",
      "Epoch 1701/3000\n",
      "2370/2370 [==============================] - 0s 28us/step - loss: 0.0385 - val_loss: 0.0463\n",
      "Epoch 1702/3000\n",
      "2370/2370 [==============================] - 0s 28us/step - loss: 0.0375 - val_loss: 0.0564\n",
      "Epoch 1703/3000\n",
      "2370/2370 [==============================] - 0s 28us/step - loss: 0.0391 - val_loss: 0.0515\n",
      "Epoch 1704/3000\n",
      "2370/2370 [==============================] - 0s 28us/step - loss: 0.0358 - val_loss: 0.0620\n",
      "Epoch 1705/3000\n",
      "2370/2370 [==============================] - 0s 28us/step - loss: 0.0384 - val_loss: 0.0396\n",
      "Epoch 1706/3000\n",
      "2370/2370 [==============================] - 0s 28us/step - loss: 0.0401 - val_loss: 0.0483\n",
      "Epoch 1707/3000\n",
      "2370/2370 [==============================] - 0s 28us/step - loss: 0.0400 - val_loss: 0.0525\n",
      "Epoch 1708/3000\n",
      "2370/2370 [==============================] - 0s 28us/step - loss: 0.0383 - val_loss: 0.0368\n",
      "Epoch 1709/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0376 - val_loss: 0.0491\n",
      "Epoch 1710/3000\n",
      "2370/2370 [==============================] - 0s 28us/step - loss: 0.0389 - val_loss: 0.0535\n",
      "Epoch 1711/3000\n",
      "2370/2370 [==============================] - 0s 28us/step - loss: 0.0369 - val_loss: 0.0524\n",
      "Epoch 1712/3000\n",
      "2370/2370 [==============================] - 0s 28us/step - loss: 0.0381 - val_loss: 0.0402\n",
      "Epoch 1713/3000\n",
      "2370/2370 [==============================] - 0s 28us/step - loss: 0.0373 - val_loss: 0.0544\n",
      "Epoch 1714/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0367 - val_loss: 0.0411\n",
      "Epoch 1715/3000\n",
      "2370/2370 [==============================] - 0s 28us/step - loss: 0.0389 - val_loss: 0.0518\n",
      "Epoch 1716/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0402 - val_loss: 0.0390\n",
      "Epoch 1717/3000\n",
      "2370/2370 [==============================] - 0s 28us/step - loss: 0.0371 - val_loss: 0.0581\n",
      "Epoch 1718/3000\n",
      "2370/2370 [==============================] - 0s 28us/step - loss: 0.0370 - val_loss: 0.0451\n",
      "Epoch 1719/3000\n",
      "2370/2370 [==============================] - 0s 28us/step - loss: 0.0394 - val_loss: 0.0520\n",
      "Epoch 1720/3000\n",
      "2370/2370 [==============================] - 0s 28us/step - loss: 0.0346 - val_loss: 0.0600\n",
      "Epoch 1721/3000\n",
      "2370/2370 [==============================] - 0s 28us/step - loss: 0.0383 - val_loss: 0.0410\n",
      "Epoch 1722/3000\n",
      "2370/2370 [==============================] - 0s 28us/step - loss: 0.0362 - val_loss: 0.0467\n",
      "Epoch 1723/3000\n",
      "2370/2370 [==============================] - 0s 33us/step - loss: 0.0335 - val_loss: 0.0496\n",
      "Epoch 1724/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0408 - val_loss: 0.0435\n",
      "Epoch 1725/3000\n",
      "2370/2370 [==============================] - 0s 28us/step - loss: 0.0378 - val_loss: 0.0394\n",
      "Epoch 1726/3000\n",
      "2370/2370 [==============================] - 0s 28us/step - loss: 0.0398 - val_loss: 0.0580\n",
      "Epoch 1727/3000\n",
      "2370/2370 [==============================] - 0s 28us/step - loss: 0.0387 - val_loss: 0.0626\n",
      "Epoch 1728/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0342 - val_loss: 0.0530\n",
      "Epoch 1729/3000\n",
      "2370/2370 [==============================] - 0s 28us/step - loss: 0.0397 - val_loss: 0.0395\n",
      "Epoch 1730/3000\n",
      "2370/2370 [==============================] - 0s 28us/step - loss: 0.0393 - val_loss: 0.0487\n",
      "Epoch 1731/3000\n",
      "2370/2370 [==============================] - 0s 28us/step - loss: 0.0411 - val_loss: 0.0562\n",
      "Epoch 1732/3000\n",
      "2370/2370 [==============================] - 0s 28us/step - loss: 0.0387 - val_loss: 0.0407\n",
      "Epoch 1733/3000\n",
      "2370/2370 [==============================] - 0s 28us/step - loss: 0.0363 - val_loss: 0.0475\n",
      "Epoch 1734/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0383 - val_loss: 0.0470\n",
      "Epoch 1735/3000\n",
      "2370/2370 [==============================] - 0s 28us/step - loss: 0.0374 - val_loss: 0.0523\n",
      "Epoch 1736/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0392 - val_loss: 0.0476\n",
      "Epoch 1737/3000\n",
      "2370/2370 [==============================] - 0s 28us/step - loss: 0.0365 - val_loss: 0.0461\n",
      "Epoch 1738/3000\n",
      "2370/2370 [==============================] - 0s 28us/step - loss: 0.0355 - val_loss: 0.0522\n",
      "Epoch 1739/3000\n",
      "2370/2370 [==============================] - 0s 28us/step - loss: 0.0384 - val_loss: 0.0411\n",
      "Epoch 1740/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0410 - val_loss: 0.0417\n",
      "Epoch 1741/3000\n",
      "2370/2370 [==============================] - 0s 28us/step - loss: 0.0388 - val_loss: 0.0453\n",
      "Epoch 1742/3000\n",
      "2370/2370 [==============================] - 0s 28us/step - loss: 0.0388 - val_loss: 0.0386\n",
      "Epoch 1743/3000\n",
      "2370/2370 [==============================] - 0s 28us/step - loss: 0.0386 - val_loss: 0.0388\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1744/3000\n",
      "2370/2370 [==============================] - 0s 28us/step - loss: 0.0362 - val_loss: 0.0403\n",
      "Epoch 1745/3000\n",
      "2370/2370 [==============================] - 0s 28us/step - loss: 0.0344 - val_loss: 0.0418\n",
      "Epoch 1746/3000\n",
      "2370/2370 [==============================] - 0s 28us/step - loss: 0.0397 - val_loss: 0.0392\n",
      "Epoch 1747/3000\n",
      "2370/2370 [==============================] - 0s 28us/step - loss: 0.0371 - val_loss: 0.0487\n",
      "Epoch 1748/3000\n",
      "2370/2370 [==============================] - 0s 28us/step - loss: 0.0373 - val_loss: 0.0385\n",
      "Epoch 1749/3000\n",
      "2370/2370 [==============================] - 0s 28us/step - loss: 0.0372 - val_loss: 0.0500\n",
      "Epoch 1750/3000\n",
      "2370/2370 [==============================] - 0s 28us/step - loss: 0.0352 - val_loss: 0.0377\n",
      "Epoch 1751/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0399 - val_loss: 0.0395\n",
      "Epoch 1752/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0392 - val_loss: 0.0399\n",
      "Epoch 1753/3000\n",
      "2370/2370 [==============================] - 0s 28us/step - loss: 0.0393 - val_loss: 0.0513\n",
      "Epoch 1754/3000\n",
      "2370/2370 [==============================] - 0s 28us/step - loss: 0.0379 - val_loss: 0.0382\n",
      "Epoch 1755/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0379 - val_loss: 0.0374\n",
      "Epoch 1756/3000\n",
      "2370/2370 [==============================] - 0s 28us/step - loss: 0.0401 - val_loss: 0.0379\n",
      "Epoch 1757/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0386 - val_loss: 0.0417\n",
      "Epoch 1758/3000\n",
      "2370/2370 [==============================] - 0s 28us/step - loss: 0.0403 - val_loss: 0.0443\n",
      "Epoch 1759/3000\n",
      "2370/2370 [==============================] - 0s 28us/step - loss: 0.0385 - val_loss: 0.0380\n",
      "Epoch 1760/3000\n",
      "2370/2370 [==============================] - 0s 28us/step - loss: 0.0355 - val_loss: 0.0453\n",
      "Epoch 1761/3000\n",
      "2370/2370 [==============================] - 0s 27us/step - loss: 0.0357 - val_loss: 0.0647\n",
      "Epoch 1762/3000\n",
      "2370/2370 [==============================] - 0s 28us/step - loss: 0.0406 - val_loss: 0.0422\n",
      "Epoch 1763/3000\n",
      "2370/2370 [==============================] - 0s 27us/step - loss: 0.0362 - val_loss: 0.0497\n",
      "Epoch 1764/3000\n",
      "2370/2370 [==============================] - 0s 28us/step - loss: 0.0396 - val_loss: 0.0547\n",
      "Epoch 1765/3000\n",
      "2370/2370 [==============================] - 0s 28us/step - loss: 0.0377 - val_loss: 0.0431\n",
      "Epoch 1766/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0353 - val_loss: 0.0598\n",
      "Epoch 1767/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0400 - val_loss: 0.0417\n",
      "Epoch 1768/3000\n",
      "2370/2370 [==============================] - 0s 31us/step - loss: 0.0418 - val_loss: 0.0410\n",
      "Epoch 1769/3000\n",
      "2370/2370 [==============================] - 0s 28us/step - loss: 0.0383 - val_loss: 0.0408\n",
      "Epoch 1770/3000\n",
      "2370/2370 [==============================] - 0s 28us/step - loss: 0.0409 - val_loss: 0.0618\n",
      "Epoch 1771/3000\n",
      "2370/2370 [==============================] - 0s 28us/step - loss: 0.0375 - val_loss: 0.0464\n",
      "Epoch 1772/3000\n",
      "2370/2370 [==============================] - 0s 28us/step - loss: 0.0408 - val_loss: 0.0467\n",
      "Epoch 1773/3000\n",
      "2370/2370 [==============================] - 0s 28us/step - loss: 0.0375 - val_loss: 0.0454\n",
      "Epoch 1774/3000\n",
      "2370/2370 [==============================] - 0s 28us/step - loss: 0.0399 - val_loss: 0.0517\n",
      "Epoch 1775/3000\n",
      "2370/2370 [==============================] - 0s 28us/step - loss: 0.0388 - val_loss: 0.0504\n",
      "Epoch 1776/3000\n",
      "2370/2370 [==============================] - 0s 28us/step - loss: 0.0383 - val_loss: 0.0409\n",
      "Epoch 1777/3000\n",
      "2370/2370 [==============================] - 0s 28us/step - loss: 0.0404 - val_loss: 0.0416\n",
      "Epoch 1778/3000\n",
      "2370/2370 [==============================] - 0s 28us/step - loss: 0.0404 - val_loss: 0.0441\n",
      "Epoch 1779/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0379 - val_loss: 0.0387\n",
      "Epoch 1780/3000\n",
      "2370/2370 [==============================] - 0s 27us/step - loss: 0.0371 - val_loss: 0.0406\n",
      "Epoch 1781/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0371 - val_loss: 0.0371\n",
      "Epoch 1782/3000\n",
      "2370/2370 [==============================] - 0s 28us/step - loss: 0.0339 - val_loss: 0.0593\n",
      "Epoch 1783/3000\n",
      "2370/2370 [==============================] - 0s 28us/step - loss: 0.0386 - val_loss: 0.0422\n",
      "Epoch 1784/3000\n",
      "2370/2370 [==============================] - 0s 32us/step - loss: 0.0391 - val_loss: 0.0415\n",
      "Epoch 1785/3000\n",
      "2370/2370 [==============================] - 0s 28us/step - loss: 0.0385 - val_loss: 0.0430\n",
      "Epoch 1786/3000\n",
      "2370/2370 [==============================] - 0s 28us/step - loss: 0.0366 - val_loss: 0.0423\n",
      "Epoch 1787/3000\n",
      "2370/2370 [==============================] - 0s 28us/step - loss: 0.0386 - val_loss: 0.0378\n",
      "Epoch 1788/3000\n",
      "2370/2370 [==============================] - 0s 28us/step - loss: 0.0394 - val_loss: 0.0447\n",
      "Epoch 1789/3000\n",
      "2370/2370 [==============================] - 0s 28us/step - loss: 0.0349 - val_loss: 0.0393\n",
      "Epoch 1790/3000\n",
      "2370/2370 [==============================] - 0s 28us/step - loss: 0.0404 - val_loss: 0.0490\n",
      "Epoch 1791/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0337 - val_loss: 0.0503\n",
      "Epoch 1792/3000\n",
      "2370/2370 [==============================] - 0s 28us/step - loss: 0.0422 - val_loss: 0.0429\n",
      "Epoch 1793/3000\n",
      "2370/2370 [==============================] - 0s 28us/step - loss: 0.0388 - val_loss: 0.0654\n",
      "Epoch 1794/3000\n",
      "2370/2370 [==============================] - 0s 28us/step - loss: 0.0360 - val_loss: 0.0728\n",
      "Epoch 1795/3000\n",
      "2370/2370 [==============================] - 0s 28us/step - loss: 0.0369 - val_loss: 0.0557\n",
      "Epoch 1796/3000\n",
      "2370/2370 [==============================] - 0s 28us/step - loss: 0.0390 - val_loss: 0.0442\n",
      "Epoch 1797/3000\n",
      "2370/2370 [==============================] - 0s 28us/step - loss: 0.0373 - val_loss: 0.0476\n",
      "Epoch 1798/3000\n",
      "2370/2370 [==============================] - 0s 28us/step - loss: 0.0363 - val_loss: 0.0495\n",
      "Epoch 1799/3000\n",
      "2370/2370 [==============================] - 0s 28us/step - loss: 0.0362 - val_loss: 0.0409\n",
      "Epoch 1800/3000\n",
      "2370/2370 [==============================] - 0s 28us/step - loss: 0.0376 - val_loss: 0.0495\n",
      "Epoch 1801/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0352 - val_loss: 0.0449\n",
      "Epoch 1802/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0413 - val_loss: 0.0648\n",
      "Epoch 1803/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0398 - val_loss: 0.0503\n",
      "Epoch 1804/3000\n",
      "2370/2370 [==============================] - 0s 30us/step - loss: 0.0369 - val_loss: 0.0501\n",
      "Epoch 1805/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0400 - val_loss: 0.0489\n",
      "Epoch 1806/3000\n",
      "2370/2370 [==============================] - 0s 30us/step - loss: 0.0385 - val_loss: 0.0491\n",
      "Epoch 1807/3000\n",
      "2370/2370 [==============================] - 0s 30us/step - loss: 0.0399 - val_loss: 0.0548\n",
      "Epoch 1808/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0345 - val_loss: 0.0677\n",
      "Epoch 1809/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0395 - val_loss: 0.0508\n",
      "Epoch 1810/3000\n",
      "2370/2370 [==============================] - 0s 34us/step - loss: 0.0406 - val_loss: 0.0486\n",
      "Epoch 1811/3000\n",
      "2370/2370 [==============================] - 0s 34us/step - loss: 0.0374 - val_loss: 0.0411\n",
      "Epoch 1812/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0334 - val_loss: 0.0483\n",
      "Epoch 1813/3000\n",
      "2370/2370 [==============================] - 0s 28us/step - loss: 0.0391 - val_loss: 0.0415\n",
      "Epoch 1814/3000\n",
      "2370/2370 [==============================] - 0s 28us/step - loss: 0.0382 - val_loss: 0.0379\n",
      "Epoch 1815/3000\n",
      "2370/2370 [==============================] - 0s 28us/step - loss: 0.0358 - val_loss: 0.0461\n",
      "Epoch 1816/3000\n",
      "2370/2370 [==============================] - 0s 28us/step - loss: 0.0379 - val_loss: 0.0485\n",
      "Epoch 1817/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0355 - val_loss: 0.0496\n",
      "Epoch 1818/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0407 - val_loss: 0.0387\n",
      "Epoch 1819/3000\n",
      "2370/2370 [==============================] - 0s 28us/step - loss: 0.0393 - val_loss: 0.0587\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1820/3000\n",
      "2370/2370 [==============================] - 0s 28us/step - loss: 0.0364 - val_loss: 0.0470\n",
      "Epoch 1821/3000\n",
      "2370/2370 [==============================] - 0s 28us/step - loss: 0.0391 - val_loss: 0.0464\n",
      "Epoch 1822/3000\n",
      "2370/2370 [==============================] - 0s 28us/step - loss: 0.0355 - val_loss: 0.0529\n",
      "Epoch 1823/3000\n",
      "2370/2370 [==============================] - 0s 28us/step - loss: 0.0357 - val_loss: 0.0459\n",
      "Epoch 1824/3000\n",
      "2370/2370 [==============================] - 0s 33us/step - loss: 0.0337 - val_loss: 0.0542\n",
      "Epoch 1825/3000\n",
      "2370/2370 [==============================] - 0s 40us/step - loss: 0.0385 - val_loss: 0.0614\n",
      "Epoch 1826/3000\n",
      "2370/2370 [==============================] - 0s 36us/step - loss: 0.0343 - val_loss: 0.0418\n",
      "Epoch 1827/3000\n",
      "2370/2370 [==============================] - 0s 30us/step - loss: 0.0355 - val_loss: 0.0406\n",
      "Epoch 1828/3000\n",
      "2370/2370 [==============================] - 0s 32us/step - loss: 0.0380 - val_loss: 0.0421\n",
      "Epoch 1829/3000\n",
      "2370/2370 [==============================] - 0s 31us/step - loss: 0.0374 - val_loss: 0.0610\n",
      "Epoch 1830/3000\n",
      "2370/2370 [==============================] - 0s 31us/step - loss: 0.0396 - val_loss: 0.0420\n",
      "Epoch 1831/3000\n",
      "2370/2370 [==============================] - 0s 31us/step - loss: 0.0375 - val_loss: 0.0409\n",
      "Epoch 1832/3000\n",
      "2370/2370 [==============================] - 0s 34us/step - loss: 0.0351 - val_loss: 0.0398\n",
      "Epoch 1833/3000\n",
      "2370/2370 [==============================] - 0s 32us/step - loss: 0.0348 - val_loss: 0.0460\n",
      "Epoch 1834/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0352 - val_loss: 0.0530\n",
      "Epoch 1835/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0387 - val_loss: 0.0658\n",
      "Epoch 1836/3000\n",
      "2370/2370 [==============================] - 0s 28us/step - loss: 0.0349 - val_loss: 0.0452\n",
      "Epoch 1837/3000\n",
      "2370/2370 [==============================] - 0s 28us/step - loss: 0.0404 - val_loss: 0.0475\n",
      "Epoch 1838/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0350 - val_loss: 0.0480\n",
      "Epoch 1839/3000\n",
      "2370/2370 [==============================] - 0s 28us/step - loss: 0.0353 - val_loss: 0.0466\n",
      "Epoch 1840/3000\n",
      "2370/2370 [==============================] - 0s 28us/step - loss: 0.0363 - val_loss: 0.0445\n",
      "Epoch 1841/3000\n",
      "2370/2370 [==============================] - 0s 28us/step - loss: 0.0342 - val_loss: 0.0560\n",
      "Epoch 1842/3000\n",
      "2370/2370 [==============================] - 0s 28us/step - loss: 0.0380 - val_loss: 0.0448\n",
      "Epoch 1843/3000\n",
      "2370/2370 [==============================] - 0s 28us/step - loss: 0.0407 - val_loss: 0.0534\n",
      "Epoch 1844/3000\n",
      "2370/2370 [==============================] - 0s 28us/step - loss: 0.0392 - val_loss: 0.0533\n",
      "Epoch 1845/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0395 - val_loss: 0.0445\n",
      "Epoch 1846/3000\n",
      "2370/2370 [==============================] - 0s 28us/step - loss: 0.0367 - val_loss: 0.0452\n",
      "Epoch 1847/3000\n",
      "2370/2370 [==============================] - 0s 28us/step - loss: 0.0366 - val_loss: 0.0451\n",
      "Epoch 1848/3000\n",
      "2370/2370 [==============================] - 0s 28us/step - loss: 0.0352 - val_loss: 0.0415\n",
      "Epoch 1849/3000\n",
      "2370/2370 [==============================] - 0s 28us/step - loss: 0.0352 - val_loss: 0.0444\n",
      "Epoch 1850/3000\n",
      "2370/2370 [==============================] - 0s 28us/step - loss: 0.0389 - val_loss: 0.0579\n",
      "Epoch 1851/3000\n",
      "2370/2370 [==============================] - 0s 28us/step - loss: 0.0399 - val_loss: 0.0481\n",
      "Epoch 1852/3000\n",
      "2370/2370 [==============================] - 0s 28us/step - loss: 0.0391 - val_loss: 0.0496\n",
      "Epoch 1853/3000\n",
      "2370/2370 [==============================] - 0s 30us/step - loss: 0.0367 - val_loss: 0.0434\n",
      "Epoch 1854/3000\n",
      "2370/2370 [==============================] - 0s 32us/step - loss: 0.0353 - val_loss: 0.0407\n",
      "Epoch 1855/3000\n",
      "2370/2370 [==============================] - 0s 28us/step - loss: 0.0367 - val_loss: 0.0454\n",
      "Epoch 1856/3000\n",
      "2370/2370 [==============================] - 0s 28us/step - loss: 0.0384 - val_loss: 0.0424\n",
      "Epoch 1857/3000\n",
      "2370/2370 [==============================] - 0s 28us/step - loss: 0.0383 - val_loss: 0.0452\n",
      "Epoch 1858/3000\n",
      "2370/2370 [==============================] - 0s 28us/step - loss: 0.0340 - val_loss: 0.0565\n",
      "Epoch 1859/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0372 - val_loss: 0.0409\n",
      "Epoch 1860/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0361 - val_loss: 0.0461\n",
      "Epoch 1861/3000\n",
      "2370/2370 [==============================] - 0s 28us/step - loss: 0.0389 - val_loss: 0.0424\n",
      "Epoch 1862/3000\n",
      "2370/2370 [==============================] - 0s 28us/step - loss: 0.0377 - val_loss: 0.0407\n",
      "Epoch 1863/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0396 - val_loss: 0.0398\n",
      "Epoch 1864/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0363 - val_loss: 0.0392\n",
      "Epoch 1865/3000\n",
      "2370/2370 [==============================] - 0s 28us/step - loss: 0.0360 - val_loss: 0.0394\n",
      "Epoch 1866/3000\n",
      "2370/2370 [==============================] - 0s 28us/step - loss: 0.0381 - val_loss: 0.0404\n",
      "Epoch 1867/3000\n",
      "2370/2370 [==============================] - 0s 28us/step - loss: 0.0358 - val_loss: 0.0541\n",
      "Epoch 1868/3000\n",
      "2370/2370 [==============================] - 0s 28us/step - loss: 0.0388 - val_loss: 0.0424\n",
      "Epoch 1869/3000\n",
      "2370/2370 [==============================] - 0s 28us/step - loss: 0.0374 - val_loss: 0.0401\n",
      "Epoch 1870/3000\n",
      "2370/2370 [==============================] - 0s 28us/step - loss: 0.0362 - val_loss: 0.0400\n",
      "Epoch 1871/3000\n",
      "2370/2370 [==============================] - 0s 28us/step - loss: 0.0352 - val_loss: 0.0383\n",
      "Epoch 1872/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0388 - val_loss: 0.0574\n",
      "Epoch 1873/3000\n",
      "2370/2370 [==============================] - 0s 27us/step - loss: 0.0352 - val_loss: 0.0559\n",
      "Epoch 1874/3000\n",
      "2370/2370 [==============================] - 0s 28us/step - loss: 0.0392 - val_loss: 0.0438\n",
      "Epoch 1875/3000\n",
      "2370/2370 [==============================] - 0s 28us/step - loss: 0.0363 - val_loss: 0.0462\n",
      "Epoch 1876/3000\n",
      "2370/2370 [==============================] - 0s 31us/step - loss: 0.0366 - val_loss: 0.0456\n",
      "Epoch 1877/3000\n",
      "2370/2370 [==============================] - 0s 36us/step - loss: 0.0362 - val_loss: 0.0449\n",
      "Epoch 1878/3000\n",
      "2370/2370 [==============================] - 0s 36us/step - loss: 0.0352 - val_loss: 0.0600\n",
      "Epoch 1879/3000\n",
      "2370/2370 [==============================] - 0s 32us/step - loss: 0.0363 - val_loss: 0.0590\n",
      "Epoch 1880/3000\n",
      "2370/2370 [==============================] - 0s 30us/step - loss: 0.0393 - val_loss: 0.0420\n",
      "Epoch 1881/3000\n",
      "2370/2370 [==============================] - 0s 30us/step - loss: 0.0403 - val_loss: 0.0606\n",
      "Epoch 1882/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0374 - val_loss: 0.0460\n",
      "Epoch 1883/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0366 - val_loss: 0.0401\n",
      "Epoch 1884/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0354 - val_loss: 0.0404\n",
      "Epoch 1885/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0342 - val_loss: 0.0472\n",
      "Epoch 1886/3000\n",
      "2370/2370 [==============================] - 0s 30us/step - loss: 0.0379 - val_loss: 0.0413\n",
      "Epoch 1887/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0369 - val_loss: 0.0407\n",
      "Epoch 1888/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0360 - val_loss: 0.0595\n",
      "Epoch 1889/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0368 - val_loss: 0.0411\n",
      "Epoch 1890/3000\n",
      "2370/2370 [==============================] - 0s 30us/step - loss: 0.0364 - val_loss: 0.0530\n",
      "Epoch 1891/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0389 - val_loss: 0.0427\n",
      "Epoch 1892/3000\n",
      "2370/2370 [==============================] - 0s 30us/step - loss: 0.0363 - val_loss: 0.0445\n",
      "Epoch 1893/3000\n",
      "2370/2370 [==============================] - 0s 30us/step - loss: 0.0395 - val_loss: 0.0379\n",
      "Epoch 1894/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0398 - val_loss: 0.0401\n",
      "Epoch 1895/3000\n",
      "2370/2370 [==============================] - 0s 30us/step - loss: 0.0367 - val_loss: 0.0448\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1896/3000\n",
      "2370/2370 [==============================] - 0s 32us/step - loss: 0.0355 - val_loss: 0.0368\n",
      "Epoch 1897/3000\n",
      "2370/2370 [==============================] - 0s 32us/step - loss: 0.0386 - val_loss: 0.0371\n",
      "Epoch 1898/3000\n",
      "2370/2370 [==============================] - 0s 30us/step - loss: 0.0382 - val_loss: 0.0563\n",
      "Epoch 1899/3000\n",
      "2370/2370 [==============================] - 0s 30us/step - loss: 0.0378 - val_loss: 0.0379\n",
      "Epoch 1900/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0385 - val_loss: 0.0401\n",
      "Epoch 1901/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0385 - val_loss: 0.0524\n",
      "Epoch 1902/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0364 - val_loss: 0.0442\n",
      "Epoch 1903/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0364 - val_loss: 0.0381\n",
      "Epoch 1904/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0384 - val_loss: 0.0463\n",
      "Epoch 1905/3000\n",
      "2370/2370 [==============================] - 0s 30us/step - loss: 0.0379 - val_loss: 0.0467\n",
      "Epoch 1906/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0387 - val_loss: 0.0398\n",
      "Epoch 1907/3000\n",
      "2370/2370 [==============================] - 0s 30us/step - loss: 0.0397 - val_loss: 0.0463\n",
      "Epoch 1908/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0374 - val_loss: 0.0390\n",
      "Epoch 1909/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0390 - val_loss: 0.0367\n",
      "Epoch 1910/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0389 - val_loss: 0.0370\n",
      "Epoch 1911/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0371 - val_loss: 0.0371\n",
      "Epoch 1912/3000\n",
      "2370/2370 [==============================] - 0s 30us/step - loss: 0.0379 - val_loss: 0.0434\n",
      "Epoch 1913/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0374 - val_loss: 0.0390\n",
      "Epoch 1914/3000\n",
      "2370/2370 [==============================] - 0s 30us/step - loss: 0.0359 - val_loss: 0.0474\n",
      "Epoch 1915/3000\n",
      "2370/2370 [==============================] - 0s 30us/step - loss: 0.0406 - val_loss: 0.0468\n",
      "Epoch 1916/3000\n",
      "2370/2370 [==============================] - 0s 30us/step - loss: 0.0390 - val_loss: 0.0385\n",
      "Epoch 1917/3000\n",
      "2370/2370 [==============================] - 0s 30us/step - loss: 0.0330 - val_loss: 0.0427\n",
      "Epoch 1918/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0350 - val_loss: 0.0488\n",
      "Epoch 1919/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0327 - val_loss: 0.0414\n",
      "Epoch 1920/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0363 - val_loss: 0.0504\n",
      "Epoch 1921/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0351 - val_loss: 0.0432\n",
      "Epoch 1922/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0386 - val_loss: 0.0410\n",
      "Epoch 1923/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0393 - val_loss: 0.0527\n",
      "Epoch 1924/3000\n",
      "2370/2370 [==============================] - 0s 30us/step - loss: 0.0385 - val_loss: 0.0470\n",
      "Epoch 1925/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0391 - val_loss: 0.0593\n",
      "Epoch 1926/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0407 - val_loss: 0.0520\n",
      "Epoch 1927/3000\n",
      "2370/2370 [==============================] - 0s 30us/step - loss: 0.0410 - val_loss: 0.0435\n",
      "Epoch 1928/3000\n",
      "2370/2370 [==============================] - 0s 30us/step - loss: 0.0362 - val_loss: 0.0476\n",
      "Epoch 1929/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0367 - val_loss: 0.0498\n",
      "Epoch 1930/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0386 - val_loss: 0.0407\n",
      "Epoch 1931/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0374 - val_loss: 0.0436\n",
      "Epoch 1932/3000\n",
      "2370/2370 [==============================] - 0s 30us/step - loss: 0.0392 - val_loss: 0.0468\n",
      "Epoch 1933/3000\n",
      "2370/2370 [==============================] - 0s 30us/step - loss: 0.0406 - val_loss: 0.0398\n",
      "Epoch 1934/3000\n",
      "2370/2370 [==============================] - 0s 30us/step - loss: 0.0402 - val_loss: 0.0431\n",
      "Epoch 1935/3000\n",
      "2370/2370 [==============================] - 0s 31us/step - loss: 0.0379 - val_loss: 0.0437\n",
      "Epoch 1936/3000\n",
      "2370/2370 [==============================] - 0s 32us/step - loss: 0.0411 - val_loss: 0.0412\n",
      "Epoch 1937/3000\n",
      "2370/2370 [==============================] - 0s 31us/step - loss: 0.0367 - val_loss: 0.0426\n",
      "Epoch 1938/3000\n",
      "2370/2370 [==============================] - 0s 32us/step - loss: 0.0358 - val_loss: 0.0481\n",
      "Epoch 1939/3000\n",
      "2370/2370 [==============================] - 0s 28us/step - loss: 0.0339 - val_loss: 0.0418\n",
      "Epoch 1940/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0372 - val_loss: 0.0583\n",
      "Epoch 1941/3000\n",
      "2370/2370 [==============================] - 0s 28us/step - loss: 0.0415 - val_loss: 0.0402\n",
      "Epoch 1942/3000\n",
      "2370/2370 [==============================] - 0s 28us/step - loss: 0.0366 - val_loss: 0.0445\n",
      "Epoch 1943/3000\n",
      "2370/2370 [==============================] - 0s 28us/step - loss: 0.0336 - val_loss: 0.0459\n",
      "Epoch 1944/3000\n",
      "2370/2370 [==============================] - 0s 28us/step - loss: 0.0358 - val_loss: 0.0416\n",
      "Epoch 1945/3000\n",
      "2370/2370 [==============================] - 0s 28us/step - loss: 0.0359 - val_loss: 0.0519\n",
      "Epoch 1946/3000\n",
      "2370/2370 [==============================] - 0s 28us/step - loss: 0.0402 - val_loss: 0.0515\n",
      "Epoch 1947/3000\n",
      "2370/2370 [==============================] - 0s 28us/step - loss: 0.0357 - val_loss: 0.0401\n",
      "Epoch 1948/3000\n",
      "2370/2370 [==============================] - 0s 28us/step - loss: 0.0356 - val_loss: 0.0416\n",
      "Epoch 1949/3000\n",
      "2370/2370 [==============================] - 0s 28us/step - loss: 0.0386 - val_loss: 0.0485\n",
      "Epoch 1950/3000\n",
      "2370/2370 [==============================] - 0s 28us/step - loss: 0.0392 - val_loss: 0.0456\n",
      "Epoch 1951/3000\n",
      "2370/2370 [==============================] - 0s 28us/step - loss: 0.0420 - val_loss: 0.0475\n",
      "Epoch 1952/3000\n",
      "2370/2370 [==============================] - 0s 28us/step - loss: 0.0384 - val_loss: 0.0465\n",
      "Epoch 1953/3000\n",
      "2370/2370 [==============================] - 0s 28us/step - loss: 0.0389 - val_loss: 0.0922\n",
      "Epoch 1954/3000\n",
      "2370/2370 [==============================] - 0s 28us/step - loss: 0.0414 - val_loss: 0.0601\n",
      "Epoch 1955/3000\n",
      "2370/2370 [==============================] - 0s 28us/step - loss: 0.0330 - val_loss: 0.0537\n",
      "Epoch 1956/3000\n",
      "2370/2370 [==============================] - 0s 28us/step - loss: 0.0370 - val_loss: 0.0525\n",
      "Epoch 1957/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0337 - val_loss: 0.0406\n",
      "Epoch 1958/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0367 - val_loss: 0.0427\n",
      "Epoch 1959/3000\n",
      "2370/2370 [==============================] - 0s 28us/step - loss: 0.0379 - val_loss: 0.0429\n",
      "Epoch 1960/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0367 - val_loss: 0.0431\n",
      "Epoch 1961/3000\n",
      "2370/2370 [==============================] - 0s 28us/step - loss: 0.0368 - val_loss: 0.0404\n",
      "Epoch 1962/3000\n",
      "2370/2370 [==============================] - 0s 31us/step - loss: 0.0383 - val_loss: 0.0387\n",
      "Epoch 1963/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0383 - val_loss: 0.0410\n",
      "Epoch 1964/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0416 - val_loss: 0.0481\n",
      "Epoch 1965/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0382 - val_loss: 0.0445\n",
      "Epoch 1966/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0380 - val_loss: 0.0411\n",
      "Epoch 1967/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0390 - val_loss: 0.0423\n",
      "Epoch 1968/3000\n",
      "2370/2370 [==============================] - 0s 33us/step - loss: 0.0385 - val_loss: 0.0467\n",
      "Epoch 1969/3000\n",
      "2370/2370 [==============================] - 0s 28us/step - loss: 0.0356 - val_loss: 0.0534\n",
      "Epoch 1970/3000\n",
      "2370/2370 [==============================] - 0s 30us/step - loss: 0.0363 - val_loss: 0.0537\n",
      "Epoch 1971/3000\n",
      "2370/2370 [==============================] - 0s 28us/step - loss: 0.0369 - val_loss: 0.0395\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1972/3000\n",
      "2370/2370 [==============================] - 0s 28us/step - loss: 0.0373 - val_loss: 0.0583\n",
      "Epoch 1973/3000\n",
      "2370/2370 [==============================] - 0s 28us/step - loss: 0.0375 - val_loss: 0.0519\n",
      "Epoch 1974/3000\n",
      "2370/2370 [==============================] - 0s 30us/step - loss: 0.0379 - val_loss: 0.0505\n",
      "Epoch 1975/3000\n",
      "2370/2370 [==============================] - 0s 31us/step - loss: 0.0378 - val_loss: 0.0399\n",
      "Epoch 1976/3000\n",
      "2370/2370 [==============================] - 0s 28us/step - loss: 0.0317 - val_loss: 0.0508\n",
      "Epoch 1977/3000\n",
      "2370/2370 [==============================] - 0s 28us/step - loss: 0.0385 - val_loss: 0.0415\n",
      "Epoch 1978/3000\n",
      "2370/2370 [==============================] - 0s 28us/step - loss: 0.0383 - val_loss: 0.0449\n",
      "Epoch 1979/3000\n",
      "2370/2370 [==============================] - 0s 28us/step - loss: 0.0359 - val_loss: 0.0431\n",
      "Epoch 1980/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0392 - val_loss: 0.0426\n",
      "Epoch 1981/3000\n",
      "2370/2370 [==============================] - 0s 31us/step - loss: 0.0354 - val_loss: 0.0400\n",
      "Epoch 1982/3000\n",
      "2370/2370 [==============================] - 0s 35us/step - loss: 0.0404 - val_loss: 0.0448\n",
      "Epoch 1983/3000\n",
      "2370/2370 [==============================] - 0s 34us/step - loss: 0.0386 - val_loss: 0.0458\n",
      "Epoch 1984/3000\n",
      "2370/2370 [==============================] - 0s 34us/step - loss: 0.0383 - val_loss: 0.0504\n",
      "Epoch 1985/3000\n",
      "2370/2370 [==============================] - 0s 35us/step - loss: 0.0396 - val_loss: 0.0400\n",
      "Epoch 1986/3000\n",
      "2370/2370 [==============================] - 0s 32us/step - loss: 0.0382 - val_loss: 0.0403\n",
      "Epoch 1987/3000\n",
      "2370/2370 [==============================] - 0s 47us/step - loss: 0.0354 - val_loss: 0.0383\n",
      "Epoch 1988/3000\n",
      "2370/2370 [==============================] - 0s 78us/step - loss: 0.0403 - val_loss: 0.0385\n",
      "Epoch 1989/3000\n",
      "2370/2370 [==============================] - 0s 30us/step - loss: 0.0359 - val_loss: 0.0568\n",
      "Epoch 1990/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0383 - val_loss: 0.0464\n",
      "Epoch 1991/3000\n",
      "2370/2370 [==============================] - 0s 57us/step - loss: 0.0396 - val_loss: 0.0394\n",
      "Epoch 1992/3000\n",
      "2370/2370 [==============================] - 0s 57us/step - loss: 0.0332 - val_loss: 0.0495\n",
      "Epoch 1993/3000\n",
      "2370/2370 [==============================] - 0s 33us/step - loss: 0.0364 - val_loss: 0.0451\n",
      "Epoch 1994/3000\n",
      "2370/2370 [==============================] - 0s 70us/step - loss: 0.0394 - val_loss: 0.0439\n",
      "Epoch 1995/3000\n",
      "2370/2370 [==============================] - 0s 58us/step - loss: 0.0353 - val_loss: 0.0409\n",
      "Epoch 1996/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0353 - val_loss: 0.0697\n",
      "Epoch 1997/3000\n",
      "2370/2370 [==============================] - 0s 84us/step - loss: 0.0376 - val_loss: 0.0744\n",
      "Epoch 1998/3000\n",
      "2370/2370 [==============================] - 0s 40us/step - loss: 0.0392 - val_loss: 0.0555\n",
      "Epoch 1999/3000\n",
      "2370/2370 [==============================] - 0s 101us/step - loss: 0.0400 - val_loss: 0.0498\n",
      "Epoch 2000/3000\n",
      "2370/2370 [==============================] - 0s 59us/step - loss: 0.0368 - val_loss: 0.0511\n",
      "Epoch 2001/3000\n",
      "2370/2370 [==============================] - 0s 31us/step - loss: 0.0390 - val_loss: 0.0398\n",
      "Epoch 2002/3000\n",
      "2370/2370 [==============================] - 0s 73us/step - loss: 0.0416 - val_loss: 0.0435\n",
      "Epoch 2003/3000\n",
      "2370/2370 [==============================] - 0s 64us/step - loss: 0.0382 - val_loss: 0.0443\n",
      "Epoch 2004/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0356 - val_loss: 0.0525\n",
      "Epoch 2005/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0377 - val_loss: 0.0486\n",
      "Epoch 2006/3000\n",
      "2370/2370 [==============================] - 0s 28us/step - loss: 0.0357 - val_loss: 0.0489\n",
      "Epoch 2007/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0405 - val_loss: 0.0423\n",
      "Epoch 2008/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0366 - val_loss: 0.0445\n",
      "Epoch 2009/3000\n",
      "2370/2370 [==============================] - 0s 28us/step - loss: 0.0394 - val_loss: 0.0444\n",
      "Epoch 2010/3000\n",
      "2370/2370 [==============================] - 0s 28us/step - loss: 0.0367 - val_loss: 0.0465\n",
      "Epoch 2011/3000\n",
      "2370/2370 [==============================] - 0s 28us/step - loss: 0.0377 - val_loss: 0.0706\n",
      "Epoch 2012/3000\n",
      "2370/2370 [==============================] - 0s 31us/step - loss: 0.0346 - val_loss: 0.0435\n",
      "Epoch 2013/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0362 - val_loss: 0.0474\n",
      "Epoch 2014/3000\n",
      "2370/2370 [==============================] - 0s 28us/step - loss: 0.0359 - val_loss: 0.0409\n",
      "Epoch 2015/3000\n",
      "2370/2370 [==============================] - 0s 28us/step - loss: 0.0359 - val_loss: 0.0484\n",
      "Epoch 2016/3000\n",
      "2370/2370 [==============================] - 0s 28us/step - loss: 0.0387 - val_loss: 0.0467\n",
      "Epoch 2017/3000\n",
      "2370/2370 [==============================] - 0s 28us/step - loss: 0.0392 - val_loss: 0.0375\n",
      "Epoch 2018/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0365 - val_loss: 0.0413\n",
      "Epoch 2019/3000\n",
      "2370/2370 [==============================] - 0s 28us/step - loss: 0.0392 - val_loss: 0.0422\n",
      "Epoch 2020/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0368 - val_loss: 0.0396\n",
      "Epoch 2021/3000\n",
      "2370/2370 [==============================] - 0s 28us/step - loss: 0.0363 - val_loss: 0.0390\n",
      "Epoch 2022/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0349 - val_loss: 0.0380\n",
      "Epoch 2023/3000\n",
      "2370/2370 [==============================] - 0s 28us/step - loss: 0.0375 - val_loss: 0.0399\n",
      "Epoch 2024/3000\n",
      "2370/2370 [==============================] - 0s 28us/step - loss: 0.0373 - val_loss: 0.0441\n",
      "Epoch 2025/3000\n",
      "2370/2370 [==============================] - 0s 28us/step - loss: 0.0331 - val_loss: 0.0395\n",
      "Epoch 2026/3000\n",
      "2370/2370 [==============================] - 0s 28us/step - loss: 0.0406 - val_loss: 0.0507\n",
      "Epoch 2027/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0359 - val_loss: 0.0441\n",
      "Epoch 2028/3000\n",
      "2370/2370 [==============================] - 0s 28us/step - loss: 0.0353 - val_loss: 0.0377\n",
      "Epoch 2029/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0372 - val_loss: 0.0413\n",
      "Epoch 2030/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0401 - val_loss: 0.0398\n",
      "Epoch 2031/3000\n",
      "2370/2370 [==============================] - 0s 30us/step - loss: 0.0391 - val_loss: 0.0455\n",
      "Epoch 2032/3000\n",
      "2370/2370 [==============================] - 0s 28us/step - loss: 0.0396 - val_loss: 0.0445\n",
      "Epoch 2033/3000\n",
      "2370/2370 [==============================] - 0s 28us/step - loss: 0.0359 - val_loss: 0.0409\n",
      "Epoch 2034/3000\n",
      "2370/2370 [==============================] - 0s 28us/step - loss: 0.0388 - val_loss: 0.0560\n",
      "Epoch 2035/3000\n",
      "2370/2370 [==============================] - 0s 28us/step - loss: 0.0375 - val_loss: 0.0461\n",
      "Epoch 2036/3000\n",
      "2370/2370 [==============================] - 0s 28us/step - loss: 0.0380 - val_loss: 0.0418\n",
      "Epoch 2037/3000\n",
      "2370/2370 [==============================] - 0s 28us/step - loss: 0.0349 - val_loss: 0.0519\n",
      "Epoch 2038/3000\n",
      "2370/2370 [==============================] - 0s 32us/step - loss: 0.0372 - val_loss: 0.0464\n",
      "Epoch 2039/3000\n",
      "2370/2370 [==============================] - 0s 30us/step - loss: 0.0410 - val_loss: 0.0435\n",
      "Epoch 2040/3000\n",
      "2370/2370 [==============================] - 0s 32us/step - loss: 0.0374 - val_loss: 0.0412\n",
      "Epoch 2041/3000\n",
      "2370/2370 [==============================] - 0s 91us/step - loss: 0.0372 - val_loss: 0.0509\n",
      "Epoch 2042/3000\n",
      "2370/2370 [==============================] - 0s 41us/step - loss: 0.0353 - val_loss: 0.0382\n",
      "Epoch 2043/3000\n",
      "2370/2370 [==============================] - 0s 32us/step - loss: 0.0379 - val_loss: 0.0486\n",
      "Epoch 2044/3000\n",
      "2370/2370 [==============================] - 0s 48us/step - loss: 0.0387 - val_loss: 0.0401\n",
      "Epoch 2045/3000\n",
      "2370/2370 [==============================] - 0s 72us/step - loss: 0.0370 - val_loss: 0.0414\n",
      "Epoch 2046/3000\n",
      "2370/2370 [==============================] - 0s 37us/step - loss: 0.0385 - val_loss: 0.0458\n",
      "Epoch 2047/3000\n",
      "2370/2370 [==============================] - 0s 32us/step - loss: 0.0380 - val_loss: 0.0447\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2048/3000\n",
      "2370/2370 [==============================] - 0s 36us/step - loss: 0.0371 - val_loss: 0.0397\n",
      "Epoch 2049/3000\n",
      "2370/2370 [==============================] - 0s 33us/step - loss: 0.0394 - val_loss: 0.0403\n",
      "Epoch 2050/3000\n",
      "2370/2370 [==============================] - 0s 33us/step - loss: 0.0386 - val_loss: 0.0433\n",
      "Epoch 2051/3000\n",
      "2370/2370 [==============================] - 0s 39us/step - loss: 0.0364 - val_loss: 0.0463\n",
      "Epoch 2052/3000\n",
      "2370/2370 [==============================] - 0s 87us/step - loss: 0.0373 - val_loss: 0.0461\n",
      "Epoch 2053/3000\n",
      "2370/2370 [==============================] - 0s 70us/step - loss: 0.0397 - val_loss: 0.0435\n",
      "Epoch 2054/3000\n",
      "2370/2370 [==============================] - 0s 28us/step - loss: 0.0380 - val_loss: 0.0477\n",
      "Epoch 2055/3000\n",
      "2370/2370 [==============================] - 0s 28us/step - loss: 0.0398 - val_loss: 0.0401\n",
      "Epoch 2056/3000\n",
      "2370/2370 [==============================] - 0s 28us/step - loss: 0.0378 - val_loss: 0.0426\n",
      "Epoch 2057/3000\n",
      "2370/2370 [==============================] - 0s 28us/step - loss: 0.0376 - val_loss: 0.0523\n",
      "Epoch 2058/3000\n",
      "2370/2370 [==============================] - 0s 28us/step - loss: 0.0393 - val_loss: 0.0371\n",
      "Epoch 2059/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0374 - val_loss: 0.0519\n",
      "Epoch 2060/3000\n",
      "2370/2370 [==============================] - 0s 28us/step - loss: 0.0367 - val_loss: 0.0483\n",
      "Epoch 2061/3000\n",
      "2370/2370 [==============================] - 0s 28us/step - loss: 0.0362 - val_loss: 0.0413\n",
      "Epoch 2062/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0367 - val_loss: 0.0431\n",
      "Epoch 2063/3000\n",
      "2370/2370 [==============================] - 0s 28us/step - loss: 0.0417 - val_loss: 0.0366\n",
      "Epoch 2064/3000\n",
      "2370/2370 [==============================] - 0s 28us/step - loss: 0.0375 - val_loss: 0.0653\n",
      "Epoch 2065/3000\n",
      "2370/2370 [==============================] - 0s 28us/step - loss: 0.0394 - val_loss: 0.0376\n",
      "Epoch 2066/3000\n",
      "2370/2370 [==============================] - 0s 28us/step - loss: 0.0390 - val_loss: 0.0491\n",
      "Epoch 2067/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0369 - val_loss: 0.0456\n",
      "Epoch 2068/3000\n",
      "2370/2370 [==============================] - 0s 28us/step - loss: 0.0393 - val_loss: 0.0414\n",
      "Epoch 2069/3000\n",
      "2370/2370 [==============================] - 0s 28us/step - loss: 0.0366 - val_loss: 0.0396\n",
      "Epoch 2070/3000\n",
      "2370/2370 [==============================] - 0s 28us/step - loss: 0.0359 - val_loss: 0.0645\n",
      "Epoch 2071/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0395 - val_loss: 0.0394\n",
      "Epoch 2072/3000\n",
      "2370/2370 [==============================] - 0s 32us/step - loss: 0.0378 - val_loss: 0.0497\n",
      "Epoch 2073/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0358 - val_loss: 0.0502\n",
      "Epoch 2074/3000\n",
      "2370/2370 [==============================] - 0s 28us/step - loss: 0.0369 - val_loss: 0.0409\n",
      "Epoch 2075/3000\n",
      "2370/2370 [==============================] - 0s 28us/step - loss: 0.0358 - val_loss: 0.0502\n",
      "Epoch 2076/3000\n",
      "2370/2370 [==============================] - 0s 28us/step - loss: 0.0370 - val_loss: 0.0431\n",
      "Epoch 2077/3000\n",
      "2370/2370 [==============================] - 0s 28us/step - loss: 0.0414 - val_loss: 0.0546\n",
      "Epoch 2078/3000\n",
      "2370/2370 [==============================] - 0s 28us/step - loss: 0.0344 - val_loss: 0.0536\n",
      "Epoch 2079/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0368 - val_loss: 0.0402\n",
      "Epoch 2080/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0368 - val_loss: 0.0394\n",
      "Epoch 2081/3000\n",
      "2370/2370 [==============================] - 0s 28us/step - loss: 0.0375 - val_loss: 0.0421\n",
      "Epoch 2082/3000\n",
      "2370/2370 [==============================] - 0s 28us/step - loss: 0.0374 - val_loss: 0.0413\n",
      "Epoch 2083/3000\n",
      "2370/2370 [==============================] - 0s 28us/step - loss: 0.0364 - val_loss: 0.0395\n",
      "Epoch 2084/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0387 - val_loss: 0.0398\n",
      "Epoch 2085/3000\n",
      "2370/2370 [==============================] - 0s 30us/step - loss: 0.0354 - val_loss: 0.0539\n",
      "Epoch 2086/3000\n",
      "2370/2370 [==============================] - 0s 30us/step - loss: 0.0367 - val_loss: 0.0403\n",
      "Epoch 2087/3000\n",
      "2370/2370 [==============================] - 0s 28us/step - loss: 0.0334 - val_loss: 0.0428\n",
      "Epoch 2088/3000\n",
      "2370/2370 [==============================] - 0s 30us/step - loss: 0.0358 - val_loss: 0.0407\n",
      "Epoch 2089/3000\n",
      "2370/2370 [==============================] - 0s 28us/step - loss: 0.0361 - val_loss: 0.0454\n",
      "Epoch 2090/3000\n",
      "2370/2370 [==============================] - 0s 28us/step - loss: 0.0378 - val_loss: 0.0367\n",
      "Epoch 2091/3000\n",
      "2370/2370 [==============================] - 0s 28us/step - loss: 0.0398 - val_loss: 0.0379\n",
      "Epoch 2092/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0363 - val_loss: 0.0451\n",
      "Epoch 2093/3000\n",
      "2370/2370 [==============================] - 0s 28us/step - loss: 0.0409 - val_loss: 0.0431\n",
      "Epoch 2094/3000\n",
      "2370/2370 [==============================] - 0s 28us/step - loss: 0.0370 - val_loss: 0.0350\n",
      "Epoch 2095/3000\n",
      "2370/2370 [==============================] - 0s 28us/step - loss: 0.0394 - val_loss: 0.0598\n",
      "Epoch 2096/3000\n",
      "2370/2370 [==============================] - 0s 28us/step - loss: 0.0364 - val_loss: 0.0443\n",
      "Epoch 2097/3000\n",
      "2370/2370 [==============================] - 0s 31us/step - loss: 0.0335 - val_loss: 0.0495\n",
      "Epoch 2098/3000\n",
      "2370/2370 [==============================] - 0s 28us/step - loss: 0.0379 - val_loss: 0.0485\n",
      "Epoch 2099/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0401 - val_loss: 0.0418\n",
      "Epoch 2100/3000\n",
      "2370/2370 [==============================] - 0s 74us/step - loss: 0.0351 - val_loss: 0.0577\n",
      "Epoch 2101/3000\n",
      "2370/2370 [==============================] - 0s 31us/step - loss: 0.0395 - val_loss: 0.0381\n",
      "Epoch 2102/3000\n",
      "2370/2370 [==============================] - 0s 63us/step - loss: 0.0378 - val_loss: 0.0525\n",
      "Epoch 2103/3000\n",
      "2370/2370 [==============================] - 0s 45us/step - loss: 0.0390 - val_loss: 0.0436\n",
      "Epoch 2104/3000\n",
      "2370/2370 [==============================] - 0s 40us/step - loss: 0.0372 - val_loss: 0.0416\n",
      "Epoch 2105/3000\n",
      "2370/2370 [==============================] - 0s 81us/step - loss: 0.0376 - val_loss: 0.0699\n",
      "Epoch 2106/3000\n",
      "2370/2370 [==============================] - 0s 31us/step - loss: 0.0377 - val_loss: 0.0423\n",
      "Epoch 2107/3000\n",
      "2370/2370 [==============================] - 0s 31us/step - loss: 0.0370 - val_loss: 0.0399\n",
      "Epoch 2108/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0388 - val_loss: 0.0475\n",
      "Epoch 2109/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0355 - val_loss: 0.0437\n",
      "Epoch 2110/3000\n",
      "2370/2370 [==============================] - 0s 81us/step - loss: 0.0401 - val_loss: 0.0498\n",
      "Epoch 2111/3000\n",
      "2370/2370 [==============================] - 0s 62us/step - loss: 0.0378 - val_loss: 0.0439\n",
      "Epoch 2112/3000\n",
      "2370/2370 [==============================] - 0s 32us/step - loss: 0.0402 - val_loss: 0.0415\n",
      "Epoch 2113/3000\n",
      "2370/2370 [==============================] - 0s 28us/step - loss: 0.0394 - val_loss: 0.0377\n",
      "Epoch 2114/3000\n",
      "2370/2370 [==============================] - 0s 28us/step - loss: 0.0390 - val_loss: 0.0499\n",
      "Epoch 2115/3000\n",
      "2370/2370 [==============================] - 0s 28us/step - loss: 0.0406 - val_loss: 0.0420\n",
      "Epoch 2116/3000\n",
      "2370/2370 [==============================] - 0s 27us/step - loss: 0.0359 - val_loss: 0.0505\n",
      "Epoch 2117/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0359 - val_loss: 0.0377\n",
      "Epoch 2118/3000\n",
      "2370/2370 [==============================] - 0s 28us/step - loss: 0.0363 - val_loss: 0.0452\n",
      "Epoch 2119/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0392 - val_loss: 0.0543\n",
      "Epoch 2120/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0429 - val_loss: 0.0418\n",
      "Epoch 2121/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0420 - val_loss: 0.0531\n",
      "Epoch 2122/3000\n",
      "2370/2370 [==============================] - 0s 28us/step - loss: 0.0373 - val_loss: 0.0379\n",
      "Epoch 2123/3000\n",
      "2370/2370 [==============================] - 0s 28us/step - loss: 0.0385 - val_loss: 0.0591\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2124/3000\n",
      "2370/2370 [==============================] - 0s 28us/step - loss: 0.0362 - val_loss: 0.0451\n",
      "Epoch 2125/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0393 - val_loss: 0.0554\n",
      "Epoch 2126/3000\n",
      "2370/2370 [==============================] - 0s 28us/step - loss: 0.0375 - val_loss: 0.0378\n",
      "Epoch 2127/3000\n",
      "2370/2370 [==============================] - 0s 28us/step - loss: 0.0408 - val_loss: 0.0479\n",
      "Epoch 2128/3000\n",
      "2370/2370 [==============================] - 0s 28us/step - loss: 0.0377 - val_loss: 0.0406\n",
      "Epoch 2129/3000\n",
      "2370/2370 [==============================] - 0s 30us/step - loss: 0.0349 - val_loss: 0.0402\n",
      "Epoch 2130/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0415 - val_loss: 0.0373\n",
      "Epoch 2131/3000\n",
      "2370/2370 [==============================] - 0s 28us/step - loss: 0.0374 - val_loss: 0.0388\n",
      "Epoch 2132/3000\n",
      "2370/2370 [==============================] - 0s 28us/step - loss: 0.0390 - val_loss: 0.0382\n",
      "Epoch 2133/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0353 - val_loss: 0.0438\n",
      "Epoch 2134/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0367 - val_loss: 0.0405\n",
      "Epoch 2135/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0374 - val_loss: 0.0402\n",
      "Epoch 2136/3000\n",
      "2370/2370 [==============================] - 0s 28us/step - loss: 0.0357 - val_loss: 0.0436\n",
      "Epoch 2137/3000\n",
      "2370/2370 [==============================] - 0s 28us/step - loss: 0.0358 - val_loss: 0.0422\n",
      "Epoch 2138/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0383 - val_loss: 0.0405\n",
      "Epoch 2139/3000\n",
      "2370/2370 [==============================] - 0s 28us/step - loss: 0.0394 - val_loss: 0.0398\n",
      "Epoch 2140/3000\n",
      "2370/2370 [==============================] - 0s 28us/step - loss: 0.0364 - val_loss: 0.0463\n",
      "Epoch 2141/3000\n",
      "2370/2370 [==============================] - 0s 28us/step - loss: 0.0364 - val_loss: 0.0493\n",
      "Epoch 2142/3000\n",
      "2370/2370 [==============================] - 0s 28us/step - loss: 0.0382 - val_loss: 0.0438\n",
      "Epoch 2143/3000\n",
      "2370/2370 [==============================] - 0s 28us/step - loss: 0.0410 - val_loss: 0.0402\n",
      "Epoch 2144/3000\n",
      "2370/2370 [==============================] - 0s 28us/step - loss: 0.0338 - val_loss: 0.0382\n",
      "Epoch 2145/3000\n",
      "2370/2370 [==============================] - 0s 28us/step - loss: 0.0384 - val_loss: 0.0426\n",
      "Epoch 2146/3000\n",
      "2370/2370 [==============================] - 0s 28us/step - loss: 0.0382 - val_loss: 0.0414\n",
      "Epoch 2147/3000\n",
      "2370/2370 [==============================] - 0s 28us/step - loss: 0.0372 - val_loss: 0.0454\n",
      "Epoch 2148/3000\n",
      "2370/2370 [==============================] - 0s 28us/step - loss: 0.0376 - val_loss: 0.0580\n",
      "Epoch 2149/3000\n",
      "2370/2370 [==============================] - 0s 28us/step - loss: 0.0403 - val_loss: 0.0497\n",
      "Epoch 2150/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0377 - val_loss: 0.0484\n",
      "Epoch 2151/3000\n",
      "2370/2370 [==============================] - 0s 33us/step - loss: 0.0405 - val_loss: 0.0458\n",
      "Epoch 2152/3000\n",
      "2370/2370 [==============================] - 0s 28us/step - loss: 0.0403 - val_loss: 0.0468\n",
      "Epoch 2153/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0383 - val_loss: 0.0536\n",
      "Epoch 2154/3000\n",
      "2370/2370 [==============================] - 0s 28us/step - loss: 0.0385 - val_loss: 0.0404\n",
      "Epoch 2155/3000\n",
      "2370/2370 [==============================] - 0s 28us/step - loss: 0.0372 - val_loss: 0.0401\n",
      "Epoch 2156/3000\n",
      "2370/2370 [==============================] - 0s 35us/step - loss: 0.0352 - val_loss: 0.0462\n",
      "Epoch 2157/3000\n",
      "2370/2370 [==============================] - 0s 32us/step - loss: 0.0365 - val_loss: 0.0442\n",
      "Epoch 2158/3000\n",
      "2370/2370 [==============================] - 0s 31us/step - loss: 0.0389 - val_loss: 0.0383\n",
      "Epoch 2159/3000\n",
      "2370/2370 [==============================] - 0s 31us/step - loss: 0.0360 - val_loss: 0.0532\n",
      "Epoch 2160/3000\n",
      "2370/2370 [==============================] - 0s 31us/step - loss: 0.0342 - val_loss: 0.0386\n",
      "Epoch 2161/3000\n",
      "2370/2370 [==============================] - 0s 31us/step - loss: 0.0409 - val_loss: 0.0418\n",
      "Epoch 2162/3000\n",
      "2370/2370 [==============================] - 0s 30us/step - loss: 0.0383 - val_loss: 0.0474\n",
      "Epoch 2163/3000\n",
      "2370/2370 [==============================] - 0s 31us/step - loss: 0.0352 - val_loss: 0.0541\n",
      "Epoch 2164/3000\n",
      "2370/2370 [==============================] - 0s 33us/step - loss: 0.0337 - val_loss: 0.0425\n",
      "Epoch 2165/3000\n",
      "2370/2370 [==============================] - 0s 30us/step - loss: 0.0334 - val_loss: 0.0539\n",
      "Epoch 2166/3000\n",
      "2370/2370 [==============================] - 0s 31us/step - loss: 0.0377 - val_loss: 0.0622\n",
      "Epoch 2167/3000\n",
      "2370/2370 [==============================] - 0s 31us/step - loss: 0.0379 - val_loss: 0.0445\n",
      "Epoch 2168/3000\n",
      "2370/2370 [==============================] - 0s 31us/step - loss: 0.0407 - val_loss: 0.0497\n",
      "Epoch 2169/3000\n",
      "2370/2370 [==============================] - 0s 31us/step - loss: 0.0378 - val_loss: 0.0423\n",
      "Epoch 2170/3000\n",
      "2370/2370 [==============================] - 0s 31us/step - loss: 0.0378 - val_loss: 0.0515\n",
      "Epoch 2171/3000\n",
      "2370/2370 [==============================] - 0s 30us/step - loss: 0.0337 - val_loss: 0.0644\n",
      "Epoch 2172/3000\n",
      "2370/2370 [==============================] - 0s 33us/step - loss: 0.0366 - val_loss: 0.0466\n",
      "Epoch 2173/3000\n",
      "2370/2370 [==============================] - 0s 31us/step - loss: 0.0369 - val_loss: 0.0466\n",
      "Epoch 2174/3000\n",
      "2370/2370 [==============================] - 0s 31us/step - loss: 0.0364 - val_loss: 0.0397\n",
      "Epoch 2175/3000\n",
      "2370/2370 [==============================] - 0s 31us/step - loss: 0.0405 - val_loss: 0.0455\n",
      "Epoch 2176/3000\n",
      "2370/2370 [==============================] - 0s 31us/step - loss: 0.0383 - val_loss: 0.0385\n",
      "Epoch 2177/3000\n",
      "2370/2370 [==============================] - 0s 31us/step - loss: 0.0367 - val_loss: 0.0437\n",
      "Epoch 2178/3000\n",
      "2370/2370 [==============================] - 0s 36us/step - loss: 0.0349 - val_loss: 0.0430\n",
      "Epoch 2179/3000\n",
      "2370/2370 [==============================] - 0s 31us/step - loss: 0.0372 - val_loss: 0.0420\n",
      "Epoch 2180/3000\n",
      "2370/2370 [==============================] - 0s 33us/step - loss: 0.0369 - val_loss: 0.0394\n",
      "Epoch 2181/3000\n",
      "2370/2370 [==============================] - 0s 31us/step - loss: 0.0377 - val_loss: 0.0431\n",
      "Epoch 2182/3000\n",
      "2370/2370 [==============================] - 0s 31us/step - loss: 0.0380 - val_loss: 0.0416\n",
      "Epoch 2183/3000\n",
      "2370/2370 [==============================] - 0s 31us/step - loss: 0.0386 - val_loss: 0.0565\n",
      "Epoch 2184/3000\n",
      "2370/2370 [==============================] - 0s 31us/step - loss: 0.0371 - val_loss: 0.0435\n",
      "Epoch 2185/3000\n",
      "2370/2370 [==============================] - 0s 30us/step - loss: 0.0366 - val_loss: 0.0401\n",
      "Epoch 2186/3000\n",
      "2370/2370 [==============================] - 0s 31us/step - loss: 0.0378 - val_loss: 0.0399\n",
      "Epoch 2187/3000\n",
      "2370/2370 [==============================] - 0s 31us/step - loss: 0.0350 - val_loss: 0.0390\n",
      "Epoch 2188/3000\n",
      "2370/2370 [==============================] - 0s 32us/step - loss: 0.0340 - val_loss: 0.0460\n",
      "Epoch 2189/3000\n",
      "2370/2370 [==============================] - 0s 30us/step - loss: 0.0397 - val_loss: 0.0527\n",
      "Epoch 2190/3000\n",
      "2370/2370 [==============================] - 0s 31us/step - loss: 0.0380 - val_loss: 0.0427\n",
      "Epoch 2191/3000\n",
      "2370/2370 [==============================] - 0s 31us/step - loss: 0.0353 - val_loss: 0.0401\n",
      "Epoch 2192/3000\n",
      "2370/2370 [==============================] - 0s 31us/step - loss: 0.0394 - val_loss: 0.0468\n",
      "Epoch 2193/3000\n",
      "2370/2370 [==============================] - 0s 31us/step - loss: 0.0379 - val_loss: 0.0574\n",
      "Epoch 2194/3000\n",
      "2370/2370 [==============================] - 0s 30us/step - loss: 0.0405 - val_loss: 0.0444\n",
      "Epoch 2195/3000\n",
      "2370/2370 [==============================] - 0s 31us/step - loss: 0.0394 - val_loss: 0.0383\n",
      "Epoch 2196/3000\n",
      "2370/2370 [==============================] - 0s 33us/step - loss: 0.0364 - val_loss: 0.0526\n",
      "Epoch 2197/3000\n",
      "2370/2370 [==============================] - 0s 31us/step - loss: 0.0347 - val_loss: 0.0455\n",
      "Epoch 2198/3000\n",
      "2370/2370 [==============================] - 0s 31us/step - loss: 0.0391 - val_loss: 0.0464\n",
      "Epoch 2199/3000\n",
      "2370/2370 [==============================] - 0s 31us/step - loss: 0.0371 - val_loss: 0.0476\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2200/3000\n",
      "2370/2370 [==============================] - 0s 30us/step - loss: 0.0369 - val_loss: 0.0439\n",
      "Epoch 2201/3000\n",
      "2370/2370 [==============================] - 0s 31us/step - loss: 0.0426 - val_loss: 0.0422\n",
      "Epoch 2202/3000\n",
      "2370/2370 [==============================] - 0s 31us/step - loss: 0.0370 - val_loss: 0.0394\n",
      "Epoch 2203/3000\n",
      "2370/2370 [==============================] - 0s 31us/step - loss: 0.0374 - val_loss: 0.0480\n",
      "Epoch 2204/3000\n",
      "2370/2370 [==============================] - 0s 31us/step - loss: 0.0376 - val_loss: 0.0440\n",
      "Epoch 2205/3000\n",
      "2370/2370 [==============================] - 0s 31us/step - loss: 0.0411 - val_loss: 0.0502\n",
      "Epoch 2206/3000\n",
      "2370/2370 [==============================] - 0s 31us/step - loss: 0.0369 - val_loss: 0.0404\n",
      "Epoch 2207/3000\n",
      "2370/2370 [==============================] - 0s 31us/step - loss: 0.0374 - val_loss: 0.0448\n",
      "Epoch 2208/3000\n",
      "2370/2370 [==============================] - 0s 32us/step - loss: 0.0375 - val_loss: 0.0734\n",
      "Epoch 2209/3000\n",
      "2370/2370 [==============================] - 0s 30us/step - loss: 0.0368 - val_loss: 0.0415\n",
      "Epoch 2210/3000\n",
      "2370/2370 [==============================] - 0s 31us/step - loss: 0.0358 - val_loss: 0.0499\n",
      "Epoch 2211/3000\n",
      "2370/2370 [==============================] - 0s 31us/step - loss: 0.0382 - val_loss: 0.0377\n",
      "Epoch 2212/3000\n",
      "2370/2370 [==============================] - 0s 31us/step - loss: 0.0388 - val_loss: 0.0388\n",
      "Epoch 2213/3000\n",
      "2370/2370 [==============================] - 0s 31us/step - loss: 0.0437 - val_loss: 0.0430\n",
      "Epoch 2214/3000\n",
      "2370/2370 [==============================] - 0s 31us/step - loss: 0.0362 - val_loss: 0.0391\n",
      "Epoch 2215/3000\n",
      "2370/2370 [==============================] - 0s 31us/step - loss: 0.0349 - val_loss: 0.0426\n",
      "Epoch 2216/3000\n",
      "2370/2370 [==============================] - 0s 31us/step - loss: 0.0388 - val_loss: 0.0391\n",
      "Epoch 2217/3000\n",
      "2370/2370 [==============================] - 0s 30us/step - loss: 0.0387 - val_loss: 0.0417\n",
      "Epoch 2218/3000\n",
      "2370/2370 [==============================] - 0s 33us/step - loss: 0.0360 - val_loss: 0.0450\n",
      "Epoch 2219/3000\n",
      "2370/2370 [==============================] - 0s 33us/step - loss: 0.0367 - val_loss: 0.0389\n",
      "Epoch 2220/3000\n",
      "2370/2370 [==============================] - 0s 33us/step - loss: 0.0366 - val_loss: 0.0549\n",
      "Epoch 2221/3000\n",
      "2370/2370 [==============================] - 0s 31us/step - loss: 0.0381 - val_loss: 0.0391\n",
      "Epoch 2222/3000\n",
      "2370/2370 [==============================] - 0s 31us/step - loss: 0.0389 - val_loss: 0.0380\n",
      "Epoch 2223/3000\n",
      "2370/2370 [==============================] - 0s 31us/step - loss: 0.0361 - val_loss: 0.0433\n",
      "Epoch 2224/3000\n",
      "2370/2370 [==============================] - 0s 31us/step - loss: 0.0406 - val_loss: 0.0388\n",
      "Epoch 2225/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0368 - val_loss: 0.0395\n",
      "Epoch 2226/3000\n",
      "2370/2370 [==============================] - 0s 28us/step - loss: 0.0405 - val_loss: 0.0392\n",
      "Epoch 2227/3000\n",
      "2370/2370 [==============================] - 0s 28us/step - loss: 0.0350 - val_loss: 0.0547\n",
      "Epoch 2228/3000\n",
      "2370/2370 [==============================] - 0s 30us/step - loss: 0.0354 - val_loss: 0.0424\n",
      "Epoch 2229/3000\n",
      "2370/2370 [==============================] - 0s 28us/step - loss: 0.0354 - val_loss: 0.0405\n",
      "Epoch 2230/3000\n",
      "2370/2370 [==============================] - 0s 30us/step - loss: 0.0382 - val_loss: 0.0564\n",
      "Epoch 2231/3000\n",
      "2370/2370 [==============================] - 0s 33us/step - loss: 0.0375 - val_loss: 0.0503\n",
      "Epoch 2232/3000\n",
      "2370/2370 [==============================] - 0s 30us/step - loss: 0.0341 - val_loss: 0.0513\n",
      "Epoch 2233/3000\n",
      "2370/2370 [==============================] - 0s 31us/step - loss: 0.0414 - val_loss: 0.0399\n",
      "Epoch 2234/3000\n",
      "2370/2370 [==============================] - 0s 31us/step - loss: 0.0358 - val_loss: 0.0397\n",
      "Epoch 2235/3000\n",
      "2370/2370 [==============================] - 0s 31us/step - loss: 0.0338 - val_loss: 0.0418\n",
      "Epoch 2236/3000\n",
      "2370/2370 [==============================] - 0s 33us/step - loss: 0.0372 - val_loss: 0.0420\n",
      "Epoch 2237/3000\n",
      "2370/2370 [==============================] - 0s 31us/step - loss: 0.0394 - val_loss: 0.0470\n",
      "Epoch 2238/3000\n",
      "2370/2370 [==============================] - 0s 31us/step - loss: 0.0376 - val_loss: 0.0434\n",
      "Epoch 2239/3000\n",
      "2370/2370 [==============================] - 0s 31us/step - loss: 0.0398 - val_loss: 0.0425\n",
      "Epoch 2240/3000\n",
      "2370/2370 [==============================] - 0s 31us/step - loss: 0.0390 - val_loss: 0.0586\n",
      "Epoch 2241/3000\n",
      "2370/2370 [==============================] - 0s 31us/step - loss: 0.0408 - val_loss: 0.0416\n",
      "Epoch 2242/3000\n",
      "2370/2370 [==============================] - 0s 31us/step - loss: 0.0388 - val_loss: 0.0392\n",
      "Epoch 2243/3000\n",
      "2370/2370 [==============================] - 0s 30us/step - loss: 0.0380 - val_loss: 0.0553\n",
      "Epoch 2244/3000\n",
      "2370/2370 [==============================] - 0s 32us/step - loss: 0.0364 - val_loss: 0.0469\n",
      "Epoch 2245/3000\n",
      "2370/2370 [==============================] - 0s 28us/step - loss: 0.0356 - val_loss: 0.0395\n",
      "Epoch 2246/3000\n",
      "2370/2370 [==============================] - 0s 28us/step - loss: 0.0395 - val_loss: 0.0409\n",
      "Epoch 2247/3000\n",
      "2370/2370 [==============================] - 0s 28us/step - loss: 0.0364 - val_loss: 0.0418\n",
      "Epoch 2248/3000\n",
      "2370/2370 [==============================] - 0s 28us/step - loss: 0.0381 - val_loss: 0.0571\n",
      "Epoch 2249/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0349 - val_loss: 0.0380\n",
      "Epoch 2250/3000\n",
      "2370/2370 [==============================] - 0s 28us/step - loss: 0.0421 - val_loss: 0.0516\n",
      "Epoch 2251/3000\n",
      "2370/2370 [==============================] - 0s 28us/step - loss: 0.0379 - val_loss: 0.0404\n",
      "Epoch 2252/3000\n",
      "2370/2370 [==============================] - 0s 28us/step - loss: 0.0348 - val_loss: 0.0632\n",
      "Epoch 2253/3000\n",
      "2370/2370 [==============================] - 0s 28us/step - loss: 0.0392 - val_loss: 0.0474\n",
      "Epoch 2254/3000\n",
      "2370/2370 [==============================] - 0s 28us/step - loss: 0.0374 - val_loss: 0.0485\n",
      "Epoch 2255/3000\n",
      "2370/2370 [==============================] - 0s 28us/step - loss: 0.0361 - val_loss: 0.0421\n",
      "Epoch 2256/3000\n",
      "2370/2370 [==============================] - 0s 28us/step - loss: 0.0382 - val_loss: 0.0511\n",
      "Epoch 2257/3000\n",
      "2370/2370 [==============================] - 0s 28us/step - loss: 0.0363 - val_loss: 0.0389\n",
      "Epoch 2258/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0363 - val_loss: 0.0444\n",
      "Epoch 2259/3000\n",
      "2370/2370 [==============================] - 0s 28us/step - loss: 0.0410 - val_loss: 0.0416\n",
      "Epoch 2260/3000\n",
      "2370/2370 [==============================] - 0s 32us/step - loss: 0.0410 - val_loss: 0.0416\n",
      "Epoch 2261/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0353 - val_loss: 0.0427\n",
      "Epoch 2262/3000\n",
      "2370/2370 [==============================] - 0s 31us/step - loss: 0.0399 - val_loss: 0.0595\n",
      "Epoch 2263/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0379 - val_loss: 0.0405\n",
      "Epoch 2264/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0435 - val_loss: 0.0415\n",
      "Epoch 2265/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0372 - val_loss: 0.0420\n",
      "Epoch 2266/3000\n",
      "2370/2370 [==============================] - 0s 28us/step - loss: 0.0367 - val_loss: 0.0492\n",
      "Epoch 2267/3000\n",
      "2370/2370 [==============================] - 0s 28us/step - loss: 0.0393 - val_loss: 0.0425\n",
      "Epoch 2268/3000\n",
      "2370/2370 [==============================] - 0s 28us/step - loss: 0.0356 - val_loss: 0.0436\n",
      "Epoch 2269/3000\n",
      "2370/2370 [==============================] - 0s 28us/step - loss: 0.0387 - val_loss: 0.0388\n",
      "Epoch 2270/3000\n",
      "2370/2370 [==============================] - 0s 28us/step - loss: 0.0369 - val_loss: 0.0400\n",
      "Epoch 2271/3000\n",
      "2370/2370 [==============================] - 0s 30us/step - loss: 0.0365 - val_loss: 0.0537\n",
      "Epoch 2272/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0387 - val_loss: 0.0449\n",
      "Epoch 2273/3000\n",
      "2370/2370 [==============================] - 0s 31us/step - loss: 0.0377 - val_loss: 0.0591\n",
      "Epoch 2274/3000\n",
      "2370/2370 [==============================] - 0s 28us/step - loss: 0.0375 - val_loss: 0.0470\n",
      "Epoch 2275/3000\n",
      "2370/2370 [==============================] - 0s 28us/step - loss: 0.0378 - val_loss: 0.0433\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2276/3000\n",
      "2370/2370 [==============================] - 0s 28us/step - loss: 0.0398 - val_loss: 0.0463\n",
      "Epoch 2277/3000\n",
      "2370/2370 [==============================] - 0s 28us/step - loss: 0.0391 - val_loss: 0.0413\n",
      "Epoch 2278/3000\n",
      "2370/2370 [==============================] - 0s 28us/step - loss: 0.0379 - val_loss: 0.0487\n",
      "Epoch 2279/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0367 - val_loss: 0.0397\n",
      "Epoch 2280/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0368 - val_loss: 0.0407\n",
      "Epoch 2281/3000\n",
      "2370/2370 [==============================] - 0s 28us/step - loss: 0.0393 - val_loss: 0.0439\n",
      "Epoch 2282/3000\n",
      "2370/2370 [==============================] - 0s 28us/step - loss: 0.0369 - val_loss: 0.0405\n",
      "Epoch 2283/3000\n",
      "2370/2370 [==============================] - 0s 28us/step - loss: 0.0413 - val_loss: 0.0449\n",
      "Epoch 2284/3000\n",
      "2370/2370 [==============================] - 0s 28us/step - loss: 0.0363 - val_loss: 0.0478\n",
      "Epoch 2285/3000\n",
      "2370/2370 [==============================] - 0s 28us/step - loss: 0.0408 - val_loss: 0.0449\n",
      "Epoch 2286/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0414 - val_loss: 0.0594\n",
      "Epoch 2287/3000\n",
      "2370/2370 [==============================] - 0s 28us/step - loss: 0.0367 - val_loss: 0.0516\n",
      "Epoch 2288/3000\n",
      "2370/2370 [==============================] - 0s 28us/step - loss: 0.0370 - val_loss: 0.0555\n",
      "Epoch 2289/3000\n",
      "2370/2370 [==============================] - 0s 28us/step - loss: 0.0374 - val_loss: 0.0633\n",
      "Epoch 2290/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0378 - val_loss: 0.0463\n",
      "Epoch 2291/3000\n",
      "2370/2370 [==============================] - 0s 28us/step - loss: 0.0335 - val_loss: 0.0460\n",
      "Epoch 2292/3000\n",
      "2370/2370 [==============================] - 0s 28us/step - loss: 0.0358 - val_loss: 0.0439\n",
      "Epoch 2293/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0390 - val_loss: 0.0427\n",
      "Epoch 2294/3000\n",
      "2370/2370 [==============================] - 0s 28us/step - loss: 0.0353 - val_loss: 0.0395\n",
      "Epoch 2295/3000\n",
      "2370/2370 [==============================] - 0s 28us/step - loss: 0.0370 - val_loss: 0.0397\n",
      "Epoch 2296/3000\n",
      "2370/2370 [==============================] - 0s 28us/step - loss: 0.0395 - val_loss: 0.0421\n",
      "Epoch 2297/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0394 - val_loss: 0.0669\n",
      "Epoch 2298/3000\n",
      "2370/2370 [==============================] - 0s 28us/step - loss: 0.0368 - val_loss: 0.0430\n",
      "Epoch 2299/3000\n",
      "2370/2370 [==============================] - 0s 28us/step - loss: 0.0364 - val_loss: 0.0400\n",
      "Epoch 2300/3000\n",
      "2370/2370 [==============================] - 0s 28us/step - loss: 0.0396 - val_loss: 0.0518\n",
      "Epoch 2301/3000\n",
      "2370/2370 [==============================] - 0s 28us/step - loss: 0.0401 - val_loss: 0.0446\n",
      "Epoch 2302/3000\n",
      "2370/2370 [==============================] - 0s 28us/step - loss: 0.0405 - val_loss: 0.0388\n",
      "Epoch 2303/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0383 - val_loss: 0.0399\n",
      "Epoch 2304/3000\n",
      "2370/2370 [==============================] - 0s 33us/step - loss: 0.0354 - val_loss: 0.0415\n",
      "Epoch 2305/3000\n",
      "2370/2370 [==============================] - 0s 28us/step - loss: 0.0382 - val_loss: 0.0410\n",
      "Epoch 2306/3000\n",
      "2370/2370 [==============================] - 0s 30us/step - loss: 0.0398 - val_loss: 0.0411\n",
      "Epoch 2307/3000\n",
      "2370/2370 [==============================] - 0s 28us/step - loss: 0.0421 - val_loss: 0.0399\n",
      "Epoch 2308/3000\n",
      "2370/2370 [==============================] - 0s 28us/step - loss: 0.0367 - val_loss: 0.0396\n",
      "Epoch 2309/3000\n",
      "2370/2370 [==============================] - 0s 28us/step - loss: 0.0362 - val_loss: 0.0435\n",
      "Epoch 2310/3000\n",
      "2370/2370 [==============================] - 0s 28us/step - loss: 0.0355 - val_loss: 0.0412\n",
      "Epoch 2311/3000\n",
      "2370/2370 [==============================] - 0s 28us/step - loss: 0.0338 - val_loss: 0.0447\n",
      "Epoch 2312/3000\n",
      "2370/2370 [==============================] - 0s 28us/step - loss: 0.0383 - val_loss: 0.0469\n",
      "Epoch 2313/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0362 - val_loss: 0.0607\n",
      "Epoch 2314/3000\n",
      "2370/2370 [==============================] - 0s 28us/step - loss: 0.0387 - val_loss: 0.0470\n",
      "Epoch 2315/3000\n",
      "2370/2370 [==============================] - 0s 30us/step - loss: 0.0350 - val_loss: 0.0592\n",
      "Epoch 2316/3000\n",
      "2370/2370 [==============================] - 0s 28us/step - loss: 0.0412 - val_loss: 0.0440\n",
      "Epoch 2317/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0366 - val_loss: 0.0411\n",
      "Epoch 2318/3000\n",
      "2370/2370 [==============================] - 0s 28us/step - loss: 0.0347 - val_loss: 0.0413\n",
      "Epoch 2319/3000\n",
      "2370/2370 [==============================] - 0s 28us/step - loss: 0.0387 - val_loss: 0.0538\n",
      "Epoch 2320/3000\n",
      "2370/2370 [==============================] - 0s 28us/step - loss: 0.0382 - val_loss: 0.0429\n",
      "Epoch 2321/3000\n",
      "2370/2370 [==============================] - 0s 27us/step - loss: 0.0400 - val_loss: 0.0736\n",
      "Epoch 2322/3000\n",
      "2370/2370 [==============================] - 0s 28us/step - loss: 0.0349 - val_loss: 0.0491\n",
      "Epoch 2323/3000\n",
      "2370/2370 [==============================] - 0s 28us/step - loss: 0.0383 - val_loss: 0.0435\n",
      "Epoch 2324/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0375 - val_loss: 0.0520\n",
      "Epoch 2325/3000\n",
      "2370/2370 [==============================] - 0s 28us/step - loss: 0.0349 - val_loss: 0.0543\n",
      "Epoch 2326/3000\n",
      "2370/2370 [==============================] - 0s 28us/step - loss: 0.0412 - val_loss: 0.0402\n",
      "Epoch 2327/3000\n",
      "2370/2370 [==============================] - 0s 28us/step - loss: 0.0371 - val_loss: 0.0480\n",
      "Epoch 2328/3000\n",
      "2370/2370 [==============================] - 0s 28us/step - loss: 0.0397 - val_loss: 0.0405\n",
      "Epoch 2329/3000\n",
      "2370/2370 [==============================] - 0s 28us/step - loss: 0.0369 - val_loss: 0.0468\n",
      "Epoch 2330/3000\n",
      "2370/2370 [==============================] - 0s 28us/step - loss: 0.0354 - val_loss: 0.0394\n",
      "Epoch 2331/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0349 - val_loss: 0.0517\n",
      "Epoch 2332/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0395 - val_loss: 0.0498\n",
      "Epoch 2333/3000\n",
      "2370/2370 [==============================] - 0s 28us/step - loss: 0.0366 - val_loss: 0.0805\n",
      "Epoch 2334/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0373 - val_loss: 0.0504\n",
      "Epoch 2335/3000\n",
      "2370/2370 [==============================] - 0s 28us/step - loss: 0.0353 - val_loss: 0.0586\n",
      "Epoch 2336/3000\n",
      "2370/2370 [==============================] - 0s 28us/step - loss: 0.0385 - val_loss: 0.0404\n",
      "Epoch 2337/3000\n",
      "2370/2370 [==============================] - 0s 28us/step - loss: 0.0349 - val_loss: 0.0452\n",
      "Epoch 2338/3000\n",
      "2370/2370 [==============================] - 0s 28us/step - loss: 0.0357 - val_loss: 0.0398\n",
      "Epoch 2339/3000\n",
      "2370/2370 [==============================] - 0s 28us/step - loss: 0.0347 - val_loss: 0.0420\n",
      "Epoch 2340/3000\n",
      "2370/2370 [==============================] - 0s 28us/step - loss: 0.0387 - val_loss: 0.0402\n",
      "Epoch 2341/3000\n",
      "2370/2370 [==============================] - 0s 28us/step - loss: 0.0380 - val_loss: 0.0413\n",
      "Epoch 2342/3000\n",
      "2370/2370 [==============================] - 0s 28us/step - loss: 0.0351 - val_loss: 0.0404\n",
      "Epoch 2343/3000\n",
      "2370/2370 [==============================] - 0s 28us/step - loss: 0.0375 - val_loss: 0.0399\n",
      "Epoch 2344/3000\n",
      "2370/2370 [==============================] - 0s 28us/step - loss: 0.0349 - val_loss: 0.0388\n",
      "Epoch 2345/3000\n",
      "2370/2370 [==============================] - 0s 28us/step - loss: 0.0368 - val_loss: 0.0426\n",
      "Epoch 2346/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0379 - val_loss: 0.0451\n",
      "Epoch 2347/3000\n",
      "2370/2370 [==============================] - 0s 32us/step - loss: 0.0374 - val_loss: 0.0522\n",
      "Epoch 2348/3000\n",
      "2370/2370 [==============================] - 0s 31us/step - loss: 0.0389 - val_loss: 0.0434\n",
      "Epoch 2349/3000\n",
      "2370/2370 [==============================] - 0s 28us/step - loss: 0.0374 - val_loss: 0.0548\n",
      "Epoch 2350/3000\n",
      "2370/2370 [==============================] - 0s 30us/step - loss: 0.0342 - val_loss: 0.0531\n",
      "Epoch 2351/3000\n",
      "2370/2370 [==============================] - 0s 28us/step - loss: 0.0366 - val_loss: 0.0427\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2352/3000\n",
      "2370/2370 [==============================] - 0s 27us/step - loss: 0.0399 - val_loss: 0.0465\n",
      "Epoch 2353/3000\n",
      "2370/2370 [==============================] - 0s 28us/step - loss: 0.0378 - val_loss: 0.0413\n",
      "Epoch 2354/3000\n",
      "2370/2370 [==============================] - 0s 28us/step - loss: 0.0361 - val_loss: 0.0454\n",
      "Epoch 2355/3000\n",
      "2370/2370 [==============================] - 0s 28us/step - loss: 0.0382 - val_loss: 0.0477\n",
      "Epoch 2356/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0372 - val_loss: 0.0410\n",
      "Epoch 2357/3000\n",
      "2370/2370 [==============================] - 0s 28us/step - loss: 0.0353 - val_loss: 0.0421\n",
      "Epoch 2358/3000\n",
      "2370/2370 [==============================] - 0s 28us/step - loss: 0.0376 - val_loss: 0.0398\n",
      "Epoch 2359/3000\n",
      "2370/2370 [==============================] - 0s 30us/step - loss: 0.0367 - val_loss: 0.0484\n",
      "Epoch 2360/3000\n",
      "2370/2370 [==============================] - 0s 28us/step - loss: 0.0389 - val_loss: 0.0502\n",
      "Epoch 2361/3000\n",
      "2370/2370 [==============================] - 0s 28us/step - loss: 0.0380 - val_loss: 0.0474\n",
      "Epoch 2362/3000\n",
      "2370/2370 [==============================] - 0s 28us/step - loss: 0.0378 - val_loss: 0.0388\n",
      "Epoch 2363/3000\n",
      "2370/2370 [==============================] - 0s 28us/step - loss: 0.0375 - val_loss: 0.0477\n",
      "Epoch 2364/3000\n",
      "2370/2370 [==============================] - 0s 28us/step - loss: 0.0359 - val_loss: 0.0400\n",
      "Epoch 2365/3000\n",
      "2370/2370 [==============================] - 0s 28us/step - loss: 0.0390 - val_loss: 0.0574\n",
      "Epoch 2366/3000\n",
      "2370/2370 [==============================] - 0s 28us/step - loss: 0.0415 - val_loss: 0.0478\n",
      "Epoch 2367/3000\n",
      "2370/2370 [==============================] - 0s 28us/step - loss: 0.0340 - val_loss: 0.0469\n",
      "Epoch 2368/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0374 - val_loss: 0.0629\n",
      "Epoch 2369/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0381 - val_loss: 0.0737\n",
      "Epoch 2370/3000\n",
      "2370/2370 [==============================] - 0s 28us/step - loss: 0.0374 - val_loss: 0.0446\n",
      "Epoch 2371/3000\n",
      "2370/2370 [==============================] - 0s 28us/step - loss: 0.0382 - val_loss: 0.0505\n",
      "Epoch 2372/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0392 - val_loss: 0.0437\n",
      "Epoch 2373/3000\n",
      "2370/2370 [==============================] - 0s 28us/step - loss: 0.0363 - val_loss: 0.0630\n",
      "Epoch 2374/3000\n",
      "2370/2370 [==============================] - 0s 28us/step - loss: 0.0373 - val_loss: 0.0561\n",
      "Epoch 2375/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0348 - val_loss: 0.0500\n",
      "Epoch 2376/3000\n",
      "2370/2370 [==============================] - 0s 28us/step - loss: 0.0386 - val_loss: 0.0459\n",
      "Epoch 2377/3000\n",
      "2370/2370 [==============================] - 0s 28us/step - loss: 0.0416 - val_loss: 0.0402\n",
      "Epoch 2378/3000\n",
      "2370/2370 [==============================] - 0s 28us/step - loss: 0.0343 - val_loss: 0.0447\n",
      "Epoch 2379/3000\n",
      "2370/2370 [==============================] - 0s 28us/step - loss: 0.0411 - val_loss: 0.0488\n",
      "Epoch 2380/3000\n",
      "2370/2370 [==============================] - 0s 28us/step - loss: 0.0376 - val_loss: 0.0402\n",
      "Epoch 2381/3000\n",
      "2370/2370 [==============================] - 0s 28us/step - loss: 0.0378 - val_loss: 0.0474\n",
      "Epoch 2382/3000\n",
      "2370/2370 [==============================] - 0s 28us/step - loss: 0.0369 - val_loss: 0.0406\n",
      "Epoch 2383/3000\n",
      "2370/2370 [==============================] - 0s 28us/step - loss: 0.0369 - val_loss: 0.0420\n",
      "Epoch 2384/3000\n",
      "2370/2370 [==============================] - 0s 28us/step - loss: 0.0421 - val_loss: 0.0434\n",
      "Epoch 2385/3000\n",
      "2370/2370 [==============================] - 0s 28us/step - loss: 0.0348 - val_loss: 0.0501\n",
      "Epoch 2386/3000\n",
      "2370/2370 [==============================] - 0s 28us/step - loss: 0.0373 - val_loss: 0.0658\n",
      "Epoch 2387/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0365 - val_loss: 0.0410\n",
      "Epoch 2388/3000\n",
      "2370/2370 [==============================] - 0s 28us/step - loss: 0.0381 - val_loss: 0.0466\n",
      "Epoch 2389/3000\n",
      "2370/2370 [==============================] - 0s 28us/step - loss: 0.0383 - val_loss: 0.0474\n",
      "Epoch 2390/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0368 - val_loss: 0.0566\n",
      "Epoch 2391/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0397 - val_loss: 0.0464\n",
      "Epoch 2392/3000\n",
      "2370/2370 [==============================] - 0s 33us/step - loss: 0.0374 - val_loss: 0.0632\n",
      "Epoch 2393/3000\n",
      "2370/2370 [==============================] - 0s 28us/step - loss: 0.0367 - val_loss: 0.0420\n",
      "Epoch 2394/3000\n",
      "2370/2370 [==============================] - 0s 30us/step - loss: 0.0391 - val_loss: 0.0522\n",
      "Epoch 2395/3000\n",
      "2370/2370 [==============================] - 0s 28us/step - loss: 0.0372 - val_loss: 0.0460\n",
      "Epoch 2396/3000\n",
      "2370/2370 [==============================] - 0s 28us/step - loss: 0.0370 - val_loss: 0.0495\n",
      "Epoch 2397/3000\n",
      "2370/2370 [==============================] - 0s 28us/step - loss: 0.0344 - val_loss: 0.0483\n",
      "Epoch 2398/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0379 - val_loss: 0.0401\n",
      "Epoch 2399/3000\n",
      "2370/2370 [==============================] - 0s 28us/step - loss: 0.0378 - val_loss: 0.0836\n",
      "Epoch 2400/3000\n",
      "2370/2370 [==============================] - 0s 30us/step - loss: 0.0364 - val_loss: 0.0556\n",
      "Epoch 2401/3000\n",
      "2370/2370 [==============================] - 0s 30us/step - loss: 0.0373 - val_loss: 0.0448\n",
      "Epoch 2402/3000\n",
      "2370/2370 [==============================] - 0s 30us/step - loss: 0.0396 - val_loss: 0.0407\n",
      "Epoch 2403/3000\n",
      "2370/2370 [==============================] - 0s 32us/step - loss: 0.0373 - val_loss: 0.0416\n",
      "Epoch 2404/3000\n",
      "2370/2370 [==============================] - 0s 30us/step - loss: 0.0388 - val_loss: 0.0522\n",
      "Epoch 2405/3000\n",
      "2370/2370 [==============================] - 0s 30us/step - loss: 0.0358 - val_loss: 0.0491\n",
      "Epoch 2406/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0358 - val_loss: 0.0516\n",
      "Epoch 2407/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0362 - val_loss: 0.0424\n",
      "Epoch 2408/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0379 - val_loss: 0.0411\n",
      "Epoch 2409/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0410 - val_loss: 0.0409\n",
      "Epoch 2410/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0364 - val_loss: 0.0413\n",
      "Epoch 2411/3000\n",
      "2370/2370 [==============================] - 0s 32us/step - loss: 0.0359 - val_loss: 0.0565\n",
      "Epoch 2412/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0367 - val_loss: 0.0512\n",
      "Epoch 2413/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0365 - val_loss: 0.0436\n",
      "Epoch 2414/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0369 - val_loss: 0.0512\n",
      "Epoch 2415/3000\n",
      "2370/2370 [==============================] - 0s 31us/step - loss: 0.0363 - val_loss: 0.0517\n",
      "Epoch 2416/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0413 - val_loss: 0.0431\n",
      "Epoch 2417/3000\n",
      "2370/2370 [==============================] - 0s 30us/step - loss: 0.0327 - val_loss: 0.0420\n",
      "Epoch 2418/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0337 - val_loss: 0.0507\n",
      "Epoch 2419/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0317 - val_loss: 0.0450\n",
      "Epoch 2420/3000\n",
      "2370/2370 [==============================] - 0s 34us/step - loss: 0.0407 - val_loss: 0.0429\n",
      "Epoch 2421/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0385 - val_loss: 0.0410\n",
      "Epoch 2422/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0380 - val_loss: 0.0678\n",
      "Epoch 2423/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0380 - val_loss: 0.0428\n",
      "Epoch 2424/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0392 - val_loss: 0.0393\n",
      "Epoch 2425/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0379 - val_loss: 0.0401\n",
      "Epoch 2426/3000\n",
      "2370/2370 [==============================] - 0s 30us/step - loss: 0.0408 - val_loss: 0.0393\n",
      "Epoch 2427/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0380 - val_loss: 0.0381\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2428/3000\n",
      "2370/2370 [==============================] - 0s 31us/step - loss: 0.0369 - val_loss: 0.0421\n",
      "Epoch 2429/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0394 - val_loss: 0.0390\n",
      "Epoch 2430/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0337 - val_loss: 0.0386\n",
      "Epoch 2431/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0392 - val_loss: 0.0454\n",
      "Epoch 2432/3000\n",
      "2370/2370 [==============================] - 0s 30us/step - loss: 0.0362 - val_loss: 0.0431\n",
      "Epoch 2433/3000\n",
      "2370/2370 [==============================] - 0s 30us/step - loss: 0.0389 - val_loss: 0.0487\n",
      "Epoch 2434/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0392 - val_loss: 0.0416\n",
      "Epoch 2435/3000\n",
      "2370/2370 [==============================] - 0s 30us/step - loss: 0.0404 - val_loss: 0.0451\n",
      "Epoch 2436/3000\n",
      "2370/2370 [==============================] - 0s 32us/step - loss: 0.0397 - val_loss: 0.0427\n",
      "Epoch 2437/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0391 - val_loss: 0.0414\n",
      "Epoch 2438/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0397 - val_loss: 0.0411\n",
      "Epoch 2439/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0368 - val_loss: 0.0418\n",
      "Epoch 2440/3000\n",
      "2370/2370 [==============================] - 0s 30us/step - loss: 0.0354 - val_loss: 0.0420\n",
      "Epoch 2441/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0347 - val_loss: 0.0411\n",
      "Epoch 2442/3000\n",
      "2370/2370 [==============================] - 0s 30us/step - loss: 0.0349 - val_loss: 0.0448\n",
      "Epoch 2443/3000\n",
      "2370/2370 [==============================] - 0s 30us/step - loss: 0.0371 - val_loss: 0.0459\n",
      "Epoch 2444/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0368 - val_loss: 0.0440\n",
      "Epoch 2445/3000\n",
      "2370/2370 [==============================] - 0s 32us/step - loss: 0.0392 - val_loss: 0.0525\n",
      "Epoch 2446/3000\n",
      "2370/2370 [==============================] - 0s 30us/step - loss: 0.0378 - val_loss: 0.0453\n",
      "Epoch 2447/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0355 - val_loss: 0.0568\n",
      "Epoch 2448/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0349 - val_loss: 0.0464\n",
      "Epoch 2449/3000\n",
      "2370/2370 [==============================] - 0s 30us/step - loss: 0.0401 - val_loss: 0.0456\n",
      "Epoch 2450/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0380 - val_loss: 0.0535\n",
      "Epoch 2451/3000\n",
      "2370/2370 [==============================] - 0s 30us/step - loss: 0.0341 - val_loss: 0.0529\n",
      "Epoch 2452/3000\n",
      "2370/2370 [==============================] - 0s 30us/step - loss: 0.0335 - val_loss: 0.0397\n",
      "Epoch 2453/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0368 - val_loss: 0.0500\n",
      "Epoch 2454/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0375 - val_loss: 0.0408\n",
      "Epoch 2455/3000\n",
      "2370/2370 [==============================] - 0s 30us/step - loss: 0.0356 - val_loss: 0.0440\n",
      "Epoch 2456/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0405 - val_loss: 0.0406\n",
      "Epoch 2457/3000\n",
      "2370/2370 [==============================] - 0s 30us/step - loss: 0.0374 - val_loss: 0.0492\n",
      "Epoch 2458/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0341 - val_loss: 0.0578\n",
      "Epoch 2459/3000\n",
      "2370/2370 [==============================] - 0s 30us/step - loss: 0.0385 - val_loss: 0.0495\n",
      "Epoch 2460/3000\n",
      "2370/2370 [==============================] - 0s 30us/step - loss: 0.0395 - val_loss: 0.0550\n",
      "Epoch 2461/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0367 - val_loss: 0.0478\n",
      "Epoch 2462/3000\n",
      "2370/2370 [==============================] - 0s 35us/step - loss: 0.0365 - val_loss: 0.0515\n",
      "Epoch 2463/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0380 - val_loss: 0.0414\n",
      "Epoch 2464/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0419 - val_loss: 0.0459\n",
      "Epoch 2465/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0359 - val_loss: 0.0458\n",
      "Epoch 2466/3000\n",
      "2370/2370 [==============================] - 0s 30us/step - loss: 0.0368 - val_loss: 0.0628\n",
      "Epoch 2467/3000\n",
      "2370/2370 [==============================] - 0s 30us/step - loss: 0.0351 - val_loss: 0.0409\n",
      "Epoch 2468/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0363 - val_loss: 0.0411\n",
      "Epoch 2469/3000\n",
      "2370/2370 [==============================] - 0s 30us/step - loss: 0.0398 - val_loss: 0.0447\n",
      "Epoch 2470/3000\n",
      "2370/2370 [==============================] - 0s 32us/step - loss: 0.0357 - val_loss: 0.0505\n",
      "Epoch 2471/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0400 - val_loss: 0.0452\n",
      "Epoch 2472/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0393 - val_loss: 0.0514\n",
      "Epoch 2473/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0371 - val_loss: 0.0404\n",
      "Epoch 2474/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0433 - val_loss: 0.0583\n",
      "Epoch 2475/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0364 - val_loss: 0.0577\n",
      "Epoch 2476/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0394 - val_loss: 0.0496\n",
      "Epoch 2477/3000\n",
      "2370/2370 [==============================] - 0s 30us/step - loss: 0.0396 - val_loss: 0.0410\n",
      "Epoch 2478/3000\n",
      "2370/2370 [==============================] - 0s 30us/step - loss: 0.0385 - val_loss: 0.0431\n",
      "Epoch 2479/3000\n",
      "2370/2370 [==============================] - 0s 31us/step - loss: 0.0344 - val_loss: 0.0609\n",
      "Epoch 2480/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0404 - val_loss: 0.0478\n",
      "Epoch 2481/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0365 - val_loss: 0.0538\n",
      "Epoch 2482/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0346 - val_loss: 0.0538\n",
      "Epoch 2483/3000\n",
      "2370/2370 [==============================] - 0s 30us/step - loss: 0.0340 - val_loss: 0.0455\n",
      "Epoch 2484/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0372 - val_loss: 0.0439\n",
      "Epoch 2485/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0359 - val_loss: 0.0444\n",
      "Epoch 2486/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0340 - val_loss: 0.0595\n",
      "Epoch 2487/3000\n",
      "2370/2370 [==============================] - 0s 32us/step - loss: 0.0396 - val_loss: 0.0432\n",
      "Epoch 2488/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0360 - val_loss: 0.0584\n",
      "Epoch 2489/3000\n",
      "2370/2370 [==============================] - 0s 30us/step - loss: 0.0367 - val_loss: 0.0437\n",
      "Epoch 2490/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0385 - val_loss: 0.0425\n",
      "Epoch 2491/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0367 - val_loss: 0.0436\n",
      "Epoch 2492/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0367 - val_loss: 0.0521\n",
      "Epoch 2493/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0342 - val_loss: 0.0454\n",
      "Epoch 2494/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0347 - val_loss: 0.0424\n",
      "Epoch 2495/3000\n",
      "2370/2370 [==============================] - 0s 30us/step - loss: 0.0357 - val_loss: 0.0436\n",
      "Epoch 2496/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0393 - val_loss: 0.0583\n",
      "Epoch 2497/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0401 - val_loss: 0.0477\n",
      "Epoch 2498/3000\n",
      "2370/2370 [==============================] - 0s 28us/step - loss: 0.0389 - val_loss: 0.0435\n",
      "Epoch 2499/3000\n",
      "2370/2370 [==============================] - 0s 28us/step - loss: 0.0386 - val_loss: 0.0574\n",
      "Epoch 2500/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0390 - val_loss: 0.0486\n",
      "Epoch 2501/3000\n",
      "2370/2370 [==============================] - 0s 28us/step - loss: 0.0397 - val_loss: 0.0428\n",
      "Epoch 2502/3000\n",
      "2370/2370 [==============================] - 0s 28us/step - loss: 0.0362 - val_loss: 0.0441\n",
      "Epoch 2503/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0376 - val_loss: 0.0514\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2504/3000\n",
      "2370/2370 [==============================] - 0s 32us/step - loss: 0.0408 - val_loss: 0.0444\n",
      "Epoch 2505/3000\n",
      "2370/2370 [==============================] - 0s 30us/step - loss: 0.0340 - val_loss: 0.0449\n",
      "Epoch 2506/3000\n",
      "2370/2370 [==============================] - 0s 28us/step - loss: 0.0395 - val_loss: 0.0467\n",
      "Epoch 2507/3000\n",
      "2370/2370 [==============================] - 0s 28us/step - loss: 0.0359 - val_loss: 0.0436\n",
      "Epoch 2508/3000\n",
      "2370/2370 [==============================] - 0s 28us/step - loss: 0.0400 - val_loss: 0.0445\n",
      "Epoch 2509/3000\n",
      "2370/2370 [==============================] - 0s 28us/step - loss: 0.0361 - val_loss: 0.0417\n",
      "Epoch 2510/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0378 - val_loss: 0.0458\n",
      "Epoch 2511/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0408 - val_loss: 0.0458\n",
      "Epoch 2512/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0362 - val_loss: 0.0432\n",
      "Epoch 2513/3000\n",
      "2370/2370 [==============================] - 0s 32us/step - loss: 0.0379 - val_loss: 0.0413\n",
      "Epoch 2514/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0384 - val_loss: 0.0452\n",
      "Epoch 2515/3000\n",
      "2370/2370 [==============================] - 0s 30us/step - loss: 0.0344 - val_loss: 0.0393\n",
      "Epoch 2516/3000\n",
      "2370/2370 [==============================] - 0s 30us/step - loss: 0.0374 - val_loss: 0.0393\n",
      "Epoch 2517/3000\n",
      "2370/2370 [==============================] - 0s 33us/step - loss: 0.0396 - val_loss: 0.0418\n",
      "Epoch 2518/3000\n",
      "2370/2370 [==============================] - 0s 33us/step - loss: 0.0362 - val_loss: 0.0393\n",
      "Epoch 2519/3000\n",
      "2370/2370 [==============================] - 0s 32us/step - loss: 0.0379 - val_loss: 0.0456\n",
      "Epoch 2520/3000\n",
      "2370/2370 [==============================] - 0s 32us/step - loss: 0.0377 - val_loss: 0.0450\n",
      "Epoch 2521/3000\n",
      "2370/2370 [==============================] - 0s 35us/step - loss: 0.0365 - val_loss: 0.0538\n",
      "Epoch 2522/3000\n",
      "2370/2370 [==============================] - 0s 33us/step - loss: 0.0359 - val_loss: 0.0395\n",
      "Epoch 2523/3000\n",
      "2370/2370 [==============================] - 0s 32us/step - loss: 0.0387 - val_loss: 0.0493\n",
      "Epoch 2524/3000\n",
      "2370/2370 [==============================] - 0s 30us/step - loss: 0.0364 - val_loss: 0.0516\n",
      "Epoch 2525/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0387 - val_loss: 0.0468\n",
      "Epoch 2526/3000\n",
      "2370/2370 [==============================] - 0s 30us/step - loss: 0.0377 - val_loss: 0.0489\n",
      "Epoch 2527/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0387 - val_loss: 0.0527\n",
      "Epoch 2528/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0353 - val_loss: 0.0556\n",
      "Epoch 2529/3000\n",
      "2370/2370 [==============================] - 0s 31us/step - loss: 0.0363 - val_loss: 0.0505\n",
      "Epoch 2530/3000\n",
      "2370/2370 [==============================] - 0s 30us/step - loss: 0.0365 - val_loss: 0.0528\n",
      "Epoch 2531/3000\n",
      "2370/2370 [==============================] - 0s 30us/step - loss: 0.0366 - val_loss: 0.0427\n",
      "Epoch 2532/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0367 - val_loss: 0.0456\n",
      "Epoch 2533/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0339 - val_loss: 0.0626\n",
      "Epoch 2534/3000\n",
      "2370/2370 [==============================] - 0s 30us/step - loss: 0.0377 - val_loss: 0.0604\n",
      "Epoch 2535/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0346 - val_loss: 0.0513\n",
      "Epoch 2536/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0419 - val_loss: 0.0415\n",
      "Epoch 2537/3000\n",
      "2370/2370 [==============================] - 0s 30us/step - loss: 0.0331 - val_loss: 0.0536\n",
      "Epoch 2538/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0361 - val_loss: 0.0432\n",
      "Epoch 2539/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0434 - val_loss: 0.0453\n",
      "Epoch 2540/3000\n",
      "2370/2370 [==============================] - 0s 30us/step - loss: 0.0383 - val_loss: 0.0420\n",
      "Epoch 2541/3000\n",
      "2370/2370 [==============================] - 0s 30us/step - loss: 0.0369 - val_loss: 0.0622\n",
      "Epoch 2542/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0404 - val_loss: 0.0417\n",
      "Epoch 2543/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0403 - val_loss: 0.0532\n",
      "Epoch 2544/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0386 - val_loss: 0.0414\n",
      "Epoch 2545/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0385 - val_loss: 0.0603\n",
      "Epoch 2546/3000\n",
      "2370/2370 [==============================] - 0s 33us/step - loss: 0.0348 - val_loss: 0.0431\n",
      "Epoch 2547/3000\n",
      "2370/2370 [==============================] - 0s 31us/step - loss: 0.0380 - val_loss: 0.0467\n",
      "Epoch 2548/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0348 - val_loss: 0.0454\n",
      "Epoch 2549/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0351 - val_loss: 0.0432\n",
      "Epoch 2550/3000\n",
      "2370/2370 [==============================] - 0s 30us/step - loss: 0.0379 - val_loss: 0.0450\n",
      "Epoch 2551/3000\n",
      "2370/2370 [==============================] - 0s 30us/step - loss: 0.0365 - val_loss: 0.0464\n",
      "Epoch 2552/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0346 - val_loss: 0.0457\n",
      "Epoch 2553/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0345 - val_loss: 0.0433\n",
      "Epoch 2554/3000\n",
      "2370/2370 [==============================] - 0s 31us/step - loss: 0.0359 - val_loss: 0.0539\n",
      "Epoch 2555/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0350 - val_loss: 0.0509\n",
      "Epoch 2556/3000\n",
      "2370/2370 [==============================] - 0s 30us/step - loss: 0.0373 - val_loss: 0.0467\n",
      "Epoch 2557/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0390 - val_loss: 0.0454\n",
      "Epoch 2558/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0332 - val_loss: 0.0550\n",
      "Epoch 2559/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0368 - val_loss: 0.0457\n",
      "Epoch 2560/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0368 - val_loss: 0.0562\n",
      "Epoch 2561/3000\n",
      "2370/2370 [==============================] - 0s 30us/step - loss: 0.0392 - val_loss: 0.0471\n",
      "Epoch 2562/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0364 - val_loss: 0.0437\n",
      "Epoch 2563/3000\n",
      "2370/2370 [==============================] - 0s 31us/step - loss: 0.0394 - val_loss: 0.0422\n",
      "Epoch 2564/3000\n",
      "2370/2370 [==============================] - 0s 30us/step - loss: 0.0363 - val_loss: 0.0537\n",
      "Epoch 2565/3000\n",
      "2370/2370 [==============================] - 0s 30us/step - loss: 0.0362 - val_loss: 0.0418\n",
      "Epoch 2566/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0383 - val_loss: 0.0462\n",
      "Epoch 2567/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0365 - val_loss: 0.0418\n",
      "Epoch 2568/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0392 - val_loss: 0.0416\n",
      "Epoch 2569/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0378 - val_loss: 0.0673\n",
      "Epoch 2570/3000\n",
      "2370/2370 [==============================] - 0s 30us/step - loss: 0.0403 - val_loss: 0.0555\n",
      "Epoch 2571/3000\n",
      "2370/2370 [==============================] - 0s 32us/step - loss: 0.0386 - val_loss: 0.0549\n",
      "Epoch 2572/3000\n",
      "2370/2370 [==============================] - 0s 30us/step - loss: 0.0373 - val_loss: 0.0463\n",
      "Epoch 2573/3000\n",
      "2370/2370 [==============================] - 0s 30us/step - loss: 0.0371 - val_loss: 0.0473\n",
      "Epoch 2574/3000\n",
      "2370/2370 [==============================] - 0s 30us/step - loss: 0.0367 - val_loss: 0.0406\n",
      "Epoch 2575/3000\n",
      "2370/2370 [==============================] - 0s 30us/step - loss: 0.0357 - val_loss: 0.0433\n",
      "Epoch 2576/3000\n",
      "2370/2370 [==============================] - 0s 30us/step - loss: 0.0372 - val_loss: 0.0444\n",
      "Epoch 2577/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0395 - val_loss: 0.0448\n",
      "Epoch 2578/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0371 - val_loss: 0.0550\n",
      "Epoch 2579/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0353 - val_loss: 0.0426\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2580/3000\n",
      "2370/2370 [==============================] - 0s 30us/step - loss: 0.0370 - val_loss: 0.0427\n",
      "Epoch 2581/3000\n",
      "2370/2370 [==============================] - 0s 30us/step - loss: 0.0374 - val_loss: 0.0396\n",
      "Epoch 2582/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0372 - val_loss: 0.0397\n",
      "Epoch 2583/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0377 - val_loss: 0.0438\n",
      "Epoch 2584/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0391 - val_loss: 0.0454\n",
      "Epoch 2585/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0346 - val_loss: 0.0418\n",
      "Epoch 2586/3000\n",
      "2370/2370 [==============================] - 0s 30us/step - loss: 0.0361 - val_loss: 0.0512\n",
      "Epoch 2587/3000\n",
      "2370/2370 [==============================] - 0s 30us/step - loss: 0.0364 - val_loss: 0.0415\n",
      "Epoch 2588/3000\n",
      "2370/2370 [==============================] - 0s 33us/step - loss: 0.0384 - val_loss: 0.0394\n",
      "Epoch 2589/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0383 - val_loss: 0.0395\n",
      "Epoch 2590/3000\n",
      "2370/2370 [==============================] - 0s 28us/step - loss: 0.0378 - val_loss: 0.0418\n",
      "Epoch 2591/3000\n",
      "2370/2370 [==============================] - 0s 28us/step - loss: 0.0366 - val_loss: 0.0453\n",
      "Epoch 2592/3000\n",
      "2370/2370 [==============================] - 0s 28us/step - loss: 0.0353 - val_loss: 0.0523\n",
      "Epoch 2593/3000\n",
      "2370/2370 [==============================] - 0s 28us/step - loss: 0.0366 - val_loss: 0.0476\n",
      "Epoch 2594/3000\n",
      "2370/2370 [==============================] - 0s 28us/step - loss: 0.0340 - val_loss: 0.0443\n",
      "Epoch 2595/3000\n",
      "2370/2370 [==============================] - 0s 28us/step - loss: 0.0367 - val_loss: 0.0503\n",
      "Epoch 2596/3000\n",
      "2370/2370 [==============================] - 0s 28us/step - loss: 0.0336 - val_loss: 0.0491\n",
      "Epoch 2597/3000\n",
      "2370/2370 [==============================] - 0s 30us/step - loss: 0.0363 - val_loss: 0.0399\n",
      "Epoch 2598/3000\n",
      "2370/2370 [==============================] - 0s 28us/step - loss: 0.0349 - val_loss: 0.0414\n",
      "Epoch 2599/3000\n",
      "2370/2370 [==============================] - 0s 28us/step - loss: 0.0384 - val_loss: 0.0401\n",
      "Epoch 2600/3000\n",
      "2370/2370 [==============================] - 0s 28us/step - loss: 0.0378 - val_loss: 0.0393\n",
      "Epoch 2601/3000\n",
      "2370/2370 [==============================] - 0s 28us/step - loss: 0.0411 - val_loss: 0.0399\n",
      "Epoch 2602/3000\n",
      "2370/2370 [==============================] - 0s 28us/step - loss: 0.0418 - val_loss: 0.0455\n",
      "Epoch 2603/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0384 - val_loss: 0.0588\n",
      "Epoch 2604/3000\n",
      "2370/2370 [==============================] - 0s 28us/step - loss: 0.0363 - val_loss: 0.0409\n",
      "Epoch 2605/3000\n",
      "2370/2370 [==============================] - 0s 28us/step - loss: 0.0367 - val_loss: 0.0607\n",
      "Epoch 2606/3000\n",
      "2370/2370 [==============================] - 0s 30us/step - loss: 0.0367 - val_loss: 0.0455\n",
      "Epoch 2607/3000\n",
      "2370/2370 [==============================] - 0s 28us/step - loss: 0.0360 - val_loss: 0.0423\n",
      "Epoch 2608/3000\n",
      "2370/2370 [==============================] - 0s 28us/step - loss: 0.0347 - val_loss: 0.0432\n",
      "Epoch 2609/3000\n",
      "2370/2370 [==============================] - 0s 28us/step - loss: 0.0362 - val_loss: 0.0473\n",
      "Epoch 2610/3000\n",
      "2370/2370 [==============================] - 0s 28us/step - loss: 0.0383 - val_loss: 0.0554\n",
      "Epoch 2611/3000\n",
      "2370/2370 [==============================] - 0s 28us/step - loss: 0.0365 - val_loss: 0.0485\n",
      "Epoch 2612/3000\n",
      "2370/2370 [==============================] - 0s 28us/step - loss: 0.0398 - val_loss: 0.0593\n",
      "Epoch 2613/3000\n",
      "2370/2370 [==============================] - 0s 28us/step - loss: 0.0398 - val_loss: 0.0544\n",
      "Epoch 2614/3000\n",
      "2370/2370 [==============================] - 0s 28us/step - loss: 0.0355 - val_loss: 0.0508\n",
      "Epoch 2615/3000\n",
      "2370/2370 [==============================] - 0s 31us/step - loss: 0.0353 - val_loss: 0.0427\n",
      "Epoch 2616/3000\n",
      "2370/2370 [==============================] - 0s 30us/step - loss: 0.0361 - val_loss: 0.0495\n",
      "Epoch 2617/3000\n",
      "2370/2370 [==============================] - 0s 30us/step - loss: 0.0355 - val_loss: 0.0459\n",
      "Epoch 2618/3000\n",
      "2370/2370 [==============================] - 0s 30us/step - loss: 0.0351 - val_loss: 0.0450\n",
      "Epoch 2619/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0369 - val_loss: 0.0423\n",
      "Epoch 2620/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0412 - val_loss: 0.0439\n",
      "Epoch 2621/3000\n",
      "2370/2370 [==============================] - 0s 30us/step - loss: 0.0384 - val_loss: 0.0410\n",
      "Epoch 2622/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0395 - val_loss: 0.0480\n",
      "Epoch 2623/3000\n",
      "2370/2370 [==============================] - 0s 30us/step - loss: 0.0368 - val_loss: 0.0400\n",
      "Epoch 2624/3000\n",
      "2370/2370 [==============================] - 0s 30us/step - loss: 0.0366 - val_loss: 0.0478\n",
      "Epoch 2625/3000\n",
      "2370/2370 [==============================] - 0s 30us/step - loss: 0.0409 - val_loss: 0.0411\n",
      "Epoch 2626/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0352 - val_loss: 0.0449\n",
      "Epoch 2627/3000\n",
      "2370/2370 [==============================] - 0s 30us/step - loss: 0.0368 - val_loss: 0.0419\n",
      "Epoch 2628/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0362 - val_loss: 0.0476\n",
      "Epoch 2629/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0359 - val_loss: 0.0517\n",
      "Epoch 2630/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0394 - val_loss: 0.0511\n",
      "Epoch 2631/3000\n",
      "2370/2370 [==============================] - 0s 41us/step - loss: 0.0358 - val_loss: 0.0440\n",
      "Epoch 2632/3000\n",
      "2370/2370 [==============================] - 0s 83us/step - loss: 0.0378 - val_loss: 0.0662\n",
      "Epoch 2633/3000\n",
      "2370/2370 [==============================] - 0s 39us/step - loss: 0.0374 - val_loss: 0.0450\n",
      "Epoch 2634/3000\n",
      "2370/2370 [==============================] - 0s 32us/step - loss: 0.0349 - val_loss: 0.0444\n",
      "Epoch 2635/3000\n",
      "2370/2370 [==============================] - 0s 32us/step - loss: 0.0378 - val_loss: 0.0479\n",
      "Epoch 2636/3000\n",
      "2370/2370 [==============================] - 0s 33us/step - loss: 0.0393 - val_loss: 0.0425\n",
      "Epoch 2637/3000\n",
      "2370/2370 [==============================] - 0s 35us/step - loss: 0.0365 - val_loss: 0.0488\n",
      "Epoch 2638/3000\n",
      "2370/2370 [==============================] - 0s 33us/step - loss: 0.0383 - val_loss: 0.0455\n",
      "Epoch 2639/3000\n",
      "2370/2370 [==============================] - 0s 33us/step - loss: 0.0342 - val_loss: 0.0544\n",
      "Epoch 2640/3000\n",
      "2370/2370 [==============================] - 0s 33us/step - loss: 0.0364 - val_loss: 0.0454\n",
      "Epoch 2641/3000\n",
      "2370/2370 [==============================] - 0s 37us/step - loss: 0.0347 - val_loss: 0.0438\n",
      "Epoch 2642/3000\n",
      "2370/2370 [==============================] - 0s 32us/step - loss: 0.0379 - val_loss: 0.0456\n",
      "Epoch 2643/3000\n",
      "2370/2370 [==============================] - 0s 33us/step - loss: 0.0379 - val_loss: 0.0449\n",
      "Epoch 2644/3000\n",
      "2370/2370 [==============================] - 0s 34us/step - loss: 0.0380 - val_loss: 0.0420\n",
      "Epoch 2645/3000\n",
      "2370/2370 [==============================] - 0s 33us/step - loss: 0.0369 - val_loss: 0.0587\n",
      "Epoch 2646/3000\n",
      "2370/2370 [==============================] - 0s 32us/step - loss: 0.0374 - val_loss: 0.0471\n",
      "Epoch 2647/3000\n",
      "2370/2370 [==============================] - 0s 33us/step - loss: 0.0397 - val_loss: 0.0455\n",
      "Epoch 2648/3000\n",
      "2370/2370 [==============================] - 0s 32us/step - loss: 0.0355 - val_loss: 0.0442\n",
      "Epoch 2649/3000\n",
      "2370/2370 [==============================] - 0s 32us/step - loss: 0.0371 - val_loss: 0.0701\n",
      "Epoch 2650/3000\n",
      "2370/2370 [==============================] - 0s 33us/step - loss: 0.0351 - val_loss: 0.0584\n",
      "Epoch 2651/3000\n",
      "2370/2370 [==============================] - 0s 34us/step - loss: 0.0336 - val_loss: 0.0408\n",
      "Epoch 2652/3000\n",
      "2370/2370 [==============================] - 0s 32us/step - loss: 0.0364 - val_loss: 0.0436\n",
      "Epoch 2653/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0397 - val_loss: 0.0406\n",
      "Epoch 2654/3000\n",
      "2370/2370 [==============================] - 0s 28us/step - loss: 0.0381 - val_loss: 0.0408\n",
      "Epoch 2655/3000\n",
      "2370/2370 [==============================] - 0s 33us/step - loss: 0.0360 - val_loss: 0.0401\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2656/3000\n",
      "2370/2370 [==============================] - 0s 28us/step - loss: 0.0367 - val_loss: 0.0465\n",
      "Epoch 2657/3000\n",
      "2370/2370 [==============================] - 0s 28us/step - loss: 0.0345 - val_loss: 0.0401\n",
      "Epoch 2658/3000\n",
      "2370/2370 [==============================] - 0s 28us/step - loss: 0.0360 - val_loss: 0.0390\n",
      "Epoch 2659/3000\n",
      "2370/2370 [==============================] - 0s 28us/step - loss: 0.0381 - val_loss: 0.0414\n",
      "Epoch 2660/3000\n",
      "2370/2370 [==============================] - 0s 28us/step - loss: 0.0359 - val_loss: 0.0403\n",
      "Epoch 2661/3000\n",
      "2370/2370 [==============================] - 0s 30us/step - loss: 0.0402 - val_loss: 0.0393\n",
      "Epoch 2662/3000\n",
      "2370/2370 [==============================] - 0s 28us/step - loss: 0.0381 - val_loss: 0.0430\n",
      "Epoch 2663/3000\n",
      "2370/2370 [==============================] - 0s 27us/step - loss: 0.0349 - val_loss: 0.0407\n",
      "Epoch 2664/3000\n",
      "2370/2370 [==============================] - 0s 28us/step - loss: 0.0328 - val_loss: 0.0407\n",
      "Epoch 2665/3000\n",
      "2370/2370 [==============================] - 0s 28us/step - loss: 0.0392 - val_loss: 0.0392\n",
      "Epoch 2666/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0350 - val_loss: 0.0599\n",
      "Epoch 2667/3000\n",
      "2370/2370 [==============================] - 0s 28us/step - loss: 0.0340 - val_loss: 0.0479\n",
      "Epoch 2668/3000\n",
      "2370/2370 [==============================] - 0s 28us/step - loss: 0.0379 - val_loss: 0.0470\n",
      "Epoch 2669/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0348 - val_loss: 0.0396\n",
      "Epoch 2670/3000\n",
      "2370/2370 [==============================] - 0s 30us/step - loss: 0.0370 - val_loss: 0.0553\n",
      "Epoch 2671/3000\n",
      "2370/2370 [==============================] - 0s 28us/step - loss: 0.0384 - val_loss: 0.0473\n",
      "Epoch 2672/3000\n",
      "2370/2370 [==============================] - 0s 28us/step - loss: 0.0410 - val_loss: 0.0435\n",
      "Epoch 2673/3000\n",
      "2370/2370 [==============================] - 0s 28us/step - loss: 0.0367 - val_loss: 0.0567\n",
      "Epoch 2674/3000\n",
      "2370/2370 [==============================] - 0s 28us/step - loss: 0.0377 - val_loss: 0.0477\n",
      "Epoch 2675/3000\n",
      "2370/2370 [==============================] - 0s 28us/step - loss: 0.0358 - val_loss: 0.0442\n",
      "Epoch 2676/3000\n",
      "2370/2370 [==============================] - 0s 28us/step - loss: 0.0429 - val_loss: 0.0433\n",
      "Epoch 2677/3000\n",
      "2370/2370 [==============================] - 0s 28us/step - loss: 0.0352 - val_loss: 0.0566\n",
      "Epoch 2678/3000\n",
      "2370/2370 [==============================] - 0s 28us/step - loss: 0.0376 - val_loss: 0.0546\n",
      "Epoch 2679/3000\n",
      "2370/2370 [==============================] - 0s 30us/step - loss: 0.0364 - val_loss: 0.0447\n",
      "Epoch 2680/3000\n",
      "2370/2370 [==============================] - 0s 28us/step - loss: 0.0362 - val_loss: 0.0395\n",
      "Epoch 2681/3000\n",
      "2370/2370 [==============================] - 0s 28us/step - loss: 0.0354 - val_loss: 0.0645\n",
      "Epoch 2682/3000\n",
      "2370/2370 [==============================] - 0s 28us/step - loss: 0.0391 - val_loss: 0.0473\n",
      "Epoch 2683/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0377 - val_loss: 0.0586\n",
      "Epoch 2684/3000\n",
      "2370/2370 [==============================] - 0s 28us/step - loss: 0.0377 - val_loss: 0.0393\n",
      "Epoch 2685/3000\n",
      "2370/2370 [==============================] - 0s 28us/step - loss: 0.0393 - val_loss: 0.0466\n",
      "Epoch 2686/3000\n",
      "2370/2370 [==============================] - 0s 28us/step - loss: 0.0352 - val_loss: 0.0407\n",
      "Epoch 2687/3000\n",
      "2370/2370 [==============================] - 0s 28us/step - loss: 0.0400 - val_loss: 0.0584\n",
      "Epoch 2688/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0388 - val_loss: 0.0423\n",
      "Epoch 2689/3000\n",
      "2370/2370 [==============================] - 0s 28us/step - loss: 0.0358 - val_loss: 0.0525\n",
      "Epoch 2690/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0404 - val_loss: 0.0678\n",
      "Epoch 2691/3000\n",
      "2370/2370 [==============================] - 0s 28us/step - loss: 0.0350 - val_loss: 0.0399\n",
      "Epoch 2692/3000\n",
      "2370/2370 [==============================] - 0s 28us/step - loss: 0.0360 - val_loss: 0.0507\n",
      "Epoch 2693/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0352 - val_loss: 0.0508\n",
      "Epoch 2694/3000\n",
      "2370/2370 [==============================] - 0s 28us/step - loss: 0.0370 - val_loss: 0.0449\n",
      "Epoch 2695/3000\n",
      "2370/2370 [==============================] - 0s 28us/step - loss: 0.0332 - val_loss: 0.0383\n",
      "Epoch 2696/3000\n",
      "2370/2370 [==============================] - 0s 27us/step - loss: 0.0391 - val_loss: 0.0429\n",
      "Epoch 2697/3000\n",
      "2370/2370 [==============================] - 0s 28us/step - loss: 0.0392 - val_loss: 0.0462\n",
      "Epoch 2698/3000\n",
      "2370/2370 [==============================] - 0s 28us/step - loss: 0.0354 - val_loss: 0.0450\n",
      "Epoch 2699/3000\n",
      "2370/2370 [==============================] - 0s 30us/step - loss: 0.0397 - val_loss: 0.0416\n",
      "Epoch 2700/3000\n",
      "2370/2370 [==============================] - 0s 32us/step - loss: 0.0367 - val_loss: 0.0376\n",
      "Epoch 2701/3000\n",
      "2370/2370 [==============================] - 0s 28us/step - loss: 0.0385 - val_loss: 0.0473\n",
      "Epoch 2702/3000\n",
      "2370/2370 [==============================] - 0s 28us/step - loss: 0.0404 - val_loss: 0.0455\n",
      "Epoch 2703/3000\n",
      "2370/2370 [==============================] - 0s 28us/step - loss: 0.0378 - val_loss: 0.0443\n",
      "Epoch 2704/3000\n",
      "2370/2370 [==============================] - 0s 28us/step - loss: 0.0391 - val_loss: 0.0455\n",
      "Epoch 2705/3000\n",
      "2370/2370 [==============================] - 0s 30us/step - loss: 0.0340 - val_loss: 0.0550\n",
      "Epoch 2706/3000\n",
      "2370/2370 [==============================] - 0s 28us/step - loss: 0.0359 - val_loss: 0.0383\n",
      "Epoch 2707/3000\n",
      "2370/2370 [==============================] - 0s 28us/step - loss: 0.0366 - val_loss: 0.0403\n",
      "Epoch 2708/3000\n",
      "2370/2370 [==============================] - 0s 28us/step - loss: 0.0373 - val_loss: 0.0422\n",
      "Epoch 2709/3000\n",
      "2370/2370 [==============================] - 0s 28us/step - loss: 0.0369 - val_loss: 0.0629\n",
      "Epoch 2710/3000\n",
      "2370/2370 [==============================] - 0s 28us/step - loss: 0.0402 - val_loss: 0.0377\n",
      "Epoch 2711/3000\n",
      "2370/2370 [==============================] - 0s 28us/step - loss: 0.0364 - val_loss: 0.0432\n",
      "Epoch 2712/3000\n",
      "2370/2370 [==============================] - 0s 28us/step - loss: 0.0367 - val_loss: 0.0415\n",
      "Epoch 2713/3000\n",
      "2370/2370 [==============================] - 0s 28us/step - loss: 0.0389 - val_loss: 0.0380\n",
      "Epoch 2714/3000\n",
      "2370/2370 [==============================] - 0s 30us/step - loss: 0.0390 - val_loss: 0.0379\n",
      "Epoch 2715/3000\n",
      "2370/2370 [==============================] - 0s 28us/step - loss: 0.0370 - val_loss: 0.0502\n",
      "Epoch 2716/3000\n",
      "2370/2370 [==============================] - 0s 28us/step - loss: 0.0375 - val_loss: 0.0485\n",
      "Epoch 2717/3000\n",
      "2370/2370 [==============================] - 0s 28us/step - loss: 0.0345 - val_loss: 0.0587\n",
      "Epoch 2718/3000\n",
      "2370/2370 [==============================] - 0s 28us/step - loss: 0.0376 - val_loss: 0.0403\n",
      "Epoch 2719/3000\n",
      "2370/2370 [==============================] - 0s 28us/step - loss: 0.0389 - val_loss: 0.0380\n",
      "Epoch 2720/3000\n",
      "2370/2370 [==============================] - 0s 28us/step - loss: 0.0390 - val_loss: 0.0456\n",
      "Epoch 2721/3000\n",
      "2370/2370 [==============================] - 0s 28us/step - loss: 0.0397 - val_loss: 0.0404\n",
      "Epoch 2722/3000\n",
      "2370/2370 [==============================] - 0s 28us/step - loss: 0.0390 - val_loss: 0.0390\n",
      "Epoch 2723/3000\n",
      "2370/2370 [==============================] - 0s 30us/step - loss: 0.0338 - val_loss: 0.0617\n",
      "Epoch 2724/3000\n",
      "2370/2370 [==============================] - 0s 28us/step - loss: 0.0380 - val_loss: 0.0507\n",
      "Epoch 2725/3000\n",
      "2370/2370 [==============================] - 0s 28us/step - loss: 0.0363 - val_loss: 0.0429\n",
      "Epoch 2726/3000\n",
      "2370/2370 [==============================] - 0s 28us/step - loss: 0.0375 - val_loss: 0.0510\n",
      "Epoch 2727/3000\n",
      "2370/2370 [==============================] - 0s 28us/step - loss: 0.0376 - val_loss: 0.0427\n",
      "Epoch 2728/3000\n",
      "2370/2370 [==============================] - 0s 28us/step - loss: 0.0406 - val_loss: 0.0458\n",
      "Epoch 2729/3000\n",
      "2370/2370 [==============================] - 0s 28us/step - loss: 0.0390 - val_loss: 0.0376\n",
      "Epoch 2730/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0371 - val_loss: 0.0432\n",
      "Epoch 2731/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0350 - val_loss: 0.0557\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2732/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0361 - val_loss: 0.0425\n",
      "Epoch 2733/3000\n",
      "2370/2370 [==============================] - 0s 28us/step - loss: 0.0361 - val_loss: 0.0393\n",
      "Epoch 2734/3000\n",
      "2370/2370 [==============================] - 0s 28us/step - loss: 0.0370 - val_loss: 0.0436\n",
      "Epoch 2735/3000\n",
      "2370/2370 [==============================] - 0s 28us/step - loss: 0.0371 - val_loss: 0.0582\n",
      "Epoch 2736/3000\n",
      "2370/2370 [==============================] - 0s 28us/step - loss: 0.0366 - val_loss: 0.0475\n",
      "Epoch 2737/3000\n",
      "2370/2370 [==============================] - 0s 28us/step - loss: 0.0316 - val_loss: 0.0485\n",
      "Epoch 2738/3000\n",
      "2370/2370 [==============================] - 0s 28us/step - loss: 0.0382 - val_loss: 0.0396\n",
      "Epoch 2739/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0368 - val_loss: 0.0400\n",
      "Epoch 2740/3000\n",
      "2370/2370 [==============================] - 0s 28us/step - loss: 0.0367 - val_loss: 0.0380\n",
      "Epoch 2741/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0385 - val_loss: 0.0571\n",
      "Epoch 2742/3000\n",
      "2370/2370 [==============================] - 0s 32us/step - loss: 0.0421 - val_loss: 0.0410\n",
      "Epoch 2743/3000\n",
      "2370/2370 [==============================] - 0s 37us/step - loss: 0.0370 - val_loss: 0.0432\n",
      "Epoch 2744/3000\n",
      "2370/2370 [==============================] - 0s 33us/step - loss: 0.0367 - val_loss: 0.0450\n",
      "Epoch 2745/3000\n",
      "2370/2370 [==============================] - 0s 32us/step - loss: 0.0352 - val_loss: 0.0460\n",
      "Epoch 2746/3000\n",
      "2370/2370 [==============================] - 0s 33us/step - loss: 0.0392 - val_loss: 0.0381\n",
      "Epoch 2747/3000\n",
      "2370/2370 [==============================] - 0s 31us/step - loss: 0.0377 - val_loss: 0.0383\n",
      "Epoch 2748/3000\n",
      "2370/2370 [==============================] - 0s 30us/step - loss: 0.0373 - val_loss: 0.0385\n",
      "Epoch 2749/3000\n",
      "2370/2370 [==============================] - 0s 28us/step - loss: 0.0354 - val_loss: 0.0506\n",
      "Epoch 2750/3000\n",
      "2370/2370 [==============================] - 0s 28us/step - loss: 0.0361 - val_loss: 0.0432\n",
      "Epoch 2751/3000\n",
      "2370/2370 [==============================] - 0s 28us/step - loss: 0.0385 - val_loss: 0.0426\n",
      "Epoch 2752/3000\n",
      "2370/2370 [==============================] - 0s 28us/step - loss: 0.0359 - val_loss: 0.0387\n",
      "Epoch 2753/3000\n",
      "2370/2370 [==============================] - 0s 28us/step - loss: 0.0346 - val_loss: 0.0466\n",
      "Epoch 2754/3000\n",
      "2370/2370 [==============================] - 0s 28us/step - loss: 0.0383 - val_loss: 0.0424\n",
      "Epoch 2755/3000\n",
      "2370/2370 [==============================] - 0s 28us/step - loss: 0.0358 - val_loss: 0.0471\n",
      "Epoch 2756/3000\n",
      "2370/2370 [==============================] - 0s 28us/step - loss: 0.0360 - val_loss: 0.0602\n",
      "Epoch 2757/3000\n",
      "2370/2370 [==============================] - 0s 30us/step - loss: 0.0371 - val_loss: 0.0470\n",
      "Epoch 2758/3000\n",
      "2370/2370 [==============================] - 0s 27us/step - loss: 0.0352 - val_loss: 0.0512\n",
      "Epoch 2759/3000\n",
      "2370/2370 [==============================] - 0s 28us/step - loss: 0.0374 - val_loss: 0.0524\n",
      "Epoch 2760/3000\n",
      "2370/2370 [==============================] - 0s 28us/step - loss: 0.0365 - val_loss: 0.0460\n",
      "Epoch 2761/3000\n",
      "2370/2370 [==============================] - 0s 28us/step - loss: 0.0345 - val_loss: 0.0506\n",
      "Epoch 2762/3000\n",
      "2370/2370 [==============================] - 0s 28us/step - loss: 0.0352 - val_loss: 0.0726\n",
      "Epoch 2763/3000\n",
      "2370/2370 [==============================] - 0s 28us/step - loss: 0.0390 - val_loss: 0.0522\n",
      "Epoch 2764/3000\n",
      "2370/2370 [==============================] - 0s 28us/step - loss: 0.0384 - val_loss: 0.0388\n",
      "Epoch 2765/3000\n",
      "2370/2370 [==============================] - 0s 28us/step - loss: 0.0371 - val_loss: 0.0390\n",
      "Epoch 2766/3000\n",
      "2370/2370 [==============================] - 0s 31us/step - loss: 0.0334 - val_loss: 0.0445\n",
      "Epoch 2767/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0389 - val_loss: 0.0394\n",
      "Epoch 2768/3000\n",
      "2370/2370 [==============================] - 0s 28us/step - loss: 0.0374 - val_loss: 0.0411\n",
      "Epoch 2769/3000\n",
      "2370/2370 [==============================] - 0s 28us/step - loss: 0.0354 - val_loss: 0.0418\n",
      "Epoch 2770/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0346 - val_loss: 0.0600\n",
      "Epoch 2771/3000\n",
      "2370/2370 [==============================] - 0s 28us/step - loss: 0.0373 - val_loss: 0.0426\n",
      "Epoch 2772/3000\n",
      "2370/2370 [==============================] - 0s 28us/step - loss: 0.0368 - val_loss: 0.0362\n",
      "Epoch 2773/3000\n",
      "2370/2370 [==============================] - 0s 28us/step - loss: 0.0364 - val_loss: 0.0546\n",
      "Epoch 2774/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0355 - val_loss: 0.0510\n",
      "Epoch 2775/3000\n",
      "2370/2370 [==============================] - 0s 28us/step - loss: 0.0370 - val_loss: 0.0404\n",
      "Epoch 2776/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0379 - val_loss: 0.0499\n",
      "Epoch 2777/3000\n",
      "2370/2370 [==============================] - 0s 28us/step - loss: 0.0388 - val_loss: 0.0483\n",
      "Epoch 2778/3000\n",
      "2370/2370 [==============================] - 0s 28us/step - loss: 0.0388 - val_loss: 0.0449\n",
      "Epoch 2779/3000\n",
      "2370/2370 [==============================] - 0s 28us/step - loss: 0.0357 - val_loss: 0.0405\n",
      "Epoch 2780/3000\n",
      "2370/2370 [==============================] - 0s 28us/step - loss: 0.0399 - val_loss: 0.0422\n",
      "Epoch 2781/3000\n",
      "2370/2370 [==============================] - 0s 28us/step - loss: 0.0381 - val_loss: 0.0448\n",
      "Epoch 2782/3000\n",
      "2370/2370 [==============================] - 0s 28us/step - loss: 0.0368 - val_loss: 0.0372\n",
      "Epoch 2783/3000\n",
      "2370/2370 [==============================] - 0s 28us/step - loss: 0.0359 - val_loss: 0.0482\n",
      "Epoch 2784/3000\n",
      "2370/2370 [==============================] - 0s 28us/step - loss: 0.0374 - val_loss: 0.0493\n",
      "Epoch 2785/3000\n",
      "2370/2370 [==============================] - 0s 28us/step - loss: 0.0344 - val_loss: 0.0452\n",
      "Epoch 2786/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0354 - val_loss: 0.0430\n",
      "Epoch 2787/3000\n",
      "2370/2370 [==============================] - 0s 31us/step - loss: 0.0389 - val_loss: 0.0495\n",
      "Epoch 2788/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0379 - val_loss: 0.0386\n",
      "Epoch 2789/3000\n",
      "2370/2370 [==============================] - 0s 28us/step - loss: 0.0360 - val_loss: 0.0399\n",
      "Epoch 2790/3000\n",
      "2370/2370 [==============================] - 0s 28us/step - loss: 0.0392 - val_loss: 0.0464\n",
      "Epoch 2791/3000\n",
      "2370/2370 [==============================] - 0s 28us/step - loss: 0.0379 - val_loss: 0.0447\n",
      "Epoch 2792/3000\n",
      "2370/2370 [==============================] - 0s 30us/step - loss: 0.0390 - val_loss: 0.0548\n",
      "Epoch 2793/3000\n",
      "2370/2370 [==============================] - 0s 28us/step - loss: 0.0356 - val_loss: 0.0485\n",
      "Epoch 2794/3000\n",
      "2370/2370 [==============================] - 0s 28us/step - loss: 0.0367 - val_loss: 0.0513\n",
      "Epoch 2795/3000\n",
      "2370/2370 [==============================] - 0s 28us/step - loss: 0.0395 - val_loss: 0.0443\n",
      "Epoch 2796/3000\n",
      "2370/2370 [==============================] - 0s 28us/step - loss: 0.0375 - val_loss: 0.0391\n",
      "Epoch 2797/3000\n",
      "2370/2370 [==============================] - 0s 28us/step - loss: 0.0388 - val_loss: 0.0660\n",
      "Epoch 2798/3000\n",
      "2370/2370 [==============================] - 0s 28us/step - loss: 0.0370 - val_loss: 0.0536\n",
      "Epoch 2799/3000\n",
      "2370/2370 [==============================] - 0s 28us/step - loss: 0.0371 - val_loss: 0.0456\n",
      "Epoch 2800/3000\n",
      "2370/2370 [==============================] - 0s 28us/step - loss: 0.0375 - val_loss: 0.0428\n",
      "Epoch 2801/3000\n",
      "2370/2370 [==============================] - 0s 30us/step - loss: 0.0396 - val_loss: 0.0490\n",
      "Epoch 2802/3000\n",
      "2370/2370 [==============================] - 0s 28us/step - loss: 0.0387 - val_loss: 0.0394\n",
      "Epoch 2803/3000\n",
      "2370/2370 [==============================] - 0s 28us/step - loss: 0.0376 - val_loss: 0.0401\n",
      "Epoch 2804/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0373 - val_loss: 0.0388\n",
      "Epoch 2805/3000\n",
      "2370/2370 [==============================] - 0s 28us/step - loss: 0.0325 - val_loss: 0.0490\n",
      "Epoch 2806/3000\n",
      "2370/2370 [==============================] - 0s 28us/step - loss: 0.0377 - val_loss: 0.0410\n",
      "Epoch 2807/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0359 - val_loss: 0.0421\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2808/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0378 - val_loss: 0.0459\n",
      "Epoch 2809/3000\n",
      "2370/2370 [==============================] - 0s 28us/step - loss: 0.0369 - val_loss: 0.0452\n",
      "Epoch 2810/3000\n",
      "2370/2370 [==============================] - 0s 31us/step - loss: 0.0386 - val_loss: 0.0471\n",
      "Epoch 2811/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0352 - val_loss: 0.0476\n",
      "Epoch 2812/3000\n",
      "2370/2370 [==============================] - 0s 28us/step - loss: 0.0401 - val_loss: 0.0443\n",
      "Epoch 2813/3000\n",
      "2370/2370 [==============================] - 0s 28us/step - loss: 0.0397 - val_loss: 0.0591\n",
      "Epoch 2814/3000\n",
      "2370/2370 [==============================] - 0s 28us/step - loss: 0.0342 - val_loss: 0.0461\n",
      "Epoch 2815/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0381 - val_loss: 0.0411\n",
      "Epoch 2816/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0395 - val_loss: 0.0507\n",
      "Epoch 2817/3000\n",
      "2370/2370 [==============================] - 0s 28us/step - loss: 0.0372 - val_loss: 0.0456\n",
      "Epoch 2818/3000\n",
      "2370/2370 [==============================] - 0s 28us/step - loss: 0.0358 - val_loss: 0.0453\n",
      "Epoch 2819/3000\n",
      "2370/2370 [==============================] - 0s 28us/step - loss: 0.0384 - val_loss: 0.0450\n",
      "Epoch 2820/3000\n",
      "2370/2370 [==============================] - 0s 28us/step - loss: 0.0376 - val_loss: 0.0445\n",
      "Epoch 2821/3000\n",
      "2370/2370 [==============================] - 0s 28us/step - loss: 0.0363 - val_loss: 0.0462\n",
      "Epoch 2822/3000\n",
      "2370/2370 [==============================] - 0s 28us/step - loss: 0.0396 - val_loss: 0.0467\n",
      "Epoch 2823/3000\n",
      "2370/2370 [==============================] - 0s 28us/step - loss: 0.0383 - val_loss: 0.0468\n",
      "Epoch 2824/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0367 - val_loss: 0.0468\n",
      "Epoch 2825/3000\n",
      "2370/2370 [==============================] - 0s 28us/step - loss: 0.0353 - val_loss: 0.0449\n",
      "Epoch 2826/3000\n",
      "2370/2370 [==============================] - 0s 28us/step - loss: 0.0391 - val_loss: 0.0503\n",
      "Epoch 2827/3000\n",
      "2370/2370 [==============================] - 0s 28us/step - loss: 0.0382 - val_loss: 0.0501\n",
      "Epoch 2828/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0397 - val_loss: 0.0479\n",
      "Epoch 2829/3000\n",
      "2370/2370 [==============================] - 0s 28us/step - loss: 0.0357 - val_loss: 0.0627\n",
      "Epoch 2830/3000\n",
      "2370/2370 [==============================] - 0s 33us/step - loss: 0.0374 - val_loss: 0.0473\n",
      "Epoch 2831/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0364 - val_loss: 0.0512\n",
      "Epoch 2832/3000\n",
      "2370/2370 [==============================] - 0s 28us/step - loss: 0.0367 - val_loss: 0.0409\n",
      "Epoch 2833/3000\n",
      "2370/2370 [==============================] - 0s 28us/step - loss: 0.0386 - val_loss: 0.0488\n",
      "Epoch 2834/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0400 - val_loss: 0.0405\n",
      "Epoch 2835/3000\n",
      "2370/2370 [==============================] - 0s 28us/step - loss: 0.0355 - val_loss: 0.0409\n",
      "Epoch 2836/3000\n",
      "2370/2370 [==============================] - 0s 30us/step - loss: 0.0360 - val_loss: 0.0574\n",
      "Epoch 2837/3000\n",
      "2370/2370 [==============================] - 0s 28us/step - loss: 0.0359 - val_loss: 0.0410\n",
      "Epoch 2838/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0384 - val_loss: 0.0436\n",
      "Epoch 2839/3000\n",
      "2370/2370 [==============================] - 0s 28us/step - loss: 0.0335 - val_loss: 0.0553\n",
      "Epoch 2840/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0358 - val_loss: 0.0442\n",
      "Epoch 2841/3000\n",
      "2370/2370 [==============================] - 0s 28us/step - loss: 0.0372 - val_loss: 0.0589\n",
      "Epoch 2842/3000\n",
      "2370/2370 [==============================] - 0s 28us/step - loss: 0.0407 - val_loss: 0.0473\n",
      "Epoch 2843/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0374 - val_loss: 0.0554\n",
      "Epoch 2844/3000\n",
      "2370/2370 [==============================] - 0s 28us/step - loss: 0.0401 - val_loss: 0.0439\n",
      "Epoch 2845/3000\n",
      "2370/2370 [==============================] - 0s 30us/step - loss: 0.0393 - val_loss: 0.0463\n",
      "Epoch 2846/3000\n",
      "2370/2370 [==============================] - 0s 28us/step - loss: 0.0350 - val_loss: 0.0529\n",
      "Epoch 2847/3000\n",
      "2370/2370 [==============================] - 0s 28us/step - loss: 0.0396 - val_loss: 0.0442\n",
      "Epoch 2848/3000\n",
      "2370/2370 [==============================] - 0s 28us/step - loss: 0.0379 - val_loss: 0.0483\n",
      "Epoch 2849/3000\n",
      "2370/2370 [==============================] - 0s 28us/step - loss: 0.0376 - val_loss: 0.0569\n",
      "Epoch 2850/3000\n",
      "2370/2370 [==============================] - 0s 28us/step - loss: 0.0398 - val_loss: 0.0499\n",
      "Epoch 2851/3000\n",
      "2370/2370 [==============================] - 0s 28us/step - loss: 0.0389 - val_loss: 0.0453\n",
      "Epoch 2852/3000\n",
      "2370/2370 [==============================] - 0s 28us/step - loss: 0.0385 - val_loss: 0.0539\n",
      "Epoch 2853/3000\n",
      "2370/2370 [==============================] - 0s 28us/step - loss: 0.0410 - val_loss: 0.0631\n",
      "Epoch 2854/3000\n",
      "2370/2370 [==============================] - 0s 30us/step - loss: 0.0376 - val_loss: 0.0571\n",
      "Epoch 2855/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0370 - val_loss: 0.0433\n",
      "Epoch 2856/3000\n",
      "2370/2370 [==============================] - 0s 28us/step - loss: 0.0371 - val_loss: 0.0432\n",
      "Epoch 2857/3000\n",
      "2370/2370 [==============================] - 0s 28us/step - loss: 0.0388 - val_loss: 0.0471\n",
      "Epoch 2858/3000\n",
      "2370/2370 [==============================] - 0s 28us/step - loss: 0.0385 - val_loss: 0.0425\n",
      "Epoch 2859/3000\n",
      "2370/2370 [==============================] - 0s 28us/step - loss: 0.0390 - val_loss: 0.0506\n",
      "Epoch 2860/3000\n",
      "2370/2370 [==============================] - 0s 28us/step - loss: 0.0345 - val_loss: 0.0459\n",
      "Epoch 2861/3000\n",
      "2370/2370 [==============================] - 0s 28us/step - loss: 0.0342 - val_loss: 0.0508\n",
      "Epoch 2862/3000\n",
      "2370/2370 [==============================] - 0s 28us/step - loss: 0.0385 - val_loss: 0.0518\n",
      "Epoch 2863/3000\n",
      "2370/2370 [==============================] - 0s 28us/step - loss: 0.0345 - val_loss: 0.0472\n",
      "Epoch 2864/3000\n",
      "2370/2370 [==============================] - 0s 28us/step - loss: 0.0386 - val_loss: 0.0569\n",
      "Epoch 2865/3000\n",
      "2370/2370 [==============================] - 0s 28us/step - loss: 0.0345 - val_loss: 0.0640\n",
      "Epoch 2866/3000\n",
      "2370/2370 [==============================] - 0s 27us/step - loss: 0.0405 - val_loss: 0.0532\n",
      "Epoch 2867/3000\n",
      "2370/2370 [==============================] - 0s 28us/step - loss: 0.0379 - val_loss: 0.0425\n",
      "Epoch 2868/3000\n",
      "2370/2370 [==============================] - 0s 28us/step - loss: 0.0405 - val_loss: 0.0512\n",
      "Epoch 2869/3000\n",
      "2370/2370 [==============================] - 0s 28us/step - loss: 0.0380 - val_loss: 0.0610\n",
      "Epoch 2870/3000\n",
      "2370/2370 [==============================] - 0s 28us/step - loss: 0.0373 - val_loss: 0.0565\n",
      "Epoch 2871/3000\n",
      "2370/2370 [==============================] - 0s 28us/step - loss: 0.0376 - val_loss: 0.0484\n",
      "Epoch 2872/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0370 - val_loss: 0.0458\n",
      "Epoch 2873/3000\n",
      "2370/2370 [==============================] - 0s 28us/step - loss: 0.0328 - val_loss: 0.0507\n",
      "Epoch 2874/3000\n",
      "2370/2370 [==============================] - 0s 30us/step - loss: 0.0374 - val_loss: 0.0463\n",
      "Epoch 2875/3000\n",
      "2370/2370 [==============================] - 0s 31us/step - loss: 0.0350 - val_loss: 0.0588\n",
      "Epoch 2876/3000\n",
      "2370/2370 [==============================] - 0s 28us/step - loss: 0.0362 - val_loss: 0.0564\n",
      "Epoch 2877/3000\n",
      "2370/2370 [==============================] - 0s 28us/step - loss: 0.0402 - val_loss: 0.0473\n",
      "Epoch 2878/3000\n",
      "2370/2370 [==============================] - 0s 28us/step - loss: 0.0370 - val_loss: 0.0424\n",
      "Epoch 2879/3000\n",
      "2370/2370 [==============================] - 0s 28us/step - loss: 0.0360 - val_loss: 0.0431\n",
      "Epoch 2880/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0389 - val_loss: 0.0527\n",
      "Epoch 2881/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0370 - val_loss: 0.0402\n",
      "Epoch 2882/3000\n",
      "2370/2370 [==============================] - 0s 28us/step - loss: 0.0377 - val_loss: 0.0409\n",
      "Epoch 2883/3000\n",
      "2370/2370 [==============================] - 0s 28us/step - loss: 0.0363 - val_loss: 0.0474\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2884/3000\n",
      "2370/2370 [==============================] - 0s 28us/step - loss: 0.0364 - val_loss: 0.0519\n",
      "Epoch 2885/3000\n",
      "2370/2370 [==============================] - 0s 28us/step - loss: 0.0380 - val_loss: 0.0443\n",
      "Epoch 2886/3000\n",
      "2370/2370 [==============================] - 0s 28us/step - loss: 0.0379 - val_loss: 0.0579\n",
      "Epoch 2887/3000\n",
      "2370/2370 [==============================] - 0s 28us/step - loss: 0.0408 - val_loss: 0.0491\n",
      "Epoch 2888/3000\n",
      "2370/2370 [==============================] - 0s 28us/step - loss: 0.0358 - val_loss: 0.0449\n",
      "Epoch 2889/3000\n",
      "2370/2370 [==============================] - 0s 30us/step - loss: 0.0387 - val_loss: 0.0508\n",
      "Epoch 2890/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0356 - val_loss: 0.0498\n",
      "Epoch 2891/3000\n",
      "2370/2370 [==============================] - 0s 28us/step - loss: 0.0368 - val_loss: 0.0497\n",
      "Epoch 2892/3000\n",
      "2370/2370 [==============================] - 0s 28us/step - loss: 0.0398 - val_loss: 0.0421\n",
      "Epoch 2893/3000\n",
      "2370/2370 [==============================] - 0s 28us/step - loss: 0.0427 - val_loss: 0.0522\n",
      "Epoch 2894/3000\n",
      "2370/2370 [==============================] - 0s 28us/step - loss: 0.0366 - val_loss: 0.0419\n",
      "Epoch 2895/3000\n",
      "2370/2370 [==============================] - 0s 28us/step - loss: 0.0363 - val_loss: 0.0549\n",
      "Epoch 2896/3000\n",
      "2370/2370 [==============================] - 0s 28us/step - loss: 0.0373 - val_loss: 0.0591\n",
      "Epoch 2897/3000\n",
      "2370/2370 [==============================] - 0s 28us/step - loss: 0.0396 - val_loss: 0.0592\n",
      "Epoch 2898/3000\n",
      "2370/2370 [==============================] - 0s 30us/step - loss: 0.0360 - val_loss: 0.0454\n",
      "Epoch 2899/3000\n",
      "2370/2370 [==============================] - 0s 28us/step - loss: 0.0361 - val_loss: 0.0530\n",
      "Epoch 2900/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0353 - val_loss: 0.0434\n",
      "Epoch 2901/3000\n",
      "2370/2370 [==============================] - 0s 28us/step - loss: 0.0334 - val_loss: 0.0497\n",
      "Epoch 2902/3000\n",
      "2370/2370 [==============================] - 0s 28us/step - loss: 0.0341 - val_loss: 0.0530\n",
      "Epoch 2903/3000\n",
      "2370/2370 [==============================] - 0s 30us/step - loss: 0.0377 - val_loss: 0.0648\n",
      "Epoch 2904/3000\n",
      "2370/2370 [==============================] - 0s 32us/step - loss: 0.0373 - val_loss: 0.0473\n",
      "Epoch 2905/3000\n",
      "2370/2370 [==============================] - 0s 28us/step - loss: 0.0385 - val_loss: 0.0557\n",
      "Epoch 2906/3000\n",
      "2370/2370 [==============================] - 0s 28us/step - loss: 0.0371 - val_loss: 0.0696\n",
      "Epoch 2907/3000\n",
      "2370/2370 [==============================] - 0s 30us/step - loss: 0.0372 - val_loss: 0.0473\n",
      "Epoch 2908/3000\n",
      "2370/2370 [==============================] - 0s 28us/step - loss: 0.0367 - val_loss: 0.0527\n",
      "Epoch 2909/3000\n",
      "2370/2370 [==============================] - 0s 28us/step - loss: 0.0348 - val_loss: 0.0557\n",
      "Epoch 2910/3000\n",
      "2370/2370 [==============================] - 0s 28us/step - loss: 0.0381 - val_loss: 0.0434\n",
      "Epoch 2911/3000\n",
      "2370/2370 [==============================] - 0s 28us/step - loss: 0.0383 - val_loss: 0.0526\n",
      "Epoch 2912/3000\n",
      "2370/2370 [==============================] - 0s 28us/step - loss: 0.0368 - val_loss: 0.0429\n",
      "Epoch 2913/3000\n",
      "2370/2370 [==============================] - 0s 28us/step - loss: 0.0393 - val_loss: 0.0581\n",
      "Epoch 2914/3000\n",
      "2370/2370 [==============================] - 0s 28us/step - loss: 0.0392 - val_loss: 0.0490\n",
      "Epoch 2915/3000\n",
      "2370/2370 [==============================] - 0s 28us/step - loss: 0.0367 - val_loss: 0.0507\n",
      "Epoch 2916/3000\n",
      "2370/2370 [==============================] - 0s 30us/step - loss: 0.0330 - val_loss: 0.0676\n",
      "Epoch 2917/3000\n",
      "2370/2370 [==============================] - 0s 28us/step - loss: 0.0361 - val_loss: 0.0428\n",
      "Epoch 2918/3000\n",
      "2370/2370 [==============================] - 0s 28us/step - loss: 0.0382 - val_loss: 0.0421\n",
      "Epoch 2919/3000\n",
      "2370/2370 [==============================] - 0s 28us/step - loss: 0.0375 - val_loss: 0.0483\n",
      "Epoch 2920/3000\n",
      "2370/2370 [==============================] - 0s 28us/step - loss: 0.0381 - val_loss: 0.0459\n",
      "Epoch 2921/3000\n",
      "2370/2370 [==============================] - 0s 28us/step - loss: 0.0371 - val_loss: 0.0583\n",
      "Epoch 2922/3000\n",
      "2370/2370 [==============================] - 0s 28us/step - loss: 0.0322 - val_loss: 0.0468\n",
      "Epoch 2923/3000\n",
      "2370/2370 [==============================] - 0s 28us/step - loss: 0.0397 - val_loss: 0.0426\n",
      "Epoch 2924/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0393 - val_loss: 0.0448\n",
      "Epoch 2925/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0404 - val_loss: 0.0401\n",
      "Epoch 2926/3000\n",
      "2370/2370 [==============================] - 0s 28us/step - loss: 0.0395 - val_loss: 0.0465\n",
      "Epoch 2927/3000\n",
      "2370/2370 [==============================] - 0s 28us/step - loss: 0.0410 - val_loss: 0.0437\n",
      "Epoch 2928/3000\n",
      "2370/2370 [==============================] - 0s 28us/step - loss: 0.0357 - val_loss: 0.0443\n",
      "Epoch 2929/3000\n",
      "2370/2370 [==============================] - 0s 28us/step - loss: 0.0406 - val_loss: 0.0476\n",
      "Epoch 2930/3000\n",
      "2370/2370 [==============================] - 0s 28us/step - loss: 0.0396 - val_loss: 0.0482\n",
      "Epoch 2931/3000\n",
      "2370/2370 [==============================] - 0s 31us/step - loss: 0.0358 - val_loss: 0.0531\n",
      "Epoch 2932/3000\n",
      "2370/2370 [==============================] - 0s 30us/step - loss: 0.0393 - val_loss: 0.0539\n",
      "Epoch 2933/3000\n",
      "2370/2370 [==============================] - 0s 31us/step - loss: 0.0350 - val_loss: 0.0438\n",
      "Epoch 2934/3000\n",
      "2370/2370 [==============================] - 0s 28us/step - loss: 0.0348 - val_loss: 0.0418\n",
      "Epoch 2935/3000\n",
      "2370/2370 [==============================] - 0s 28us/step - loss: 0.0335 - val_loss: 0.0399\n",
      "Epoch 2936/3000\n",
      "2370/2370 [==============================] - 0s 28us/step - loss: 0.0350 - val_loss: 0.0420\n",
      "Epoch 2937/3000\n",
      "2370/2370 [==============================] - 0s 28us/step - loss: 0.0392 - val_loss: 0.0548\n",
      "Epoch 2938/3000\n",
      "2370/2370 [==============================] - 0s 27us/step - loss: 0.0413 - val_loss: 0.0495\n",
      "Epoch 2939/3000\n",
      "2370/2370 [==============================] - 0s 28us/step - loss: 0.0333 - val_loss: 0.0626\n",
      "Epoch 2940/3000\n",
      "2370/2370 [==============================] - 0s 28us/step - loss: 0.0374 - val_loss: 0.0437\n",
      "Epoch 2941/3000\n",
      "2370/2370 [==============================] - 0s 28us/step - loss: 0.0324 - val_loss: 0.0429\n",
      "Epoch 2942/3000\n",
      "2370/2370 [==============================] - 0s 30us/step - loss: 0.0371 - val_loss: 0.0418\n",
      "Epoch 2943/3000\n",
      "2370/2370 [==============================] - 0s 28us/step - loss: 0.0369 - val_loss: 0.0403\n",
      "Epoch 2944/3000\n",
      "2370/2370 [==============================] - 0s 28us/step - loss: 0.0366 - val_loss: 0.0634\n",
      "Epoch 2945/3000\n",
      "2370/2370 [==============================] - 0s 28us/step - loss: 0.0374 - val_loss: 0.0495\n",
      "Epoch 2946/3000\n",
      "2370/2370 [==============================] - 0s 28us/step - loss: 0.0384 - val_loss: 0.0417\n",
      "Epoch 2947/3000\n",
      "2370/2370 [==============================] - 0s 32us/step - loss: 0.0347 - val_loss: 0.0536\n",
      "Epoch 2948/3000\n",
      "2370/2370 [==============================] - 0s 28us/step - loss: 0.0377 - val_loss: 0.0522\n",
      "Epoch 2949/3000\n",
      "2370/2370 [==============================] - 0s 28us/step - loss: 0.0401 - val_loss: 0.0463\n",
      "Epoch 2950/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0351 - val_loss: 0.0403\n",
      "Epoch 2951/3000\n",
      "2370/2370 [==============================] - 0s 30us/step - loss: 0.0406 - val_loss: 0.0445\n",
      "Epoch 2952/3000\n",
      "2370/2370 [==============================] - 0s 28us/step - loss: 0.0360 - val_loss: 0.0463\n",
      "Epoch 2953/3000\n",
      "2370/2370 [==============================] - 0s 28us/step - loss: 0.0356 - val_loss: 0.0514\n",
      "Epoch 2954/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0336 - val_loss: 0.0395\n",
      "Epoch 2955/3000\n",
      "2370/2370 [==============================] - 0s 28us/step - loss: 0.0403 - val_loss: 0.0402\n",
      "Epoch 2956/3000\n",
      "2370/2370 [==============================] - 0s 28us/step - loss: 0.0415 - val_loss: 0.0428\n",
      "Epoch 2957/3000\n",
      "2370/2370 [==============================] - 0s 28us/step - loss: 0.0411 - val_loss: 0.0404\n",
      "Epoch 2958/3000\n",
      "2370/2370 [==============================] - 0s 28us/step - loss: 0.0354 - val_loss: 0.0559\n",
      "Epoch 2959/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0378 - val_loss: 0.0430\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2960/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0395 - val_loss: 0.0414\n",
      "Epoch 2961/3000\n",
      "2370/2370 [==============================] - 0s 28us/step - loss: 0.0360 - val_loss: 0.0401\n",
      "Epoch 2962/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0382 - val_loss: 0.0573\n",
      "Epoch 2963/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0365 - val_loss: 0.0590\n",
      "Epoch 2964/3000\n",
      "2370/2370 [==============================] - 0s 28us/step - loss: 0.0387 - val_loss: 0.0414\n",
      "Epoch 2965/3000\n",
      "2370/2370 [==============================] - 0s 28us/step - loss: 0.0376 - val_loss: 0.0415\n",
      "Epoch 2966/3000\n",
      "2370/2370 [==============================] - 0s 28us/step - loss: 0.0341 - val_loss: 0.0472\n",
      "Epoch 2967/3000\n",
      "2370/2370 [==============================] - 0s 28us/step - loss: 0.0381 - val_loss: 0.0420\n",
      "Epoch 2968/3000\n",
      "2370/2370 [==============================] - 0s 30us/step - loss: 0.0370 - val_loss: 0.0407\n",
      "Epoch 2969/3000\n",
      "2370/2370 [==============================] - 0s 28us/step - loss: 0.0382 - val_loss: 0.0424\n",
      "Epoch 2970/3000\n",
      "2370/2370 [==============================] - 0s 28us/step - loss: 0.0384 - val_loss: 0.0432\n",
      "Epoch 2971/3000\n",
      "2370/2370 [==============================] - 0s 28us/step - loss: 0.0376 - val_loss: 0.0430\n",
      "Epoch 2972/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0407 - val_loss: 0.0436\n",
      "Epoch 2973/3000\n",
      "2370/2370 [==============================] - 0s 28us/step - loss: 0.0351 - val_loss: 0.0554\n",
      "Epoch 2974/3000\n",
      "2370/2370 [==============================] - 0s 28us/step - loss: 0.0381 - val_loss: 0.0525\n",
      "Epoch 2975/3000\n",
      "2370/2370 [==============================] - 0s 28us/step - loss: 0.0352 - val_loss: 0.0486\n",
      "Epoch 2976/3000\n",
      "2370/2370 [==============================] - 0s 28us/step - loss: 0.0420 - val_loss: 0.0413\n",
      "Epoch 2977/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0363 - val_loss: 0.0444\n",
      "Epoch 2978/3000\n",
      "2370/2370 [==============================] - 0s 33us/step - loss: 0.0390 - val_loss: 0.0514\n",
      "Epoch 2979/3000\n",
      "2370/2370 [==============================] - 0s 28us/step - loss: 0.0371 - val_loss: 0.0554\n",
      "Epoch 2980/3000\n",
      "2370/2370 [==============================] - 0s 28us/step - loss: 0.0355 - val_loss: 0.0528\n",
      "Epoch 2981/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0363 - val_loss: 0.0450\n",
      "Epoch 2982/3000\n",
      "2370/2370 [==============================] - 0s 28us/step - loss: 0.0358 - val_loss: 0.0546\n",
      "Epoch 2983/3000\n",
      "2370/2370 [==============================] - 0s 28us/step - loss: 0.0364 - val_loss: 0.0662\n",
      "Epoch 2984/3000\n",
      "2370/2370 [==============================] - 0s 28us/step - loss: 0.0361 - val_loss: 0.0442\n",
      "Epoch 2985/3000\n",
      "2370/2370 [==============================] - 0s 28us/step - loss: 0.0396 - val_loss: 0.0499\n",
      "Epoch 2986/3000\n",
      "2370/2370 [==============================] - 0s 30us/step - loss: 0.0369 - val_loss: 0.0666\n",
      "Epoch 2987/3000\n",
      "2370/2370 [==============================] - 0s 28us/step - loss: 0.0427 - val_loss: 0.0481\n",
      "Epoch 2988/3000\n",
      "2370/2370 [==============================] - 0s 28us/step - loss: 0.0404 - val_loss: 0.0456\n",
      "Epoch 2989/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0362 - val_loss: 0.0443\n",
      "Epoch 2990/3000\n",
      "2370/2370 [==============================] - 0s 28us/step - loss: 0.0375 - val_loss: 0.0463\n",
      "Epoch 2991/3000\n",
      "2370/2370 [==============================] - 0s 31us/step - loss: 0.0410 - val_loss: 0.0443\n",
      "Epoch 2992/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0392 - val_loss: 0.0561\n",
      "Epoch 2993/3000\n",
      "2370/2370 [==============================] - 0s 28us/step - loss: 0.0393 - val_loss: 0.0490\n",
      "Epoch 2994/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0382 - val_loss: 0.0462\n",
      "Epoch 2995/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0380 - val_loss: 0.0445\n",
      "Epoch 2996/3000\n",
      "2370/2370 [==============================] - 0s 28us/step - loss: 0.0376 - val_loss: 0.0521\n",
      "Epoch 2997/3000\n",
      "2370/2370 [==============================] - 0s 28us/step - loss: 0.0365 - val_loss: 0.0432\n",
      "Epoch 2998/3000\n",
      "2370/2370 [==============================] - 0s 28us/step - loss: 0.0327 - val_loss: 0.0503\n",
      "Epoch 2999/3000\n",
      "2370/2370 [==============================] - 0s 28us/step - loss: 0.0363 - val_loss: 0.0596\n",
      "Epoch 3000/3000\n",
      "2370/2370 [==============================] - 0s 28us/step - loss: 0.0348 - val_loss: 0.0512\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "411.3968336582184"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "start_seq = time.time()\n",
    "seq_history=seq_model.fit(x_train, y_train, epochs=3000, batch_size=50, verbose=1, validation_data=(x_test, y_test));\n",
    "end_seq = time.time()\n",
    "time_seq = end_seq - start_seq\n",
    "time_seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2370/2370 [==============================] - 0s 18us/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.008189283659532353"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seq_model.evaluate(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from matplotlib import pyplot\n",
    "plt.plot(seq_history.history['loss'], label='FF training Loss')\n",
    "plt.plot(seq_history.history['val_loss'], label='FF testing Loss')\n",
    "plt.yscale(\"log\")\n",
    "plt.grid()\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rolling_seq_prediction(model,x_test,y_test):\n",
    "    predictions = list()\n",
    "    for i in x_test.index:\n",
    "        # make one-step forecast\n",
    "        X = x_test.loc[i].to_numpy().reshape(1,x_test.shape[1]) # reshape single line to have just 1 row\n",
    "        y = y_test.loc[i]\n",
    "        yhat = seq_model.predict(X).reshape(1) # reshape prediction to be just a 1D array\n",
    "        # store forecast\n",
    "        predictions.append(yhat[0])\n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "yhat = rolling_seq_prediction(seq_model,x_test,y_test)\n",
    "raw_seq_predictions = pd.DataFrame(yhat,columns = ['seq_prediction'], index = test_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Invert scaling for prediction data\n",
    "x_test = pd.DataFrame(x_test, columns = x_columns, index = test_index)\n",
    "unscaled_seq_predictions = pd.concat([x_test, raw_seq_predictions], axis=1)\n",
    "#unscaled_seq_predictions = pd.concat([unscaled_seq_predictions, test_data.google_45d], axis=1)\n",
    "unscaled_seq_predictions = pd.DataFrame(scaler.inverse_transform(unscaled_seq_predictions), columns=unscaled_seq_predictions.columns, index=unscaled_seq_predictions.index)\n",
    "\n",
    "# Invert scaling for actual data\n",
    "unscaled_data = pd.concat([x_test, y_test], axis=1)\n",
    "#unscaled_data = pd.concat([unscaled_data, test_data.google_45d], axis=1)\n",
    "unscaled_data = pd.DataFrame(scaler.inverse_transform(unscaled_data), columns=unscaled_data.columns, index=unscaled_data.index)\n",
    "#unscaled_data = unscaled_data['google_45d_sta']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "backup['unscaled_seq_predictions'] = unscaled_seq_predictions\n",
    "backup['unscaled_data'] = unscaled_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>google_open</th>\n",
       "      <th>google_high</th>\n",
       "      <th>google_low</th>\n",
       "      <th>google_close</th>\n",
       "      <th>google</th>\n",
       "      <th>google_volume</th>\n",
       "      <th>google_ra_04</th>\n",
       "      <th>google_std_04</th>\n",
       "      <th>google_ra_09</th>\n",
       "      <th>google_std_09</th>\n",
       "      <th>...</th>\n",
       "      <th>nasdaq_ra_04</th>\n",
       "      <th>nasdaq_std_04</th>\n",
       "      <th>nasdaq_ra_09</th>\n",
       "      <th>nasdaq_std_09</th>\n",
       "      <th>nasdaq_ra_18</th>\n",
       "      <th>nasdaq_std_18</th>\n",
       "      <th>google_daily_vol</th>\n",
       "      <th>s&amp;p_daily_vol</th>\n",
       "      <th>nasdaq_daily_vol</th>\n",
       "      <th>seq_prediction</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Date</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2018-07-31</th>\n",
       "      <td>1220.010010</td>\n",
       "      <td>1227.588013</td>\n",
       "      <td>1205.599976</td>\n",
       "      <td>1217.260010</td>\n",
       "      <td>1217.260010</td>\n",
       "      <td>1644700.0</td>\n",
       "      <td>1218.500000</td>\n",
       "      <td>1.753611</td>\n",
       "      <td>1237.301409</td>\n",
       "      <td>24.147664</td>\n",
       "      <td>...</td>\n",
       "      <td>7650.895020</td>\n",
       "      <td>29.550020</td>\n",
       "      <td>7786.610073</td>\n",
       "      <td>109.258547</td>\n",
       "      <td>7805.587565</td>\n",
       "      <td>85.060270</td>\n",
       "      <td>0.307801</td>\n",
       "      <td>0.089192</td>\n",
       "      <td>0.152314</td>\n",
       "      <td>1060.496162</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-08-01</th>\n",
       "      <td>1228.000000</td>\n",
       "      <td>1233.469971</td>\n",
       "      <td>1210.209961</td>\n",
       "      <td>1220.010010</td>\n",
       "      <td>1220.010010</td>\n",
       "      <td>1567200.0</td>\n",
       "      <td>1219.003337</td>\n",
       "      <td>1.515790</td>\n",
       "      <td>1239.374268</td>\n",
       "      <td>21.433011</td>\n",
       "      <td>...</td>\n",
       "      <td>7669.693359</td>\n",
       "      <td>38.687654</td>\n",
       "      <td>7767.384347</td>\n",
       "      <td>109.753646</td>\n",
       "      <td>7798.026217</td>\n",
       "      <td>85.881161</td>\n",
       "      <td>0.285090</td>\n",
       "      <td>0.083101</td>\n",
       "      <td>0.146205</td>\n",
       "      <td>1063.190143</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-08-02</th>\n",
       "      <td>1205.900024</td>\n",
       "      <td>1229.880005</td>\n",
       "      <td>1204.790039</td>\n",
       "      <td>1226.150024</td>\n",
       "      <td>1226.150024</td>\n",
       "      <td>1531300.0</td>\n",
       "      <td>1220.790009</td>\n",
       "      <td>3.781604</td>\n",
       "      <td>1236.241420</td>\n",
       "      <td>21.550840</td>\n",
       "      <td>...</td>\n",
       "      <td>7702.942505</td>\n",
       "      <td>73.619602</td>\n",
       "      <td>7761.944336</td>\n",
       "      <td>106.402575</td>\n",
       "      <td>7798.359340</td>\n",
       "      <td>82.521365</td>\n",
       "      <td>0.271257</td>\n",
       "      <td>0.086055</td>\n",
       "      <td>0.164864</td>\n",
       "      <td>1091.418702</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-08-03</th>\n",
       "      <td>1229.619995</td>\n",
       "      <td>1230.000000</td>\n",
       "      <td>1215.060059</td>\n",
       "      <td>1223.709961</td>\n",
       "      <td>1223.709961</td>\n",
       "      <td>1089600.0</td>\n",
       "      <td>1221.782501</td>\n",
       "      <td>3.932133</td>\n",
       "      <td>1230.528564</td>\n",
       "      <td>18.079647</td>\n",
       "      <td>...</td>\n",
       "      <td>7748.444946</td>\n",
       "      <td>69.648419</td>\n",
       "      <td>7744.768555</td>\n",
       "      <td>81.003961</td>\n",
       "      <td>7798.808594</td>\n",
       "      <td>82.581619</td>\n",
       "      <td>0.214106</td>\n",
       "      <td>0.085981</td>\n",
       "      <td>0.165144</td>\n",
       "      <td>1091.916170</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-08-06</th>\n",
       "      <td>1225.000000</td>\n",
       "      <td>1226.088013</td>\n",
       "      <td>1215.796997</td>\n",
       "      <td>1224.770020</td>\n",
       "      <td>1224.770020</td>\n",
       "      <td>1081700.0</td>\n",
       "      <td>1224.239991</td>\n",
       "      <td>0.749575</td>\n",
       "      <td>1221.940003</td>\n",
       "      <td>3.444802</td>\n",
       "      <td>...</td>\n",
       "      <td>7835.844971</td>\n",
       "      <td>33.708070</td>\n",
       "      <td>7747.243327</td>\n",
       "      <td>90.494779</td>\n",
       "      <td>7792.345052</td>\n",
       "      <td>87.849464</td>\n",
       "      <td>0.151643</td>\n",
       "      <td>0.065983</td>\n",
       "      <td>0.139857</td>\n",
       "      <td>1082.353677</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 40 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            google_open  google_high   google_low  google_close       google  \\\n",
       "Date                                                                           \n",
       "2018-07-31  1220.010010  1227.588013  1205.599976   1217.260010  1217.260010   \n",
       "2018-08-01  1228.000000  1233.469971  1210.209961   1220.010010  1220.010010   \n",
       "2018-08-02  1205.900024  1229.880005  1204.790039   1226.150024  1226.150024   \n",
       "2018-08-03  1229.619995  1230.000000  1215.060059   1223.709961  1223.709961   \n",
       "2018-08-06  1225.000000  1226.088013  1215.796997   1224.770020  1224.770020   \n",
       "\n",
       "            google_volume  google_ra_04  google_std_04  google_ra_09  \\\n",
       "Date                                                                   \n",
       "2018-07-31      1644700.0   1218.500000       1.753611   1237.301409   \n",
       "2018-08-01      1567200.0   1219.003337       1.515790   1239.374268   \n",
       "2018-08-02      1531300.0   1220.790009       3.781604   1236.241420   \n",
       "2018-08-03      1089600.0   1221.782501       3.932133   1230.528564   \n",
       "2018-08-06      1081700.0   1224.239991       0.749575   1221.940003   \n",
       "\n",
       "            google_std_09  ...  nasdaq_ra_04  nasdaq_std_04  nasdaq_ra_09  \\\n",
       "Date                       ...                                              \n",
       "2018-07-31      24.147664  ...   7650.895020      29.550020   7786.610073   \n",
       "2018-08-01      21.433011  ...   7669.693359      38.687654   7767.384347   \n",
       "2018-08-02      21.550840  ...   7702.942505      73.619602   7761.944336   \n",
       "2018-08-03      18.079647  ...   7748.444946      69.648419   7744.768555   \n",
       "2018-08-06       3.444802  ...   7835.844971      33.708070   7747.243327   \n",
       "\n",
       "            nasdaq_std_09  nasdaq_ra_18  nasdaq_std_18  google_daily_vol  \\\n",
       "Date                                                                       \n",
       "2018-07-31     109.258547   7805.587565      85.060270          0.307801   \n",
       "2018-08-01     109.753646   7798.026217      85.881161          0.285090   \n",
       "2018-08-02     106.402575   7798.359340      82.521365          0.271257   \n",
       "2018-08-03      81.003961   7798.808594      82.581619          0.214106   \n",
       "2018-08-06      90.494779   7792.345052      87.849464          0.151643   \n",
       "\n",
       "            s&p_daily_vol  nasdaq_daily_vol  seq_prediction  \n",
       "Date                                                         \n",
       "2018-07-31       0.089192          0.152314     1060.496162  \n",
       "2018-08-01       0.083101          0.146205     1063.190143  \n",
       "2018-08-02       0.086055          0.164864     1091.418702  \n",
       "2018-08-03       0.085981          0.165144     1091.916170  \n",
       "2018-08-06       0.065983          0.139857     1082.353677  \n",
       "\n",
       "[5 rows x 40 columns]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unscaled_seq_predictions.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>google_open</th>\n",
       "      <th>google_high</th>\n",
       "      <th>google_low</th>\n",
       "      <th>google_close</th>\n",
       "      <th>google</th>\n",
       "      <th>google_volume</th>\n",
       "      <th>google_ra_04</th>\n",
       "      <th>google_std_04</th>\n",
       "      <th>google_ra_09</th>\n",
       "      <th>google_std_09</th>\n",
       "      <th>...</th>\n",
       "      <th>nasdaq_ra_04</th>\n",
       "      <th>nasdaq_std_04</th>\n",
       "      <th>nasdaq_ra_09</th>\n",
       "      <th>nasdaq_std_09</th>\n",
       "      <th>nasdaq_ra_18</th>\n",
       "      <th>nasdaq_std_18</th>\n",
       "      <th>google_daily_vol</th>\n",
       "      <th>s&amp;p_daily_vol</th>\n",
       "      <th>nasdaq_daily_vol</th>\n",
       "      <th>google_45d</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Date</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2018-07-31</th>\n",
       "      <td>1220.010010</td>\n",
       "      <td>1227.588013</td>\n",
       "      <td>1205.599976</td>\n",
       "      <td>1217.260010</td>\n",
       "      <td>1217.260010</td>\n",
       "      <td>1644700.0</td>\n",
       "      <td>1218.500000</td>\n",
       "      <td>1.753611</td>\n",
       "      <td>1237.301409</td>\n",
       "      <td>24.147664</td>\n",
       "      <td>...</td>\n",
       "      <td>7650.895020</td>\n",
       "      <td>29.550020</td>\n",
       "      <td>7786.610073</td>\n",
       "      <td>109.258547</td>\n",
       "      <td>7805.587565</td>\n",
       "      <td>85.060270</td>\n",
       "      <td>0.307801</td>\n",
       "      <td>0.089192</td>\n",
       "      <td>0.152314</td>\n",
       "      <td>1156.050049</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-08-01</th>\n",
       "      <td>1228.000000</td>\n",
       "      <td>1233.469971</td>\n",
       "      <td>1210.209961</td>\n",
       "      <td>1220.010010</td>\n",
       "      <td>1220.010010</td>\n",
       "      <td>1567200.0</td>\n",
       "      <td>1219.003337</td>\n",
       "      <td>1.515790</td>\n",
       "      <td>1239.374268</td>\n",
       "      <td>21.433011</td>\n",
       "      <td>...</td>\n",
       "      <td>7669.693359</td>\n",
       "      <td>38.687654</td>\n",
       "      <td>7767.384347</td>\n",
       "      <td>109.753646</td>\n",
       "      <td>7798.026217</td>\n",
       "      <td>85.881161</td>\n",
       "      <td>0.285090</td>\n",
       "      <td>0.083101</td>\n",
       "      <td>0.146205</td>\n",
       "      <td>1161.219971</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-08-02</th>\n",
       "      <td>1205.900024</td>\n",
       "      <td>1229.880005</td>\n",
       "      <td>1204.790039</td>\n",
       "      <td>1226.150024</td>\n",
       "      <td>1226.150024</td>\n",
       "      <td>1531300.0</td>\n",
       "      <td>1220.790009</td>\n",
       "      <td>3.781604</td>\n",
       "      <td>1236.241420</td>\n",
       "      <td>21.550840</td>\n",
       "      <td>...</td>\n",
       "      <td>7702.942505</td>\n",
       "      <td>73.619602</td>\n",
       "      <td>7761.944336</td>\n",
       "      <td>106.402575</td>\n",
       "      <td>7798.359340</td>\n",
       "      <td>82.521365</td>\n",
       "      <td>0.271257</td>\n",
       "      <td>0.086055</td>\n",
       "      <td>0.164864</td>\n",
       "      <td>1171.089966</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-08-03</th>\n",
       "      <td>1229.619995</td>\n",
       "      <td>1230.000000</td>\n",
       "      <td>1215.060059</td>\n",
       "      <td>1223.709961</td>\n",
       "      <td>1223.709961</td>\n",
       "      <td>1089600.0</td>\n",
       "      <td>1221.782501</td>\n",
       "      <td>3.932133</td>\n",
       "      <td>1230.528564</td>\n",
       "      <td>18.079647</td>\n",
       "      <td>...</td>\n",
       "      <td>7748.444946</td>\n",
       "      <td>69.648419</td>\n",
       "      <td>7744.768555</td>\n",
       "      <td>81.003961</td>\n",
       "      <td>7798.808594</td>\n",
       "      <td>82.581619</td>\n",
       "      <td>0.214106</td>\n",
       "      <td>0.085981</td>\n",
       "      <td>0.165144</td>\n",
       "      <td>1186.869995</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-08-06</th>\n",
       "      <td>1225.000000</td>\n",
       "      <td>1226.088013</td>\n",
       "      <td>1215.796997</td>\n",
       "      <td>1224.770020</td>\n",
       "      <td>1224.770020</td>\n",
       "      <td>1081700.0</td>\n",
       "      <td>1224.239991</td>\n",
       "      <td>0.749575</td>\n",
       "      <td>1221.940003</td>\n",
       "      <td>3.444802</td>\n",
       "      <td>...</td>\n",
       "      <td>7835.844971</td>\n",
       "      <td>33.708070</td>\n",
       "      <td>7747.243327</td>\n",
       "      <td>90.494779</td>\n",
       "      <td>7792.345052</td>\n",
       "      <td>87.849464</td>\n",
       "      <td>0.151643</td>\n",
       "      <td>0.065983</td>\n",
       "      <td>0.139857</td>\n",
       "      <td>1166.089966</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 40 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            google_open  google_high   google_low  google_close       google  \\\n",
       "Date                                                                           \n",
       "2018-07-31  1220.010010  1227.588013  1205.599976   1217.260010  1217.260010   \n",
       "2018-08-01  1228.000000  1233.469971  1210.209961   1220.010010  1220.010010   \n",
       "2018-08-02  1205.900024  1229.880005  1204.790039   1226.150024  1226.150024   \n",
       "2018-08-03  1229.619995  1230.000000  1215.060059   1223.709961  1223.709961   \n",
       "2018-08-06  1225.000000  1226.088013  1215.796997   1224.770020  1224.770020   \n",
       "\n",
       "            google_volume  google_ra_04  google_std_04  google_ra_09  \\\n",
       "Date                                                                   \n",
       "2018-07-31      1644700.0   1218.500000       1.753611   1237.301409   \n",
       "2018-08-01      1567200.0   1219.003337       1.515790   1239.374268   \n",
       "2018-08-02      1531300.0   1220.790009       3.781604   1236.241420   \n",
       "2018-08-03      1089600.0   1221.782501       3.932133   1230.528564   \n",
       "2018-08-06      1081700.0   1224.239991       0.749575   1221.940003   \n",
       "\n",
       "            google_std_09  ...  nasdaq_ra_04  nasdaq_std_04  nasdaq_ra_09  \\\n",
       "Date                       ...                                              \n",
       "2018-07-31      24.147664  ...   7650.895020      29.550020   7786.610073   \n",
       "2018-08-01      21.433011  ...   7669.693359      38.687654   7767.384347   \n",
       "2018-08-02      21.550840  ...   7702.942505      73.619602   7761.944336   \n",
       "2018-08-03      18.079647  ...   7748.444946      69.648419   7744.768555   \n",
       "2018-08-06       3.444802  ...   7835.844971      33.708070   7747.243327   \n",
       "\n",
       "            nasdaq_std_09  nasdaq_ra_18  nasdaq_std_18  google_daily_vol  \\\n",
       "Date                                                                       \n",
       "2018-07-31     109.258547   7805.587565      85.060270          0.307801   \n",
       "2018-08-01     109.753646   7798.026217      85.881161          0.285090   \n",
       "2018-08-02     106.402575   7798.359340      82.521365          0.271257   \n",
       "2018-08-03      81.003961   7798.808594      82.581619          0.214106   \n",
       "2018-08-06      90.494779   7792.345052      87.849464          0.151643   \n",
       "\n",
       "            s&p_daily_vol  nasdaq_daily_vol   google_45d  \n",
       "Date                                                      \n",
       "2018-07-31       0.089192          0.152314  1156.050049  \n",
       "2018-08-01       0.083101          0.146205  1161.219971  \n",
       "2018-08-02       0.086055          0.164864  1171.089966  \n",
       "2018-08-03       0.085981          0.165144  1186.869995  \n",
       "2018-08-06       0.065983          0.139857  1166.089966  \n",
       "\n",
       "[5 rows x 40 columns]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unscaled_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "seq_accuracy=np.sqrt(metrics.mean_squared_error(unscaled_data.google_45d, unscaled_seq_predictions.seq_prediction))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/matthias/anaconda3/lib/python3.7/site-packages/pandas/plotting/_converter.py:129: FutureWarning: Using an implicitly registered datetime converter for a matplotlib plotting method. The converter was registered by pandas on import. Future versions of pandas will require you to explicitly register matplotlib converters.\n",
      "\n",
      "To register the converters:\n",
      "\t>>> from pandas.plotting import register_matplotlib_converters\n",
      "\t>>> register_matplotlib_converters()\n",
      "  warnings.warn(msg, FutureWarning)\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYAAAAD8CAYAAAB+UHOxAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDMuMC4zLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvnQurowAAIABJREFUeJzsnXd4VGW+gN8zk0kmvRDSCJ1QQkhCbwIBAVFs6CrY1sbavV73WtfGddX1qru6ll0rdgHFBRVRRCBSpLcQQiAB0iA9pM1k+rl/fDOTmWSSTEI68z5PnkzOfOecbyYz3+/7dUmWZTx48ODBw4WHoqsn4MGDBw8eugaPAPDgwYOHCxSPAPDgwYOHCxSPAPDgwYOHCxSPAPDgwYOHCxSPAPDgwYOHCxSPAPDgwYOHCxSPAPDgwYOHCxSPAPDgwYOHCxSvrp5Ac4SHh8uDBg3q6mk0iUajwd/fv6un0Wp66ryhZ869J84Zeu68wTP3/fv3l8my3Lelcd1aAAwaNIh9+/Z19TSaJDU1lZSUlK6eRqvpqfOGnjn3njhn6LnzBs/cJUnKdWdciyYgSZKWS5JUIklSusOxv0qSlCZJ0iFJkn6RJCnGelySJOlNSZKyrc+PczjnVkmSsqw/t7blRXnw4MGDh/bDHR/AJ8CCBsdelWU5UZblZGAd8Kz1+KVAnPXnLuDfAJIkhQHPAZOBScBzkiSFnvfsPXjw4MFDm2lRAMiyvBWoaHCs2uFPf8BWUvQq4DNZsAsIkSQpGrgE2CjLcoUsy+eAjTQWKh48ePDgoRNpsw9AkqQXgT8CVcBs6+F+QL7DsALrsaaOtxqj0UhBQQE6na4tp7crwcHBHDt2rKun0Wq6Yt5qtZrY2FhUKlWn3teDBw9N02YBIMvyU8BTkiQ9CTyAMPFIroY2c7wRkiTdhTAfERkZSWpqqtPzAQEBREZG0q9fPyTJ1WU7D7PZjFKp7NI5tIXOnrcsy1RVVXH48GFqa2vP61q1tbWNPhPdnZ44Z+i58wbP3N2lPaKAvgJ+RAiAAqC/w3OxwFnr8ZQGx1NdXUyW5feB9wEmTJggN/SGHzt2jNjY2C5f/AFqamoIDAzs6mm0mq6Yd2BgILW1tUyYMOG8rtMTozt64pyh584bPHN3lzYlgkmSFOfw55VApvXx98AfrdFAU4AqWZYLgQ3AfEmSQq3O3/nWY22iOyz+HlqH53/mwUP3o0UNQJKkFYjde7gkSQWInf5lkiSNACxALnCPdfh64DIgG9ACtwPIslwhSdJfgb3Wcc/LsuzkWPbg4UIkq7iG0lo904aGd/VUPFyAuBMFdIMsy9GyLKtkWY6VZfkjWZavlWU5wRoKeoUsy2esY2VZlu+XZXmoLMtjZFne53Cd5bIsD7P+fNyRL6ozWLNmDUFBQWRmZjY55rbbbmP16tUALF26lIyMjFbf59ChQ6xfv77V56WkpLhMojMajTz33HPExcWRkJDApEmT+OmnnwCReFdWVtbqe3loO/Ne38qNH+zu6ml4uEDx1AJqIytWrGDq1KmsXLnSrfEffvgh8fHxrb5PWwVAUzzzzDMUFRWRnp5Oeno6P/zwAzU1Ne12fQ8ePPQcPAKgDdTW1rJjxw7efvttJwEgyzIPPPAA8fHxLFy4kJKSEvtzjjvygIAA+/HVq1dz2223AfDNN9+QkJBAUlISM2fOxGAw8Oyzz7Jq1SqSk5NZtWoVGo2GO+64g4kTJzJ27Fi+++47AOrq6liyZAmJiYksXryYurq6RvPWarV88MEHvPrqq/j4+AAi0ur6669vNPYf//gHCQkJJCQk8MYbbwCiRsnChQtJSkoiISGBVatWAbB//35mzZrF+PHjueSSSygsLDyft/eCwWJxGQjnwUOn0a1rAbXE//5wlIyz1S0PbAXxMUE8d8XoZsesXbuWBQsWEBcXR1hYGAcOHGDcuHGsWbOG48ePc+TIEYqLi4mPj+eOO+5w+97PP/88GzZsoF+/flRWVuLt7c3zzz/Pvn37ePvttwH4y1/+wpw5c1i+fDmVlZVMmjSJuXPn8t577+Hn50daWhppaWmMGzeu0fWzs7MZMGAAQUFBzc5j//79fPzxx+zevRtZlpk8eTKzZs3i1KlTxMTE8OOPPwJQVVWF0WjkwQcf5LvvvqNv376sWrWKp556iuXLl7v9ui9Uiqq7PpfFw4WNRwNoAytWrGDJkiUALFmyhBUrVgCwdetWbrjhBpRKJTExMcyZM6dV150+fTq33XYbH3zwAWaz2eWYX375hZdffpnk5GRSUlLQ6XTk5eWxdetWbr75ZgASExNJTExs8+vbvn07ixYtwt/fn4CAAK655hq2bdvGmDFj+PXXX3n88cfZtm0bwcHBHD9+nPT0dObNm0dycjIvvPACBQUFbb73hUROuaarp+DhAqdHawAt7dQ7gvLycjZv3kx6uqiNZ7FYkCSJV155BXAv3NFxjGNG87vvvsvu3bv58ccfSU5O5tChQ43OlWWZb7/9lhEjRjR7XVcMGzaMvLy8FvMAZNm1aWL48OHs37+f9evX8+STTzJ//nwWLVrE6NGj2blzZ7P39tCYXzNKWh7kwUMH4tEAWsnq1av54x//SG5uLunp6eTn5zN48GC2b9/OzJkzWblyJWazmcLCQrZs2eLyGpGRkRw7dgyLxcKaNWvsx0+ePMnkyZN5/vnnCQ8PJz8/n8DAQCcn7SWXXMJbb71lX6QPHjwIwMyZM/nyyy8BSE9PJy0trdF9/fz8uPPOO3nssccwGAwAFBYW8sUXXziNmzlzJmvXrkWr1aLRaFizZg0zZszg7Nmz+Pn5cfPNN/PII49w4MABRowYQWlpqV0AGI1Gjh492ta394Jh07Filu84bf+7KaHrwUNH4hEArWTFihUsWrTI6di1117LV199xaJFi4iLi2PMmDHce++9zJo1y2mcbYf+8ssvc/nllzNnzhyio6Ptzz/66KOMGTOGhIQEZs6cSVJSErNnzyYjI8PuBH7mmWcwGo0kJiaSkJDAM888A8C9995LbW0tiYmJvPLKK0yaNMnl/F944QXCw8OJj48nISGBq6++mr59nftGjBs3jttuu41JkyYxefJkli5dytixYzly5AiTJk0iOTmZF198kaeffhpvb29Wr17N448/TlJSEsnJyfz+++/n/T73RswWmc925pBdUsufvz5MfHQQD84ZBoDR7BEA7Y7JAN8/CEVHunom3RdZlrvtz/jx4+WGZGRkNDrWVVRXV7s9NiEhQT516lQHzsZ9WjPv9qQ9/ndbtmw5/4l0MrY5bz5WLA98fJ088PF18uhnf5ZPl9bK76ZmywMfXydr9MaunaQLeuJ7bWPLli2yXHRUlp8LkuVXh8tyZX5XT8lt2uN9B/bJbqyxHg2gE5g3bx5jxoxh8ODBXT0VD12IwWyxP756bAyDwv1RKcVX0GjyaADtjq5S/K4tgi+vB137Rgz2BjwCoBPYuHEjX331VVdPw0MXU6k12B/PGRkBgEopzIKOwsFDO1FnFQAXPwulmZD6N/fP3fQ8fDS/Y+bVjfAIAA8eOokKjRGAUdFB9to/Ng3AZPEIgHbHpgGMXgR9R0Jlnvvnbvs75O+Ggv0dM7cG5JZr+HxXLnqT6/DvjsIjADx46CTOaQ2oVQp+emgGapXox+AxATWmqErHW5uyMJ9vprSuSvxWh4DKF4yNs+ObxMtX/E7/9vzm4CZvb87mmbXpXPX2DvJrOm8z4BEAHjx0EhUaA2F+3k7HVF7iK+gxAdXz8k/H+PvGE2zLKj2/C9lMQOrg1gkAYx2YrGMNLTcwkmWZY4Xn51/Yl3uO4ZEBlNUaWPZ7Hf9OPXn+AtANPALAg4dO4pzGQKh/AwGgED4AjwmoHtt7dPR8y7zoKsEnGBRKUPmBUeveeZUO3WstphaHb8sq49J/buPzXbltmuap0lpOl2m4dlwsvzw8k7ERSv7v50xu+Wh3h9eL8giANqBUKklOTmb69OkkJyeTk5NDamoqwcHBJCcnk5yczNy5czt8Hp988gkPPPAAILKIP/vssybH5uTktMkR7VjS2sP5UaE1ENZQAHhMQI3oFyLML+e7q6auEnyDxePWaACVDgu5GwKgrFYPwJubslo7Q/QmMwvf3A7ARXHhhPl7c3+yD68vTmJefCQKRcc2UurRpSC6Cl9fXw4dOuRUUiEnJ4cZM2awbt2687p2W/v13nPPPc0+bxMAN954Y1un5uE8qdQa6R/q53TMYwJqjMm66z1vAaCrFOYfsGoAbggAow4Ofm49xx/MRkqqdTy9Np3ZIyO4YdIANmYUE+yrYtLgMACq6oRzv7RGT3ZJDcMi3G+3Wlqjp85oZulFgxkdI+YqSRKLxsa6/zrPA48G0IksW7aMW265hTlz5hAXF8cHH3wAiB6gs2fP5sYbb2TMmDEAfPHFF/as27vvvtteHO7jjz9m+PDhzJo1ix07djhd+7XXXgNE1c+5c+eSlJTEuHHjOHnyJE888QTbtm0jOTmZt99+G7PZzKOPPsrEiRNJTEzkvffeA5ovae2h7Xy84zSnyzSNNQCbCcgjAOzY3ovccu35mUDqKoUDGKwaQAsmIIsF1twNGd/BjEcgpD/nNHVc8fZ2fsko5tPfc9AaTDy08iC3Lt9Ddoko0XJOI8J7FRKs3JPf3B0aUV4rzp0ypE/rXls70bM1gJ+eaP8076gxcOnLzQ6pq6sjOTkZi8XC0KFD7fV8bAsswHXXXcdTTz3V6Ny0tDR27dqFRqNh7NixLFy4EIA9e/aQnp7O4MGDOXbsGKtWrWLHjh2oVCruu+8+vvzyS+bNm8dzzz3H/v37CQ4OZvbs2YwdO7bRPW666SaeeOIJFi1ahE6nw2Kx8PLLL/Paa6+xbt06ampq+OijjwgODmbv3r3o9XqmT5/O/PnzOXjw4HmVtO7JGEwWvL3af0+UVmriH/szGBDmx2Vjop2es2kAnlIQ9djeC5NFprBaZzcJtRpdJYRb25e3ZAKSZdjwF8hYC/NfgGkPYsxcz4FTJXgHKPjD+FhW7y9gxZ58tAYzPl4K7vvyALGhfmzOLCHM35vJg8P4z8EzPLpgBD5e7mnxFVbh0SfAu4WRHUPPFgBdhCsTEOCWCeiqq67C19cXX19fZs+ezZ49ewgJCWHSpEn2TOFNmzaxf/9+Jk6cCAiBExERwe7du0lJSbHX7lm8eDEnTpxwun5NTQ1nzpyx1ytSq9Uu5/HLL7+QlpZmt+9XVVWRlZV13iWteyq/ZhSz9LN9bHx4JnGR7qvwLVFWq+fDIwZGRAby3QPT7eGfNuw+AI8GYMcx+uVkSW3bBYCTBmB1AssyOFbN/fZPUHcORl4Gu/8NU+6DqcKvVmdSIMlm/n5dMr4qJav3F/DXdRn0C/HlxUUJ3P7JXk4UiyihUD8VSyYN4Kf0IjZmFHN5YoxbU7T5D/r4+7TtNZ4nPVsAtLBT7440LNls+9vf399+TJZlbr31Vv72N+fMxbVr17ZY8ll2s6qkLMu89dZbXHLJJU7H169f71ZJ697GZ9YIjtTjpe0qAJ74Ng2tSeafNyQ3WvwBvKwmII8AqMfoEBF1qrSWmcP7NjO6GXSV4OtgAkIGkx5U1k2R2QhHvhaPszfCkNkw/0W7gNCZJbwwExcRQIifiquTY1h76CwvXJ1AyogInrx0JC+tFz3BQ/28uWhYOP1CfFm1N99tAVDexRqAxwfQyXz33XfodDrKy8tJTU217/Idufjii1m9erXd/l5RUUFubi6TJ08mNTWV8vJyjEYj33zzTaNzg4KCiI2NZe3atQDo9Xq0Wq3LstL//ve/MRqFA+vEiRNoNBq3S1r3JmRZJrtYvDf7civa7brHCqv59VgJVw9TMTLKdRc2b48JqBEms4y/t5JAtRfZpS3H4btCYTaASeesAYCzH8DRfDxgKlz5Jijql0SNScJXKRPq740kSby+OJldT17MbGsZj7tmDuWFqxMA4cRXKiSun9CfbVll5Fe4F3JaoRHJgX7erQ/8aA88AqCTmTRpEgsXLmTKlCk888wzxMQ03inEx8fzwgsvMH/+fBITE5k3bx6FhYVER0ezbNkypk6dyty5c122fQT4/PPPefPNN0lMTGTatGkUFRWRmJiIl5cXSUlJvP322yxdupT4+HjGjRtHQkICd999NyaTqcWS1r2R0ho9Z6tEY57dpyvaLfZ6zcEzeCkkZsWqmhzjMQE1xmS24KVUkBgbzL6cc226hpfJKjicNACc/QC51iCKPx+DO36GkAFO19CYIEBV/1mQJImoYGeT6uBwoblXWyOBrpsQi0KCr/e55wwuq9XTx9+ny7Tunm0C6iJqaxvvSlJSUkhJSWnx3OHDh/P++++3eO7ixYtZvHhxo/Nvv/12br/99kbHly1bZn8cFxfH5s2bG43ZtGkTIPwECoWCl156iZdeeqnROFv/4QuFUqsddn58JL9kFHOsqNoektdWcss1fL4zl/mjIwn0rmlynMcE1BiTRUallJg2NJxXNxynrFZPeEDrbOR2AdBIA7AKALMJ9n4IMWMhyLW5ptYI0a5daHYG9hHXrdWLfIGYEF9mDe/L1/vyeXju8Bbj+MtrDV1m/gGPBuDBgz0U7/IksRDsOtW8GaioSsfRs1XNjvlsZy4WWeaZy+ObHecxATXGZJZRKiSmDRWhkd8fOtvqa3iZrP2WG2kAVtNM+mo4lwMzH3V5fqXWQJ1Jwter+f9LTLAv14zrx79vHm8/Nn90FMXVes5UNp93cCi/ksMFlUQGtSBlOhCPAOhEli1bxiOPPNLV0/DQAFskxph+wQzs48fOk+XNjv/LmiMsfHM7L60/hsHkeud+vKiGEVGBRAc3H8HiMQE1xmix4KVQkBQbwoy4cF5af4zfs8tadQ27AFCHit+OJiCLGba+BpEJMOIyQPiBtIb6rN+TpRpMeOGrbP7/olBI/OP6ZCYOCrMfi4sIACC7pGn/xeH8Sm75cDeBai8evaRxf+/OokcKAHcjXTx0H7rz/8ymAfQJ8GbqkD7sPl3eZCEuk9nCntMVRAWpeX/rKRb9a4fLL3pWSQ3DrAtBc3gpPSaghpjMwgSkUEi8c9M4hvT15+4v9pNV3LQprSEqo80E5JAJDFBXAb/9H5Rnid2/1fb+5e48Jrzwq13QnCytxYQCH0XrP7dD+wbYr2Ejv0JLuXWjcfRsFbd8tJsQfxWr7prK8HaMOmstLQoASZKWS5JUIklSusOxVyVJypQkKU2SpDWSJIU4PPekJEnZkiQdlyTpEofjC6zHsiVJeqKtE1ar1ZSXl3frBcWDM7IsU15e3mROQldTVqvH20tBoI8XU4f2oUZnIqOJQmTHCmuo1Zv4y8JRvH/LeM5W1nH5W9tYf6TQPqaqzkhxtd6tL7a30mMCaojZIuNlfV+C1CqW3zYRL4XE67+eaOHMepp0Aq+9VwiAviNh1JX28TllGrQGM7d/spetJ0o5WVqLRfJCRevr84f6e9PH39u+MTCaLVz+1nbGv/ArmUXV3PflAQJ8vPhq6RRi2prj0E644wT+BHgbcKw0thF4UpZlkyRJ/wc8CTwuSVI8sAQYDcQAv0qSNNx6zjvAPKAA2CtJ0veyLGe0dsKxsbEUFBRQWnqepWIbYLHISFLjOP3m0Ol03XZRa46umLdarSY2tnPqm7SWsloD4dZQP1tK/u7T5YyJbewI/u2ECM2dNCiMqGA1Sf1DuPvz/Tz6zWESY4OJDfXjUL4oQxznhgbgMQE1xmi22J3jALGhfkwYFNZI0/rtRCmr9xfw5pJk5+9tcYaDE7ihBnBOOH7/sNwp5POc1kgff28igtTc+eleAN7390GSWy4G54qhfQPsGsDe0xX2ekFvbcomt1zL369Lon+YX3OX6BRaFACyLG+VJGlQg2O/OPy5C/iD9fFVwEpZlvXAaUmSsoFJ1ueyZVk+BSBJ0krr2FYLAJVK1e69dXPKNKS8lspt0wax7MrmnXaOpKamuizF0N3pqfPuKMpq9fSxRplEBqmJCPQhw0UhMlmWWb2/gMmDw+zhgJFBat66YSyzX0vl8125zBkRwf98fYiYYDUTHOzCTaFUSEiSpxaQIyaLbDeN2RgS7s9vJ0qxWGR7ZM3q/QX8cPgs/3vl6PoaSwX74MOLGQzgHQBKawiuymGnff3nENLf6fqVWgORQWq++tNkXtlwnPJaPRNUEXA2u02vISpYTVqB2Aj8klGMj5cCg9nCj0cK8fZSMG90ZJuu2960hw/gDuAn6+N+gGMAbIH1WFPHuwVPrxXWrcPWf5iHC4tyjZ5wh1C8EVGBHC9qbG/em3OOnHIt101wXjz6h/kxbVg4X+3K44YPdhGkVvHZnZMJ9m06/t8RlVKBwWMCsiM0AOelaVC4PwaThbNV9ZE1h/JFjkBeuYbi9FRRzO1MfQtHrcJBAwuIgNiJcO1HjRZ/gMo6I6H+KkL8vHlp0Rjeu2UCgX5qt8pBuyLM35sKjQFZlvnlaBEz4voyzOobGB0TRJDavc9GR3NeeQCSJD0FmIAvbYdcDJNxLWhcfuIlSboLuAsgMjKS1NTU85miS3QmmexKC5F+ElV6me3ZIgnIpK1p1f1qa2s7ZH4dTU+dN7T/3GVZ5nSJlnCFl/26/kY9O4tMbNq8BaWDKeKjI3rUSgg8l0VqqvPOcISPka16E1OjlfxxtExBxj4KMtybs0K2cDonl9TUonZ7Xe1BV31OysrrMMs43buqQtji127aSUK4kmqDTH6FEAYZK58hWfs5/+n3BNNMu4iynpOqHYRq8xZ7xVWGPQ3lgIvXdKZMS/9AhdM944pK6avX8nsb3oOqEgPVOhMff7eZs1U6Lu1vodgiXkOgpfn3tTPf9zYLAEmSbgUuBy6W6z2yBYCjeI0FbEG8TR13Qpbl94H3ASZMmCC7k1zVWt7Zks1r+47b/44M8iEq2BeFBCkp092+TmpqqlvJX92NnjpvOL+5l1TreHDFQQxmC18unYyftxcVGgOaDRu5KDGOlBlDACgLLODnnMMMTJhgr+0uyzIPpv7CwqRYLpmb1Ojas2SZJWUahoT7N/IjtTRn9dZfiIqJISUloU2vq6Poqs/JvzJ3olBASspU+7GRVTpe3rOJ4NhhpEwZyObMYmAfILNIswokUHsridIVgJeak6YInjPcwhsDxjB9WDgavQlvL4Xd52Jj9f4CZFnGQCbDB0aRkjKm/knteqj4vU3vQb5PDmuyj5ItR6CQ8rjv6pmcWZPOkbIiZibVf9Zc0Znve5tMQJIkLQAeB66UZdmx6MX3wBJJknwkSRoMxAF7gL1AnCRJgyVJ8kY4ir8/v6m3ncyiGiKDfHhp0Rj+6+I41tw3ndhQX3s6t4feybIfjrL7dAUH8yrZkS1i/U9ZHXW20D2AJKvz17EMQY3eRI3OxPBI145dSZIY2jegTSn9DU1AtXoTX+7O7fB2gN0Vo8XSaKEO9RcmE9t39FBeJQoJRqjP4SuJMN5wQwGUZ8PMR7na/DKlhPLbiVLKavXM+8dvLHhjKyXVOqfrPvLNYR5dnUaFxkBog37NKFUiY7gNhFiv9fXefCYMCqNPgA8aa55BbGjXO39tuBMGugLYCYyQJKlAkqQ7EVFBgcBGSZIOSZL0LoAsy0eBrxHO3Z+B+2VZNsuybAIeADYAx4CvrWM7nHVpZ9mS6dzU5GRJLaOig7hx8gD+PG84MSG+BKlVVNW17Z/tofvza0Yx648U8dDFcfh5K9mYUYTJbOFUmUgYstV0ARgWEUBEoA87HBLCbAtHw1ow7YG3UoHeWB9ueO8X+3lqTTr7cttWB6enYzLLTlFAAD5eSlRKyV5y4WB+JSOiglgQVN++ccg5UdvHED2OGuteLvV4CQ+tPMjZKh0nSzVO4boNCfFrYJdXKM/LBwDCoT0/Xjh8F08URpDk/iFNntfZuBMFdIOLwx81M/5F4EUXx9cD61s1u3bglZ+PI0mQMkKUlF366T4yCquZMsQ5kijI14tqnUcD6I1o9Cae/S6d4ZEB3D97GMcKq/l6XwFrDp7BVyUWltjQ+igRSRJlCLZnlyHLMpIkUVQlkng6Im2/b6CPvR5Rea2ebVkiGSm3XGNvO3ghYXLIA3AkwMeLWp0Ji0XmcH4lCxNjmJSfRa2spho/YrTZgERJQDywnyF9/TlRXMuJ4lpeWjSGv6w5QrWufkHXm5xj/EMaagAKFVjatiY4ahO20OLLE2NYOCa6W5Vb75GZwO5iNFs4U1lHbrmW48U15FfUscmqDfQLdU7ACFKrMJgs6IytT/zw0L35+y8nOFul42/XjMHbS8ELixJ45dpEls4YwrSh4SydMaTRgjN+UBhltQZ7ldAiqwbQEQIgKkhNofU+W7Pq81tyyjXtfq+egKlBHoANfx8vNHoTOeUaqnUmkvsHM8J4jEOWoZTI1pIP4cMp0ovF94aJorrn4gn9uXHyANQqhV2DAFEF1pFGyaUKL5AtIrqolTi2/oxzMBt2p8Ufenk10DPn6uwp/T+nFzllZtoKTdmwhexV64wuG3d46JkcKajik99Pc/OUAYwfKHbTEYFqrp/YOBTQkfhoUb//6Jkq+oX4UmwXAO3fuSkqWM0OawmCLZmlhAf44O+jJKfMdU35d7ZkU1Sl469Xdy+ncXvRnAZQozfZE+3GRqroo8liv3w18eSIQbET7MJ65vC+TBnSh5HRgdbzVdQ4aPklVgHw9+uS2J5dxrz4BrH5SuvyaDGBonUVOx3NSe62h+wKerUAsO2gAn282HC02J5heGTZJfg2aMAQZBMAdSYiuq40h4d25unv0gkP8OGxBSNbdd6o6EAkCTIKq63VHXUEqr3w827/r0xUsJoavYmqOiO/nShlXnwkZbV6Tpe51gDWpRWSWVTNXTOHdIts0vbGaLbUh246EGDVAA7lV+LvrWSo4TiSbGG/JY5IhbWCa79x5JYLwRkToibQId4+SO3lZAKy+XVGRAVy7XgXWeoKBwFA6wSAbRMZ3oWlnt2hV5uA8qxdeW6cPIBjhdX8nF7EsIiARos/iA8HYE/Z9tDzMVttxYsn9m914o2ftxdDwv1JK6iitEbP7lOiAFxHEG11LG9IL6KqzsjsEREMCQ8cnrMQAAAgAElEQVTgdJmmUbVRi0Ump0yDLLvfdKSnYXaRCQwQoPaiUmtk64lSkvqHoDyzBxmJQ5ZhlGPtuNZvAofyK4nyk5wWf4BAtfAh2CiubsGvo7Ce30Y/wNr7p/PTQzPbdG5n0asFwKlSDX7eSm6aPBAQJV6nNjD92AhyMAF56B3YQgYbhfe5ydxRkaQeL+HSf24lp1zDQ3Pj2nN6dmwL0Fd78lAqJC6KC2fKkDDqjGYO5DlHAhXX6KgzmvFSSHy9L79XlpAwmmWUisZLk7+PFxmF1eSUa7l12iDI24UUMYq0l69HOWQ2uxXJyBHxHMqvZEhI401egNqLU2W1bDlewhe7cnnvt5P4eSvp49/E58OuAbTNL5jcP4S+gV3T7N1derUAOJhfSUK/YAb08WOU1aY7d5TrGhy2D0FmofslZ9sNWYZPr4Rd73b+vXsxNm3O3ZIMDblzxmDUKiVh/t58/8BFbjf6bi39rBUhD+VXMn5gKMG+KqYO7YOXQiL1uHPRw9Olwix04+QBFFfrGz3fGzBZLKhcaACBPmJB7hvow/xRfaFgL/QXpcZK+k7hdtOTvL8jn9IaPUOCGy9tgT4q8ivquP3jvTy9Np2IIDUf3Tqx6a5dNh+AufduCnutD0BnNHP0TBV/miky7q6fEMvyHaedGjc4MiDMj5nD+/LOlmyuHdePiI5Q980m+PZOGPdHGHZx/fGqAjj9m/gJj4Ohc+x1yj20HZs211YBEBGoJvXRFIJ9VR3qyIsN9SXUT8U5rTD/AASqVUwfFs7q/flEB6vZllXK1KHh+Fg7iC29aAjrjxSxcm8ecxs6L3s4Ig/AtQYAMDDMD6n0OOirof9kQPyPtQYzf/spk8mDw5gSrWt0fqC6frlbedcUJg8Oaz4qx8kH0DvptRpAWkEVJovMuAEiPOz26YPZ9tgcewu+hkiSxPNXjsZgtvDCj8daf0OLGfZ/CoZmQvcK9kLGWvjiGtA7aBoOBaz44hp49yJIfRnqPMXpWkuNzsjUv23io+2n6zWAhgk+rSAiUN3hURySJHH7dJGXMntkX/vxR+aPoFxj4Lnvj7I5s4TvDp2xN6+JCVHzh/GxbM4s6XVmy6Y0gACrAIgO8YX83eKgVQAsTIzmtmmD+O7+6ay6eyoB3i40CKtPYOGYaKYM6dNySOZ5+gB6Ar1WAPznQAFqlaJViTSDwv25d9ZQvj981h6W5zYZ38EP/yWaTTRF1ob6x2cP1j8u2AtKb3iyAK56B3TVkPo3ON7peXM9nq9251FYpeOv6zLsxcLaqgF0Jg/OGcam/5nFyKgg+7ExscH8595pbHkkhSuSYqiqM6IxmFCrFHgpFcTHBGGRobiq8W63J2Myu3YC2yJrwvxUkL8H/MIhTGj4wyICWHblaJKaybIN8BHnR7ubzX2ePoCeQK8UAJVaA2sPneHq5H6t/vLfmzKUmGA1H2471bqbZllbJBQ30eLAoIUjq8FfqPhorALmyGrY9S8YOA18AmHszXDfTuuYUjixAc4eat1cLlD0JjPLd5y2d9nacFRU1+wupXebw1ZLqCFjB4QyONyfEF8VVXVGavUm+064r7WHQcOEpu6MLMvNJlvKsozJ4toJrLEmcQX7eQsNoP/kVplKddaIqtCmnL4NuQB8AL1SAADcMX0wt00f1Orz1Colc0ZFsOd0hftdmmQZTvwsHufvcV1Aatc7UJUPl1o1BG05mPSw8TmISoQ/fFw/1tsflD5izLqHYdXNopm1h2b57uBZiqv1vL44GcCeMNQTNICWCLYJAJ3Jbgu3RZgU1+jYdaq8R0QEPfJNGtNe3twovNWGyZq46SoPwGbSi1LWQMVJuwPYXaq0rfQJeXwAPZMQP28eWzDSSZ1uDdOHhqMxmDmc76YN/txp0Wpu8EzQV0HhYefnTQbY8wEMmwejrhDHtOWQ8T1UF8DFz4Cfg6lKksA/XGgJtcVCcPz+Vptey4WCxSLz7taTxEcHcdmYKMIDfKiqM+KtVKBW9fyPebCfN7IMRVU6/L2dBcDDqw6z5P1drN5f0PoLV52BsrZ1vWoNeeVaHl+dxrcHCqjQGNh9utzlOFvmvqtM4HtThnJJXCCLvJ3t/+5iCwG3+QVbxOMDuDCp7wtb4d4JtgV/2kPi96ktzs9n/iAW8kl3iRKz6hAhANJWQXB/GDKn8TX9wqAsS+w+vHxh++tQ7bKFQo9FazDxzb78xjVY2sDWrFJOlWq4e9YQJEliiLW6Z5CvqtvVX2kLtl3rmco6uwkoSO0cxHcwr5VBA9Vn4cO58NV17TJHV+SUaXj0m8PM/nsqaw6dYcnE/igkUaTx8dVp/Dv1JEazhfe3nqRWb7Jr3a6cwDEhvrwXvgrfX5+0Hkhu1VyuSo7h0LPziI9xc2Po8QFcmIT6ezMk3N9uQmiRs4fEbmHwDIhMEOGcjuz5AEIHwbC54m+/PlByDE5uhjHXOTWntuMXLsYAzHpMfAg3Pd/m19QdeXtzNo+uTuPo2cb9d1vL+iOFBPp4sSBB9IMa0lcIABcbyR6JowDwszozGwq2Ri1NLWb49X/h9Dbx94HP4FyOeGzQwIolUHMWKk7BuVzaE1mW+a8VB0l5LZXvD5/lj1MHsu2x2bx8bSJ3TB9MaY2en9IL+b+fM/nPgQJeWp/J2oNnMFn7IihdxebLMhxeJR57+Tr3+XUDSZIaV/xsDo8P4MIluX8Ih/IrG+9OK04J2z3gZayB9Y8J80zsRPDygcGzIG93vc2+6Ajk7YSJf6pf6P3DIWcbyGZIXOx6An59wGANFY2dAMk3ikijdtgtdwcqNAY+/T3H/vh8MFtkNmYUc/GoCHvI5pyRwtke10sKOzkWF7P5ABy5bnwsWSW1djs3IAITtv8DPr8aNjwF3z8Ia+8X1S3/c5f4bM5dJsbmbGvX+a5LK+T7w2e5YVJ/tj02m+euGG3PeH768nh2/eViXrtOdFb7cnceAL+fLMNorbzpygREebYwxyTdAEt/bdf5uqQrfQCVeaCr6vDbeARAEyQPCKG0Rm8v0wuAUQdvjoVVt8Du95i8+x7Y+wGMvxUWfy7GDJkFZn19nPKeD8RuZexN9dfxs5ajiEqEiCaKlPmHOzyOgL4jwKgFrZtmqW7Oh9tOoTEI1bryPOsvFVfrOKc1MtEh5Hf+6Ch+f2KO3SHc03F0XAa4KEh3y9SBSMDdX+yrj7LZtxwCo2HIbNj5tjhWWwyblkHmOpj/Ikz/bzEmbVW7zven9EL6hfjywtVjmkyqHNhHaGlpBWKh+/1kuV2AuXICk7VR/E55EqI6oRJqV/kAZAt8+ydYfmmbSlG3Bo8AaIKIQPGhrXTcUVWfEb+zNsBPj1EbMATu2Q6Xv16/YA+cJnYOp1KFCp7xHcRfBb4OjiebAGhq9+84BiAgQvgKQDiEezi23f9Uq6+lUnt+GoCt/G/D+O6YEN9uX4vFXUJ8XWsAv/55Jt/cM5XE2BBeuy6JXacq+PPXhzAb9cL0E38V3LBSLJq+YVCeBTv+CRPugCn3ioCDaQ/C6a2Qv7fxjavPtmkRyinTMjwywLUpx0r/sHoTzoLRUdToTNz2sZiDSw0geyOED4fQga2eT5voIh9AdOFGyN8FU+9zbR5uRzwCoAlsFUPrHGOWHRffQTM4nPQ8RI52PtEnEPpNgFO/CRVbV+lc9gHEjktSwpg/ND0B//qMUHxDIdharraqDZEe3YzPd+aiNZp55vJ4oIGQbQO2RKiOaNbSXQhy1AB86jOTh0UE2subXD22H09dNor1R4rYtnUTmOpgwBRhy055Aq74pzgpagxc+kp9DH2Stelf3u9O91SaNELj3f8xLVGhMfCfA6LBuizL5JZr7Dv8pvDz9rIL6McWjOCT2yfaC/g1aghj0ELODhFJ11l0hQ+gppghpz6FgRdB8k0tjz9PPAKgCdTWkhFOSSuOi++Cl5tOQhkyC84egPdnib8HzXB+fsq9cMcGCIxqegLxV9U/lqRepQFkFlUzJNyf+JggAny8zl8A2Pr19mIBoFYp7WVMXPkAbNxx0WBUSglL3i5xoP+U+ieHL4DL34DbfxbRaDb8wiAgqj7owHZPXRmYdEKbbYHPd+by568Ps/NkOWW1BjQGM4P6tNyrYFAfP/qF+DI43J8ZcX1Zfe80Zg3vy5jYYOeBOduEaTVubovXbDe6wgew4S8ozXq44o1OqQfWa4vBnS92DcDQUABI8HSxcPhmpro+eUhKfUmIEZdBULTz835hznH/rvALg/85IcJFbX97+fYKDaCkRm/frQf7qqisO18TkB6VUmpz2eeewsAwP7JKapsVAEqFREyIL2HlB0TkmeNnz8sbJtzu+sSIUfUCwGIBhQJvg9XflL9bBB80syAdzBdlq5fvyOGeWaI8w8Dw5jUAgKcXxmMwW+wRTSOiAvn0DhcJXlkbQeUHA6e3eM12Q2n9PGlKOud+Wb9C+mpyB93A4PCOKT3eEI8G0AS+1rojOlMDE1BApFj8m6P/FLjsNfjvI3DDirZPIjASIoWZRGgB/XqFACiu1tkFQKi/qlkNwJ0cgZJqHRGB6qbL+vYSbCaVllqW9g/xZZAmDQZMdf/iNgGwfAG8EAHZm/DRWwVAbTH6stNNZu/Ksmi84+2lYFNmMZutfbcHt2ACAkjqH9JkhV476f+Bg1+IKrktfffak7ChEJ0Em1/s+BwcTTms+2/oE0fegGs79l4OeARAE/hatNys3Eidvn5xkstPcUIf3HLGpUIBk/4EIQPaeVKhnRIa1pHIskxJjZ4Iq+03xNfbyQlstshc9fZ2rnx7O3P+nsqfPtvX4jWLqnVEuVvgqwcz0GpSKalpvvjb2IAyQuQqYf93lwFThYlFXyuiXoqP4m2ob0bzz48/54GvDrg8NbdcyzmtkftShqKUJN7beorwAG/7fNuMLIs8htW3Q3QiLPzH+V2vtSi94NqPhBns26VweKVzFd/2omAfvDpEbDCveANZ0XmlSzwCoAnCtz/LC6qPCSneBUXpsOOfSHm/81PdaB755nC7ZK+2Gp/AjvkAdiIZhdUYTBZ7aGCwn8opDPRYYTWHC6qoqjNyqlTDr8dKmm3TWVKt41B+JcNcFFLrbdw1cwgTB4VyzVgX/WsdmGIUkTRvnWxFn4D4K+GpIrhnmwh/1JYLE5BPEAalP9HVaWzKLKG8tnHhOZv555LRUVw6JhqzRWbq0PDzz8DO/V3kMYy9GW5dJzTiziY8TtTvyt0Ba+6GbR0ghGyFJK/7BAZd1P7XbwaPAGgC7yKx25m37y54dzpsfBZ9eAL/NolaPseLu2Ah9g4AQ23n37edKKnRsfDN7QAOGoCzCWjXKeHzWHXXVL6+W5gwPtp+mh/TCslq8J5vPVHKlW/vwGi2cN/soZ3xErqUyCA139wzrXltx2xkXPFqdltG8vcDFvSmVoQwqnzr61Bpy/DRV2Dwi2SPcQgX+Zy0J9w15FBeJX7eSoZHBnKHtQDjzDhrWHTODpE/0xYOfAo+QSJiyasL/Ttjb4GUv4jHmevaPxkz93dhahq9qH2v6wYeAeAKixmpxsHml7gY/usQ6Zf/gA6xcNlqzXcqPkE9WgMockiqswmAqCA1FRqDPdpq16kKBof7ExWsZuyAEEL9VLy5KYv7vzrAbR/vRZZlquqMrNqbx+2f7CXYV8Wnd0xqMeTwgkCWYf2j+GoKKE28GxDmmVbjFw4aoQGc0PhzRDGCQeYcQhR15FU0vt6h/EoSY4NRKiTGDghl48MzuWZcLFTmwyeXtb2Q4anfYMSlojpuVyJJkPK48OuVnRDVANqDrF9FTkbB3s51bjvgEQCuqDiN5LjTTryeT47Bte/utB86d57lC9qET4Cw0fZQHEs+9A8T9uEBVjtxwTktZovMntPlTBkinIIqpYKfHprJugcv4uG5wzlTWcdPp40k/e8vPP7tEdEy8d6pTBsa3vhmFyJ73hcx+xc9zKCpwpF4sqQNnxe/MNCWo6or4Zg2iEHJFyMhM12dw7kGDnuT2cKxwhqSYusbscRFBooEsBJrb4yja1o/B4tZ9MOwhT93B2Init9Faed/raNr4KvrYeOzwsfQGod9O9KiAJAkabkkSSWSJKU7HLtOkqSjkiRZJEma0GD8k5IkZUuSdFySpEscji+wHsuWJOmJ9n0Z7URRuvjgVZx0Ph47iV+POYeClXeJAAgUfVAbqKBf7Mq119XpzpyzOnu/+tNkYqyN0G2CIK9Cy7HCaqp1Jns1VoCoYDUJ/YK5Mlk0ZP/ptFiA7p89lI9unWBv83fBU7Affn5ChB3PeZbB1hDMU2XNtChtCv9wOHcaP+M5TskxjJp0MUgKpnhlNdr45FZoMZgtDI90UXPJFlZaclRUtm0N2gpRKysgovXz7yj6jhQJnEXpLY9tjqNrYPWdzkEi3VUAAJ8ACxocSweuAbY6HpQkKR5YAoy2nvMvSZKUkiQpgXeAS4F44Abr2O5DWZaw9W94CsqFAHhc+Qg/Rt2HxTuwUW+Acw3KF+SUabjxg11kl3SgicY7AJAb9R1+em06z31/tOPu205UaMTiPTq6PslngE0AlGvt9v/Jg/s0OndQHz9igtXUGGFkVCCPXjISVXct9Vl3TuzsvrgWvrgWP00nJO+d+AmQYNG7oFDg7+NFdLCak6Vt0QDCxe4bOOMVy8DoCIgaw2T5UKPPfVaxuH4SmfD5onoTZflJYfrxtgqGjLWtm4Mt9t4xI76rUamFU/jsgbb7AaxmOmKS4S6HsvEBXfM6W/wGybK8FahocOyYLMvHXQy/Clgpy7JeluXTQDYwyfqTLcvyKVmWDcBK69iu51wubH8DbJmTu/8tyub6BLPLZzq/hFzPqbJaavT12YDRwWonc4bJbOGqd3bw+8lytmW1spdwa/CxfpkczFNaQ8/pVnROY0CpkAh0qGPfx98bP28luRVCANjs/w2RJImpVlNPnKvdZndi62vCtlucAWf2M+rY6x1e1IuzB0Usv7peuPYN9GmbqdKhDtXA4ckimidxCcONmfSpyawfZ9Thf+A9rlT8zsDiTaK8+T5r2YiVN4G2TJQwiZ0kamK1hlqrAOhOGgCIMhonN8OXf2h9lz5thejwpykVtZh8Q2HeX+Gqf3XMXN2gvTOB+wG7HP4usB4DyG9w3GU7H0mS7gLuAoiMjCQ1NbVdJyhZjPQ7s57agEFo/AcxYd9/42Oo4FzIGAKVftT5RhNYeozqwGGY63Tkny1mxS/Oi7q3bCArr5BaXxOpqansOmuyhyruTc9isLF9a6vbiCguIB7Yve1X6vxEKOCpyvooD3ffq9ra2nZ/X4s0Fl7fr+Ph8Wqi/F3vK9Kz9Ph7yWzd6twvoY+PzJYjeRRrLUyK8mpybmFGa50YTWm7z7+9UJq0TNv9PqWRc8gc9RARxanEH3udzFXPUBTdQXVsZJlpuXso7zOJ4w7vi05TR5HW/c+Fjegz5YywPh4bqSY1NRUv4wCmoGRCzWZSUwcDMCzrQ2ac+YEZ3qBJj0UF6H/7B2nnAplYegyjVwAZ0Uvw1+QyrGA5u9evoM4vusn7OhJR/Jv4rGfkUJfT+k1OR3zGAXx9ZxMbU0NM9gYq/n056QlPuB23PzBnJYNzRGLozmJv9KmpQCJUAQ5z7ai5u6K9BYCrwF8Z15qGSx1KluX3gfcBJkyYIKekpLTb5ADIXA9blzc6HFp5BAbPJPDyN+C9WQQNnUyfM8EE+KrQ+fsS6HPWrgUMiAyjRmciIMBIbdhwPtucRlxEgEhpDwwmcWIC4/66kX/dNI7LxjT/gc8qrnF/R3u8Do7B5KR46DcOgMI9ecARANx9r1JTU90e6y7v/XaSYm0mper+LElxnca+Mn8/kYZaUlJmOR2/1zePv6wRr+GaixJISe7n6nTG1Or5/uQmbp43kfED3Wzr19lkb4LtBqLmP0jU0BSQZ1H1+k+MzF/JyKsfBd+QFi/Ras7lwm/VRI9bQPSkFPvhD7N3U2c0k5IyrVWXKznhT9bxdZwLjmfO3HqhVXA4nsTq44yfNQuppgi2/cJ+1XgSTOn4awug70h8SjOZeOotQEL10AGSAiNFNNAby5kcWAgTFoimNDFjm5/E7+nisz778ja9Zx3xGa/nJti3nD7rHmZW5Wq45j33TvvmE/E7agxTF1zf5LCOnbsz7W1ELQAc3faxwNlmjncsZw+Jhuq73q0/VnhI/L7+c7jkJZh8T/1zg2ZAn6Fw7w6Y/wK+KgU6g5nDBZUk9q9XrcP8vSmt0fPlMT0PfHWQkVGBfHbnJAaE+VFwro7TVsfbe781cCY3YMvxEua9vpXvDp1x7/XYTEAOoaCpx4Wq7NtCeYCOZqfVfr/5eL2z/GRpLUcK6jOXK7QGQv0bx3Mvmdifhy6O4/oJsfZGLq7oE+DD/830676LP4jmP5KyPmJEksiK+5Oo6bTlpY65p62ZS4NQQh8vhXMxQzfZr49lnuFV0obe63S8PGwsY6STaLQa2PEGyGaWme9gR19rWfOJS8XrrjgpspBtiVsh/UWF3Iy1sO3v8NElLecGaEpELR51cPPjuooJd8CEO+HI1+6Xiz5zQBR5vGtry2M7ifYWAN8DSyRJ8pEkaTAQB+wB9gJxkiQNliTJG+Eo/r6d792YA5/CsR9EdESetUFL4WHhzY+/EqbeL7L8bNjKr4YOBL8wfFVKzmkNZBbWkNw/hI9uncDnd04i1M+bM5V1bMw1cfv0Qay8ayrRwb7Ehvpy5pwWW0mauha+fBnWVoh7c9xs8uJtzXbV12C2yOw6Vc6WTOGs05vMXZOdDBjNFvacrsDbS8Gh/Eq7f+TJb4/wyDeH7ePOaQz0cSEAFAqJh+cN55U/JPX8qJ68XcJO7FOv1dUGDhOlQfa8J8orlze/MXCbfcvhvVlCq/XvK3wADqhVSvRN1O9pSH6Flue+SyfjbDWpx0sJ9PFiUJDz8lATMREfyYQ+bS3s+xjjmCUc0YZycvhSmHyvWNxmPCIGj1zofIP4q8R3L3O9KDlhCxFtitoS0QipO/dzjhglmrdo3PD7lWRCZS70G9/hNf5bgzthoCuAncAISZIKJEm6U5KkRZIkFQBTgR8lSdoAIMvyUeBrIAP4GbhflmWzLMsm4AFgA3AM+No6tmPRlkNQP7EDWXO3iKE/ewiiG3SJuuZDmP20KLbmgK+3kqySWkwWmeT+oVw8KpIZcX2Jjwki0MeLe5N8eO6K0fYyvbGhfpTVGuwLoNbQvAAoOCeSaoqrG6fXu8S6qGTknGXSi7+y5P1dqJQSi8b2wyKDwdzBjsYmSCuoQmswc8f0wcgy/HaiBK3BxIG8c5ytrHeUVWhcawC9BpPBmtTjwuRyyUuQuEQkEdlS/8+HzPWw7s9Coz3+Iwye2WixbI0G8J8DZ/h0Zy6XvbmNNQfPkDIyolFNfkP/6ehlL/psuB9kM3mjhfYc1bcvXPqycNgOv0Q0oJm41PkG8VeK37YQ65Zi6WtLuiwyxm1s5dyrCyDtm3pNQJbtbWMBUb9rxWIhpBM6r9CbO7gTBXSDLMvRsiyrZFmOlWX5I1mW11gf+8iyHCnL8iUO41+UZXmoLMsjZFn+yeH4elmWh1ufe7GjXpAT2goIGQiL3hN2x4/mQ21R4y9o4nUw69FGpztWXUxyMAFdNz6WtGXzmRzt7ELpZ41tt5WJqGtBAGQWiXHpZ9ws8OYTBMDps0XU6k28c+M4dj81lzH9xNx0hq4RALbwzaUzBhMe4M2WzFL25pzDZJGp0ZvQ6E1YLDLntAbCenPJ5sLD1qQeF0XYlCphKw6MERE758PZg/DtnSKU0MaY6xoN81Ep3NYAMgqr6B/my4NzhhHk68V14xvXGxo5MIZc2WrWGX87p0zCXGfL5QCEEBpxaeOG7aGDnO3+hYdpFo1VA+jOBFgFwG+vwn+W1jfO2fxXUVHVpBfCYN2fhR9k8Zf1jZ26Cd1HF+kItOUiq3HgNJj+kEhIAUi4xq3TA61115UKyd4iEkRIoqtCV7GhVgFgXdib0wAsFpkT1nGFVTrnZt5NoRJzsBi09PH3ZmFiNAE+Xq67l3USsiyzObOEkVGBhAf4MGt4BL+dKGV7Vql9TFG1jmqdEYtM79YAbB21mkvqiRl7fgJAXwtf/1HE6t/4tTA3gctOWT5eSvQtfCY+25nDsu+PcqSgisTYEP5n/gj2PT2PmcMb775jQnz5NvRO9nhNwDD7OX5ME268/qG+jca6xNbkSOkNhS1pAKXdLwS0ITYfx2mrTb9gv/i97e/it74GDq+A9NUw+0kY4DLwsUvp3Q1htOX1zriLnxM1Rfz6ONlnm+O26YP58Ughlya4F7oWGyp2QjYB4Lgg55RpyCyqYUGC2DWcqaxDYzCTMqIvqcdLyavQMsavBYeXNdzMZDQ6NQWxOYA7UgCUVOsoqdGT0M95jpszS9ife45lV4i8vjkjI/j2QAEfbDuNWqVAZ7RQXKWzh4e58gH0GvJ2iRryzS1cMWPh+HqhnbbUFMgVBz6FyjzR1SsgAm77UThUlY2/yj4qBbpmNIAtx0t49rt6S+xN0UEt3r7/1D9w/dqRDHpnHznlWu6eOYQ+AW7W6B93q/hOGjRw6Cswm1zOG4tFxMp3dwEQYBUARmti5umtzg7hyjz48RERXHLRnzt/fm7QezUAWbZqANakFoUCZj0GE+90+xKDw/3Z+9Rc/vfK0S0PRhQ4UyklTpXWZ+qaLcIxe+vHe7jni/32cro2ITEvXnyIcivcSNm3tvGzmAxOAsBmquqopLBzGgPXvvs7l7+1nc93Oec4bMsqw89byU1TRKPui+Lq6/LMjxfCrqhaZ88g7bUagMUiIoAGtpDSP+xiQIbjPzU/rimKjggzku0+6uAmyyT7eCkxmCwugwPOVNbx8KpDjIwK5KNbJzB+YCjz41sut7x4Yn/iIgKo0Bh49+bxPHnZqBbPseMXBvNfgP6ThTtLFfsAACAASURBVKmsPEtUCz34pfPCWWctA9HdTUCOzWmCBwhfwC6HpK6iI0I4TPsvUHRtlF5T9F4BoK8WvTz9GpcVaA2SJLndaUqhkOgX4uvkjLU5hG2VMH+xltO1+QnmjRJfOldVFhvfQAlImE0Gp2xamwmoLSF/zXG6TMOuU+Xc/cV+8ivqCPDx4pWfMp3MVdV1RsL8ve1lGYJ9VbyxOJlBffy43VoauKhaR3mteB96rQ+g7LgoAdFSTZeYsWKxOPJ128oJlGZC3xEtjwPUKvE/aegH0JvM3PflAcxmmXdvHs/FoyL59t5pbuWjqJQKVt8zjd8enW3XZltNVKL4XZgGP/4PfHcfbH21/v2wZwF3cyewIzd9I+owbX6h/li1Nby7LZpeJ9F7BYC9l+75CYDWYjMD2aiqMyLLsj1SaJO1qFxmUQ2xob5EBKkJD/Amz92yvUoVFpMRf+/GJqD7vzxIaY2bEUVu8PjqNJa8v4s9pyv455JkvrlnKjV6E6v25dnHVNUZCfZ1Dt28emw/Uh+dzdgBoYT5e/N7drldEIb69/Awz6YosHYu69+CnVeSREjoqVRRvbM15O0W/oO+I90a7uMlPhd6o7MAeOnHYxzOr+TV65IY5Ebf3oYE+6nOT5MLHw5eauEItjVcT/0bvJEods32OkDdXAMAkU+05CuIGCk6likdtAKbAPDtvnkrvVgAWGPrO1kA2CKBbNQZzJTU6KnRiQ96xlkR8XO8qJqRUWLH1T/Mr1Hd9r05Fa57sCq9wWwgQN1YABRV69hwtKjdXovZuiN78tKRXJXcj1HRQSTGBvNjWqF9TKULAeDIf80ZxvbsMpbvOA2IJLpeSdERUPkLH0BLTH1A7BY3/AVyd7Y8HoSJafl88djNhuE+XjYNoF4z3JxZzKc7c/nTjMFt38GfL0oviBwtQkG1ZaCwfpar8kRBuRzRNMhuY+/OxF9Zn/MQFA0LHJL9qqwCQN0B2d/tRO8VAJVWW3Unq1+xDSIitAYT2daa7DPiwjlbpaO0Rs+pUg0jrAIgOlhNqUOrvZwyDde9u5PbP9nT+AYKLyxmIwGOTmDv+n9jWkF91dJ3tmRzxVvb3dcuGqBUSEweHMbds+oXtcvGRHO4oIp8q8mqqs5IiF/TAuCPUwcxIy6cE8W1qFUK/Lx7adxB0RGISnAvyUehEFU7QwbCN7c2nRWb+aMoPAbCpGnDzbaBNt+QzkED2HmyHLVKwWML3NMiOozoJKEB1J0T2fjjb4Nb1goz0NZXITC6/XtqdwZjb4brPxOPbRpAd81mprcKgOqz8POTEDoYIjq36nRsWAMBYDTbWxlekSRq2q8/UojJIjMiSkRdhPp5O1UXLTgnkqd2ZJc3MunIShU0EgD1j9McSi98tTuPI2eqeH5d23Lu6gxm/LydnVcLrbWN1h8RWoArE5AjCoXEq39IIthX1Xvt/+UnRQioLSTTHdTBIiihtlg0A3fFyhvFjhhEMhHAVe+47QNwpQEUV+uJDFJ3fSntqMR6oRY6CK74JwydDbesEX6Sa963hz33OPqNF7+rzojcHVeRTt2E3ikAfAJFZuQNK8Dbr+Xx7YjNBxBq3RXXGcxkldQSpPZiZpxwatlq/9hMQH38vanUGuwRQ0XV9TvCigblfGWFF0rZ7DIMFCCrpJY6gxmT2UKZVavYeqKMap0beQYNqDOa7Q5mG/3D/EiMDeaHtLOiPaPWSFAzAgBEU5ePbp3AM5d3rxYQ7YLFDMut7TJiJ7XuXFuYo6a0+XG6KtBZNbtWmBPqBUC9BlBcrSMysBssrNFJ9Y8da/5HJ8JdqeL721NRWdcco6Zjiv+1I71XAFz7YaPaKJ2BzQQUbo2N1hrMZJfUEhcZSGSQD74qJQfyKvFSSPauTaH+3lhk7CWlix0EQK3eObRTlrzwkkwufQAgwk4zCqvIKdegN1m4ecoADGYLv7po5t0SdQazUza0jesm9Cf9TDUbjhZjMFsI8W15Zz9hUBiXtlAZtUeSs104Lecug8SmKzy6xLbwuRIAjhFCBXvrNYBWmBPqTUBmiqp09lyOiCA34/Y7koh4UTQPRAey3oRjD+NubP+H3ioAupCIQDUqpURfa9PzOqsPIC4iAEmS7AKif5ifXQ23OUZtu/3mBIBZocIbEwE+9Quzbadn43B+FccKhdlpycQBxASrnRy37lJnbGwCAlg8oT/9w3z56zpR0Ks5E1CvJ/1bUaRv0t2tL1zmSgBoyuG14XDwi/pjp36DOqsG0Iodpe1zoTNauPmj3fz3qkNCAwjqBhqASl0fzdSdun61B0rvesd2N44AAo8AaHeUColZw/va+9oWVNZRrjEwLEJU8rQJgIF96k1TDQWALWcAQOMgAMwWmbP/396Zx8lVVnn/e3pfk053kib7vpEgWQkBhCwSSNAJiygqGBaH11EcdF58RWc+wquDsqkz6jt8BgeGIA7KoDMgE8EQaJZAIBEIJITsJOlsnc7S+5p+3j+e53ZVV6qra01VdZ3v51Ofe+9zb937q+que+45z3nOU99JDqcoyffddLOyhNHlRfzDFdMYWprPh9W1DNj4C8qzmphUWcLyc4bx2o7Iw0At7aeClpnOy8nqnqQdMtgAdLbD1mdtRk80oUYvQ63pmK/tg6dsv8CmJ31tH/63XwgofA8g3/3tXt9Zy86aRt7YdYzm9lNUpoIHAL4wUH8zACI2Iww0BJSJ/NvKedy2aCJAdz18zwCMcAZgbIXPTTzNA2hoY/wQu7+x1WcAahpaaeoUcjhFccCT+avz3uQrY47yiZFlTNm9ikv2/wt/M+AN8nOyWf6JYX2GgZraOulqsPnXdS0dfGXVBtsH0Ms8AytmjmBypf1MobKA+jW7q2wWS7QVHrNzbYjA8wCMgXd+bdf3rrPL6VfbkgJeNlBEISD78/7thn092lPCAwCYfhVM/FTKh0miwnsgUA8gM8nKEgpys7qzcjwD4N1Q/WvieAbgRHM7R+pb2X3UhoygZwiotqGdDrLJ5RSV/vPm1h+EV+6DR5cyf2gnf9X2RwCKBtk86lmjykKGgY43tXPt3f9K1k8mwfqHeGL9Xl50A9YKe0nbzM4Svrt8GgW5WT28mX7N8d22qmPNVru95Q/2hjxhcfTnLB5iK9R+tBr2v2ULFpaN8e2ffYOtAbX1OZAs3yTrYeANBDvZ3MH8cb506DEVkQ/+SgiTl8L1v0+p+vhxwyttkWLVPwNJ3fykfoCdUKaDorxshg+0T/5ex5x/J+4glx65/3gzNz+2ga4uw22LJvHCliM9DUBjG6XkcM6wIiqGlPgu9PG67tVb3r6CLLHvGT7QnldEuGTKEP60OfggsUdf38M5WXagFi98j/zROcBw9xl6/3EumjKUzXdfRk6yUwrPBIc/gF8tsZOZAHxzs70pT78ScmJIby0qt5MWbf2j7UvIKYClP7QVPwHKx9v6Qduft0/KEdws/fuGvrd8Gg+8sI3zx5dz7sjUzUvvN3ijmYf1MfVlksmAX27y8AY9TRxa0l1P6K8vHs/NF47j8/N8M2QW5GYzpDSfh1/dzUeHG/jll2ZzzsiB5OdkdfcBPFS1i5se20An2RTnBNSQ8aYEHHkebXO/yhfbvwfA5ME+L6FyQAEnmzvoCDJpzPrdx5ggB2k1uRzKH8d1e+9ivNhSv4FpoIFkxM0f7LiSU20w0P3d3vx/0N4Q+wQfHX6D9NobYdpnYPIyX1tRhQ0DQcTxZO9hY/rwAZw7qownvjKf2xZPClrKXEkQwz6RbAUhyZBfb3Lwbp7j/eqtDCjI5fufOfu0EbH3X/MJCnKz+eGKGSyaYvPDSwtyaGjrpPpEMz97cTsA7SaHXAmo+nl0G4y5CL6yhsIr7mHElLkAjCj1xea9kr0nAsYVALR2nmJO0VGO5o/ihuZvYUwXX83+o/sM6iRycr81sov+wZZfBjv5R/EQW+o3FpY/CJ/5ua2SCXYkqb9HkVdiJ1jJzo94RGlpQQ7jBhdz68XjY9OoRE+Kl7TWX3cC8abUCyfmumjqUN77/qU9nqiL83NoauvkgRe2ddcF6iSb7K4AA9Bc66uwCDzwuTlwH9Dly/oZ7PoZahtPNwAlbTXMaX8bpl/NC9dcT8sTq7lg19vQaZI+2XxK8MF/2uU5n7Ux3ex8W8541vWxj/Icfb59dbTaOkLjLrHt33jHhp1EoGCALWOeFdm1crOzePmOhbHpU6Ljy8/4xm6kMGoAEsgx97QdbidpYDilJD+HN3bZchDLZpzFnzYfppNs6Aqo+Nl0tGcqXbZ7gjzlu9l7HsDyn7/Gd+YVsNDv7d9pdjMYDZ9JdpZQMnUxJbtXM0pqrAE41Wmn1Mwv7bX2fL/FGHj/dzDqfCgfZ9vKx8PRrfGd3zW3AKYu921XTLAvj8t/HL9rKYln/MJkKwgLDQElEK8UQ7RZMsX5ORxtaGNwSR4/vtrWmekgG0755fN3ttsnjR4GwIV+Tvk8hYoSX1jhvg1+xceMYXzXXraXnAcLvmHbXFbLPTmPcu6Ln4d7zoJfzoFfXxnV50hrDn9ga/D7j/I9a4at6T8qyNy/ipJGqAeQQLzR/KPLo0u7q3Ejgm+5aDxlLlMoKzu3R2iH5lq7LPYre+2FCnqEgHoO/ukeEdp8jIE08mbZAiZ7GSYVE3jFzOKS7HdpbxoJC74G+9+2Rc8yjQ+est/n9Kt8bcvuh46W/pm+qGQU+h98BhhcEl2aoFcJ9Jo5IwB463tLWDx9ZE8PoMkzAH4egIi9afkdN6DQZ+uHU8vBjc/ZjdodANSXjOtx7Z9k38IvO1dw8AsvwqU/gGEzoTN+k82kDG0NvrrtgWWZu7rgg6dh0tKeZcWLymHgiDOnUVEShHoACeS5b1zErqONUafdPX7LeeyqaWKoq95YOaAA8vMDDIAbRRo4nD6rp6fgaSiilSfyfsT4Vw/D6EF0nawmC2gs6ZkpUps7nAebP8+KQpd5kpNnOz77G49fCQc22uyex1fA5ffaGbsAjmyGhkMw7fvJ1agoCUI9gAQyY8RAVsyM/klxzphyPuc3XgA47cbe7QEUBVRUzM7t0QcA8I3FE3lk/hHGZx2mPqcc/uurdG1bTYMppKNkeI9jv325rTnvVTUlp8DmwUczj20qc8BN5fjuE3Z6wtV3wLp/tm271tplLCN9FSWFUQOQbgTe2Ls9gAADkJXT01AA/3vpFBaU1tBBNneX3A2tdeTseJ7nT80jP7enM3jVrJF8fO8VvoFgOc4QnDo9jTTt+PBZePlHPds2PWlLFE+/GtZ8Hzb9ztb6GXo2lCZp6kRFSTB9GgAReVREakRks19buYisEZEdbjnItYuI/FxEdorI+yIy2+89K93xO0RkZWI+TgYQeGNvrrVeQeAgITdzWA+2/xne/Q012cN54XglXZf+EICnT10StO5/z/M5A9BbGKjlpK/+Sarz3n/Y2kkfrwP8wnOjzrPzSAyeAht+ZSd6H3Nh0mQqSqIJxwN4DLg8oO1OYK0xZhKw1m0DLAMmudetwENgDQZwFzAfOA+4yzMaSoQE3tibjtqn/8B+huy80w3Af1wLTTXUF46gqf0UO8Z+kX03rOctM61vA+B5AJ1BPICWE3DfGHjpHyP/PMmg0VVFXX0H4BfSmngpZGXblM/qDbY0w6j5SZGoKGeCPg2AMeZV4HhA8wpglVtfBVzp1/64sawHykRkGHAZsMYYc9wYcwJYw+lGRQmH7DzrAXix+Kba4DMqBQkBeRTk207ld/edoLHQ9lEUhCj6Btg+ADjdAzDGZsoAvP7TsD5C0mk8AoXlUGMntOHT/wTf3g3TPm23Z13vO3ZUhNM8KkoaEW0fQKUx5hCAW3oFL0YA/jNcV7u23tqVSMlyg7y8chCBo4A9Aj2FLl8RuMMTvkBZUS7v7DtBq5swPD9sD8AvFXTni3DvGHj1QV/byV4mOE8VurqsAZh9Awydbtsqp/ccR1F6FnzpaZhzI5SNTopMRTkTxDsNNFi+ownRfvoJRG7Fho+orKykqqoqbuLiTWNj4xnXN2rfPiYAr1a9RFd2PvOPVVM/oIStATrmtrTRUnOILa49p6ORi4CdE27mWFcJo4u7eP2jA4wTm0X00eYPkEO9G4HBR3cyA9iw/nWaSqqRrg7mbfgGRW110FbH0cELGFL7JrufuZd9Y65NyGeH2L/z3PZ6LuzqZMeRJhqHf4kJLY+yaVstp3YFnjMXSq+CV16JRS6QnP+TeJCuukG1h0u0BuCIiAwzxhxyIR5X/JpqwD9vcSRw0LUvDGivCnZiY8zDwMMAc+fONQsXLgx2WEpQVVXFGdf3xmbYDRcvmGcHJK1rpHD8DCoDdWwro6SkzKfvxMewDibOmEN1XQmXzhzBT9ZsZ/iEqbDhPc6fN5tZo0N0y2xrhS0wb9Y5MGIOvP5P0OKbYGbIktvgrRzGn3iN8Tf8ImGjZGP+zo9sgTdg0swLYcbVwNeJsZ5nnyTl/yQOpKtuUO3hEu2v9FnAy+RZCTzj1/5llw10PlDnQkQvAEtFZJDr/F3q2pRI8er8PDgZDr4LHU299AEEjBfwJhV30+95N/v1u233TvidwG3QcBhefcDOhesx6jyYvRJO7oU9sT81JwyvA1hTOxWlbw9ARJ7EPr0PFpFqbDbPvcBTInILsA/wfP7VwHJgJ9AM3ARgjDkuIj8ENrjjfmCMCexYVsLBMwBdHXbkKoTXB+BNKl5YBnRy7qiBiNjJYCAcA+DXCbz623Y8wNJ/hAtvt/PVlgy1k5kUlME7j8OERdF/xkTS4AxASYZVNVWUIPRpAIwxX+hl15Igxxrg672c51Hg0YjUKafjdQLnFNg5YiFEFpDfgLEeHkAtpQW5TB5ayrYjDUDP6QOD4k1S8vavYNtq+NTdvpLFo11VzNwCOPc62PgoNB3r2bGaKpzcBwgM0BwERdGRwOmG5wGMnAfX/8FOIDI8yLyj2Xk9R+328AAss8f41sP2ALattrXOL7g9+HGzV9rrepOopBon9sCA4dZYKUqGowYg3fCe6kuHwYjZsPLZ4PHswBBQQB8AwKxRvk7fsMcBAMy6ofdO3sqzbY597fbez9V8vLsK6Rnn+G47oYuiKGoA0o56l3kzYHjo4wJDQK0nbVueb26CJdN885UW5PRVCsKvpLV/aeRgFA/2zVMQjIcugF/ODX2ORHF8Dwwam5xrK0qKoeWg0w0vrj5iTujjTisZUQuFg3qUjKgoyefRG+ey5sMjZGX1UbLa3wMo7MMAFFXYp/zXfgqNNbDs3p77G5wRM+b0EhaJpK0Bmmp8UzsqSoajBiDdmH2jLVY2to8iZYFpoNUbe0wc77F4aiWLp4aREZMTgQdQVGFnD/vLY7ajOtAAeHQ09/BI4kr1X+Dle2wp5wtus227XrLLIVMTc01FSTM0BJRuZGX1ffOHnmWjm47ZSczHXBD9dSPxAIoH21j7yb2+zudgtITYFwvtTfD0jbae/0aXeNbZDmvusjf/SZcl5rqKkmaoAeiv+BeD27vOLsdeFP35/PsA+npqLxpsJ48BO2G9/yQywcYmxJutz9l0z8pzoP6gvf6GX9kMoKX3QLY6vooCagD6L/5poHvXQU4hDJ8d+j2h8I/V9xW39x+XYLps7N2jsca3nigPYMef7eC4WV+CzhabcfTK/TYcNOlTibmmoqQhagD6K/4hoL3rYNS8nnH8RBI4PaX/k37D4eDt8aKry4Z+Jl4KZWNs2x9vh7Z6O3JZUZRu1AD0V7JyoL0Bnv8uHN4MY2II/0SKl6nkdTq31vn2NfoZgER4AM21doKa4bOgzNUl3PeGHbtQOT3+11OUNEYNQH/FGzG8/l8AE17HcbwYerbNtZ/5Jbvtf6P3HyCWCA+gzs1HUDYKBjoDkFcCi/4+/tdSlDRHe8P6K17NILDz+Y6Iw8CrS38AFRP7Pq70LLh9ExzaZLf9b/SHNsHA0fZGnQgPoK7aLgeOtGUvRi+wE72XavE3RQlEDUB/JcvvTztybnxq31zYS/2f3vDKTviHgA6+B8NnQltdgjwAPwMAcPPz8b+GovQTNATUX2lx1bYLyyO/cccLr/Cc96TfWmdTMYfPtMYhUR5AXkmPmkeKogRHDUB/pf6gXS67HyYnaeBTXqkdCew96R/fY5eDJ1vjkKg+gIEjz2yJCUVJU9QA9Fe8sEtfReMSSVaWLQtxYq/drj/gNI1IrAfghX8URQmJGoD+yhU/gflfhVHzk6tj0mWw7U/Q0QJ1fgYgYR7AAZ3sRVHCRA1Af6ViAiy7L/llD865xo5H2PFnqK+22UnFQxLjAXS02mqfXvqnoigh0SwgJbGMvRiKh8IHT9uJ5QcMt6EhzwOIZ0loL8SkISBFCQv1AJTEkp0D06+C7S9AzVbfzbmgzNYq6miJ37UCU0AVRQmJGgAl8Zxzra0OemSz7+bspYjGsx9ADYCiRIQaACXxjPQbheyVhygIGCMQD+qqAUlu5pOipBHaB6AkHhG45hEbAhp/iW3rHiR2HDrbbP9ArNTth5LK+JxLUTIANQDKmeGcz/bc9jyAJ66Bzla4u+7090SKjgFQlIjQEJCSHDwPoLM1fuesP6AGQFEiQA2AkhziXavHGPUAFCVCYjIAInK7iGwWkS0i8k3XVi4ia0Rkh1sOcu0iIj8XkZ0i8r6IxDA/oZL2FAzsud3VFdv5Wk5AR7MaAEWJgKgNgIjMAP4aOA84F/i0iEwC7gTWGmMmAWvdNsAyYJJ73Qo8FINuJd3JyoZ8PyPQ1dH7seHgTQSjBkBRwiYWD2AasN4Y02yM6QReAa4CVgCr3DGrgCvd+grgcWNZD5SJyLAYrq+kO4V+YaBTMRqA+kN2WaopoIoSLrFkAW0G7hGRCqAFWA5sBCqNMYcAjDGHRGSoO34EsN/v/dWu7ZD/SUXkVqyHQGVlJVVVVTFITCyNjY0pra83UkX3nM4cSt3666++TGduacjjoXftQ4+8xdnAW+9/RMvOhrjqjJVU+b4jJV11g2oPl6gNgDFmq4jcB6wBGoFNQGeItwQr+GKCnPdh4GGAuXPnmoULF0YrMeFUVVWRyvp6I2V07x0JjbsAuGjBfCgZ2scbQmjfuBu2wvxPLrFTUqYQKfN9R0i66gbVHi4xdQIbYx4xxsw2xlwMHAd2AEe80I5b1rjDqwH/Mo0jgYOxXF9Jc+IZAmpzT/15JbGdR1EyiFizgIa65WjgauBJ4FlgpTtkJfCMW38W+LLLBjofqPNCRUqG4p8Keqo9tnO1NQICecWxnUdRMohYRwL/3vUBdABfN8acEJF7gadE5BZgH3CtO3Y1tp9gJ9AM3BTjtZV0x98D6AoVPQyD9kbIL9WpIBUlAmIyAMaYTwZpOwYsCdJugK/Hcj2lnxFXD6Bewz+KEiE6ElhJHnHtA2iEfDUAihIJagCU5FFU4VuPRwhIPQBFiQg1AErymHSZnSwG4tMJnN/3OAJFUXyoAVCSR24BzLnRrscjDVQNgKJEhBoAJblk59llrAagvUFDQIoSITohjJJcsty/YDTF4Dpa4aPnYOAo7QRWlChQA6Akl1g8gO1/gt/f4tvWEJCiRISGgJTkkp1rl9F0AjcctsuxbjhKR0t8NClKhqAGQEku3SGgKNJAm46CZMN1v4Gpn4YZ18RXm6L0czQEpCSXWEJATbVQPNjOLnbdb+KrS1EyAPUAlOQSSwio+RgUDY6vHkXJINQAKMklyxmAaENAxRV9H6coSlDUACjJJRYPoKlWPQBFiQE1AEpy6TYAUfQBNNdC8ZD46lGUDEINgJJcvE7gSENAne3QWmc7gRVFiQo1AEpyycoGJPIQUPMxuyzSPgBFiRY1AEryyc6NPATUXGuXGgJSlKhRA6Akn+y8yA1A01G71BCQokSNGgAl+WTlRF4MrskLAakBUJRoUQOgJJ9wPQBjqDz8kq382R0CUgOgKNGipSCU5BNuH8CxnUz76J/ho+m+OkD+E8srihIR6gEoySfcEFDzcbtsb3SDwCogS/+FFSVa9NejJJ/svPDSQFvr7LKz1aaBavhHUWJCDYCSfMINAXkGoKPF1QFSA6AosaAGQEk+YRuAk3bZ2ap1gBQlDsRkAETkWyKyRUQ2i8iTIlIgIuNE5C0R2SEivxORPHdsvtve6faPjccHUPoBOQX2pt4X/h5Ac616AIoSI1EbABEZAfwtMNcYMwPIBq4D7gN+ZoyZBJwAvElbbwFOGGMmAj9zxykK5BZGZgDaGuy6egCKEhOxhoBygEIRyQGKgEPAYuBpt38VcKVbX+G2cfuXiIjEeH2lP5BbBB3NfR/nGYD6A3apHoCixETUBsAYcwB4ENiHvfHXAX8BThpjvNKO1cAItz4C2O/e2+mO10peig0BhTOhu9cHcHK/XaoBUJSYiHogmIgMwj7VjwNOAv8JLAtyqPHeEmKf/3lvBW4FqKyspKqqKlqJCaexsTGl9fVGqumecqyOQY0nWN+Hpk8c+phy4NTxvWQD726vpq4m9HtSgVT7vsMlXXWDag+XWEYCfwrYY4w5CiAifwAuAMpEJMc95Y8EDrrjq4FRQLULGQ0Ejgee1BjzMPAwwNy5c83ChQtjkJhYqqqqSGV9vZFyupueg7p3+9a0PQtOQHaX7S+YdeGlMGRy4vXFSMp932GSrrpBtYdLLH0A+4DzRaTIxfKXAB8CLwOfdcesBJ5x68+6bdz+l4wxp3kASgaSG24IqK7ntoaAFCUmYukDeAvbmfsO8IE718PAd4C/E5Gd2Bj/I+4tjwAVrv3vgDtj0K30J3KLoLMF+noe8DcAWgdIUWImpmJwxpi7gLsCmncD5wU5thW4NpbrKf2U3EK77Gz1rQdiTE8DUFSudYAUJUb0F6Qkn9wiuwwVBupo7jlvcEllYjUpSgagBkBJPjkFdhlqLECLTQE1XjJZydAEi1KU/o8aACX5dHsAIUYDu/BPR+5Au60eGVAlbAAACPtJREFUgKLEjBoAJfl4cf9QHoAzAG35g+y2egCKEjNqAJTk020AQvQBOAPQlZVnt4uHJFiUovR/1AAoyScCD6B7QLmmgCpKzKgBUJKPfxpob5xmAAYkVJKiZAJqAJTk090J3LcH0JHrbvyF5QkWpSj9n5gGgilKXOhOAw3VB3AScovZNuVrDC5YCmMuPDPaFKUfowZAST7hDARrPQkFA+nIK4OL7zgzuhSln6MhICX59NUJ3NkGH6+DgSPPnCZFyQDUACjJJ68Ycouh/mDw/W/+Ek7sgYVaP1BR4okaACX5iED5ODjx8en76g7Aqw/C1E/DxCVnXJqi9GfUACipwaCxcHxPz7Zju+Dpm8F0wWU/SoosRenPqAFQUoNBY60H0NXla3v+u7B/PSy5CwaNSZYyRem3qAFQUoPycXCqDRoO+doaDsH4RbDga8nTpSj9GDUASmpQMdEuj2zxtTUfg9JhydGjKBmAGgAlNRg1344H2P683TYGmmqhuCK5uhSlH6MGQEkNcgthwmLY9id7829vtCGhIp34XVEShRoAJXWYshwaDsKhTfbpH6BYDYCiJAo1AErqMPkyQKwX0HzMtqkHoCgJQw2AkjoUD7Z9AdtWqwegKGcANQBKajFlGRx+34aBAIq0E1hREoUaACW1mHqFXb5yHyA69aOiJBAtB62kFoMnwSXfgbYGGL0A8kuSrUhR+i1qAJTUY9H3kq1AUTKCqENAIjJFRN7ze9WLyDdFpFxE1ojIDrcc5I4XEfm5iOwUkfdFZHb8PoaiKIoSKVEbAGPMNmPMTGPMTGAO0Az8F3AnsNYYMwlY67YBlgGT3OtW4KFYhCuKoiixEa9O4CXALmPMXmAFsMq1rwKudOsrgMeNZT1QJiJa6EVRFCVJxMsAXAc86dYrjTGHANxyqGsfAez3e0+1a1MURVGSgBhjYjuBSB5wEJhujDkiIieNMWV++08YYwaJyP8APzbGvO7a1wL/xxjzl4Dz3YoNEVFZWTnnt7/9bUz6EkljYyMlJemXpZKuuiE9taejZkhf3aDaFy1a9BdjzNw+DzTGxPTChnb+7Le9DRjm1ocB29z6vwJfCHZcb685c+aYVObll19OtoSoSFfdxqSn9nTUbEz66jZGtQMbTRj373iEgL6AL/wD8Cyw0q2vBJ7xa/+yywY6H6gzLlSkKIqinHliGgcgIkXApcD/8mu+F3hKRG4B9gHXuvbVwHJgJzZj6KZYrq0oiqLERsx9AIlERI4Ce5OtIwSDgdpki4iCdNUN6ak9HTVD+uoG1T7GGNNnHZWUNgCpjohsNOF0tKQY6aob0lN7OmqG9NUNqj1ctBicoihKhqIGQFEUJUNRAxAbDydbQJSkq25IT+3pqBnSVzeo9rDQPgBFUZQMRT0ARVGUDCWjDICIjBKRl0Vkq4hsEZHbXXtvJaynisibItImIncEnOtb7hybReRJESno5Zor3Xl3iMhKv/bPu7LYW0Tk/hTU/byInBSR5wLab3MlvY2I9Dlhb5y13+50bxGRb4a45uUiss3pvNOvPSztKab5ERHZ5P5XnhaRXmsEpJjux0Rkj/jKxc/s7RwpqP01P90HReS/00j7YhF5x51jlYiEHusVznDh/vLClqaY7dZLge3A2cD9wJ2u/U7gPrc+FJgH3APc4XeeEcAeoNBtPwXcGOR65cButxzk1gcBFdhBckPccauAJami2+1bAnwGeC6gfRYwFvgYGHwGv/MZwGagCDuA8UVgUpDrZQO7gPFAHrAJODsS7SmmeYDfcT/1rp8Guh8DPpuE32bM2gOO+z3w5XTQjn2g3w9Mdsf9ALgllPaM8gCMMYeMMe+49QZgK/amGLSEtTGmxhizAegIcrocoNBZ2CJsQbxALgPWGGOOG2NOAGuAy7F/uO3GmKPuuBeBa1JIN8aYtUBDkPZ3jTEf96Y1gdqnAeuNMc3GmE7gFeCqIJc8D9hpjNltjGkHfuuuFbb2FNNcD3ZCJaAQ6LXTLpV0R0oqaheRUmAxENIDSCHtFUCbMWa7O24NIe4rkGEhIH9EZCz2ifAtei9hHRRjzAHgQexT/CFsXaM/Bzm0txLYO4GpIjLW3YivBEalkO6EEIt27JPRxSJSIbYEyXKCf2dxLTueCppF5N+Bw8BU4Bfpohu4x4WufiYi+eHoTiHtYG++az0jnAbaa4FcEfEGkX22l/d3k5EGwMVRfw98M5I/rt/7B2Et7jhgOFAsItcHOzRIm3HewN8AvwNew4YkOlNId9yJVbsxZitwH/ap5nms2xvsOwv6nUd6PUgdzcaYm7B/r63A59NE93exBmseNgT6nXCunSLaPQILXYYk2dqNjftcB/xMRN7GevAh7ysZZwBEJBf7R/qNMeYPrvmIuNnJ3LKmj9N8CthjjDlqjOkA/gBcICLz/TqP/gprmf0t8EhcyMUY80djzHxjzAJsaewdKaQ7rsRJO8aYR4wxs40xFwPHgR2uA87T/lVCfOfprNkYcwr7wBDSpU8V3S4sYowxbcC/Y8MWIUkV7e5aFU7z//R1vVTSbox50xjzSWPMecCr9HFfiakaaLrh4qiPAFuNMT/12+WVsL6XniWse2MfcL5z01qwHaYbjTFvAd3ZDiJSDvzI6/0HlmKfjBCRocaYGrfva8DnUkV3PImjdv/vbDRwNbDAeVP+33kOMElExgEHsE9EX0xHzU7HBGPMTrf+GeCjVNft9g0zxhxymq7EhjdCXS9ltDuuxSZAtIZxvZTR7vf+fKzXdU/IC5owe+n7wwu4COvmvQ+8517LsZ0na7HWci1Q7o4/C2tt64GTbn2A2/d/sT/GzcCvgfxernkzNua/E7jJr/1J4EP3ui4Fdb8GHMUaimrgMtf+t267E/vU8W9nUPtr7vvaROisqeXYTIxdwN/7tYelPVU0Yz30dcAH7u/1G/yyglJVt2t/yU/3E0BJuvyfuH1VwOVJuK/E+r0/gA0VbsOGokJq15HAiqIoGUrG9QEoiqIoFjUAiqIoGYoaAEVRlAxFDYCiKEqGogZAURQlQ1EDoCiKkqGoAVAURclQ1AAoiqJkKP8fH7zy3E2HNwAAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot prediction vs actual\n",
    "plt.plot(unscaled_data['google_45d'], label='Adjusted Close')\n",
    "plt.plot(unscaled_seq_predictions['seq_prediction'], label='FF predicted')\n",
    "plt.grid()\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LSTM model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Split into train and test set\n",
    "And here you have to be careful according to https://machinelearningmastery.com/time-series-forecasting-long-short-term-memory-network-python/ :\n",
    "\n",
    "\"A benefit of this type of network is that it can learn and remember over long sequences and does not rely on a pre-specified window lagged observation as input. In Keras, this is referred to as stateful, and involves setting the “stateful” argument to “True” when defining an LSTM layer. By default, an LSTM layer in Keras maintains state between data within one batch. A batch of data is a fixed-sized number of rows from the training dataset that defines how many patterns to process before updating the weights of the network. State in the LSTM layer between batches is cleared by default, therefore we must make the LSTM stateful. This gives us fine-grained control over when state of the LSTM layer is cleared, by calling the reset_states() function.\"\n",
    "\n",
    "But in order to can use \"stateful = True\" the data has to be a multiple of the batch size. Therefore I have to use modulo to get the test size correctly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 10\n",
    "train_test_split = 0.9\n",
    "x_train, x_test, y_train, y_test, test_data, test_index, x_columns = get_train_test_data(df_scaled, train_test_split, batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train=x_train.to_numpy()\n",
    "y_train=y_train.to_numpy()\n",
    "x_test = x_test.to_numpy()\n",
    "y_test = y_test.to_numpy()\n",
    "# Reshape data for (Sample, Timesteps, Features)\n",
    "x_train = x_train.reshape(x_train.shape[0], 1, x_train.shape[1])\n",
    "x_test_reshaped = x_test.reshape(x_test.shape[0], 1, x_test.shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_1 (LSTM)                (None, 1, 100)            56000     \n",
      "_________________________________________________________________\n",
      "lstm_2 (LSTM)                (None, 100)               80400     \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 1)                 101       \n",
      "=================================================================\n",
      "Total params: 136,501\n",
      "Trainable params: 136,501\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "lstm_model = Sequential()\n",
    "lstm_model.add(LSTM(units=100, input_shape = (x_train.shape[1], x_train.shape[2]), dropout = 0.2, return_sequences = True))\n",
    "#lstm_model.add(Dropout(0.2))\n",
    "lstm_model.add(LSTM(units = 100, dropout = 0.2))\n",
    "#lstm_model.add(Dropout(0.2))\n",
    "lstm_model.add(Dense(1))\n",
    "lstm_model.compile(optimizer = 'adam', loss='mse')\n",
    "lstm_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/matthias/anaconda3/lib/python3.7/site-packages/tensorflow/python/ops/math_grad.py:1250: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "Train on 2370 samples, validate on 270 samples\n",
      "Epoch 1/100\n",
      "2370/2370 [==============================] - 4s 2ms/step - loss: 0.0244 - val_loss: 0.0175\n",
      "Epoch 2/100\n",
      "2370/2370 [==============================] - 3s 1ms/step - loss: 0.0092 - val_loss: 0.0162\n",
      "Epoch 3/100\n",
      "2370/2370 [==============================] - 3s 1ms/step - loss: 0.0084 - val_loss: 0.0161\n",
      "Epoch 4/100\n",
      "2370/2370 [==============================] - 3s 1ms/step - loss: 0.0085 - val_loss: 0.0193\n",
      "Epoch 5/100\n",
      "2370/2370 [==============================] - 3s 1ms/step - loss: 0.0081 - val_loss: 0.0161\n",
      "Epoch 6/100\n",
      "2370/2370 [==============================] - 3s 1ms/step - loss: 0.0077 - val_loss: 0.0179\n",
      "Epoch 7/100\n",
      "2370/2370 [==============================] - 2s 991us/step - loss: 0.0075 - val_loss: 0.0197\n",
      "Epoch 8/100\n",
      "2370/2370 [==============================] - 3s 1ms/step - loss: 0.0072 - val_loss: 0.0155\n",
      "Epoch 9/100\n",
      "2370/2370 [==============================] - 2s 1ms/step - loss: 0.0069 - val_loss: 0.0177\n",
      "Epoch 10/100\n",
      "2370/2370 [==============================] - 2s 866us/step - loss: 0.0069 - val_loss: 0.0165\n",
      "Epoch 11/100\n",
      "2370/2370 [==============================] - 3s 1ms/step - loss: 0.0068 - val_loss: 0.0169\n",
      "Epoch 12/100\n",
      "2370/2370 [==============================] - 3s 1ms/step - loss: 0.0066 - val_loss: 0.0207\n",
      "Epoch 13/100\n",
      "2370/2370 [==============================] - 1s 614us/step - loss: 0.0062 - val_loss: 0.0158\n",
      "Epoch 14/100\n",
      "2370/2370 [==============================] - 1s 617us/step - loss: 0.0062 - val_loss: 0.0193\n",
      "Epoch 15/100\n",
      "2370/2370 [==============================] - 1s 616us/step - loss: 0.0060 - val_loss: 0.0189\n",
      "Epoch 16/100\n",
      "2370/2370 [==============================] - 1s 618us/step - loss: 0.0062 - val_loss: 0.0177\n",
      "Epoch 17/100\n",
      "2370/2370 [==============================] - 2s 755us/step - loss: 0.0055 - val_loss: 0.0197\n",
      "Epoch 18/100\n",
      "2370/2370 [==============================] - 2s 847us/step - loss: 0.0059 - val_loss: 0.0165\n",
      "Epoch 19/100\n",
      "2370/2370 [==============================] - 2s 1ms/step - loss: 0.0056 - val_loss: 0.0210\n",
      "Epoch 20/100\n",
      "2370/2370 [==============================] - 3s 1ms/step - loss: 0.0055 - val_loss: 0.0162\n",
      "Epoch 21/100\n",
      "2370/2370 [==============================] - 2s 929us/step - loss: 0.0054 - val_loss: 0.0211\n",
      "Epoch 22/100\n",
      "2370/2370 [==============================] - 1s 425us/step - loss: 0.0057 - val_loss: 0.0169\n",
      "Epoch 23/100\n",
      "2370/2370 [==============================] - 1s 425us/step - loss: 0.0055 - val_loss: 0.0200\n",
      "Epoch 24/100\n",
      "2370/2370 [==============================] - 2s 893us/step - loss: 0.0054 - val_loss: 0.0173\n",
      "Epoch 25/100\n",
      "2370/2370 [==============================] - 1s 425us/step - loss: 0.0053 - val_loss: 0.0164\n",
      "Epoch 26/100\n",
      "2370/2370 [==============================] - 1s 425us/step - loss: 0.0054 - val_loss: 0.0182\n",
      "Epoch 27/100\n",
      "2370/2370 [==============================] - 1s 424us/step - loss: 0.0053 - val_loss: 0.0187\n",
      "Epoch 28/100\n",
      "2370/2370 [==============================] - 3s 1ms/step - loss: 0.0051 - val_loss: 0.0175\n",
      "Epoch 29/100\n",
      "2370/2370 [==============================] - 3s 1ms/step - loss: 0.0052 - val_loss: 0.0201\n",
      "Epoch 30/100\n",
      "2370/2370 [==============================] - 1s 502us/step - loss: 0.0052 - val_loss: 0.0223\n",
      "Epoch 31/100\n",
      "2370/2370 [==============================] - 1s 494us/step - loss: 0.0053 - val_loss: 0.0178\n",
      "Epoch 32/100\n",
      "2370/2370 [==============================] - 3s 1ms/step - loss: 0.0051 - val_loss: 0.0218\n",
      "Epoch 33/100\n",
      "2370/2370 [==============================] - 2s 874us/step - loss: 0.0051 - val_loss: 0.0307\n",
      "Epoch 34/100\n",
      "2370/2370 [==============================] - 3s 1ms/step - loss: 0.0049 - val_loss: 0.0177\n",
      "Epoch 35/100\n",
      "2370/2370 [==============================] - 2s 845us/step - loss: 0.0050 - val_loss: 0.0186\n",
      "Epoch 36/100\n",
      "2370/2370 [==============================] - 2s 833us/step - loss: 0.0049 - val_loss: 0.0205\n",
      "Epoch 37/100\n",
      "2370/2370 [==============================] - 1s 485us/step - loss: 0.0048 - val_loss: 0.0190\n",
      "Epoch 38/100\n",
      "2370/2370 [==============================] - 1s 432us/step - loss: 0.0047 - val_loss: 0.0244\n",
      "Epoch 39/100\n",
      "2370/2370 [==============================] - 1s 455us/step - loss: 0.0049 - val_loss: 0.0180\n",
      "Epoch 40/100\n",
      "2370/2370 [==============================] - 1s 451us/step - loss: 0.0048 - val_loss: 0.0168\n",
      "Epoch 41/100\n",
      "2370/2370 [==============================] - 1s 455us/step - loss: 0.0047 - val_loss: 0.0197\n",
      "Epoch 42/100\n",
      "2370/2370 [==============================] - 1s 461us/step - loss: 0.0046 - val_loss: 0.0179\n",
      "Epoch 43/100\n",
      "2370/2370 [==============================] - 1s 596us/step - loss: 0.0047 - val_loss: 0.0181\n",
      "Epoch 44/100\n",
      "2370/2370 [==============================] - 2s 1ms/step - loss: 0.0048 - val_loss: 0.0194\n",
      "Epoch 45/100\n",
      "2370/2370 [==============================] - 3s 1ms/step - loss: 0.0046 - val_loss: 0.0166\n",
      "Epoch 46/100\n",
      "2370/2370 [==============================] - 2s 642us/step - loss: 0.0047 - val_loss: 0.0160\n",
      "Epoch 47/100\n",
      "2370/2370 [==============================] - 1s 495us/step - loss: 0.0046 - val_loss: 0.0161\n",
      "Epoch 48/100\n",
      "2370/2370 [==============================] - 1s 488us/step - loss: 0.0046 - val_loss: 0.0165\n",
      "Epoch 49/100\n",
      "2370/2370 [==============================] - 1s 504us/step - loss: 0.0044 - val_loss: 0.0171\n",
      "Epoch 50/100\n",
      "2370/2370 [==============================] - 1s 495us/step - loss: 0.0046 - val_loss: 0.0163\n",
      "Epoch 51/100\n",
      "2370/2370 [==============================] - 1s 489us/step - loss: 0.0048 - val_loss: 0.0179\n",
      "Epoch 52/100\n",
      "2370/2370 [==============================] - 1s 487us/step - loss: 0.0046 - val_loss: 0.0192\n",
      "Epoch 53/100\n",
      "2370/2370 [==============================] - 1s 489us/step - loss: 0.0046 - val_loss: 0.0181\n",
      "Epoch 54/100\n",
      "2370/2370 [==============================] - 1s 494us/step - loss: 0.0046 - val_loss: 0.0169\n",
      "Epoch 55/100\n",
      "2370/2370 [==============================] - 1s 501us/step - loss: 0.0043 - val_loss: 0.0193\n",
      "Epoch 56/100\n",
      "2370/2370 [==============================] - 1s 495us/step - loss: 0.0042 - val_loss: 0.0180\n",
      "Epoch 57/100\n",
      "2370/2370 [==============================] - 2s 729us/step - loss: 0.0044 - val_loss: 0.0182\n",
      "Epoch 58/100\n",
      "2370/2370 [==============================] - 3s 1ms/step - loss: 0.0044 - val_loss: 0.0178\n",
      "Epoch 59/100\n",
      "2370/2370 [==============================] - 2s 999us/step - loss: 0.0042 - val_loss: 0.0175\n",
      "Epoch 60/100\n",
      "2370/2370 [==============================] - 2s 749us/step - loss: 0.0043 - val_loss: 0.0161\n",
      "Epoch 61/100\n",
      "2370/2370 [==============================] - 1s 485us/step - loss: 0.0042 - val_loss: 0.0172\n",
      "Epoch 62/100\n",
      "2370/2370 [==============================] - 1s 485us/step - loss: 0.0042 - val_loss: 0.0181\n",
      "Epoch 63/100\n",
      "2370/2370 [==============================] - 1s 485us/step - loss: 0.0042 - val_loss: 0.0173\n",
      "Epoch 64/100\n",
      "2370/2370 [==============================] - 1s 485us/step - loss: 0.0041 - val_loss: 0.0185\n",
      "Epoch 65/100\n",
      "2370/2370 [==============================] - 1s 487us/step - loss: 0.0044 - val_loss: 0.0178\n",
      "Epoch 66/100\n",
      "2370/2370 [==============================] - 1s 485us/step - loss: 0.0044 - val_loss: 0.0165\n",
      "Epoch 67/100\n",
      "2370/2370 [==============================] - 1s 487us/step - loss: 0.0041 - val_loss: 0.0182\n",
      "Epoch 68/100\n",
      "2370/2370 [==============================] - 1s 485us/step - loss: 0.0043 - val_loss: 0.0163\n",
      "Epoch 69/100\n",
      "2370/2370 [==============================] - 1s 486us/step - loss: 0.0043 - val_loss: 0.0165\n",
      "Epoch 70/100\n",
      "2370/2370 [==============================] - 1s 493us/step - loss: 0.0040 - val_loss: 0.0191\n",
      "Epoch 71/100\n",
      "2370/2370 [==============================] - 1s 490us/step - loss: 0.0042 - val_loss: 0.0178\n",
      "Epoch 72/100\n",
      "2370/2370 [==============================] - 1s 487us/step - loss: 0.0042 - val_loss: 0.0179\n",
      "Epoch 73/100\n",
      "2370/2370 [==============================] - 1s 478us/step - loss: 0.0039 - val_loss: 0.0179\n",
      "Epoch 74/100\n",
      "2370/2370 [==============================] - 1s 424us/step - loss: 0.0041 - val_loss: 0.0182\n",
      "Epoch 75/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2370/2370 [==============================] - 1s 425us/step - loss: 0.0040 - val_loss: 0.0182\n",
      "Epoch 76/100\n",
      "2370/2370 [==============================] - 1s 424us/step - loss: 0.0041 - val_loss: 0.0178\n",
      "Epoch 77/100\n",
      "2370/2370 [==============================] - 1s 425us/step - loss: 0.0043 - val_loss: 0.0183\n",
      "Epoch 78/100\n",
      "2370/2370 [==============================] - 1s 422us/step - loss: 0.0041 - val_loss: 0.0185\n",
      "Epoch 79/100\n",
      "2370/2370 [==============================] - 1s 422us/step - loss: 0.0040 - val_loss: 0.0195\n",
      "Epoch 80/100\n",
      "2370/2370 [==============================] - 1s 424us/step - loss: 0.0040 - val_loss: 0.0187\n",
      "Epoch 81/100\n",
      "2370/2370 [==============================] - 1s 423us/step - loss: 0.0039 - val_loss: 0.0237\n",
      "Epoch 82/100\n",
      "2370/2370 [==============================] - 1s 424us/step - loss: 0.0040 - val_loss: 0.0229\n",
      "Epoch 83/100\n",
      "2370/2370 [==============================] - 1s 424us/step - loss: 0.0041 - val_loss: 0.0223\n",
      "Epoch 84/100\n",
      "2370/2370 [==============================] - 1s 429us/step - loss: 0.0039 - val_loss: 0.0191\n",
      "Epoch 85/100\n",
      "2370/2370 [==============================] - 1s 428us/step - loss: 0.0040 - val_loss: 0.0192\n",
      "Epoch 86/100\n",
      "2370/2370 [==============================] - 1s 425us/step - loss: 0.0042 - val_loss: 0.0205\n",
      "Epoch 87/100\n",
      "2370/2370 [==============================] - 1s 431us/step - loss: 0.0041 - val_loss: 0.0184\n",
      "Epoch 88/100\n",
      "2370/2370 [==============================] - 1s 480us/step - loss: 0.0039 - val_loss: 0.0193\n",
      "Epoch 89/100\n",
      "2370/2370 [==============================] - 1s 560us/step - loss: 0.0041 - val_loss: 0.0186\n",
      "Epoch 90/100\n",
      "2370/2370 [==============================] - 3s 1ms/step - loss: 0.0039 - val_loss: 0.0196\n",
      "Epoch 91/100\n",
      "2370/2370 [==============================] - 2s 863us/step - loss: 0.0039 - val_loss: 0.0188\n",
      "Epoch 92/100\n",
      "2370/2370 [==============================] - 2s 850us/step - loss: 0.0039 - val_loss: 0.0191\n",
      "Epoch 93/100\n",
      "2370/2370 [==============================] - 3s 1ms/step - loss: 0.0038 - val_loss: 0.0178\n",
      "Epoch 94/100\n",
      "2370/2370 [==============================] - 3s 1ms/step - loss: 0.0039 - val_loss: 0.0210\n",
      "Epoch 95/100\n",
      "2370/2370 [==============================] - 2s 640us/step - loss: 0.0038 - val_loss: 0.0182\n",
      "Epoch 96/100\n",
      "2370/2370 [==============================] - 1s 615us/step - loss: 0.0037 - val_loss: 0.0203\n",
      "Epoch 97/100\n",
      "2370/2370 [==============================] - 1s 619us/step - loss: 0.0037 - val_loss: 0.0202\n",
      "Epoch 98/100\n",
      "2370/2370 [==============================] - 1s 614us/step - loss: 0.0038 - val_loss: 0.0185\n",
      "Epoch 99/100\n",
      "2370/2370 [==============================] - 1s 609us/step - loss: 0.0038 - val_loss: 0.0266\n",
      "Epoch 100/100\n",
      "2370/2370 [==============================] - 1s 614us/step - loss: 0.0036 - val_loss: 0.0195\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "167.52475452423096"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "start_lstm = time.time()\n",
    "lstm_model.reset_states()\n",
    "history=lstm_model.fit(x_train, y_train, epochs=100, batch_size=batch_size, verbose=1, validation_data=(x_test_reshaped, y_test));\n",
    "end_lstm = time.time()\n",
    "time_lstm = end_lstm - start_lstm\n",
    "time_lstm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2370/2370 [==============================] - 1s 277us/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.002759826799141408"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lstm_model.evaluate(x_train, y_train, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x7f053101b898>"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYAAAAD8CAYAAAB+UHOxAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDMuMC4zLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvnQurowAAIABJREFUeJzsnXdYFNf6xz+zywLSFVAELBixo1ix11xN1FiuGk0zamK66eam3l9uyk2/KaaaYnrRxESNJbFh7xV7VxBQRECQImV+fxyWZWGpAovL+3kent2dnXKGnTnf85bzjqbrOoIgCELdw2DvBgiCIAj2QQRAEAShjiICIAiCUEcRARAEQaijiAAIgiDUUUQABEEQ6igiAIIgCHUUEQBBEIQ6igiAIAhCHcXJ3g0oDT8/P7158+aV2vby5cu4u7tXbYNqOXXxnKFunnddPGeom+dd0XPesWPHBV3X/cuzbq0WgObNm7N9+/ZKbRsZGcnAgQOrtkG1nLp4zlA3z7sunjPUzfOu6Dlrmna6vOuKC0gQBKGOIgIgCIJQRxEBEARBqKOIAAiCINRRRAAEQRDqKCIAgiAIdRQRAEEQhDqKCIBQfVw8CUdX2LsVgiCUgAiAUH1seB9+nWbvVgiCUAIiAEL1kRoHWSmQl2vvlgiCYINaKQCapt2kadrslJQUezdFuBpS49Vr1iX7tkMQBJvUSgHQdX2Rruv3eHt727spwtWQdl69ZoqQC0JtpFYKgOAA5OXBZbMAiAUgCLUREQChekhPhLwc9V5cQIJQKxEBEKqHtHjLe3EBCUKtRARAqB5Sz1neiwtIEGolIgBC9ZBWSADEBSQItRIRAKF6EBeQINR6avUjIYVrmNRz4OINedkiAIJQSxELQKge0uLBsxG4eIkACEItRSwAoXpIPQcejdR7iQEIQq1ELAChekiLVwLg6i1ZQIJQFmvfht+m1/hhxQIQqh5dV2UgPAMgI0n9CYJQMifXQnxUjR9WLACh6slKhex0iwUgLiBBKJ3LCZBxEbIzavSwIgBC1WOeA+AZAK5e4gIShLIw3zOXYmv0sCIAQtVjLgPt0VCygAShLHJzIP2iep8aV6OHFgEQqh7zaMYjQLmAcrMgO9O+bRKE2kr6BUBX78UCEK55ClxA+TEAkDiAIJRE4bIpl87W6KEdUgAys3O5kqvbuxl1l9R4MLqAq49FACQOIAi2SUuwvL8kLqCrJvylv5l/NNvezai7pOVPAtM0FQMA9WzginApDt4Lg/MHq759glCbMFsAJnexAKoCFycj2XliAdiN1PwyEKCygKDigeDYXZB8BmK2VW3bBKG2YX5yXuOOEgOoClxNBrLz7N2KOkxaoTIQlXUBJZ1SrzV8Qwi1mGMrcM5KtHcrqp608+DsAb7XSRZQVeBqMkoMwJ6knVNzAKCQC6iyAlCzJrFQS7mSDj9MoPOupyHptL1bU7WknQd3f/AKUtZzbs25rx1TAJyMYgHYi5wsVfrBI18ACiyACrqAxAIQCpN0EvQ86mWeh29GOpYIpJ1Tc2Y8GwO6dVZQNeOQAuBiMpCda+9W1FEK5gA0VK/OHoAmLiDh6rh4AoDDrR5QgwlHEoHLCep+8QpSn2vwmndIAXB1MnJFgsD2IbVQGQgAg6His4Hz8iA5/+YWF5AABQKQ4N8HJi9QA4rPB8HJdXZuWBWQdg7cG4JXoPosAnB1uJgMXBELwD6YHwVpDgJDxQvCpZ2DnEzwbqqEIyutatsoXHskHgc3X3JMHhDYGe5eAW6+8O1o2PSxqkB7LZJzJd9l2kgEoKpwNUkMoNr540FY+VLx5WlFLACoeEE4s/unWW/1WsOZEUIt5OIJaNDC8tkvFO5eCa1vhL+egXVv269tV8Pl/ElgHv5Qrz44udao1euQAuDiZCBbsoCqj8uJsOdHOLys+Hep50AzqKwGM67eFXMBFRUAcQMJF09Cg+usl7l6wc3fQdPecGCBfdp1tZjnAJgnTnoF1uiAxyEFwNVk5IpYANXH4SWg56lRWV6Rf3RaPLj5gcFoWebiVbGZwEmnAA2a9lSfU0QA6jTZGXApxtoCMGMwqIHCuQMqVfRaIy1fANzzkyY8A8UFdLW4msQCqFYOLlKvORmQWuRivXgSfJpaL6uMC8g7GHyaqc+SCVS3MWf72BIAgOBuoOdC3J6aa1NVYRYAc9acV6C4gK4WFyexAKqNzEtwYjUEhKnPicesv084DA3bWC+rjAuofnMwuapAn7iA6jYXj6vXkgQgqKt6Pbu9ZtpTlRRNm/YKVJPBilrW1YRDCoBDl4JIv1iQEmfFhaM0PT23+rMhjv4NuVeg72Pqc+Jx67ZdPg/+RQTAxUtlAZW3bUmnoH7+6N8rSCyAuo75evctQQA8GqqMsRg7C0D6RcirYPrh5QR1f5jqqc9eger+Sq+ZkheOKQBORvJ0yM51QBVY8SJ8M7r48t0/0uLkD9XfWR5cqAJWbUeDUz1rAUg4rF6LCoCrl4oZXClHOueVdBVHqN9cfRYBEC6eUBky9eqXvE5wVzi7s+baVJSsNHi/E2z7omLbmctAmClIBa0Zq9cxBcCkApCZ9pwOfCUdfpwI5w9V7X4TDkHKmeIPj04+o14Tj1bt8QqTnQFHl0ObkWB0UsWrLhYWgPxz9W9tvV1FCsKZJ4DVD1GvNewTFWohRVNAbRHUTd0XZp96UfJyq7fGTvxeZeUeX1Wx7dLOW8+ZMQtADWUCOaQAuJjUaWXa0w8UtxuOLINjK6p2vxdPqtfkaOvlZgG4UI0CcHwVZKdD25vU5wYtrGMACYdVTXOvYOvtSisId3ItvBVqSf00vxZYAIGQcbG44Al1h/IIQHA39VqSG2jRIzB7YPW5SGN3qdfoLcX990mnIG6v7Wv48nk1B8CMp1gAV42rk7IAsnLsaAGYR8PmjrkqyEqz5A0nF6mDUmABHKfaOLBQPeWreV/12belurhzc9TnhEPg30ql5hWmtIJwO75W57T1c/W5mADUfH0UoRaRkwUpMcXnABQloCNoRtuB4MTjsPsHOLev+h4wZBaAjCRrKzwvF74cCp/1g1cbw3sdYdcPlu/NZSDMeDRU51FD17tDCkCtsADM/vCU6NLXqwjmzhGsBSA701KCoagLKCMZ1rypppxfDbquAsCtbgCjSS3zvQ7ycixtSThc3P8PJbuArlyGw0vVxLGd3ymBSzqlCsi5+ap17DA9XqhFJJ1W8aOyLABnN2jUHs7uKP7duv+BIf+aPby46tsISgDM1/6ZzZblZ3eqTr7XQzDwGXB2h+UvKHdUTpYaFBV2ARmMqipoXRYATdNu0jRtdkpKBUsI51NlMYDMFIixcUGVh6u1ADKSVOddmKSThd4XEoCUGADyNKfiLqD982H1q3ByTeXaYSbhsHLFmEf/oCwAUCOszBQ1J6Co/x9KdgEd+Uu5lAY/ryaK7f3ZkgKqaWqdsiyAAwtwzai58rlCDWPOACpLAEC5gc7utHbBJJ1W11W3qSpd9NCSqm9jZopyhXYYrwYu0Vss3x39S43o+z8JA/8FQ/5PZfgcW1FoDoC/9f686rgA6Lq+SNf1e7y9vSu1vYuTOq2rdgGt+A/MuUGNsCuK2QIo6qsvLz9OhN/utl5m9v+7+1tbAMmnAEjxbqcEp3B7Y3er18KjkspwZpN6NZdngEICcAwSjqj3pVoARQRt/3z13IA+j6oCX1s+U+dodv+AuhlAzQQtSuYlmHsnLU58W+HTEWopOVdgy2xL51gRAQjqpgYZha3g9e8qC7PPI9B6OMTuLN+D188dgHlTyjcL3TwBLagzNImwvteOLFPLzBlMLYeomfJ7frIuA1EYr5qbDVwrBeBqMVsAWaW5gNLOl34h5ObAgT9UTm5FR/EZySqK7+6vRrZFR/KxuwpG7TZJjlajiJit1kGrpJPKBx/Q0bpN+e8vNggHdGtLoXBw6mo4s0n5KgvfiG6+4OKtMoFKygCCQs8FLmQBZF6CI39D+zHK7I24Dy4cgQuHrQXA2V2ds60bIm4PoOObuE25k0pixX8g8o3ynmn1cWCBZWAg2GbzR7B0Jvx8q3KRXDyhBhBuDcre1jwhzBwITjmrfP+db1edapsRavnhQlZA8hnY9b31fZabA3/cD/t/V+0oq8SE+R5rnC8AF4/D5Qvqmo2PglZDLesaTRA2Qbk+zYOmwjEAUEkUuVlln28V4NACkFmaBTD/Hph3Z8nfn1xjmYxha+JVaVzI/2FbXq9eC8cBsjPhy2HwQWdY+jSkJRTf/lC+nzIzxXrbiyehQYgqtVDYBZR8BgxOJPvkz841u4GyM+H8gfzg2I6rS4M7vQma9bK4ZkC9970u3wI4pCoZmss3FMbJFYzO1kHgw0vVRd7+n+pz+7GWfOjCAgCqLIQtAYhVed/GvCy1P1skHFGjwO1fVl8GyKVY+OoG9T8qiaw0+HUa/H7ftVe6+OCfKlurukmOVvEq/zYQsw2WPmXJACp83ZWEXyvlbtz8Ccy9E74bq+IHfR5V3/u3UenFZgHIzoQfJ8GCB2FNoQHC1s9UFl/XqWqQsfAhy28Wsx3+fNz6eozdpe5Jd19L/aroLSpmBipuVphOk9TAcutn6rNHEQEY9io8GlX2+VYBDioAZQSBdV11iLG7LRksRdk3X010gooLgHk0HPoP9Vp4tJ54VHV8AR1h62w1eWTffOvtDy4Ck5t6H1/oQkg6qS7g+s2UPz4r1bJ/72DS3YItxwA4v18FadvepHzt5/ZV7DzMpMSoHOumvYp/53udigEkHFYlegsXgTOjaZbZwGb2/QbeTSC4u/rs5ALdpqn3RQWgpLkAZ3eCd1OynH3V/myx7m0KHrNXmtV1Nax/V1lIfz5asshGb1G/RexOOL6yetpRHei6Oq/59159IkFZ/PWMOt5t86Dv4ypD7OTa8rl/QGWftRulrtVz+8HdD4a/ZZlVrmnKCji5Vt07q15W90jTXhD5GuydqwZWq16B0GEw8l0Y8m91bf31HPx8G3wxRA0m1rxpOW7sLuXCBGgcrgY7ZzarGJd30+Ju0cadwL+txXJwLxIDKI/YVREOKQAuTmUEgZNPq84oN8v2xKmcLNUJtx+jOq7CLpXykHBYjXqb98s/XrT1dwCjZsGDW8GvJSx72pIjfPkCnNmoOkPNYBGA3Gy1H7MFABZhST4DPk3JdXJTGQQX8nPzzRdYz/vV65lyuIGuXIbV/7VOlzP7NG0KQEvVscbvte3/N1O4IFz6RTWnoP0Y65TRng9AvyetA81Qsk80dicEdeF8w75qglpGkvX3icchah6E9FefY7aV3L7KcikOdnwDDdsr4d/+le31Tm9QlphnY1jz1rVjBSSdVOUKUmNLFtmq4Ohydc/1f1Jd34Ofh+uGQF52+QUAYPRH8PQZmLEdpi6xDCrMtBmhRt/L/w82fQjd7oLJC6FZX2UJzJ0MaDDibdUR931MuWw2fwQn1sCg56DjJNj9o3Ijp19UiQtmATC5KhE4uRZORCr3T9EOXdMg/Bb13tVbbWMnHFIAzBZAVk4JFkDhUXW8jVHx8VXKd99hvOpwK2MB+IUqZTe5WVsA5w+qjsD3OtX5D31VjU53fKO+N5da7jhR5T6b25oSrSoe1g8Bn+ZqWREBAFSHbJ6cFbsb6jVQfkmv4LLjAInH4YvrlTn852OWTur0RnD2tBSAK4xvSwpG2Lb8/2YKF4Tb85O6sc3uHzP1fGDIC5a6KGa8glQnlFPIL3o5UZ13UBfON+yn9neoSIrfunfUaGzMp0qQq6NWzIb31ch+0g/QYpDKuLpso47LqQ2qk+j3BERvhlPV/ChDXVfCXdHaNEWJ3qpe3fxg46yqF67cHDUwWTITfEOh9wy13GCEcV+oWeetb6y64zWJULGr7V+q4w19BZycYeJ3yiKN263Ex3w/aRqM+hDGfAKP7IEBT0H/mUpEtnyq1geLAAA0jVDLs9OVJWGLsJvzn5vR0Pb3NYRjCkBZFkB8lPrnG53VyLUo+35THWeLAWr0cbESFoB/G3XxeDexzthJOKQ6fycX9Tmknxp9rH9X+SQPLlIXX0CY+jO3zzwHoEGIxaRNOq0sh7RzFlHwbWmxamJ3qwtT09RFWZoAHPlbPWM1NQ7Cb1cuDXPq6JlN0KSHbfdO4dFZaRaA2QWUdBpWvQrXDba+aUrD1vR4s3UT2IVUz5bKbRT1q+X7iydhz89qBOgdpI5V1RZA6jnYMUf5dBuEwA2vK1//6les17uSrlyOzftA5ztU5lNhF0JKTMWqpZaHHXPgq2Gw6OGrqywZvUUF+q9/UblLKuK+ys5UpVBsTb5KPK6ybN5qAV8NVS6+kf+z3BegAr+TfrAEd6sCg1EJisEJ/jlbzR8wH+uO39VvGHGv9TYmVwi/Vfn4QQ3c2t6k6v6Yn0ncuJNl/Sb5cQCneur+toVXY7UPW4OqGsQxBaBgHkApFoBvKDRsa20NgLpZDy1RvkSjSY24k0+XHCsoSlaqGq2bR8M+Ta0DuQmHio+UBz6tJnJt+lCZjW1HqU47IEyNcjOSLSJUP0SNYExuql1m95J5xOIXqlwhKWdVANjcyTbpqW4yW2mpKTHw8y1qH/dEwoh3lKsi8g1l4p4/YNv9A0rMzPiVwwJY9LA6t5s+KL+v09ZksNidgKZuPE2DDuOUYKWdV4G7JU+qm7z3w2r94G5qeU4VZlds/ECNBPs9oT43bAM9pivfdeHrKmabslCa9VWdSZ+HlQWwZCZ82hfeba+SAipaRwbU9XZspfXIPOeKmvzk6q0yXP561vbIPTdbuTxObSh5/9Fb1f+u40R1TWz4oOR18/Lg1Hr44wH4X3t4NQA+joCPe6plZhfdwT9VWYbjq1QnOH4OPHHY4qqrbv7xMkxfBUFdrJfXb6bcpbYGOkXp84i6njd9pAZBhQvVNYlQryH9i1uzhRn/NYwvwWVYQzjZ9ejVhHkeQKkWQJMI5RY4skzdHObO6OhfkH1ZdSigfty8HNWJNwgp++AXiuTD+zSxTE83p7UVdX2E9INmfZT7QM+z1NoJ6Khez+1Tvliji7oJNU1l2ySfsbiBfJpCUpYlN3//78plFBiuPjfpoV6jt6g2FebwUnWO4+dYArB9H1NZGOveUZ+blSAArt7KjM1IKv3/4+qlxC/hEIz4X/E2lIZ5MljiMcs8hLM7ldiZU0w7jFNt/bC7mm+gGWHwc5Z5BMHdlQsjfp+qHHm1XL4A275UpnxhERz4NOz+Cda+BTfnz084vUFZnE3zO4auU2H9eyoJILiHmhy0dy58908Y9KyKgxQtp2ELXVdzRY4sU26KLneo5bt/UNfr7b/B0RWw5RP1fxr0rPX2JyLzH6WoKeukKJmXVDC17SjlJom4D1b8nxKMzGTlt08+rc5NM6rfNvm0che2GqoGBA1ClAWw4X0lVKHXK1EK7Kz+P0UfHlQTuDUoX1ppaQR3U4J+er3y+RfGwx8Gv1C2oJXnN65mHFIADAYNJ62EGED6RXVzdL9bCcDu75ULxfwQ8/1/qA6tWf4NYe7Ukk5a3melwaE/1c1f9EcsWhLZp6nqHLNSLdPabfnKBz4N39ykjh2c31mbzcP4KMsEKfPxzKmgZveST1PgqEUAouapV7MF0KiDKtQWvQXCxlsf+/AStZ1fqGVZlzvVKHLTh2oafWlmuF8rlZVkLhFhC5f8yWDN+6kOsCL4tlTxkK2fKxeKpikXUIuBlnUatVfCmZWqUkrbjrK+yc3ZRjHbqkYAon5VT0Tr84j18nr1ocfd6n934ZhyF5zaoH5L84Q4Zze4N9+9ZrZuIu5VcZfVr6qOted90OYm1fGWxPYvVefvEaDEumlPNTBY9z81Keq6IdBiMFxJVXGdwC7QulBKojmoe2ylshqKHuvsdkC3DB66TYW1b8PXw/PPw0Ndy7quBht+ocp/3makxbVipt1oFWTd9T10nQI3vGHX4GeV0PdRJQBFLQlQwexrAMcTgOwM2PwxA5yyycxuXvx7cypkQJjF3xi/TwmAudxxp4kWM9Ds4754QvmtAXZ+q1LWjM7QochoPuGQ6jDN5YwLMnaiLemhDdsWb1fzfkpQGraxdPKejZQgxEepGEDhEXb9ZirIl3xGHc8zADiqOgCDSQWh3P0to2ejk+r4isYBMlOUH7PnfdbLTa7KClj2L3WBl2bKDn+r7Ikr9ZupkeGoDyo+8jEYVVsWPqSm0Ddqr1xmRWMIE78veR9egep/EbMNuK/k9crL/vlKVBu1K/5dxH2w8UPlIrrxTXXM7kVmdZs7fjPO7jD2M5UBtfZtNWfAvaEaGHS/q9gh3C5Hw/rn1FyTUbPgkz5qmy6TVRrkyP8podQ0GPk+HI9UmSxmAcjOVK4Yn6bqGjq93nJ9m4neqkb3ZvF39YZR76vKli2HKLdiaQJVmMBwmL5aWcgBHcq3TW2n5fXKam45xN4tqTT2t0GqGoMJtn3FdMMi26UgzL7ZgDB1A4Ml0Hp8tXL/mF0woEZXTq7WgeBT69Xruv8V960mHFEjIWO+tnrnC0BKvgBoRssovTCaBuM+t/iTzQSEqRuuaIkEn6YqUyl+r5ooZRYso5NFtBqHW/vZm/RUYpdV6MEsx1Yq/3TrEcXb1PVOta82Nr4rTKN2ZQd0u0+Hx/dXLKWvMB0nqoD62rctD/6wNfIqjeBuVRMITj6jhLT9WNvfezRUs0/3/KSKj+Vm2XaxFEXTVAf+8G647Vcl+EufKl7jPieLtgffUaIx+mMlJqM/UtfC0qfUSN88CRHUNdH9LpWaaA7IHv1bWQY3vKGClbYm0kVvUemtZjcbKFfbP/6j3Bvl7fzNODk7TucP+bGnf1osu2sQxxMAoxN0n0ZPbR8+aTbSN+OjVKfu0VClHXo3tVgFh/5UP2bzQpF7g0GN5s0CkJerRksejeBclLIYClM0yFs4Z//8QdUBFs50KIuAMNW+7MsWqwIsM27PbLZkBZkxC0zRTrlphDLVC0+FP7w0P1W0R/Fjm+rBjJ3F3RyVwWC4uhvFyVm1I3qzCrxpxopnUAR1Uy6zkh4aYovsTEvGkZn9v6vXotZfYXrPUHGVxU8CWslBdFsYDGoS4ahZah+7f7T+fsMHeKadVJ2+Z34dmTbDlZWh56mqk0UD7F3uVDGkrbPV532/qdTO0KFq5H94qfVgJi9Xpc3aui4Eh8HxBACgyxSyMNEr8dfi38VHWXccAWFqWW626hhb3Vjcl114LsC5fcptMuTfakS67m3LjZOdoVw1hdMh3f3VjZd8xvYD08siIAzQLe0wYxaW7PTigTQ/swAUCU6FDFSB5b9fUAG+3GwV9G51Q8mZDzU4K7FMOt+u3CJnNkLDdqW7pWxREAco53yA7Ez4aZLKWNn/h2X5vvlKXEuzZhqEQLsxKjbSqH3lgo7+rZVw7PzWco1lXoJNs7jgG1E8P/6GN+Detda1Z8y4+0LHCSo1NiVGzVJtP0YNmFrfoCzUwjPFEw6ptF1zRovgkDimALj7stLQh4hLy60LseVkqQu7qAAkHlO+5Ywka/ePmQYtVMduTnMDNemn98PKTD69UXUWq14BdGsBMBhUxkviMSUipeXK28KcCQTWFkDhUX9RAQjurtJEzR2eGaMTjHxPBb1X/1fl92emqNHjtYCpHvR+SL0PKuccgsI07qRSQ8vjBsrJgrl3wInVahLd4ifUBK/E4yq+Ys4SK42++TVois5srghd7lTFxczX3dbZkJnCqeY3F1/X6GSdj16UHveqAcO8KSqA3SE/GaDVDYAGh5dZ1jXHisQCcGgcUwCAhU7DcNEzVUqcmYRDyqS2EoAOymxek+8LLRoIAzWay8lQgcdT65UgeAeptDt3f/j7efisv8qY6Xy7KjtbGO8mKtCq51ZcAHyvy69JpFl3+vXqWzJrihZgazMSnjxSvMgUqEBwt2mqENW6d5R10mJQxdpkT7pNU6PStqMrvq2zm4r7nFxT+uSonCuqkzz6txLM2+YqoVw6UwV/oWT/f2Ead4JJP6m6NpWl3Wj1O+/8RsVuNn0EocNI87QRRyqzPR2haW8lgF5BltG9R0MVHynsGozeqqytonWZBIfCYQXgjCmEg6Z2KnXQfLMXBIALjarNYhC7S+UoF01fA8vIO/GYSukzxwhM9aDXg2pS0pXLKu969EfFg2M+TVXADSouAAajciF4BRWPHZhH/kUtAE0DF8+S9znk32oy2YlIlUrp4lGxNtkTF0+462/1W1WGLneoWbmrX7X9fWo8fDdGdYbD31apj43aw4B/Kb/5hlkqmO4dbHv7orQZbvHTVwZnN5WVdmCBmluQcVGVI6gsEfeo1/ZjrbOxWt+oruO4PRD5usoQatKjdrkAhSrHYQXAZNBY4DxS5e9v/1K5cGJ3qVz4wr5bn2aWJ1a1HWV7Z+b1DyxUmTeFg8Q9H1C1Zh7YZJ15URjzpCfNYJ1rX176P1l8Eg9YLIKKTqap5wPD/qvel5Xh42h0uyt/jsPbyh9emBNr1Mzc2F0wdraa1Wum76NqRJ+VUj73T1XS5U4143jDe8pCNT8AvTK0uUlNUur1kPXyVvnxhM/6q8qYTSNU+QfBoXG8eQD5OBthlR7B0w1aqLIAZoJ7WI98NE25BWK2qYwIW3g3Ub7jvXPV58I+XScXS2W/kjC7aCqaAWSmpGJYvi1Vbr1HQMX3GTbBuhxzXUHTVKmLiydg4QyV7ZJ2zlK/3bcl3Lmo+FwNo0mJwooXi0+kq24COqhc/LM7lCVyNRidbE9SathWTbJzdlcpu36VcDEJ1xyOKwAGjcs5BngoUhVFSz6tMnFaDCy+cq8HlIVQz8f2zoxOapR98YTqIMzlBcqLd74FUFH3T1n0e1wVIqvMlHJNK7m8g6NjNKkyBF9cDwseUMt8W6og6eDnS3aJNWwDt/5s+7vqZugrKmhvfuBIVaNpMPrD6tm3UGtxWAEwGfJLQbh6q6qepWEr86co9fNTQSuT0WF20VS1ALh6X9OTUOyKWwOYsljVugnqcvW1YaqbZr2tn8csCFWA4wqAEbJKKgZXGRq0UKVwC/v/y4tXoBrBlRRjEOyDV+OKW3OC4EA4rAA4GzQyc8pZwrk8NGqvav9URgA0zfKgC0EQhFqmfMiSAAAgAElEQVSCwwqAyQjZuTq5eTpGQxWksnW+Ha4bdHUpfYIgCLUIh00Ddc4/M5sF4SqD0SSTYgRBcCgcVgBMRjXqL/GpYIIgCHUchxUAswVQ4lPBBEEQ6jgOKwAWC0AEQBAEwRaOKwAFMQBxAQmCINjCYQXAOb+8vVgAgiAItnFcATBIEFgQBKE0HFYAzC6gzKpKAxUEQXAwHFcA8l1AWWIBCIIg2MRhBcDsAqqyiWCCIAgOhuMKgASBBUEQSsVhBcAkQWBBEIRScVwBMMcAxAUkCIJgE4cVAEspCLEABEEQbOGwAmA0aDgZNIkBCIIglIDDCgCAi5NBLABBEIQScGgBcDUZJQYgCIJQAg4vAGIBCIIg2MZhHwkJ4GIySCkIwWHJzs4mJiaGzMxMezelxvD29ubgwYP2bkaNUtI5u7q6EhwcjMlkqvS+HVsAnIxSCkJwWGJiYvD09KR58+ZoWhU89/oaIDU1FU9PT3s3o0axdc66rpOYmEhMTAwhISGV3reDu4AMEgMQHJbMzEx8fX3rTOcvWNA0DV9f36u2/hxbAJyMkgYqODTS+dddquK3d2wBMEkaqCBUJx4eHsWWHT58mIEDBxIeHk7btm255557+OuvvwgPDyc8PBwPDw9at25NeHg4kydPJjIyEk3T+PLLLwv2sWvXLjRN4+233y62/z/++IMDBw5UuK0LFy7k9ddfL3Wd2NhYxo8fX+F92+Lrr7/moYceqpJ9VRcOLQAuTpIGKgg1zcMPP8xjjz3G7t27OXjwIDNmzGDYsGHs3r2b3bt3061bN3744Qd2797Nt99+C0BYWBi//PJLwT5+/vlnOnXqZHP/pQlATk5Oie0aNWoUTz/9dKltDwwM5Ndffy3rFB0GhxYAsQAEoeaJi4sjODi44HNYWFiZ2zRt2pTMzEzOnTuHrussW7aMG2+8sdh6W7ZsYeHChcycOZPw8HCOHz/OwIEDefbZZxkwYADvv/8+ixYtIiIigs6dO3P99ddz7tw5wHpEPmXKFB5++GF69+5NixYtCjr9U6dO0aFDh4L1//nPf3LDDTcQGhrKU089VdCOL7/8klatWjFw4ECmT59eoZH+Tz/9RFhYGB06dOBf//oXALm5uUyZMoUOHToQFhbGu+++C8AHH3xA9+7d6dixI5MmTSr3McqLQ2cBqXkAYgEIjs9/Fu3nQOylKt1nu0Av/u+m9hXe7rHHHmPw4MH07t2boUOHMnXqVHx8fMrcbvz48cybN4/OnTvTpUsXXFxciq0TERHBqFGjGDlypJWrJjk5mTVr1gCQlJTE5s2b0TSNL774gjfffJN33nmn2L7i4uJYv349hw4dYtSoUTZdP7t372bXrl24uLjQunVrZsyYgdFo5OWXX2bnzp14enoyePDgEq2VosTGxvKvf/2LHTt2UL9+fYYOHcoff/xBkyZNOHv2LPv27Ss4H4DXX3+dvXv34ufnV7CsKnFoC0CVghABEISaZOrUqRw8eJAJEyYQGRlJz549ycrKKnO7m2++mXnz5vHTTz9xyy23VOiYEydOLHgfExPDsGHDCAsL46233mL//v02txkzZgwGg4F27doVWAlFGTJkCN7e3ri6utKuXTtOnz7N1q1bGTBgAA0aNMBkMjFhwoRyt3Pbtm0MHDgQf39/nJycuO2221i7di0tWrTgxIkTzJgxg2XLluHl5QVAx44dufvuu/n+++9xcqr68brDWwBZOeICEhyfyozUq5PAwECmTZvGtGnT6NChA/v27aNr166lbhMQEIDJZGL58uW8//77bNy4sdzHc3d3L3g/Y8YMHn/8cUaNGkVkZCQvvviizW0KWxi6rpe5jtFoJCcnp8R1y0NJ29avX589e/bw119/8dFHHzF37ly++uorFi9ezLJly1ixYgUvv/wy+/fvr1IhcGwLIF8AruYHEwShYixbtozs7GwA4uPjSUxMJCgoqFzbvvTSS7zxxhsYjcYS1/H09CQ1NbXE71NSUgqO980331Sg5eWjR48erFmzhqSkJHJycvjtt9/KvW1ERARr1qzhwoUL5Obm8tNPPzFgwAAuXLhAXl4e48aNK3Av5eXlER0dTf/+/XnzzTdJTk4mLS2tSs/FwS0ApW9ZOXm4mkq+oARBqBzp6elWAd/HH3+cmJgYHnnkEVxdXQF46623CAgIKNf+evfuXeY6kyZNYvr06XzwwQc2M3ZefPFFJkyYQFBQED179uTkyZPlPJvyERQUxLPPPktERASBgYG0a9cOb29vm+t+/fXX/PHHHwWfN2/ezGuvvcagQYPQdZ3hw4czevRo9uzZw9SpU8nLUx6L1157jdzcXG6//XaSkpLQNI3HHnusXLGUiqDV5tFxt27d9O3bt1dq28jISI47NePlPw+w+9//wMfNuYpbV/uIjIxk4MCB9m5GjVMXzzsyMpJGjRrRtm1bezelRqktpSDS0tLw8PAgJyeHsWPHMm3aNMaOHVstxyrtnA8ePFjsGtA0bYeu693Ks2+HdgEVtgAEQRCqihdffJHw8HA6dOhASEgIY8aMsXeTKoVju4CclNtHMoEEQahKbM1QvhZxcAvALABiAQiCIBTFoQXAxUmdnlgAgiAIxakxAdA0bYymaZ9rmrZA07ShNXFMswUgMQBBEITilEsANE37StO085qm7Suy/AZN0w5rmnZM07RSqyzpuv6HruvTgSnAxNLWrSrMQWCxAARBEIpTXgvga+CGwgs0TTMCHwE3Au2AWzRNa6dpWpimaX8W+WtYaNPn87erdiwxABEAQagOrqVy0KBq+yxZsqTgc3lKRJeXKVOmXHOVRMslALqurwUuFlncAzim6/oJXdevAD8Do3Vdj9J1fWSRv/Oa4g1gqa7rO6v2NGxTEAMQF5Ag1Bj2LAddFkUFoDwloh2Zq0kDDQKiC32OASJKWX8GcD3grWlaS13XP7W1kqZp9wD3ADRq1IjIyMhKNS4tLY3dO7YBsCdqP15JRyq1n2uJtLS0Sv+/rmXq4nmnpaXh7e1dakmEmqJoG86ePUv9+vULljdv3txqndzcXC5fvlywLD09ncDAQFJTUzl+/Dj+/v4sWbKEoUOHkpWVZbXtxo0bWbBgAZGRkbz00kt89913ADzxxBMkJiZSr149Zs2aRatWrfj99995/fXXMRqNeHl5sXDhQl544QUyMjJYu3Ytjz/+OJmZmezcuZN33nmH++67D09PT3bt2sX58+d56aWXGDNmDHl5eTzxxBNs2LCBZs2akZeXxx133FEs9z87O5uMjAyr9uq6zgsvvMDy5cvRNI2ZM2cybtw44uPjmTJlCqmpqeTk5PDuu+8SERHBgw8+WGD93H777Tz00EPk5uaW+DtnZmZe1bV/NQJg63lkJU4r1nX9A+CDsnaq6/psYDaomcCVneEZGRlJj649Ye1KmrdsxcCezSq1n2uJujgjFurmeUdGRuLq6mqZIbr0aYiPqtqDBITBjWW7R4rOUn3iiSe46aabSiwHbTQacXd3L9jOzc0NJycnJk6cyNKlS+ncuTPdunXD09MTFxcXq/337t2b0aNHW5WDHjJkCJ9++imhoaFs2bKFmTNnsmrVKt566y2WL19OUFAQycnJ+Pj48PLLL7N9+3Y+/PBDQJVqcHZ2xtPTE5PJRGJiIps2bSooEX3HHXfw66+/Ehsby/79+zl//nyBW6voeZtMJurVq2e1/LfffuPAgQNERUVx4cIFunfvzrBhw1i4cCHDhw/nueeeIzc3l/T0dI4cOcL58+cLrJvk5OSCukclzQR2dXWlc+fOZf5GJXE1WUAxQJNCn4OB2KvYX5VTkAUkMQBBqDFqshx0WloaGzduZMKECYSHh3PvvfcSFxcHQJ8+fZgyZQqff/45ubnl6wNslYhev349EyZMwGAwEBAQwKBBg8q1L/O2t9xyC0ajkUaNGjFgwAC2bdtG9+7dmTNnDi+++CJRUVF4enqWWBK6OrkaC2AbEKppWghwFpgE3FolraoiZB6AUGcox0i9JqmpctB5eXn4+Piwe/fuYt99+umnbNmyhcWLFxMeHm5znaLYKhFdHeWf+/fvz9q1a1m8eDF33HEHM2fOZPLkyTZLQlcn5U0D/QnYBLTWNC1G07S7dF3PAR4C/gIOAnN1Xbf95AU74Ww0oGkyD0AQapKaLAft5eVFSEgI8+bNA1SHu2fPHgCOHz9OREQEL730En5+fkRHR5dZStoWffv25bfffiMvL49z585VyOfev39/fvnlF3Jzc0lISGDt2rX06NGD06dP07BhQ6ZPn85dd93Fzp07bZaErm7KZQHoum7THtN1fQmwxNZ3tQFN03B1ksdCCkJ1URvKQf/www/cf//9vPLKK2RnZzNp0iQ6derEzJkzOXr0KLquM2TIEDp16kTTpk15/fXXCQ8P55lnnilXm8aNG8fKlSvp0KEDrVq1IiIiosTyz/feey+PPvooAE2aNGHjxo1s2rSJTp06oWkab775JgEBAXzzzTe89dZbmEwmPDw8+Pbbbzl79myxktDVjq7rtfava9euemVZvXq1ruu6Hv6fv/Tnf4+q9H6uJcznXNeoi+e9evVq/cCBA/ZuRo1z6dIluxw3NTVV13Vdv3Dhgt6iRQs9Li6uxo5d2jnbugaA7Xo5+1iHrgYKKhCclpVj72YIgnANM3LkSJKTk7ly5QovvPBCuS2a2o7DC0CnYB/WH7tAbp6O0WArc1UQBKF0HHWeiUNXAwUYFR5IQmoWm08k2rspgiAItYpaKQCapt2kadrslJSUq97X4DYN8XBxYuHuWjVFQRCqBL0WP9JVqF6q4revlQKg6/oiXdfvKSnSXhFcTUaGtmvEkn1xZOVINpDgOLi6upKYmCgiUAfRdZ3ExMSCTKvK4vAxAFBuoPm7zrLmcAJD2ztG8EYQgoODiYmJISEhwd5NqTEyMzOvutO71ijpnF1dXa1ScCtDnRCAPi39aODuzMI9sVYCoOs6miaBYeHaxGQyERISYu9m1CiRkZFXVfvmWqQ6z7lWuoCqGpPRwPCwAFYcPMflrBy2n7rI8PfXcfuXW8jJlVnCgiDUTeqEAACM6hREZnYed3y5hfGfbiIuJYMNxxL5dM1xezdNEATBLtQZAejWrD5BPvXYE5PCvf1bsP5fgxnZsTHvrTjK3phkezdPEAShxqkTMQAAg0Hju7t6ANDCXz3G7tUxYWw/lcSjv+xm8Yx+1HMuuQCVIAiCo1FnLABQHb+58wfwdjPxzs2dOJFwmdeXHrRjywRBEGqeOiUAtujT0o8pvZvz7ebT7IkWV5AgCHWHOi8AAE8MbYWfhwv/XrCPvDyZVCMIQt2gVgpAVZaCKA+eriaeG96WPTEp/LI9uuwNBEEQHIBaKQBVWQqivIwOD6RHSAPeWHaIpMtXauy4giAI9qJWCoA90DSNl0a3JzUzh9eXHrJ3cwRBEKodEYBCtAnw4u6+IfyyPZr3Vxy1d3MEQRCqlTozD6C8PHVDGxIvX+HdFUfI03UevT5U6gUJguCQiAAUwWjQeHNcRwwavL9SPVD68aGt7d0sQRCEKkcEwAYGg8br/+yIQdP4YNUxXJ2NPDCwpb2bJQiCUKWIAJSAwaDx37FhZGTn8uayw3i5mri9ZzN7N0sQBKHKEAEoBYNB4+0JnbiclcMLC/bh6erE6PAgezdLEAShSpAsoDIwGQ18eGsXeob48vjcPaw4cM7eTRIEQagSRADKgavJyOd3dqNDkDcP/LiTjccu2LtJgiAIV40IQDnxcHHi6yndCfF15+5vt7PrTJK9myQIgnBV1MoYgKZpNwE3tWxZuzJv6rs7891dPRj/6SZu+XwzzX3d8fd0obG3K5N7NadDUM2VrhAEQbhaaqUFYI9aQOWloZcrP06PYHzXYILru5GamcPSqHhGzlrPoz/vIvpiur2bKAiCUC5qpQVQ2wmu78YrY8IKPqdkZPPpmuN8tf4kS6LieefmTtzUKdCOLRQEQSibWmkBXGt41zPxrxvaEDlzIJ2aeKtHTO6Ns3ezBEEQSkUEoApp7F2POVN70LmJDw//vItl+0QEBEGovYgAVDEeLk58Pa0HnYK9eejHXWw5kWjvJgmCINhEBKAa8HBx4ptpPWjk5crLiw/IYyYFQaiViABUE56uJp4c1op9Zy+xaG+svZsjCIJQDBGAamR0pyDaNfbi7b8Pk5WTa+/mCIIgWCECUI0YDBpP39iG6IsZ/LD5TJnr5+Xp5OTm1UDLBEEQRACqnf6t/Onb0o9Zq44Sm5yBrtuOB8QmZzDonUheXLS/hlsoCEJdRQSgBnj6xjYkZ2TT+/VVhL+0nHGfbOSnrWcKgsOJaVnc8eUWTiem8+feOHIlaCwIQg0gAlADdAjyZtFDffm/m9oxsmNjMrNzeWZ+FBM+28SO0xeZMmcbMUkZ3NGzGcnp2eyPTbF3kwVBqANIKYgaokOQd0GxOF3X+W3nWf675CDjPtmEk0Fj9uSuhAX58N3m06w7eoGOwT52brEgCI5OrRSA2loNtKrQNI3xXYMZ0qYhH0ceo3vzBgxu0wiAdo29WHskgQcHWc7dHDfQNM0u7RUEwTGplS6g2lwNtCqp7+7McyPaMbR9QMGy/q382XkmibSsnIJlz/4exS2fb5YMIUEQqpRaKQB1mf6hfmTn6gUlJA7GXeKnrdFsPnGR7zeftnPrBEFwJEQAahldm9fH1WRg3VH12MlZq47i6eJEj5AGvPP3Ec6nZtq5hYIgOAoiALUMFycjPVv4svZoAofiL7EkKp6pfZrzxriOZOXk8d/FB+3dREEQHAQRgFpIv1B/TiRc5rnf9+Hp4sS0viGE+Llz34AW/LE7lk3HS68wuiQqjn5vrmKDPLxeEIRSEAGohfQP9QNgx+kkpvRpjo+bMwAPDGpJkwb1eOCHHfxn0X52nUkqNrM4ITWLZ3+PIiYpgzu/2sqvO2JqvP2CIFwbiADUQlo29CDAyxUPFyfu6htSsNzVZOTT27vSvXkDfth8hrEfb+TG99dxOvFywTovLtxPelYu8+/vTUSLBjw5bw/vLj8iJakFQShGrZwHUNfRNI0XR7VH0ygY/ZtpH+jN7MnduJSZzbKoeP679CBjP97I55O7seNcDouj4pg5rDWdm9ZnzpQePPt7FO+vPMrOM0m8M6ETDb1c7XRWgiDUNsQCqKXc0CGAYYXmBxTFy9XEzd2b8PsDffBydeKWzzczZ18W7Rp7cU//FgA4Oxl4a3xHXhnTgW2nLjLsvbUs2ye1hgRBUIgFcI0T4ufO/Af6MP3b7ew+k8Sb4ztiMlp0XdM0bu/ZjJ4tfHn0l13c9/1O6pmMtArw5Dp/dy5n5RB/KYsLqVm0auRB31B/+oX6EdrQo1wzj3Vd59tNan7Cnb2bV9dpCoJQDYgAOAAN3J35+Z6eLFoeWVBvqCgtG3ow//4+LI6KJSrmEofiL7Hh2AW8XE0EeLvStIEb+86msPrwAQBu6dGEV8eEYTCULAK6rvO/5UeYteoYmgZdmtYnLNixZ28LgiMhAuAgmIwGGriW7tFzdjIwtnMwYzuXvE5MUjpzNpziy/UnMWgar4zpgKZpXMnJ48+9seg69A31o6GnC2/9dZiPI48zrkswa44k8PyCffx+f+9SRUMQhNqDCIBgRXB9N54f0RZnJwOfRB7HyaDRtrEXs1Yd42xyRsF6QT71OJucwa0RTXlldAcW7DnLY7/sYe72aCb1aGrHMxAEobyIAAjF0DSNp4a1Jic3j8/XnQSgUxMfXh3bAX9PF9YfvcCmE4mM6xrMo0NCMRg0xoQH8dOWaN5Ydohh7QOo7+5cxlEEQbA3IgCCTTRN49nhbQnx8yDA24VBrRsWBIXbB3pz74Driq3/0pj2jPhgPdO+2UaIrzs5eTpNGtTjoUGh1HM22uM0BEEoBREAoUQ0TePWiPK7c9oEePH0DW2Ys+EkF9KyMGoaC/fEsuLAeT66rQstG3oQn5LJJ5HH2HEmiVfHhNGpiTz4RhDsRa0UAEd/IIwjM71/C6bnz0MAWHMkgcd+2c1Ns9YztH0jlkbFk6fr+LiZmPDZJl4Z3YGbuzexY4sFoe5SKyeC1ZUHwtQFBrTyZ8nD/QgL9mbx3jjGdQ1i9ZMDWf7YAHo0b8BTv+3l2d+jyMrJLXEfV3Ly+HTNcXacvliDLRcEx6dWWgCCYxHg7crP03uSmpWDdz1TwfKvp3bnrb8P89maExyMu8Qnt3UlwNu6VIWu6zz7e1RBUbuIkAY8MKgl/UP95BGZgnCV1EoLQHA8DAbNqvMHcDIaeObGtnx0axcOx6cycta6giehmflkzXF+3RHDAwOv44WR7TidmM6dX21l0uzNHIi9VK5j7zubwtFzqVV2LgCZ2bnM2x5t9ehOQbjWEAtAsDsjOjamVSMP7v1uB7d8vpm+of6M7RwIwJvLDjOqUyAzh7XOL2vRlLnbY/jf34cZOWsdt0Y0pZe77dpGSZev8MayQ/y8LRofNxNLHu5HoE+9gu9zcvNIz87Fy9Vkc/uSyMvTeWLuHhZHxfHj1jN8PbVHMXEThGsBsQCEWkFoI08WPNSHBwa25Pj5NB77ZQ+P/bKHLk19eHN8xwJ3j4uTkTt6NiPyyUFM7tWcn7ZG8/rWTJIuXynYl67r/LYjhsHvRPLrjhgm92pGdk4ej/y8i5zcPABSMrKZOHsz3V5ZwXO/R1mV1C6Ld5YfZnFUHKPDA9l3NoVbP9/MxULHF4RrBREAodbg6WriyWGtWffUIObd14tHrw9l9uRuuJqKzyHwdjPx4qj2zJnSnbjLedz2xRaS069wOSuHx+fu4Yl5e7jO34PFD/fjpdEdeHVsGNtOJfH+yqMkpmVxy+zN7I1J5h/tGjFvewyD3o7kkZ93cepC6UIwb3s0H60+zqTuTXhvYjizJ3fj2Pk0bpm9mWPnq9bNJAjVjbiAhFqHwaDRvXkDujdvUOa6/Vv580hnF2btSePWz7eQlZPLiQuXeez6Vjw0uCXG/LpEYzoHseHYBT5cfYzfd53lQloWX9zZnQGt/Dl/KZMv15/k202nWbw3jkk9mvDwkFAaeloHpLedusizv0fRp6UvL+fXSBrUuiFzpnTnnu928I931zKqU2D+ti6cu5TJhbQrdAr2kYlwQq1EBEC45gnzd+KzOzpy77c78Kpn4oe7Iujd0q/Yev8Z3Z5d0cnEp2TyzdQeRLTwBaChlyvPDG/LXf1CmLXyGD9tPcOCXbF8c1cPujStD8D5S5k88MNOgnzq8fFtXa1Kbvdu6ceamQOZve4E3248zYLdsVbHHdclmHdu7lSN/wFBqBwiAIJDMKh1Q5Y92o/6bs4l1iFyc3bit/t7k3Elt1i6KUBDT1deHtOBaX1DmDJnK3d+uZXv746gXaAXD/64k7TMHL67y3bA19fDhWdubMv0fi2Ytz0Gg6bSXzcdT+TnbdFM7tVMZj0LtQ4RAMFhaOHvUeY63vVMZWbshPi589P0nkycvYk7vtxCv1B/FT+YFE6bAK9St/XzcOH+gZY6SUPaNmLlofO8uGg/8+/vbXPuws4zSaSkZ9O/lX+By6oqycnN48etZ2jawI2IEF9xRwkFiAAIgg0CfeopEfhsM4uj4pjapzmjw4MqvB8PFyeeGtaamb/uZcHuWMZ0tt5HSkY2U77ayqXMHJo0qMfkns25uXuTKk0rXbA7ln8v2A+As9FA95D6/HdsGM183avsGMK1iWQBCUIJBNd345d7e/L8iLY8O7xtpfczrkswHYO9eX3pIdKvWE8c+2LdCS5l5vD8iLY09qrHq0sOMuKDdcQWevbC1aDrOl9tOEloQw++mdaDO3s3Y090Cq8tOVRs3YwruTX6vGhdl2dT2xsRAEEoheD6btzdr4VV0LeiGAwa/3dTO+IvZfL60kMFHV9iWhZfrT/JiLDG3N2vBXPv68W8+3qRkp7NbV9s4XxqZsE+dpxOYmlUXIWPvfXkRfbHXmJa3xAGtPLnuRHtmNY3hGX74zkUb5lJfT41k75vrKLbK8t5/Jfd/Lk3lszskuszXS2rDp2j+6sr2Xj8QrUdQygbEQBBqAG6NmvA3X1D+HbTaWavPQHAJ5HHycjO5bF/tCpYr3vzBnw9rTvnLmVy+xdbWH3oPLd/sYVxn2zk/h92Mmvl0RKPkZ2bx8ki8xjmbDiFj5uJMYXcV9P6NMfDxYlZq44BaiT+wh/7SM3KoV+oP6sOn+ehH3fx0I+7qvJfUEBy+hWe+jWKC2lZPPTjriqzdoSKIzEAQaghnh3elvhLmby29BA68O3m04ztHEzLhtbB667NGvDFnd2YOmcbU7/ehp+HM88Nb8uBuEu8s/wIAGGF4rjRF9P5edsZ5m6PISE1i0eGhPLo9aHEJGXw94F47htwnVXg18fNmcm9mvHJmuMcO5/K4fg0/tp/jqdvbMN9A64jN0/nw1XHeHfFEVYePMeQto0Ktv1szXFOXrjMyI6B9LrOt1JB6xcX7ic5/QqzbunMM/OjuO/7Hcy9t5fNCX9C9SICIAg1hMGg8c7NnUhKv8LrSw9hMmo8en2ozXV7X+fH93dHcCjuEuO7NqGes5HcPB0NeGf5EfoEOjEvdicHYy9x4sJlDJpKhXV1NvL+yqOcT83CxcmApmnc0atZsf3f3a8FX288xWtLDrEnJpmwIG/u7hsCgNGgcf/A61i45yz/WXSAPi39cDUZ+WXbGV7Lb/fP26Lx93ThwYHXMaVPSLn/B3/vj+eP3bE8MiSUmzoF4uJk4J7vdvDvBft4Y1zHaqvwGpOUztmkjIK5H4JCBEAQahAXJyOf3t6V+77fQbdmDWjSwK3EdYvOhjYaNN6a0Akno8bc7TE0yUymXWMvxncLZkx4EIE+9dB1nea+bny0+jgAN3UKpLF3vWL7buDuzB09m/HZ2hOYjBrf3x2BU6E4h7OTgZdGd+C2L7bw2ZoT9GzRgOf/2Ee/UIN+BGsAAAuYSURBVD8+vb0ra48kMGfDKV768wB9Q/2trJjd0cmsPHiOBwa2tLI8EtOyeO6PfbRt7MWDg9TDnoa2D2DG4JbMWnUMN2cnnh/R1qod5SEnNw+DpmEowRq5nJXDbV9s4XRiOgNb+/PCyHZcZyNlOC9PJ/5SplXBQEdHBEAQahhPVxM/3N2zUtsaDRpvju/EPxpc5B+DBxX7XtM0Zg5rQ0NPV95bcYR7Cz2drSh392vB77vOMrVPiM35DX1a+jGiY2M+jjzG1xuNNGngxoe3dsHdxYkbwxrTPaQB/d9czXsrjvDhrV0ASL+Sw4M/7ORscgaRhxP4fHI3Arxd2XE6iRk/7iQlPZs5U7rj7GTp5B+7vhXpV3L5cv1JTly4zKxbOpc7DfZwfCpT52ylc7P6fJTfhqL8Z9F+oi+mc1ffEOZui2bYu2u5p38LnhzaukA0dF1n5q97+X1XDPPu60XXZmWXIXEEJAgsCNcgpjJ873f2bs7OF/5Bh6CSn6rn7+nCxqcHW01cK8rzI9piNGjk6fDlnd2tOmY/Dxem9Qnhz71xBc9meH/lUc4mZ/DIkFBOJKRx04freW3pQSZ+tgmjUePX+3sVa5PBoPHCyHa8MS6MjccuMPbjDew4nVTm/2DbqYtM+HQjF9KusHhvHCsOnCu2zpKoOOZuj+GBgS15YWQ7Vs8cyJjOQXwceZxn5keRl5/2OnvtCX7bGYOTwcALf+wvqBrr6IgACIKDUh5/elnulsbe9fhxek9+u78XIX7FJ45N798CL1cn/rf8MAfjLvHFupNM7NaEx/7RivkP9MHFycBna07wj3aN+HNGPzoGl1wOY2L3pnx/dwSXMnIY98lGpn29jX1nU2yu+/f+eG7/Ygt+Hi789Vh/Qht68OKi/WRcsaSuxqVk8Mz8KDoFe/NIfqzFz8OFt8Z35OHBLfllezTPzI9i+YFzvL7sECM6NubdieEciLvE95tPl/m/cwRqpQtIHgovCLWH8FJqGHnXM3HvgOt466/DHD2fhk89E88MbwNA6wBPFj3Ul71nU8r9CM+eLXxZM3Mg32w6xWdrTjBy1nomdW/CsyPa4uVqIk/X+d/yI8xadZSOQd58NaU7vh4uvDKmAxNnb+bD1UeZOawNO04nMfPXPWTn5vHepM5W8zg0TStIvf1g1THm7oimQ6A3b4/vhKvJQL9QP975+wgjOgbi7+lSsN2ZxHT+2h/P1lMXeXhwKGHB1/4zy2ulAOi6vghY1K1bt+n2bosgCKUzpXdzvlp/ktOJ6bw7sRM+bpZifPXdnRnQyr9C+3N3ceKBgS25vWczPlp1jM/XnWDNkQSeG9GWT7Znsj/xKP/sEsSrY8IKgswRLXz5Z5cgZq89QUJqFvN2xBDoXY8vJnezabmYRcBkNLA4Ko7PJ3cr2Nd/RrVn2Htr+feCfQxs7c/emBR2nE7iULx63kM9k5HNJxL5dloPOudXi72Sk8fyA+cIrl+PjsHe18zzqmulAAiCcO3g7uLEG+M6su30RasJZ1eLl6uJZ4a35cawxsyct4eHftyFkwFe+2cYk7o3KdbJPju8LSsOnGPudvUUuKduaIOHS8ldnKZpzBgSyowh1qm4Lfw9uKd/Cz5afZyl++LxdHUiLMib50e0ZVj7AAwGjVs/38wdX25lztTuJKRm8eayQ5xKTM/f3p2x4UFM7tUcb7fa/ahQEQBBEK6a69s14vp2jcpesRKEN/Fh0Yy+/Lz1DFriCW7p0dTmen4eLvw4vSe5efpVl95+9PpWdGvWgOZ+7jRr4FYsxfSXe3px6+ebufmzTeg6tGrkwew7upKUfoX5O8/yzvIjrD92gZ+m97SZnqrrOpFHEth28iIXL18h8fIV2gd68ej1rYqtW52IAAiCUOtxNRmZ0ieEyMjSg7OlZT1VBJPRwKA2DUv8PsDblZ/v6cl//jxA35Z+TOgaXBBQn9i9KXO3RfPUb3uZs/EUd/W1nii372wKry4+yKYTiTgZNOq7O+NsNLD8wDnCm/gwsHXJx61qRAAEQRAqQUMv1xLnHkzoFsxf++N5c9khBrRSE+WSLl/htaUHmbcjBp96Jv4zqj23RjTFZDSQlZPLje+v44UF+/j70QE19swGSQMVBEGoYjRN47VxYbg5G3li7m5+3RHDkP+tYf7Os9zdN4TImYO4s3fzguwkFycj/x0bRvTFDN5beaTG2ikCIAiCUA009HTllTFh7IlJ4cl5e2ju68afD/fluRHtbM507tnCl5u7BfPFupMcjLtkY49Vj7iABEEQqokRHRsTk9QG73ombu7WpMR6RWaeHd6WlQfP88z8KH67v3e1PCK0MCIAgiAI1ci9A0outVEUHzdn/n1TO7aevMiVnLxqjwWIAAiCINQiRocHVer505VBYgCCIAh1FBEAQRCEOooIgCAIQh1FBEAQBKGOIgIgCIJQRxEBEARBqKOIAAiCINRRRAAEQRDqKJqu6/ZuQ4lompYAVPbhnH7AhSpszrVAXTxnqJvnXRfPGermeVf0nJvpul6ux7DVagG4GjRN267rejd7t6MmqYvnDHXzvOviOUPdPO/qPGdxAQmCINRRRAAEQRDqKI4sALPt3QA7UBfPGermedfFc4a6ed7Vds4OGwMQBEEQSseRLQBBEAShFBxOADRNu0HTtMOaph3TNO1pe7enutA0rYmmaas1TTuoadp+TdMeyV/eQNO05ZqmHc1/rW/vtlY1mqYZNU3bpWnan/mfQzRN25J/zr9omuZs7zZWNZqm+Wia9qumaYfyf/Nejv5ba5r2WP61vU/TtJ80TXN1xN9a07SvNE07r2navkLLbP62muKD/P5tr6Zptp9KX04cSgA0TTMCHwE3Au2AWzRNa2ffVlUbOcATuq63BXoCD+af69PASl3XQ4GV+Z8djUeAg4U+vwG8m3/OScBddmlV9fI+sEzX9TZAJ9T5O+xvrWlaEPAw0E3X9Q6AEZiEY/7WXwM3FFlW0m97IxCa/3cP8MnVHNihBADoARzTdf2ErutXgJ+B0XZuU7Wg63qcrus789+nojqEINT5fpO/2jfAGPu0sHrQNC0YGAF8kf9ZAwYDv+av4ojn7AX0B74E0HX9iq7ryTj4b416YmE9TdOcADcgDgf8rXVdXwtcLLK4pN92NPCtrtgM+Gia1riyx3Y0AQgCogt9jslf5tBomtYc6AxsARrpuh4HSiSAhvZrWbXwHvAUkJf/2RdI1nU9J/+zI/7mLYAEYE6+6+sLTdPcceDfWtf1s8DbwBlUx58C7MDxf2szJf22VdrHOZoAaDaWOXSak6ZpHsBvwKO6rl+yd3uqE03TRgLndV3fUXixjVUd7Td3AroAn+i63hm4jAO5e2yR7/MeDYQAgYA7yv1RFEf7rcuiSq93RxOAGKBJoc/BQKyd2lLtaJpmQnX+P+i6Pj9/8TmzSZj/et5e7asG+gCjNE07hXLvDUZZBD75bgJwzN88BojRdX1L/udfUYLgyL/19cBJXdcTdF3PBuYDvXH839pMSb9tlfZxjiYA24DQ/EwBZ1TQaKGd21Qt5Pu+/799+3WpGIoCOP5demDTbBCL1fiCQbC9bBN8wb9CTP4DNqPJYLDow6p2MYiKij+SxWo2HMO9D155IA/m4N7vB8bGNtg9nLGznW1HwHNEHExsGgHDvDwEzv97bG2JiN2IWIyIJVJuryJiC7gGNvNuRcUMEBFfwGfTNCt51QbwRMG5JrV++k3TzOVzfRxz0bmeMC23I2A7fw3UB77HraKZRERREzAAXoEPYK/r8bQY5xrp0e8euMvTgNQTvwTe8nyh67G2FP86cJGXl4Eb4B04BXpdj6+FeFeB25zvM2C+9FwD+8AL8AgcA70Scw2ckN5z/JDu8Hem5ZbUAjrM17cH0ldSMx/bP4ElqVKltYAkSX9kAZCkSlkAJKlSFgBJqpQFQJIqZQGQpEpZACSpUhYASarUL8tGTX6qQzx3AAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from matplotlib import pyplot\n",
    "plt.plot(history.history['loss'], label='LSTM training Loss')\n",
    "plt.plot(history.history['val_loss'], label='LSTM testing Loss')\n",
    "plt.yscale(\"log\")\n",
    "plt.grid()\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Get predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rolling_lstm_prediction(model,x_test,y_test):\n",
    "    predictions = list()\n",
    "    for i in range(len(x_test)):\n",
    "        # make one-step forecast\n",
    "        X = x_test[i,:] # select single line\n",
    "        X = X.reshape(1, 1, len(X))  # reshape for LSTM\n",
    "        y = y_test[i]\n",
    "        yhat = lstm_model.predict(X, batch_size=1)[0] # reshape prediction to be just a 1D array\n",
    "        # store forecast\n",
    "        predictions.append(yhat[0])\n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate predictions\n",
    "yhat = rolling_lstm_prediction(lstm_model,x_test,y_test);\n",
    "raw_lstm_predictions = pd.DataFrame(yhat,columns = ['lstm_prediction'], index = test_index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Convert data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test = pd.DataFrame(x_test, columns = x_columns, index = test_index)\n",
    "y_test = pd.DataFrame(y_test, columns = ['google_45d'], index = test_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Invert scaling for prediction data\n",
    "unscaled_lstm_predictions = pd.concat([x_test, raw_lstm_predictions], axis=1)\n",
    "unscaled_lstm_predictions = pd.DataFrame(scaler.inverse_transform(unscaled_lstm_predictions), columns=unscaled_lstm_predictions.columns, index=unscaled_lstm_predictions.index)\n",
    "\n",
    "# Invert scaling for actual data\n",
    "test_data_unscaled = pd.concat([x_test, y_test], axis=1)\n",
    "test_data_unscaled = pd.DataFrame(scaler.inverse_transform(test_data_unscaled), columns=test_data_unscaled.columns, index=test_data_unscaled.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "backup['unscaled_lstm_predictions'] = unscaled_lstm_predictions\n",
    "backup['test_data_unscaled'] = test_data_unscaled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of feedforward model is 129.24 \n",
      "Accuracy of LSTM model is 80.0\n"
     ]
    }
   ],
   "source": [
    "lstm_accuracy=np.sqrt(metrics.mean_squared_error(unscaled_data.google_45d, unscaled_lstm_predictions.lstm_prediction))\n",
    "print('Accuracy of feedforward model is {} \\nAccuracy of LSTM model is {}'.format(round(seq_accuracy,2), round(lstm_accuracy)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/matthias/anaconda3/lib/python3.7/site-packages/pandas/plotting/_converter.py:129: FutureWarning: Using an implicitly registered datetime converter for a matplotlib plotting method. The converter was registered by pandas on import. Future versions of pandas will require you to explicitly register matplotlib converters.\n",
      "\n",
      "To register the converters:\n",
      "\t>>> from pandas.plotting import register_matplotlib_converters\n",
      "\t>>> register_matplotlib_converters()\n",
      "  warnings.warn(msg, FutureWarning)\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot prediction vs actual\n",
    "plt.plot(test_data_unscaled['google_45d'], label='Data')\n",
    "plt.plot(unscaled_seq_predictions['seq_prediction'], label='Seq Predicted')\n",
    "plt.plot(unscaled_lstm_predictions['lstm_prediction'], label='LSTM Predicted')\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('USD')\n",
    "plt.grid()\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run time for feedforward model is 411.3968336582184 seconds \n",
      "Run time for LSTM model is 167.52475452423096 seconds\n"
     ]
    }
   ],
   "source": [
    "print('Run time for feedforward model is {} seconds \\nRun time for LSTM model is {} seconds'.format(time_seq,time_lstm))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "backup.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
