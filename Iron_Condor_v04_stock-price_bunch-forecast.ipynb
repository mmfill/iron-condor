{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Notebook for training for stock price with bunch forecast\n",
    "Forecast for last 10% of data. Forecast is done in one go.\n",
    "- Sequential model\n",
    "- LSTM model\n",
    "- not stationary\n",
    "- bunch forecast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from matplotlib import cm\n",
    "import numpy as np\n",
    "import scipy.stats as si\n",
    "from IPython.display import Image\n",
    "import datetime\n",
    "import time\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dense, Activation, Dropout\n",
    "from keras.layers import LSTM\n",
    "from sklearn import preprocessing, metrics\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "backup = pd.HDFStore('backup_v04.h5')\n",
    "#df = backup['df']\n",
    "#unscaled_seq_predictions = backup['unscaled_seq_predictions']\n",
    "#unscaled_data = backup['unscaled_data']\n",
    "#unscaled_lstm_predictions = backup['unscaled_lstm_predictions']\n",
    "#test_data_unscaled = backup['test_data_unscaled']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Iron Condor\n",
    "The iron condor is a investment strategy using four option with the same strike date. More precise, the iron condor uses two vertical spreads, one put spread and a call spread. The put spread consists at of a short put option (P_s) with a strike price below the actual stock price and a long put option (P_l) with a strike price below the short put option P_s. The call spread consists of a short call option (C_s) above the actual stock price and a long call option (C_l) above the short call option C_s. All options have the same strike date and the spread on both legs can vary but is chosen mostly the same.\n",
    "![title](Condor_strategy.png)\n",
    "\"A trader who buys an iron condor speculates that the spot price of the underlying instrument will be between the short strikes when the options expire where the position is the most profitable. Thus, the iron condor is an options strategy considered when the trader has a neutral outlook for the market. Buying iron condors are popular with traders who seek regular income from their trading capital. An iron condor buyer will attempt to construct the trade so that the short strikes are close enough that the position will earn a desirable net credit, but wide enough apart so that it is likely that the spot price of the underlying will remain between the short strikes for the duration of the options contract. The trader would typically play iron condors every month (if possible) thus generating monthly income with the strategy.\" [https://en.wikipedia.org/wiki/Iron_condor]\n",
    "\n",
    "### Goal of the project\n",
    "The goal of the project is to find a strategy to maximise the profit with an iron condor. The ideal short and long spreads should be found at any time based on data of the underlying stock and of the indices S&P500 and Nasdaq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data(var_name,name):\n",
    "    # reads in csv into DataFrame, keeps Adj Close and Volume and calculates rolling averages and \n",
    "    # rolling standard deviation of Adj Close for 4, 9 and 18 days\n",
    "    # var_name: path to csv file\n",
    "    # name: name of the column df\n",
    "    df = pd.read_csv('data/'+var_name+'.csv',index_col='Date', parse_dates=True)\n",
    "    df.rename(columns={\"Adj Close\": name, \"Volume\": name+'_volume', \"Open\": name+'_open'}, inplace = True)\n",
    "    df.rename(columns={\"High\": name+'_high', \"Low\": name+'_low', \"Close\": name+'_close'}, inplace = True)\n",
    "    #df.drop(['Open','High','Low','Close'],axis=1, inplace = True)\n",
    "    #if name == 'google':\n",
    "        #df['google_45d'] = df['google'].shift(-32) # stock price in 45 days (approx. 32 trading days)\n",
    "    df[name+'_ra_04'] = df[name].rolling('4d').mean()\n",
    "    df[name+'_std_04'] = df[name].rolling('4d').std()\n",
    "    df[name+'_ra_09'] = df[name].rolling('9d').mean()\n",
    "    df[name+'_std_09'] = df[name].rolling('9d').std()\n",
    "    df[name+'_ra_18'] = df[name].rolling('18d').mean()\n",
    "    df[name+'_std_18'] = df[name].rolling('18d').std()\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigma_fct(df, name, days = 10):\n",
    "    # For Black-Scholes I need the volatility of return of the underlying assets in the last days\n",
    "    # based on https://www.wikihow.com/Calculate-Historical-Stock-Volatility\n",
    "    # First the ln of daily returns is calculated for each element of names\n",
    "    for name in names:\n",
    "        a=pd.DataFrame()\n",
    "        a['ln_daily_return'] = np.log(df[name] / df[name].shift(1))\n",
    "        # next the rolling average (mean) over certain days is calculated\n",
    "        a['rol_avg'] =  a['ln_daily_return'].rolling(str(days)+'d').mean()\n",
    "        # next deviation from the mean is calculated\n",
    "        a['dev_mean'] = a['ln_daily_return'] - a['rol_avg']\n",
    "        # next variance is calculated for certain days\n",
    "        a['dev_mean_sq'] = a['dev_mean']**2\n",
    "        a['variance'] = a['dev_mean_sq'].rolling(str(days)+'d').sum()/(a['dev_mean_sq'].rolling(str(days)+'d').count()-1)\n",
    "        # volatility is square root of variance\n",
    "        # do you need annual volatility?\n",
    "        df[name + '_daily_vol'] = np.sqrt(a['variance'])*np.sqrt(252) # 252 trading days in a year\n",
    "        #df.drop(['ln_daily_return'])#,'rol_avg','dev_mean','dev_mean_sq','variance'])\n",
    "        #return df[name + '_daily_vol']\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stationary_timeseries(df, damned_list = ['volume','std','short','daily']):\n",
    "    # makes columns in df that are not in damned list stationary. Means it calculates the \n",
    "    # difference row by row\n",
    "    columns = df.columns\n",
    "    df_sta = pd.DataFrame()\n",
    "    for column in columns:\n",
    "        if any(elem in column for elem in damned_list):\n",
    "            df_sta[column] = df[column]\n",
    "        else:\n",
    "            df_sta[column+'_sta'] = df[column] - df[column].shift(1)\n",
    "    df_sta.dropna(inplace = True)  # to make sure we have all the data on all the days\n",
    "    return df_sta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_df(var_name_list,name_list):\n",
    "    # takes list paths of csv files and names and passes it on to get_data.\n",
    "    # Gets df from get_data and merges them on 'Date'\n",
    "    # Calls sigma_fct to get sigma of daily returns \n",
    "    # var_name_list: list of paths to csv files\n",
    "    # name: list of names the columns of df\n",
    "    i=0\n",
    "    for var_name in var_name_list:\n",
    "        if i == 0:\n",
    "            df = get_data(var_name, name_list[i])\n",
    "        else:\n",
    "            df = pd.merge(df,get_data(var_name, name_list[i]),on='Date')\n",
    "        i += 1\n",
    "    df.dropna(inplace = True)  # to make sure we have all the data on all the days\n",
    "    \n",
    "    df = sigma_fct(df,names)\n",
    "    df=df[23:] # drop first entries to get clean results\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_target_data(df, name):\n",
    "    # function to create the target data\n",
    "    # name: name of the column the target data is created of\n",
    "    df[name+'_45d'] = df[name].shift(-32) # stock price in 45 days (approx. 32 trading days)\n",
    "    df.dropna(inplace = True)  # to make sure we have all the data on all the days\n",
    "    # df['short_call'] shows the perfect short call. If you would have shorted a call option with a strike price \n",
    "    # exactly the same as the stock price 45 days later, you made maximum profit. Therefore this is the target \n",
    "    # value for the short call option. The minimum is 0, so we never short a call option with a strike price\n",
    "    # below the stock price right now. The same for the short put option\n",
    "    #df['short_call'] = [x/y-1 if x/y > 1 else 0 for x,y in df[[name+'_45d', name]].values]\n",
    "    #df['short_put'] = [1-x/y if x/y < 1 else 0 for x,y in df[[name+'_45d', name]].values]\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>google_open</th>\n",
       "      <th>google_high</th>\n",
       "      <th>google_low</th>\n",
       "      <th>google_close</th>\n",
       "      <th>google</th>\n",
       "      <th>google_volume</th>\n",
       "      <th>google_ra_04</th>\n",
       "      <th>google_std_04</th>\n",
       "      <th>google_ra_09</th>\n",
       "      <th>google_std_09</th>\n",
       "      <th>...</th>\n",
       "      <th>nasdaq_ra_04</th>\n",
       "      <th>nasdaq_std_04</th>\n",
       "      <th>nasdaq_ra_09</th>\n",
       "      <th>nasdaq_std_09</th>\n",
       "      <th>nasdaq_ra_18</th>\n",
       "      <th>nasdaq_std_18</th>\n",
       "      <th>google_daily_vol</th>\n",
       "      <th>s&amp;p_daily_vol</th>\n",
       "      <th>nasdaq_daily_vol</th>\n",
       "      <th>google_45d</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Date</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2008-11-21</th>\n",
       "      <td>130.764862</td>\n",
       "      <td>134.182053</td>\n",
       "      <td>123.188263</td>\n",
       "      <td>130.725006</td>\n",
       "      <td>130.725006</td>\n",
       "      <td>20565700</td>\n",
       "      <td>136.935478</td>\n",
       "      <td>8.750923</td>\n",
       "      <td>143.875715</td>\n",
       "      <td>10.798013</td>\n",
       "      <td>...</td>\n",
       "      <td>1392.540009</td>\n",
       "      <td>68.742234</td>\n",
       "      <td>1452.251430</td>\n",
       "      <td>95.188063</td>\n",
       "      <td>1541.462141</td>\n",
       "      <td>127.530682</td>\n",
       "      <td>0.716470</td>\n",
       "      <td>0.802275</td>\n",
       "      <td>0.738519</td>\n",
       "      <td>156.946732</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2008-11-24</th>\n",
       "      <td>134.127258</td>\n",
       "      <td>134.470963</td>\n",
       "      <td>124.040070</td>\n",
       "      <td>128.239334</td>\n",
       "      <td>128.239334</td>\n",
       "      <td>20184700</td>\n",
       "      <td>129.482170</td>\n",
       "      <td>1.757636</td>\n",
       "      <td>137.580147</td>\n",
       "      <td>9.599201</td>\n",
       "      <td>...</td>\n",
       "      <td>1428.184998</td>\n",
       "      <td>61.992083</td>\n",
       "      <td>1420.705017</td>\n",
       "      <td>68.914436</td>\n",
       "      <td>1498.502503</td>\n",
       "      <td>101.345711</td>\n",
       "      <td>0.529842</td>\n",
       "      <td>0.864400</td>\n",
       "      <td>0.777819</td>\n",
       "      <td>155.761169</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2008-11-25</th>\n",
       "      <td>133.838348</td>\n",
       "      <td>142.794769</td>\n",
       "      <td>133.160873</td>\n",
       "      <td>140.498383</td>\n",
       "      <td>140.498383</td>\n",
       "      <td>21623100</td>\n",
       "      <td>134.368858</td>\n",
       "      <td>8.668457</td>\n",
       "      <td>137.997038</td>\n",
       "      <td>8.831976</td>\n",
       "      <td>...</td>\n",
       "      <td>1468.375000</td>\n",
       "      <td>5.154837</td>\n",
       "      <td>1426.994298</td>\n",
       "      <td>65.073432</td>\n",
       "      <td>1483.279999</td>\n",
       "      <td>90.035297</td>\n",
       "      <td>0.833833</td>\n",
       "      <td>0.791313</td>\n",
       "      <td>0.710049</td>\n",
       "      <td>156.573120</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2008-11-26</th>\n",
       "      <td>139.616684</td>\n",
       "      <td>147.178345</td>\n",
       "      <td>137.584305</td>\n",
       "      <td>145.499634</td>\n",
       "      <td>145.499634</td>\n",
       "      <td>12760800</td>\n",
       "      <td>138.079117</td>\n",
       "      <td>8.880829</td>\n",
       "      <td>137.425609</td>\n",
       "      <td>8.059325</td>\n",
       "      <td>...</td>\n",
       "      <td>1489.616659</td>\n",
       "      <td>36.971751</td>\n",
       "      <td>1434.144287</td>\n",
       "      <td>74.235572</td>\n",
       "      <td>1487.035382</td>\n",
       "      <td>87.259170</td>\n",
       "      <td>0.812863</td>\n",
       "      <td>0.758392</td>\n",
       "      <td>0.707779</td>\n",
       "      <td>149.923050</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2008-11-28</th>\n",
       "      <td>144.747452</td>\n",
       "      <td>147.671494</td>\n",
       "      <td>143.601746</td>\n",
       "      <td>145.933014</td>\n",
       "      <td>145.933014</td>\n",
       "      <td>5150200</td>\n",
       "      <td>143.977010</td>\n",
       "      <td>3.020363</td>\n",
       "      <td>136.698456</td>\n",
       "      <td>8.236770</td>\n",
       "      <td>...</td>\n",
       "      <td>1510.799967</td>\n",
       "      <td>39.935485</td>\n",
       "      <td>1450.814982</td>\n",
       "      <td>86.016593</td>\n",
       "      <td>1480.791532</td>\n",
       "      <td>79.789023</td>\n",
       "      <td>0.871477</td>\n",
       "      <td>0.804582</td>\n",
       "      <td>0.756923</td>\n",
       "      <td>148.936752</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 40 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            google_open  google_high  google_low  google_close      google  \\\n",
       "Date                                                                         \n",
       "2008-11-21   130.764862   134.182053  123.188263    130.725006  130.725006   \n",
       "2008-11-24   134.127258   134.470963  124.040070    128.239334  128.239334   \n",
       "2008-11-25   133.838348   142.794769  133.160873    140.498383  140.498383   \n",
       "2008-11-26   139.616684   147.178345  137.584305    145.499634  145.499634   \n",
       "2008-11-28   144.747452   147.671494  143.601746    145.933014  145.933014   \n",
       "\n",
       "            google_volume  google_ra_04  google_std_04  google_ra_09  \\\n",
       "Date                                                                   \n",
       "2008-11-21       20565700    136.935478       8.750923    143.875715   \n",
       "2008-11-24       20184700    129.482170       1.757636    137.580147   \n",
       "2008-11-25       21623100    134.368858       8.668457    137.997038   \n",
       "2008-11-26       12760800    138.079117       8.880829    137.425609   \n",
       "2008-11-28        5150200    143.977010       3.020363    136.698456   \n",
       "\n",
       "            google_std_09  ...  nasdaq_ra_04  nasdaq_std_04  nasdaq_ra_09  \\\n",
       "Date                       ...                                              \n",
       "2008-11-21      10.798013  ...   1392.540009      68.742234   1452.251430   \n",
       "2008-11-24       9.599201  ...   1428.184998      61.992083   1420.705017   \n",
       "2008-11-25       8.831976  ...   1468.375000       5.154837   1426.994298   \n",
       "2008-11-26       8.059325  ...   1489.616659      36.971751   1434.144287   \n",
       "2008-11-28       8.236770  ...   1510.799967      39.935485   1450.814982   \n",
       "\n",
       "            nasdaq_std_09  nasdaq_ra_18  nasdaq_std_18  google_daily_vol  \\\n",
       "Date                                                                       \n",
       "2008-11-21      95.188063   1541.462141     127.530682          0.716470   \n",
       "2008-11-24      68.914436   1498.502503     101.345711          0.529842   \n",
       "2008-11-25      65.073432   1483.279999      90.035297          0.833833   \n",
       "2008-11-26      74.235572   1487.035382      87.259170          0.812863   \n",
       "2008-11-28      86.016593   1480.791532      79.789023          0.871477   \n",
       "\n",
       "            s&p_daily_vol  nasdaq_daily_vol  google_45d  \n",
       "Date                                                     \n",
       "2008-11-21       0.802275          0.738519  156.946732  \n",
       "2008-11-24       0.864400          0.777819  155.761169  \n",
       "2008-11-25       0.791313          0.710049  156.573120  \n",
       "2008-11-26       0.758392          0.707779  149.923050  \n",
       "2008-11-28       0.804582          0.756923  148.936752  \n",
       "\n",
       "[5 rows x 40 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "var_names = ['GOOG','S&P','Nasdaq2']\n",
    "names = ['google','s&p','nasdaq']\n",
    "df = get_df(var_names, names)\n",
    "df = get_target_data(df, 'google')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "backup['df'] = df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Price of options\n",
    "It is really hard to get data about historic stock options. Be it as api channel or csv download. I managed only to find sources to pay: 500 USD for every year. Because that is too expensive for this project I decided to calculate the option price with the **Black–Scholes formula**, which gives a theoretical estimate of the price of European-style options.\n",
    "\n",
    "Based on the formula for non-divident paying options I calculate the option prices and add a small random term to create a bit of noise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def black_scholes(S, K, sigma, r=0.03, T=45/365, option = 'call'):\n",
    "    \n",
    "    #S: spot price\n",
    "    #K: strike price\n",
    "    #T: time to maturity in years, 45 days as default\n",
    "    #r: risk-free interest rate, assumed to be constant between t and T\n",
    "    #sigma: standard deviation (volatility) of RETURNS of underlying asset\n",
    "    \n",
    "    d1 = (np.log(S / K) + (r + 0.5 * sigma ** 2) * T) / (sigma * np.sqrt(T))\n",
    "    #d2 = (np.log(S / K) + (r - 0.5 * sigma ** 2) * T) / (sigma * np.sqrt(T))\n",
    "    d2 = d1 - sigma * np.sqrt(T)\n",
    "    \n",
    "    if option == 'call':\n",
    "        result = (S * si.norm.cdf(d1, 0.0, 1.0) - K * np.exp(-r * T) * si.norm.cdf(d2, 0.0, 1.0))\n",
    "    if option == 'put':\n",
    "        result = (K * np.exp(-r * T) * si.norm.cdf(-d2, 0.0, 1.0) - S * si.norm.cdf(-d1, 0.0, 1.0))\n",
    "        \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def option_prices(date, short_spread, long_spread, strike_days = 45):\n",
    "    # calculates prices of options and calculates initial return (net premium)\n",
    "    date = date_fct(date)\n",
    "    P_s_strike_price, P_l_strike_price, C_s_strike_price, C_l_strike_price = strike_prices(date, short_spread, long_spread)\n",
    "    sigma = get_sigma(date)\n",
    "    stock_price = stock_price_fct(date)\n",
    "    P_s_price = black_scholes(stock_price, P_s_strike_price, sigma, T=strike_days/365, option = 'put')\n",
    "    P_l_price = black_scholes(stock_price, P_l_strike_price, sigma, T=strike_days/365, option = 'put')\n",
    "    C_s_price = black_scholes(stock_price, C_s_strike_price, sigma, T=strike_days/365, option = 'call')\n",
    "    C_l_price = black_scholes(stock_price, C_l_strike_price, sigma, T=strike_days/365, option = 'call')\n",
    "    initial_return = P_s_price + C_s_price - P_l_price - C_l_price\n",
    "    \n",
    "    return P_s_price, P_l_price, C_s_price, C_l_price, initial_return"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now it is time to calculate the strike prices and option prices of all 4 options and the initial return by setting up the iron condor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate return of options\n",
    "There are five different regimes for the return. R: return, P(C) put(call) option, s(l): short(long), ir: initial return, stock_price: stock price\n",
    "- stock price is below long put option: R = ir + P_l_strike_price - P_s_strike_price\n",
    "- stock price is between long and short put option: R = ir + stock_price - P_s_strike_price\n",
    "- stock price is between short call and short put option: R = ir\n",
    "- stock price is between short and long call option: R = ir - stock_price + C_s_strike price\n",
    "- stock price is above long call option: R = ir - C_l_strike_price + C_s_strike_price"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define return function of iron condor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ic_return(trading_date, short_spread, long_spread, strike_days = 45):\n",
    "    P_s_strike_price, P_l_strike_price, C_s_strike_price, C_l_strike_price = strike_prices(trading_date, short_spread, long_spread)\n",
    "    P_s_price, P_l_price, C_s_price, C_l_price, initial_return = option_prices(trading_date,short_spread, long_spread, strike_days)\n",
    "    strike_date = date_fct(trading_date, 45)\n",
    "    final_stock_price = stock_price_fct(strike_date)\n",
    "    if final_stock_price < P_l_strike_price:\n",
    "        final_result = initial_return + P_l_strike_price - P_s_strike_price\n",
    "    elif final_stock_price >= P_l_strike_price and final_stock_price < P_s_strike_price:\n",
    "        final_result = initial_return + stock_price - P_s_strike_price\n",
    "    elif final_stock_price >= P_s_strike_price and final_stock_price < C_s_strike_price:\n",
    "        final_result = initial_return\n",
    "    elif final_stock_price >= C_s_strike_price and final_stock_price < C_l_strike_price:\n",
    "        final_result = initial_return - final_stock_price + C_s_strike_price\n",
    "    elif final_stock_price >= C_l_strike_price:\n",
    "        final_result = initial_return - C_l_strike_price + C_s_strike_price\n",
    "    maximum_result = initial_return\n",
    "    return final_result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Scale data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/matthias/anaconda3/lib/python3.7/site-packages/sklearn/preprocessing/data.py:334: DataConversionWarning: Data with input dtype int64, float64 were all converted to float64 by MinMaxScaler.\n",
      "  return self.partial_fit(X, y)\n"
     ]
    }
   ],
   "source": [
    "scaler = preprocessing.MinMaxScaler(feature_range=(-1, 1))\n",
    "df_scaled = pd.DataFrame(scaler.fit_transform(df), columns=df.columns, index = df.index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Split into train and test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_train_test_data(data, train_test_split, batch_size):\n",
    "    # data is dataframe to get train test data out of\n",
    "    train_size_raw = data.shape[0]*train_test_split\n",
    "    train_size = int(train_size_raw - train_size_raw % batch_size) # make train_size multiple of batch_size for \"statefull = True\" in LSTM\n",
    "    train_data = data.iloc[0:train_size]\n",
    "    test_size_raw = data.shape[0] - train_size\n",
    "    test_size = int(test_size_raw - test_size_raw%batch_size) # multiple of batch_size\n",
    "    test_data = data.iloc[train_size : train_size + test_size]\n",
    "    x_train = train_data.drop(columns=['google_45d']) # google_45d is not for training, just for reversing stationarity\n",
    "    y_train = train_data['google_45d']\n",
    "    x_test = test_data.drop(columns=['google_45d'])\n",
    "    y_test = test_data['google_45d']\n",
    "    train_index=x_train.index\n",
    "    x_columns = x_train.columns\n",
    "    test_index = x_test.index\n",
    "    return x_train, x_test, y_train, y_test, test_data, test_index, x_columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 10\n",
    "train_test_split = 0.9\n",
    "x_train, x_test, y_train, y_test, test_data, test_index, x_columns = get_train_test_data(df_scaled, train_test_split, batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sequential model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_6 (Dense)              (None, 128)               5120      \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 16)                2064      \n",
      "_________________________________________________________________\n",
      "dropout_5 (Dropout)          (None, 16)                0         \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 4)                 68        \n",
      "_________________________________________________________________\n",
      "dropout_6 (Dropout)          (None, 4)                 0         \n",
      "_________________________________________________________________\n",
      "dense_9 (Dense)              (None, 1)                 5         \n",
      "=================================================================\n",
      "Total params: 7,257\n",
      "Trainable params: 7,257\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Build the model architecture\n",
    "seq_model = Sequential()\n",
    "seq_model.add(Dense(128, activation='relu', input_shape=(x_train.shape[1],)))\n",
    "seq_model.add(Dropout(0.2))\n",
    "seq_model.add(Dense(16, activation='relu'))\n",
    "seq_model.add(Dropout(0.2))\n",
    "seq_model.add(Dense(4, activation='relu'))\n",
    "seq_model.add(Dropout(0.2))\n",
    "seq_model.add(Dense(1))\n",
    "\n",
    "# Compile the model using a loss function and an optimizer.\n",
    "seq_model.compile(optimizer='adam',loss='mean_squared_error')\n",
    "seq_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 2370 samples, validate on 270 samples\n",
      "Epoch 1/3000\n",
      "2370/2370 [==============================] - 1s 310us/step - loss: 0.1499 - val_loss: 0.3492\n",
      "Epoch 2/3000\n",
      "2370/2370 [==============================] - 0s 55us/step - loss: 0.0909 - val_loss: 0.0729\n",
      "Epoch 3/3000\n",
      "2370/2370 [==============================] - 0s 78us/step - loss: 0.0749 - val_loss: 0.0651\n",
      "Epoch 4/3000\n",
      "2370/2370 [==============================] - 0s 126us/step - loss: 0.0659 - val_loss: 0.0650\n",
      "Epoch 5/3000\n",
      "2370/2370 [==============================] - 0s 120us/step - loss: 0.0663 - val_loss: 0.0630\n",
      "Epoch 6/3000\n",
      "2370/2370 [==============================] - 0s 98us/step - loss: 0.0582 - val_loss: 0.0543\n",
      "Epoch 7/3000\n",
      "2370/2370 [==============================] - 0s 57us/step - loss: 0.0570 - val_loss: 0.0499\n",
      "Epoch 8/3000\n",
      "2370/2370 [==============================] - 0s 124us/step - loss: 0.0546 - val_loss: 0.0423\n",
      "Epoch 9/3000\n",
      "2370/2370 [==============================] - 0s 111us/step - loss: 0.0496 - val_loss: 0.0682\n",
      "Epoch 10/3000\n",
      "2370/2370 [==============================] - 0s 94us/step - loss: 0.0499 - val_loss: 0.0555\n",
      "Epoch 11/3000\n",
      "2370/2370 [==============================] - 0s 54us/step - loss: 0.0482 - val_loss: 0.0584\n",
      "Epoch 12/3000\n",
      "2370/2370 [==============================] - 0s 63us/step - loss: 0.0456 - val_loss: 0.0536\n",
      "Epoch 13/3000\n",
      "2370/2370 [==============================] - 0s 91us/step - loss: 0.0476 - val_loss: 0.0600\n",
      "Epoch 14/3000\n",
      "2370/2370 [==============================] - 0s 135us/step - loss: 0.0453 - val_loss: 0.0600\n",
      "Epoch 15/3000\n",
      "2370/2370 [==============================] - 0s 113us/step - loss: 0.0439 - val_loss: 0.0520\n",
      "Epoch 16/3000\n",
      "2370/2370 [==============================] - 0s 78us/step - loss: 0.0469 - val_loss: 0.0573\n",
      "Epoch 17/3000\n",
      "2370/2370 [==============================] - 0s 92us/step - loss: 0.0436 - val_loss: 0.0502\n",
      "Epoch 18/3000\n",
      "2370/2370 [==============================] - 0s 75us/step - loss: 0.0439 - val_loss: 0.0408\n",
      "Epoch 19/3000\n",
      "2370/2370 [==============================] - 0s 52us/step - loss: 0.0440 - val_loss: 0.0522\n",
      "Epoch 20/3000\n",
      "2370/2370 [==============================] - 0s 45us/step - loss: 0.0440 - val_loss: 0.0410\n",
      "Epoch 21/3000\n",
      "2370/2370 [==============================] - 0s 45us/step - loss: 0.0428 - val_loss: 0.0412\n",
      "Epoch 22/3000\n",
      "2370/2370 [==============================] - 0s 43us/step - loss: 0.0417 - val_loss: 0.0529\n",
      "Epoch 23/3000\n",
      "2370/2370 [==============================] - 0s 30us/step - loss: 0.0452 - val_loss: 0.0449\n",
      "Epoch 24/3000\n",
      "2370/2370 [==============================] - 0s 30us/step - loss: 0.0422 - val_loss: 0.0462\n",
      "Epoch 25/3000\n",
      "2370/2370 [==============================] - 0s 30us/step - loss: 0.0411 - val_loss: 0.0615\n",
      "Epoch 26/3000\n",
      "2370/2370 [==============================] - 0s 31us/step - loss: 0.0423 - val_loss: 0.0491\n",
      "Epoch 27/3000\n",
      "2370/2370 [==============================] - 0s 31us/step - loss: 0.0419 - val_loss: 0.0396\n",
      "Epoch 28/3000\n",
      "2370/2370 [==============================] - 0s 30us/step - loss: 0.0412 - val_loss: 0.0552\n",
      "Epoch 29/3000\n",
      "2370/2370 [==============================] - 0s 36us/step - loss: 0.0447 - val_loss: 0.0446\n",
      "Epoch 30/3000\n",
      "2370/2370 [==============================] - 0s 44us/step - loss: 0.0417 - val_loss: 0.0548\n",
      "Epoch 31/3000\n",
      "2370/2370 [==============================] - 0s 90us/step - loss: 0.0400 - val_loss: 0.0437\n",
      "Epoch 32/3000\n",
      "2370/2370 [==============================] - 0s 85us/step - loss: 0.0399 - val_loss: 0.0321\n",
      "Epoch 33/3000\n",
      "2370/2370 [==============================] - 0s 48us/step - loss: 0.0415 - val_loss: 0.0377\n",
      "Epoch 34/3000\n",
      "2370/2370 [==============================] - 0s 43us/step - loss: 0.0404 - val_loss: 0.0460\n",
      "Epoch 35/3000\n",
      "2370/2370 [==============================] - 0s 44us/step - loss: 0.0417 - val_loss: 0.0399\n",
      "Epoch 36/3000\n",
      "2370/2370 [==============================] - 0s 43us/step - loss: 0.0424 - val_loss: 0.0402\n",
      "Epoch 37/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0423 - val_loss: 0.0382\n",
      "Epoch 38/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0423 - val_loss: 0.0495\n",
      "Epoch 39/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0420 - val_loss: 0.0378\n",
      "Epoch 40/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0419 - val_loss: 0.0394\n",
      "Epoch 41/3000\n",
      "2370/2370 [==============================] - 0s 28us/step - loss: 0.0413 - val_loss: 0.0443\n",
      "Epoch 42/3000\n",
      "2370/2370 [==============================] - 0s 30us/step - loss: 0.0430 - val_loss: 0.0266\n",
      "Epoch 43/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0417 - val_loss: 0.0376\n",
      "Epoch 44/3000\n",
      "2370/2370 [==============================] - 0s 30us/step - loss: 0.0402 - val_loss: 0.0450\n",
      "Epoch 45/3000\n",
      "2370/2370 [==============================] - 0s 30us/step - loss: 0.0422 - val_loss: 0.0685\n",
      "Epoch 46/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0412 - val_loss: 0.0392\n",
      "Epoch 47/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0406 - val_loss: 0.0295\n",
      "Epoch 48/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0404 - val_loss: 0.0398\n",
      "Epoch 49/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0405 - val_loss: 0.0359\n",
      "Epoch 50/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0414 - val_loss: 0.0387\n",
      "Epoch 51/3000\n",
      "2370/2370 [==============================] - 0s 35us/step - loss: 0.0415 - val_loss: 0.0415\n",
      "Epoch 52/3000\n",
      "2370/2370 [==============================] - 0s 32us/step - loss: 0.0392 - val_loss: 0.0308\n",
      "Epoch 53/3000\n",
      "2370/2370 [==============================] - 0s 31us/step - loss: 0.0409 - val_loss: 0.0321\n",
      "Epoch 54/3000\n",
      "2370/2370 [==============================] - 0s 31us/step - loss: 0.0389 - val_loss: 0.0379\n",
      "Epoch 55/3000\n",
      "2370/2370 [==============================] - 0s 31us/step - loss: 0.0390 - val_loss: 0.0318\n",
      "Epoch 56/3000\n",
      "2370/2370 [==============================] - 0s 30us/step - loss: 0.0385 - val_loss: 0.0554\n",
      "Epoch 57/3000\n",
      "2370/2370 [==============================] - 0s 30us/step - loss: 0.0383 - val_loss: 0.0466\n",
      "Epoch 58/3000\n",
      "2370/2370 [==============================] - 0s 30us/step - loss: 0.0423 - val_loss: 0.0329\n",
      "Epoch 59/3000\n",
      "2370/2370 [==============================] - 0s 30us/step - loss: 0.0417 - val_loss: 0.0539\n",
      "Epoch 60/3000\n",
      "2370/2370 [==============================] - 0s 30us/step - loss: 0.0391 - val_loss: 0.0351\n",
      "Epoch 61/3000\n",
      "2370/2370 [==============================] - 0s 30us/step - loss: 0.0374 - val_loss: 0.0480\n",
      "Epoch 62/3000\n",
      "2370/2370 [==============================] - 0s 31us/step - loss: 0.0361 - val_loss: 0.0422\n",
      "Epoch 63/3000\n",
      "2370/2370 [==============================] - 0s 30us/step - loss: 0.0396 - val_loss: 0.0331\n",
      "Epoch 64/3000\n",
      "2370/2370 [==============================] - 0s 30us/step - loss: 0.0414 - val_loss: 0.0456\n",
      "Epoch 65/3000\n",
      "2370/2370 [==============================] - 0s 31us/step - loss: 0.0376 - val_loss: 0.0479\n",
      "Epoch 66/3000\n",
      "2370/2370 [==============================] - 0s 31us/step - loss: 0.0387 - val_loss: 0.0342\n",
      "Epoch 67/3000\n",
      "2370/2370 [==============================] - 0s 30us/step - loss: 0.0397 - val_loss: 0.0455\n",
      "Epoch 68/3000\n",
      "2370/2370 [==============================] - 0s 31us/step - loss: 0.0394 - val_loss: 0.0510\n",
      "Epoch 69/3000\n",
      "2370/2370 [==============================] - 0s 31us/step - loss: 0.0373 - val_loss: 0.0364\n",
      "Epoch 70/3000\n",
      "2370/2370 [==============================] - 0s 31us/step - loss: 0.0391 - val_loss: 0.0516\n",
      "Epoch 71/3000\n",
      "2370/2370 [==============================] - 0s 32us/step - loss: 0.0375 - val_loss: 0.0435\n",
      "Epoch 72/3000\n",
      "2370/2370 [==============================] - 0s 32us/step - loss: 0.0390 - val_loss: 0.0379\n",
      "Epoch 73/3000\n",
      "2370/2370 [==============================] - 0s 32us/step - loss: 0.0404 - val_loss: 0.0236\n",
      "Epoch 74/3000\n",
      "2370/2370 [==============================] - 0s 31us/step - loss: 0.0389 - val_loss: 0.0555\n",
      "Epoch 75/3000\n",
      "2370/2370 [==============================] - 0s 31us/step - loss: 0.0395 - val_loss: 0.0512\n",
      "Epoch 76/3000\n",
      "2370/2370 [==============================] - 0s 32us/step - loss: 0.0406 - val_loss: 0.0363\n",
      "Epoch 77/3000\n",
      "2370/2370 [==============================] - 0s 30us/step - loss: 0.0403 - val_loss: 0.0288\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 78/3000\n",
      "2370/2370 [==============================] - 0s 32us/step - loss: 0.0389 - val_loss: 0.0398\n",
      "Epoch 79/3000\n",
      "2370/2370 [==============================] - 0s 31us/step - loss: 0.0408 - val_loss: 0.0501\n",
      "Epoch 80/3000\n",
      "2370/2370 [==============================] - 0s 30us/step - loss: 0.0407 - val_loss: 0.0338\n",
      "Epoch 81/3000\n",
      "2370/2370 [==============================] - 0s 30us/step - loss: 0.0414 - val_loss: 0.0340\n",
      "Epoch 82/3000\n",
      "2370/2370 [==============================] - 0s 30us/step - loss: 0.0403 - val_loss: 0.0387\n",
      "Epoch 83/3000\n",
      "2370/2370 [==============================] - 0s 32us/step - loss: 0.0389 - val_loss: 0.0374\n",
      "Epoch 84/3000\n",
      "2370/2370 [==============================] - 0s 36us/step - loss: 0.0388 - val_loss: 0.0394\n",
      "Epoch 85/3000\n",
      "2370/2370 [==============================] - 0s 36us/step - loss: 0.0397 - val_loss: 0.0666\n",
      "Epoch 86/3000\n",
      "2370/2370 [==============================] - 0s 33us/step - loss: 0.0411 - val_loss: 0.0418\n",
      "Epoch 87/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0395 - val_loss: 0.0461\n",
      "Epoch 88/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0411 - val_loss: 0.0449\n",
      "Epoch 89/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0384 - val_loss: 0.0421\n",
      "Epoch 90/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0369 - val_loss: 0.0264\n",
      "Epoch 91/3000\n",
      "2370/2370 [==============================] - 0s 31us/step - loss: 0.0407 - val_loss: 0.0448\n",
      "Epoch 92/3000\n",
      "2370/2370 [==============================] - 0s 32us/step - loss: 0.0374 - val_loss: 0.0461\n",
      "Epoch 93/3000\n",
      "2370/2370 [==============================] - 0s 32us/step - loss: 0.0408 - val_loss: 0.0439\n",
      "Epoch 94/3000\n",
      "2370/2370 [==============================] - 0s 32us/step - loss: 0.0370 - val_loss: 0.0396\n",
      "Epoch 95/3000\n",
      "2370/2370 [==============================] - 0s 31us/step - loss: 0.0379 - val_loss: 0.0465\n",
      "Epoch 96/3000\n",
      "2370/2370 [==============================] - 0s 32us/step - loss: 0.0381 - val_loss: 0.0373\n",
      "Epoch 97/3000\n",
      "2370/2370 [==============================] - 0s 33us/step - loss: 0.0390 - val_loss: 0.0431\n",
      "Epoch 98/3000\n",
      "2370/2370 [==============================] - 0s 31us/step - loss: 0.0386 - val_loss: 0.0679\n",
      "Epoch 99/3000\n",
      "2370/2370 [==============================] - 0s 34us/step - loss: 0.0370 - val_loss: 0.0425\n",
      "Epoch 100/3000\n",
      "2370/2370 [==============================] - 0s 32us/step - loss: 0.0404 - val_loss: 0.0363\n",
      "Epoch 101/3000\n",
      "2370/2370 [==============================] - 0s 31us/step - loss: 0.0390 - val_loss: 0.0434\n",
      "Epoch 102/3000\n",
      "2370/2370 [==============================] - 0s 32us/step - loss: 0.0414 - val_loss: 0.0449\n",
      "Epoch 103/3000\n",
      "2370/2370 [==============================] - 0s 31us/step - loss: 0.0380 - val_loss: 0.0430\n",
      "Epoch 104/3000\n",
      "2370/2370 [==============================] - 0s 32us/step - loss: 0.0362 - val_loss: 0.0448\n",
      "Epoch 105/3000\n",
      "2370/2370 [==============================] - 0s 32us/step - loss: 0.0384 - val_loss: 0.0333\n",
      "Epoch 106/3000\n",
      "2370/2370 [==============================] - 0s 32us/step - loss: 0.0358 - val_loss: 0.0464\n",
      "Epoch 107/3000\n",
      "2370/2370 [==============================] - 0s 35us/step - loss: 0.0377 - val_loss: 0.0450\n",
      "Epoch 108/3000\n",
      "2370/2370 [==============================] - 0s 31us/step - loss: 0.0389 - val_loss: 0.0534\n",
      "Epoch 109/3000\n",
      "2370/2370 [==============================] - 0s 31us/step - loss: 0.0396 - val_loss: 0.0403\n",
      "Epoch 110/3000\n",
      "2370/2370 [==============================] - 0s 31us/step - loss: 0.0374 - val_loss: 0.0327\n",
      "Epoch 111/3000\n",
      "2370/2370 [==============================] - 0s 32us/step - loss: 0.0399 - val_loss: 0.0362\n",
      "Epoch 112/3000\n",
      "2370/2370 [==============================] - 0s 32us/step - loss: 0.0390 - val_loss: 0.0429\n",
      "Epoch 113/3000\n",
      "2370/2370 [==============================] - 0s 32us/step - loss: 0.0362 - val_loss: 0.0389\n",
      "Epoch 114/3000\n",
      "2370/2370 [==============================] - 0s 32us/step - loss: 0.0375 - val_loss: 0.0478\n",
      "Epoch 115/3000\n",
      "2370/2370 [==============================] - 0s 32us/step - loss: 0.0387 - val_loss: 0.0484\n",
      "Epoch 116/3000\n",
      "2370/2370 [==============================] - 0s 32us/step - loss: 0.0373 - val_loss: 0.0539\n",
      "Epoch 117/3000\n",
      "2370/2370 [==============================] - 0s 32us/step - loss: 0.0386 - val_loss: 0.0257\n",
      "Epoch 118/3000\n",
      "2370/2370 [==============================] - 0s 36us/step - loss: 0.0407 - val_loss: 0.0268\n",
      "Epoch 119/3000\n",
      "2370/2370 [==============================] - 0s 39us/step - loss: 0.0377 - val_loss: 0.0386\n",
      "Epoch 120/3000\n",
      "2370/2370 [==============================] - 0s 35us/step - loss: 0.0411 - val_loss: 0.0434\n",
      "Epoch 121/3000\n",
      "2370/2370 [==============================] - 0s 35us/step - loss: 0.0402 - val_loss: 0.0322\n",
      "Epoch 122/3000\n",
      "2370/2370 [==============================] - 0s 35us/step - loss: 0.0383 - val_loss: 0.0449\n",
      "Epoch 123/3000\n",
      "2370/2370 [==============================] - 0s 36us/step - loss: 0.0377 - val_loss: 0.0457\n",
      "Epoch 124/3000\n",
      "2370/2370 [==============================] - 0s 34us/step - loss: 0.0359 - val_loss: 0.0414\n",
      "Epoch 125/3000\n",
      "2370/2370 [==============================] - 0s 30us/step - loss: 0.0354 - val_loss: 0.0612\n",
      "Epoch 126/3000\n",
      "2370/2370 [==============================] - 0s 32us/step - loss: 0.0367 - val_loss: 0.0520\n",
      "Epoch 127/3000\n",
      "2370/2370 [==============================] - 0s 32us/step - loss: 0.0403 - val_loss: 0.0283\n",
      "Epoch 128/3000\n",
      "2370/2370 [==============================] - 0s 30us/step - loss: 0.0364 - val_loss: 0.0428\n",
      "Epoch 129/3000\n",
      "2370/2370 [==============================] - 0s 30us/step - loss: 0.0367 - val_loss: 0.0434\n",
      "Epoch 130/3000\n",
      "2370/2370 [==============================] - 0s 30us/step - loss: 0.0415 - val_loss: 0.0438\n",
      "Epoch 131/3000\n",
      "2370/2370 [==============================] - 0s 31us/step - loss: 0.0390 - val_loss: 0.0403\n",
      "Epoch 132/3000\n",
      "2370/2370 [==============================] - 0s 31us/step - loss: 0.0387 - val_loss: 0.0423\n",
      "Epoch 133/3000\n",
      "2370/2370 [==============================] - 0s 30us/step - loss: 0.0402 - val_loss: 0.0490\n",
      "Epoch 134/3000\n",
      "2370/2370 [==============================] - 0s 31us/step - loss: 0.0363 - val_loss: 0.0342\n",
      "Epoch 135/3000\n",
      "2370/2370 [==============================] - 0s 31us/step - loss: 0.0366 - val_loss: 0.0410\n",
      "Epoch 136/3000\n",
      "2370/2370 [==============================] - 0s 30us/step - loss: 0.0376 - val_loss: 0.0456\n",
      "Epoch 137/3000\n",
      "2370/2370 [==============================] - 0s 31us/step - loss: 0.0373 - val_loss: 0.0466\n",
      "Epoch 138/3000\n",
      "2370/2370 [==============================] - 0s 30us/step - loss: 0.0383 - val_loss: 0.0448\n",
      "Epoch 139/3000\n",
      "2370/2370 [==============================] - 0s 30us/step - loss: 0.0385 - val_loss: 0.0430\n",
      "Epoch 140/3000\n",
      "2370/2370 [==============================] - 0s 33us/step - loss: 0.0391 - val_loss: 0.0377\n",
      "Epoch 141/3000\n",
      "2370/2370 [==============================] - 0s 31us/step - loss: 0.0395 - val_loss: 0.0412\n",
      "Epoch 142/3000\n",
      "2370/2370 [==============================] - 0s 30us/step - loss: 0.0362 - val_loss: 0.0418\n",
      "Epoch 143/3000\n",
      "2370/2370 [==============================] - 0s 30us/step - loss: 0.0381 - val_loss: 0.0327\n",
      "Epoch 144/3000\n",
      "2370/2370 [==============================] - 0s 30us/step - loss: 0.0378 - val_loss: 0.0369\n",
      "Epoch 145/3000\n",
      "2370/2370 [==============================] - 0s 30us/step - loss: 0.0342 - val_loss: 0.0416\n",
      "Epoch 146/3000\n",
      "2370/2370 [==============================] - 0s 31us/step - loss: 0.0370 - val_loss: 0.0399\n",
      "Epoch 147/3000\n",
      "2370/2370 [==============================] - 0s 30us/step - loss: 0.0377 - val_loss: 0.0334\n",
      "Epoch 148/3000\n",
      "2370/2370 [==============================] - 0s 30us/step - loss: 0.0403 - val_loss: 0.0501\n",
      "Epoch 149/3000\n",
      "2370/2370 [==============================] - 0s 30us/step - loss: 0.0374 - val_loss: 0.0525\n",
      "Epoch 150/3000\n",
      "2370/2370 [==============================] - 0s 30us/step - loss: 0.0348 - val_loss: 0.0372\n",
      "Epoch 151/3000\n",
      "2370/2370 [==============================] - 0s 30us/step - loss: 0.0366 - val_loss: 0.0545\n",
      "Epoch 152/3000\n",
      "2370/2370 [==============================] - 0s 30us/step - loss: 0.0363 - val_loss: 0.0394\n",
      "Epoch 153/3000\n",
      "2370/2370 [==============================] - 0s 31us/step - loss: 0.0386 - val_loss: 0.0388\n",
      "Epoch 154/3000\n",
      "2370/2370 [==============================] - 0s 30us/step - loss: 0.0368 - val_loss: 0.0407\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 155/3000\n",
      "2370/2370 [==============================] - 0s 31us/step - loss: 0.0363 - val_loss: 0.0467\n",
      "Epoch 156/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0374 - val_loss: 0.0446\n",
      "Epoch 157/3000\n",
      "2370/2370 [==============================] - 0s 31us/step - loss: 0.0371 - val_loss: 0.0392\n",
      "Epoch 158/3000\n",
      "2370/2370 [==============================] - 0s 31us/step - loss: 0.0384 - val_loss: 0.0448\n",
      "Epoch 159/3000\n",
      "2370/2370 [==============================] - 0s 31us/step - loss: 0.0409 - val_loss: 0.0466\n",
      "Epoch 160/3000\n",
      "2370/2370 [==============================] - 0s 30us/step - loss: 0.0399 - val_loss: 0.0455\n",
      "Epoch 161/3000\n",
      "2370/2370 [==============================] - 0s 31us/step - loss: 0.0379 - val_loss: 0.0346\n",
      "Epoch 162/3000\n",
      "2370/2370 [==============================] - 0s 32us/step - loss: 0.0397 - val_loss: 0.0453\n",
      "Epoch 163/3000\n",
      "2370/2370 [==============================] - 0s 31us/step - loss: 0.0382 - val_loss: 0.0688\n",
      "Epoch 164/3000\n",
      "2370/2370 [==============================] - 0s 30us/step - loss: 0.0378 - val_loss: 0.0502\n",
      "Epoch 165/3000\n",
      "2370/2370 [==============================] - 0s 31us/step - loss: 0.0398 - val_loss: 0.0578\n",
      "Epoch 166/3000\n",
      "2370/2370 [==============================] - 0s 30us/step - loss: 0.0378 - val_loss: 0.0427\n",
      "Epoch 167/3000\n",
      "2370/2370 [==============================] - 0s 30us/step - loss: 0.0369 - val_loss: 0.0451\n",
      "Epoch 168/3000\n",
      "2370/2370 [==============================] - 0s 30us/step - loss: 0.0375 - val_loss: 0.0479\n",
      "Epoch 169/3000\n",
      "2370/2370 [==============================] - 0s 32us/step - loss: 0.0332 - val_loss: 0.0449\n",
      "Epoch 170/3000\n",
      "2370/2370 [==============================] - 0s 30us/step - loss: 0.0342 - val_loss: 0.0312\n",
      "Epoch 171/3000\n",
      "2370/2370 [==============================] - 0s 31us/step - loss: 0.0378 - val_loss: 0.0393\n",
      "Epoch 172/3000\n",
      "2370/2370 [==============================] - 0s 32us/step - loss: 0.0373 - val_loss: 0.0424\n",
      "Epoch 173/3000\n",
      "2370/2370 [==============================] - 0s 30us/step - loss: 0.0377 - val_loss: 0.0537\n",
      "Epoch 174/3000\n",
      "2370/2370 [==============================] - 0s 30us/step - loss: 0.0370 - val_loss: 0.0472\n",
      "Epoch 175/3000\n",
      "2370/2370 [==============================] - 0s 31us/step - loss: 0.0387 - val_loss: 0.0394\n",
      "Epoch 176/3000\n",
      "2370/2370 [==============================] - 0s 30us/step - loss: 0.0369 - val_loss: 0.0630\n",
      "Epoch 177/3000\n",
      "2370/2370 [==============================] - 0s 30us/step - loss: 0.0370 - val_loss: 0.0357\n",
      "Epoch 178/3000\n",
      "2370/2370 [==============================] - 0s 31us/step - loss: 0.0410 - val_loss: 0.0463\n",
      "Epoch 179/3000\n",
      "2370/2370 [==============================] - 0s 30us/step - loss: 0.0353 - val_loss: 0.0640\n",
      "Epoch 180/3000\n",
      "2370/2370 [==============================] - 0s 30us/step - loss: 0.0378 - val_loss: 0.0377\n",
      "Epoch 181/3000\n",
      "2370/2370 [==============================] - 0s 30us/step - loss: 0.0400 - val_loss: 0.0429\n",
      "Epoch 182/3000\n",
      "2370/2370 [==============================] - 0s 30us/step - loss: 0.0360 - val_loss: 0.0411\n",
      "Epoch 183/3000\n",
      "2370/2370 [==============================] - 0s 30us/step - loss: 0.0397 - val_loss: 0.0378\n",
      "Epoch 184/3000\n",
      "2370/2370 [==============================] - 0s 30us/step - loss: 0.0384 - val_loss: 0.0418\n",
      "Epoch 185/3000\n",
      "2370/2370 [==============================] - 0s 30us/step - loss: 0.0359 - val_loss: 0.0422\n",
      "Epoch 186/3000\n",
      "2370/2370 [==============================] - 0s 31us/step - loss: 0.0349 - val_loss: 0.0405\n",
      "Epoch 187/3000\n",
      "2370/2370 [==============================] - 0s 30us/step - loss: 0.0386 - val_loss: 0.0386\n",
      "Epoch 188/3000\n",
      "2370/2370 [==============================] - 0s 30us/step - loss: 0.0406 - val_loss: 0.0436\n",
      "Epoch 189/3000\n",
      "2370/2370 [==============================] - 0s 30us/step - loss: 0.0346 - val_loss: 0.0420\n",
      "Epoch 190/3000\n",
      "2370/2370 [==============================] - 0s 31us/step - loss: 0.0400 - val_loss: 0.0341\n",
      "Epoch 191/3000\n",
      "2370/2370 [==============================] - 0s 30us/step - loss: 0.0396 - val_loss: 0.0457\n",
      "Epoch 192/3000\n",
      "2370/2370 [==============================] - 0s 31us/step - loss: 0.0336 - val_loss: 0.0450\n",
      "Epoch 193/3000\n",
      "2370/2370 [==============================] - 0s 30us/step - loss: 0.0363 - val_loss: 0.0621\n",
      "Epoch 194/3000\n",
      "2370/2370 [==============================] - 0s 30us/step - loss: 0.0376 - val_loss: 0.0355\n",
      "Epoch 195/3000\n",
      "2370/2370 [==============================] - 0s 31us/step - loss: 0.0389 - val_loss: 0.0420\n",
      "Epoch 196/3000\n",
      "2370/2370 [==============================] - 0s 30us/step - loss: 0.0370 - val_loss: 0.0450\n",
      "Epoch 197/3000\n",
      "2370/2370 [==============================] - 0s 30us/step - loss: 0.0375 - val_loss: 0.0540\n",
      "Epoch 198/3000\n",
      "2370/2370 [==============================] - 0s 30us/step - loss: 0.0389 - val_loss: 0.0303\n",
      "Epoch 199/3000\n",
      "2370/2370 [==============================] - 0s 30us/step - loss: 0.0343 - val_loss: 0.0477\n",
      "Epoch 200/3000\n",
      "2370/2370 [==============================] - 0s 30us/step - loss: 0.0349 - val_loss: 0.0369\n",
      "Epoch 201/3000\n",
      "2370/2370 [==============================] - 0s 30us/step - loss: 0.0396 - val_loss: 0.0540\n",
      "Epoch 202/3000\n",
      "2370/2370 [==============================] - 0s 30us/step - loss: 0.0365 - val_loss: 0.0604\n",
      "Epoch 203/3000\n",
      "2370/2370 [==============================] - 0s 30us/step - loss: 0.0379 - val_loss: 0.0335\n",
      "Epoch 204/3000\n",
      "2370/2370 [==============================] - 0s 30us/step - loss: 0.0319 - val_loss: 0.0370\n",
      "Epoch 205/3000\n",
      "2370/2370 [==============================] - 0s 32us/step - loss: 0.0349 - val_loss: 0.0511\n",
      "Epoch 206/3000\n",
      "2370/2370 [==============================] - 0s 30us/step - loss: 0.0339 - val_loss: 0.0391\n",
      "Epoch 207/3000\n",
      "2370/2370 [==============================] - 0s 30us/step - loss: 0.0381 - val_loss: 0.0473\n",
      "Epoch 208/3000\n",
      "2370/2370 [==============================] - 0s 30us/step - loss: 0.0384 - val_loss: 0.0610\n",
      "Epoch 209/3000\n",
      "2370/2370 [==============================] - 0s 30us/step - loss: 0.0348 - val_loss: 0.0343\n",
      "Epoch 210/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0374 - val_loss: 0.0489\n",
      "Epoch 211/3000\n",
      "2370/2370 [==============================] - 0s 58us/step - loss: 0.0377 - val_loss: 0.0566\n",
      "Epoch 212/3000\n",
      "2370/2370 [==============================] - 0s 52us/step - loss: 0.0384 - val_loss: 0.0461\n",
      "Epoch 213/3000\n",
      "2370/2370 [==============================] - 0s 32us/step - loss: 0.0368 - val_loss: 0.0310\n",
      "Epoch 214/3000\n",
      "2370/2370 [==============================] - 0s 33us/step - loss: 0.0395 - val_loss: 0.0388\n",
      "Epoch 215/3000\n",
      "2370/2370 [==============================] - 0s 31us/step - loss: 0.0359 - val_loss: 0.0544\n",
      "Epoch 216/3000\n",
      "2370/2370 [==============================] - 0s 32us/step - loss: 0.0365 - val_loss: 0.0453\n",
      "Epoch 217/3000\n",
      "2370/2370 [==============================] - 0s 32us/step - loss: 0.0341 - val_loss: 0.0286\n",
      "Epoch 218/3000\n",
      "2370/2370 [==============================] - 0s 32us/step - loss: 0.0375 - val_loss: 0.0355\n",
      "Epoch 219/3000\n",
      "2370/2370 [==============================] - 0s 32us/step - loss: 0.0358 - val_loss: 0.0470\n",
      "Epoch 220/3000\n",
      "2370/2370 [==============================] - 0s 32us/step - loss: 0.0354 - val_loss: 0.0518\n",
      "Epoch 221/3000\n",
      "2370/2370 [==============================] - 0s 31us/step - loss: 0.0367 - val_loss: 0.0469\n",
      "Epoch 222/3000\n",
      "2370/2370 [==============================] - 0s 31us/step - loss: 0.0398 - val_loss: 0.0361\n",
      "Epoch 223/3000\n",
      "2370/2370 [==============================] - 0s 30us/step - loss: 0.0357 - val_loss: 0.0490\n",
      "Epoch 224/3000\n",
      "2370/2370 [==============================] - 0s 32us/step - loss: 0.0371 - val_loss: 0.0440\n",
      "Epoch 225/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0384 - val_loss: 0.0423\n",
      "Epoch 226/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0346 - val_loss: 0.0408\n",
      "Epoch 227/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0365 - val_loss: 0.0512\n",
      "Epoch 228/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0378 - val_loss: 0.0464\n",
      "Epoch 229/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0356 - val_loss: 0.0417\n",
      "Epoch 230/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0363 - val_loss: 0.0470\n",
      "Epoch 231/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0354 - val_loss: 0.0423\n",
      "Epoch 232/3000\n",
      "2370/2370 [==============================] - 0s 28us/step - loss: 0.0364 - val_loss: 0.0536\n",
      "Epoch 233/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0352 - val_loss: 0.0417\n",
      "Epoch 234/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0361 - val_loss: 0.0485\n",
      "Epoch 235/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0315 - val_loss: 0.0358\n",
      "Epoch 236/3000\n",
      "2370/2370 [==============================] - 0s 28us/step - loss: 0.0410 - val_loss: 0.0334\n",
      "Epoch 237/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0363 - val_loss: 0.0356\n",
      "Epoch 238/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0370 - val_loss: 0.0345\n",
      "Epoch 239/3000\n",
      "2370/2370 [==============================] - 0s 30us/step - loss: 0.0354 - val_loss: 0.0402\n",
      "Epoch 240/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0349 - val_loss: 0.0364\n",
      "Epoch 241/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0330 - val_loss: 0.0436\n",
      "Epoch 242/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0385 - val_loss: 0.0546\n",
      "Epoch 243/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0360 - val_loss: 0.0414\n",
      "Epoch 244/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0371 - val_loss: 0.0375\n",
      "Epoch 245/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0368 - val_loss: 0.0539\n",
      "Epoch 246/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0352 - val_loss: 0.0310\n",
      "Epoch 247/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0380 - val_loss: 0.0457\n",
      "Epoch 248/3000\n",
      "2370/2370 [==============================] - 0s 30us/step - loss: 0.0367 - val_loss: 0.0475\n",
      "Epoch 249/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0417 - val_loss: 0.0684\n",
      "Epoch 250/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0396 - val_loss: 0.0331\n",
      "Epoch 251/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0370 - val_loss: 0.0420\n",
      "Epoch 252/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0388 - val_loss: 0.0563\n",
      "Epoch 253/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0400 - val_loss: 0.0499\n",
      "Epoch 254/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0409 - val_loss: 0.0523\n",
      "Epoch 255/3000\n",
      "2370/2370 [==============================] - 0s 31us/step - loss: 0.0355 - val_loss: 0.0320\n",
      "Epoch 256/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0361 - val_loss: 0.0434\n",
      "Epoch 257/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0381 - val_loss: 0.0305\n",
      "Epoch 258/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0393 - val_loss: 0.0554\n",
      "Epoch 259/3000\n",
      "2370/2370 [==============================] - 0s 28us/step - loss: 0.0357 - val_loss: 0.0378\n",
      "Epoch 260/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0363 - val_loss: 0.0491\n",
      "Epoch 261/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0373 - val_loss: 0.0390\n",
      "Epoch 262/3000\n",
      "2370/2370 [==============================] - 0s 28us/step - loss: 0.0395 - val_loss: 0.0342\n",
      "Epoch 263/3000\n",
      "2370/2370 [==============================] - 0s 28us/step - loss: 0.0329 - val_loss: 0.0448\n",
      "Epoch 264/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0357 - val_loss: 0.0360\n",
      "Epoch 265/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0355 - val_loss: 0.0443\n",
      "Epoch 266/3000\n",
      "2370/2370 [==============================] - 0s 30us/step - loss: 0.0369 - val_loss: 0.0529\n",
      "Epoch 267/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0370 - val_loss: 0.0445\n",
      "Epoch 268/3000\n",
      "2370/2370 [==============================] - 0s 30us/step - loss: 0.0387 - val_loss: 0.0555\n",
      "Epoch 269/3000\n",
      "2370/2370 [==============================] - 0s 30us/step - loss: 0.0361 - val_loss: 0.0620\n",
      "Epoch 270/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0360 - val_loss: 0.0445\n",
      "Epoch 271/3000\n",
      "2370/2370 [==============================] - 0s 31us/step - loss: 0.0356 - val_loss: 0.0612\n",
      "Epoch 272/3000\n",
      "2370/2370 [==============================] - 0s 32us/step - loss: 0.0355 - val_loss: 0.0398\n",
      "Epoch 273/3000\n",
      "2370/2370 [==============================] - 0s 31us/step - loss: 0.0351 - val_loss: 0.0492\n",
      "Epoch 274/3000\n",
      "2370/2370 [==============================] - 0s 31us/step - loss: 0.0401 - val_loss: 0.0435\n",
      "Epoch 275/3000\n",
      "2370/2370 [==============================] - 0s 81us/step - loss: 0.0364 - val_loss: 0.0295\n",
      "Epoch 276/3000\n",
      "2370/2370 [==============================] - 0s 116us/step - loss: 0.0362 - val_loss: 0.0413\n",
      "Epoch 277/3000\n",
      "2370/2370 [==============================] - 0s 104us/step - loss: 0.0369 - val_loss: 0.0416\n",
      "Epoch 278/3000\n",
      "2370/2370 [==============================] - 0s 60us/step - loss: 0.0407 - val_loss: 0.0367\n",
      "Epoch 279/3000\n",
      "2370/2370 [==============================] - 0s 50us/step - loss: 0.0367 - val_loss: 0.0486\n",
      "Epoch 280/3000\n",
      "2370/2370 [==============================] - 0s 30us/step - loss: 0.0353 - val_loss: 0.0492\n",
      "Epoch 281/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0393 - val_loss: 0.0330\n",
      "Epoch 282/3000\n",
      "2370/2370 [==============================] - 0s 30us/step - loss: 0.0371 - val_loss: 0.0390\n",
      "Epoch 283/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0335 - val_loss: 0.0433\n",
      "Epoch 284/3000\n",
      "2370/2370 [==============================] - 0s 28us/step - loss: 0.0372 - val_loss: 0.0435\n",
      "Epoch 285/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0385 - val_loss: 0.0358\n",
      "Epoch 286/3000\n",
      "2370/2370 [==============================] - 0s 28us/step - loss: 0.0391 - val_loss: 0.0434\n",
      "Epoch 287/3000\n",
      "2370/2370 [==============================] - 0s 30us/step - loss: 0.0386 - val_loss: 0.0653\n",
      "Epoch 288/3000\n",
      "2370/2370 [==============================] - 0s 28us/step - loss: 0.0351 - val_loss: 0.0525\n",
      "Epoch 289/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0371 - val_loss: 0.0368\n",
      "Epoch 290/3000\n",
      "2370/2370 [==============================] - 0s 28us/step - loss: 0.0370 - val_loss: 0.0403\n",
      "Epoch 291/3000\n",
      "2370/2370 [==============================] - 0s 30us/step - loss: 0.0357 - val_loss: 0.0480\n",
      "Epoch 292/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0379 - val_loss: 0.0396\n",
      "Epoch 293/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0366 - val_loss: 0.0430\n",
      "Epoch 294/3000\n",
      "2370/2370 [==============================] - 0s 28us/step - loss: 0.0372 - val_loss: 0.0452\n",
      "Epoch 295/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0349 - val_loss: 0.0394\n",
      "Epoch 296/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0338 - val_loss: 0.0537\n",
      "Epoch 297/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0349 - val_loss: 0.0470\n",
      "Epoch 298/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0375 - val_loss: 0.0561\n",
      "Epoch 299/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0357 - val_loss: 0.0408\n",
      "Epoch 300/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0413 - val_loss: 0.0377\n",
      "Epoch 301/3000\n",
      "2370/2370 [==============================] - 0s 30us/step - loss: 0.0361 - val_loss: 0.0476\n",
      "Epoch 302/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0363 - val_loss: 0.0382\n",
      "Epoch 303/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0367 - val_loss: 0.0458\n",
      "Epoch 304/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0376 - val_loss: 0.0408\n",
      "Epoch 305/3000\n",
      "2370/2370 [==============================] - 0s 30us/step - loss: 0.0365 - val_loss: 0.0482\n",
      "Epoch 306/3000\n",
      "2370/2370 [==============================] - 0s 28us/step - loss: 0.0348 - val_loss: 0.0500\n",
      "Epoch 307/3000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2370/2370 [==============================] - 0s 30us/step - loss: 0.0400 - val_loss: 0.0445\n",
      "Epoch 308/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0361 - val_loss: 0.0495\n",
      "Epoch 309/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0345 - val_loss: 0.0457\n",
      "Epoch 310/3000\n",
      "2370/2370 [==============================] - 0s 30us/step - loss: 0.0350 - val_loss: 0.0340\n",
      "Epoch 311/3000\n",
      "2370/2370 [==============================] - 0s 30us/step - loss: 0.0358 - val_loss: 0.0667\n",
      "Epoch 312/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0368 - val_loss: 0.0383\n",
      "Epoch 313/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0382 - val_loss: 0.0407\n",
      "Epoch 314/3000\n",
      "2370/2370 [==============================] - 0s 28us/step - loss: 0.0368 - val_loss: 0.0571\n",
      "Epoch 315/3000\n",
      "2370/2370 [==============================] - 0s 28us/step - loss: 0.0349 - val_loss: 0.0405\n",
      "Epoch 316/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0358 - val_loss: 0.0359\n",
      "Epoch 317/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0386 - val_loss: 0.0525\n",
      "Epoch 318/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0355 - val_loss: 0.0570\n",
      "Epoch 319/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0368 - val_loss: 0.0451\n",
      "Epoch 320/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0371 - val_loss: 0.0319\n",
      "Epoch 321/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0375 - val_loss: 0.0372\n",
      "Epoch 322/3000\n",
      "2370/2370 [==============================] - 0s 28us/step - loss: 0.0372 - val_loss: 0.0452\n",
      "Epoch 323/3000\n",
      "2370/2370 [==============================] - 0s 30us/step - loss: 0.0382 - val_loss: 0.0369\n",
      "Epoch 324/3000\n",
      "2370/2370 [==============================] - 0s 30us/step - loss: 0.0331 - val_loss: 0.0561\n",
      "Epoch 325/3000\n",
      "2370/2370 [==============================] - 0s 30us/step - loss: 0.0347 - val_loss: 0.0424\n",
      "Epoch 326/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0408 - val_loss: 0.0650\n",
      "Epoch 327/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0343 - val_loss: 0.0321\n",
      "Epoch 328/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0388 - val_loss: 0.0454\n",
      "Epoch 329/3000\n",
      "2370/2370 [==============================] - 0s 28us/step - loss: 0.0339 - val_loss: 0.0323\n",
      "Epoch 330/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0391 - val_loss: 0.0474\n",
      "Epoch 331/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0373 - val_loss: 0.0388\n",
      "Epoch 332/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0346 - val_loss: 0.0591\n",
      "Epoch 333/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0354 - val_loss: 0.0570\n",
      "Epoch 334/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0386 - val_loss: 0.0573\n",
      "Epoch 335/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0359 - val_loss: 0.0483\n",
      "Epoch 336/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0340 - val_loss: 0.0500\n",
      "Epoch 337/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0349 - val_loss: 0.0491\n",
      "Epoch 338/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0346 - val_loss: 0.0413\n",
      "Epoch 339/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0393 - val_loss: 0.0543\n",
      "Epoch 340/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0357 - val_loss: 0.0460\n",
      "Epoch 341/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0366 - val_loss: 0.0424\n",
      "Epoch 342/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0362 - val_loss: 0.0398\n",
      "Epoch 343/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0364 - val_loss: 0.0579\n",
      "Epoch 344/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0332 - val_loss: 0.0323\n",
      "Epoch 345/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0389 - val_loss: 0.0459\n",
      "Epoch 346/3000\n",
      "2370/2370 [==============================] - 0s 30us/step - loss: 0.0359 - val_loss: 0.0433\n",
      "Epoch 347/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0369 - val_loss: 0.0505\n",
      "Epoch 348/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0343 - val_loss: 0.0395\n",
      "Epoch 349/3000\n",
      "2370/2370 [==============================] - 0s 28us/step - loss: 0.0328 - val_loss: 0.0360\n",
      "Epoch 350/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0350 - val_loss: 0.0483\n",
      "Epoch 351/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0399 - val_loss: 0.0533\n",
      "Epoch 352/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0355 - val_loss: 0.0341\n",
      "Epoch 353/3000\n",
      "2370/2370 [==============================] - 0s 28us/step - loss: 0.0369 - val_loss: 0.0425\n",
      "Epoch 354/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0391 - val_loss: 0.0413\n",
      "Epoch 355/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0371 - val_loss: 0.0452\n",
      "Epoch 356/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0379 - val_loss: 0.0514\n",
      "Epoch 357/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0362 - val_loss: 0.0378\n",
      "Epoch 358/3000\n",
      "2370/2370 [==============================] - 0s 30us/step - loss: 0.0381 - val_loss: 0.0445\n",
      "Epoch 359/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0342 - val_loss: 0.0538\n",
      "Epoch 360/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0372 - val_loss: 0.0692\n",
      "Epoch 361/3000\n",
      "2370/2370 [==============================] - 0s 28us/step - loss: 0.0367 - val_loss: 0.0371\n",
      "Epoch 362/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0363 - val_loss: 0.0372\n",
      "Epoch 363/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0371 - val_loss: 0.0460\n",
      "Epoch 364/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0335 - val_loss: 0.0513\n",
      "Epoch 365/3000\n",
      "2370/2370 [==============================] - 0s 28us/step - loss: 0.0347 - val_loss: 0.0444\n",
      "Epoch 366/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0373 - val_loss: 0.0478\n",
      "Epoch 367/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0427 - val_loss: 0.0333\n",
      "Epoch 368/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0390 - val_loss: 0.0431\n",
      "Epoch 369/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0369 - val_loss: 0.0313\n",
      "Epoch 370/3000\n",
      "2370/2370 [==============================] - 0s 28us/step - loss: 0.0357 - val_loss: 0.0480\n",
      "Epoch 371/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0386 - val_loss: 0.0543\n",
      "Epoch 372/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0391 - val_loss: 0.0499\n",
      "Epoch 373/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0333 - val_loss: 0.0321\n",
      "Epoch 374/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0395 - val_loss: 0.0293\n",
      "Epoch 375/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0397 - val_loss: 0.0533\n",
      "Epoch 376/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0358 - val_loss: 0.0350\n",
      "Epoch 377/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0371 - val_loss: 0.0476\n",
      "Epoch 378/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0369 - val_loss: 0.0310\n",
      "Epoch 379/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0347 - val_loss: 0.0585\n",
      "Epoch 380/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0351 - val_loss: 0.0365\n",
      "Epoch 381/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0391 - val_loss: 0.0371\n",
      "Epoch 382/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0346 - val_loss: 0.0443\n",
      "Epoch 383/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0353 - val_loss: 0.0521\n",
      "Epoch 384/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0398 - val_loss: 0.0538\n",
      "Epoch 385/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0369 - val_loss: 0.0456\n",
      "Epoch 386/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0348 - val_loss: 0.0365\n",
      "Epoch 387/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0377 - val_loss: 0.0453\n",
      "Epoch 388/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0351 - val_loss: 0.0504\n",
      "Epoch 389/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0360 - val_loss: 0.0539\n",
      "Epoch 390/3000\n",
      "2370/2370 [==============================] - 0s 28us/step - loss: 0.0340 - val_loss: 0.0416\n",
      "Epoch 391/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0349 - val_loss: 0.0603\n",
      "Epoch 392/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0366 - val_loss: 0.0615\n",
      "Epoch 393/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0379 - val_loss: 0.0609\n",
      "Epoch 394/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0354 - val_loss: 0.0548\n",
      "Epoch 395/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0360 - val_loss: 0.0516\n",
      "Epoch 396/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0344 - val_loss: 0.0591\n",
      "Epoch 397/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0393 - val_loss: 0.0534\n",
      "Epoch 398/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0360 - val_loss: 0.0454\n",
      "Epoch 399/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0353 - val_loss: 0.0370\n",
      "Epoch 400/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0354 - val_loss: 0.0490\n",
      "Epoch 401/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0361 - val_loss: 0.0374\n",
      "Epoch 402/3000\n",
      "2370/2370 [==============================] - 0s 32us/step - loss: 0.0382 - val_loss: 0.0489\n",
      "Epoch 403/3000\n",
      "2370/2370 [==============================] - 0s 34us/step - loss: 0.0370 - val_loss: 0.0345\n",
      "Epoch 404/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0349 - val_loss: 0.0434\n",
      "Epoch 405/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0389 - val_loss: 0.0440\n",
      "Epoch 406/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0358 - val_loss: 0.0405\n",
      "Epoch 407/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0355 - val_loss: 0.0630\n",
      "Epoch 408/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0397 - val_loss: 0.0473\n",
      "Epoch 409/3000\n",
      "2370/2370 [==============================] - 0s 28us/step - loss: 0.0367 - val_loss: 0.0676\n",
      "Epoch 410/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0377 - val_loss: 0.0386\n",
      "Epoch 411/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0384 - val_loss: 0.0420\n",
      "Epoch 412/3000\n",
      "2370/2370 [==============================] - 0s 31us/step - loss: 0.0391 - val_loss: 0.0444\n",
      "Epoch 413/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0344 - val_loss: 0.0406\n",
      "Epoch 414/3000\n",
      "2370/2370 [==============================] - 0s 28us/step - loss: 0.0392 - val_loss: 0.0421\n",
      "Epoch 415/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0327 - val_loss: 0.0488\n",
      "Epoch 416/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0371 - val_loss: 0.0355\n",
      "Epoch 417/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0336 - val_loss: 0.0366\n",
      "Epoch 418/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0359 - val_loss: 0.0536\n",
      "Epoch 419/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0374 - val_loss: 0.0436\n",
      "Epoch 420/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0361 - val_loss: 0.0586\n",
      "Epoch 421/3000\n",
      "2370/2370 [==============================] - 0s 33us/step - loss: 0.0355 - val_loss: 0.0510\n",
      "Epoch 422/3000\n",
      "2370/2370 [==============================] - 0s 32us/step - loss: 0.0356 - val_loss: 0.0390\n",
      "Epoch 423/3000\n",
      "2370/2370 [==============================] - 0s 31us/step - loss: 0.0346 - val_loss: 0.0420\n",
      "Epoch 424/3000\n",
      "2370/2370 [==============================] - 0s 32us/step - loss: 0.0345 - val_loss: 0.0484\n",
      "Epoch 425/3000\n",
      "2370/2370 [==============================] - 0s 32us/step - loss: 0.0357 - val_loss: 0.0414\n",
      "Epoch 426/3000\n",
      "2370/2370 [==============================] - 0s 32us/step - loss: 0.0357 - val_loss: 0.0464\n",
      "Epoch 427/3000\n",
      "2370/2370 [==============================] - 0s 31us/step - loss: 0.0402 - val_loss: 0.0460\n",
      "Epoch 428/3000\n",
      "2370/2370 [==============================] - 0s 32us/step - loss: 0.0359 - val_loss: 0.0471\n",
      "Epoch 429/3000\n",
      "2370/2370 [==============================] - 0s 31us/step - loss: 0.0347 - val_loss: 0.0530\n",
      "Epoch 430/3000\n",
      "2370/2370 [==============================] - 0s 32us/step - loss: 0.0388 - val_loss: 0.0582\n",
      "Epoch 431/3000\n",
      "2370/2370 [==============================] - 0s 32us/step - loss: 0.0352 - val_loss: 0.0503\n",
      "Epoch 432/3000\n",
      "2370/2370 [==============================] - 0s 32us/step - loss: 0.0339 - val_loss: 0.0590\n",
      "Epoch 433/3000\n",
      "2370/2370 [==============================] - 0s 32us/step - loss: 0.0376 - val_loss: 0.0584\n",
      "Epoch 434/3000\n",
      "2370/2370 [==============================] - 0s 31us/step - loss: 0.0370 - val_loss: 0.0371\n",
      "Epoch 435/3000\n",
      "2370/2370 [==============================] - 0s 31us/step - loss: 0.0386 - val_loss: 0.0442\n",
      "Epoch 436/3000\n",
      "2370/2370 [==============================] - 0s 31us/step - loss: 0.0382 - val_loss: 0.0489\n",
      "Epoch 437/3000\n",
      "2370/2370 [==============================] - 0s 31us/step - loss: 0.0331 - val_loss: 0.0338\n",
      "Epoch 438/3000\n",
      "2370/2370 [==============================] - 0s 31us/step - loss: 0.0365 - val_loss: 0.0467\n",
      "Epoch 439/3000\n",
      "2370/2370 [==============================] - 0s 30us/step - loss: 0.0370 - val_loss: 0.0533\n",
      "Epoch 440/3000\n",
      "2370/2370 [==============================] - 0s 30us/step - loss: 0.0372 - val_loss: 0.0509\n",
      "Epoch 441/3000\n",
      "2370/2370 [==============================] - 0s 31us/step - loss: 0.0336 - val_loss: 0.0466\n",
      "Epoch 442/3000\n",
      "2370/2370 [==============================] - 0s 30us/step - loss: 0.0398 - val_loss: 0.0358\n",
      "Epoch 443/3000\n",
      "2370/2370 [==============================] - 0s 31us/step - loss: 0.0389 - val_loss: 0.0678\n",
      "Epoch 444/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0381 - val_loss: 0.0443\n",
      "Epoch 445/3000\n",
      "2370/2370 [==============================] - 0s 30us/step - loss: 0.0363 - val_loss: 0.0385\n",
      "Epoch 446/3000\n",
      "2370/2370 [==============================] - 0s 30us/step - loss: 0.0362 - val_loss: 0.0420\n",
      "Epoch 447/3000\n",
      "2370/2370 [==============================] - 0s 31us/step - loss: 0.0393 - val_loss: 0.0379\n",
      "Epoch 448/3000\n",
      "2370/2370 [==============================] - 0s 30us/step - loss: 0.0359 - val_loss: 0.0570\n",
      "Epoch 449/3000\n",
      "2370/2370 [==============================] - 0s 30us/step - loss: 0.0322 - val_loss: 0.0514\n",
      "Epoch 450/3000\n",
      "2370/2370 [==============================] - 0s 30us/step - loss: 0.0339 - val_loss: 0.0300\n",
      "Epoch 451/3000\n",
      "2370/2370 [==============================] - 0s 30us/step - loss: 0.0359 - val_loss: 0.0401\n",
      "Epoch 452/3000\n",
      "2370/2370 [==============================] - 0s 30us/step - loss: 0.0352 - val_loss: 0.0535\n",
      "Epoch 453/3000\n",
      "2370/2370 [==============================] - 0s 30us/step - loss: 0.0365 - val_loss: 0.0392\n",
      "Epoch 454/3000\n",
      "2370/2370 [==============================] - 0s 31us/step - loss: 0.0372 - val_loss: 0.0535\n",
      "Epoch 455/3000\n",
      "2370/2370 [==============================] - 0s 30us/step - loss: 0.0359 - val_loss: 0.0546\n",
      "Epoch 456/3000\n",
      "2370/2370 [==============================] - 0s 30us/step - loss: 0.0352 - val_loss: 0.0500\n",
      "Epoch 457/3000\n",
      "2370/2370 [==============================] - 0s 30us/step - loss: 0.0354 - val_loss: 0.0567\n",
      "Epoch 458/3000\n",
      "2370/2370 [==============================] - 0s 31us/step - loss: 0.0343 - val_loss: 0.0343\n",
      "Epoch 459/3000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2370/2370 [==============================] - 0s 30us/step - loss: 0.0404 - val_loss: 0.0547\n",
      "Epoch 460/3000\n",
      "2370/2370 [==============================] - 0s 30us/step - loss: 0.0343 - val_loss: 0.0487\n",
      "Epoch 461/3000\n",
      "2370/2370 [==============================] - 0s 30us/step - loss: 0.0321 - val_loss: 0.0495\n",
      "Epoch 462/3000\n",
      "2370/2370 [==============================] - 0s 30us/step - loss: 0.0341 - val_loss: 0.0566\n",
      "Epoch 463/3000\n",
      "2370/2370 [==============================] - 0s 30us/step - loss: 0.0350 - val_loss: 0.0406\n",
      "Epoch 464/3000\n",
      "2370/2370 [==============================] - 0s 30us/step - loss: 0.0344 - val_loss: 0.0436\n",
      "Epoch 465/3000\n",
      "2370/2370 [==============================] - 0s 30us/step - loss: 0.0376 - val_loss: 0.0527\n",
      "Epoch 466/3000\n",
      "2370/2370 [==============================] - 0s 30us/step - loss: 0.0349 - val_loss: 0.0328\n",
      "Epoch 467/3000\n",
      "2370/2370 [==============================] - 0s 30us/step - loss: 0.0351 - val_loss: 0.0483\n",
      "Epoch 468/3000\n",
      "2370/2370 [==============================] - 0s 30us/step - loss: 0.0366 - val_loss: 0.0424\n",
      "Epoch 469/3000\n",
      "2370/2370 [==============================] - 0s 30us/step - loss: 0.0372 - val_loss: 0.0599\n",
      "Epoch 470/3000\n",
      "2370/2370 [==============================] - 0s 30us/step - loss: 0.0379 - val_loss: 0.0543\n",
      "Epoch 471/3000\n",
      "2370/2370 [==============================] - 0s 31us/step - loss: 0.0347 - val_loss: 0.0512\n",
      "Epoch 472/3000\n",
      "2370/2370 [==============================] - 0s 30us/step - loss: 0.0352 - val_loss: 0.0433\n",
      "Epoch 473/3000\n",
      "2370/2370 [==============================] - 0s 30us/step - loss: 0.0360 - val_loss: 0.0497\n",
      "Epoch 474/3000\n",
      "2370/2370 [==============================] - 0s 30us/step - loss: 0.0391 - val_loss: 0.0555\n",
      "Epoch 475/3000\n",
      "2370/2370 [==============================] - 0s 30us/step - loss: 0.0335 - val_loss: 0.0490\n",
      "Epoch 476/3000\n",
      "2370/2370 [==============================] - 0s 30us/step - loss: 0.0350 - val_loss: 0.0457\n",
      "Epoch 477/3000\n",
      "2370/2370 [==============================] - 0s 30us/step - loss: 0.0356 - val_loss: 0.0422\n",
      "Epoch 478/3000\n",
      "2370/2370 [==============================] - 0s 31us/step - loss: 0.0345 - val_loss: 0.0455\n",
      "Epoch 479/3000\n",
      "2370/2370 [==============================] - 0s 57us/step - loss: 0.0364 - val_loss: 0.0579\n",
      "Epoch 480/3000\n",
      "2370/2370 [==============================] - 0s 122us/step - loss: 0.0354 - val_loss: 0.0504\n",
      "Epoch 481/3000\n",
      "2370/2370 [==============================] - 0s 131us/step - loss: 0.0349 - val_loss: 0.0473\n",
      "Epoch 482/3000\n",
      "2370/2370 [==============================] - 0s 145us/step - loss: 0.0394 - val_loss: 0.0519\n",
      "Epoch 483/3000\n",
      "2370/2370 [==============================] - 0s 105us/step - loss: 0.0364 - val_loss: 0.0422\n",
      "Epoch 484/3000\n",
      "2370/2370 [==============================] - 0s 82us/step - loss: 0.0348 - val_loss: 0.0551\n",
      "Epoch 485/3000\n",
      "2370/2370 [==============================] - 0s 40us/step - loss: 0.0368 - val_loss: 0.0722\n",
      "Epoch 486/3000\n",
      "2370/2370 [==============================] - 0s 32us/step - loss: 0.0388 - val_loss: 0.0411\n",
      "Epoch 487/3000\n",
      "2370/2370 [==============================] - 0s 33us/step - loss: 0.0332 - val_loss: 0.0475\n",
      "Epoch 488/3000\n",
      "2370/2370 [==============================] - 0s 32us/step - loss: 0.0326 - val_loss: 0.0580\n",
      "Epoch 489/3000\n",
      "2370/2370 [==============================] - 0s 32us/step - loss: 0.0373 - val_loss: 0.0611\n",
      "Epoch 490/3000\n",
      "2370/2370 [==============================] - 0s 33us/step - loss: 0.0353 - val_loss: 0.0601\n",
      "Epoch 491/3000\n",
      "2370/2370 [==============================] - 0s 31us/step - loss: 0.0355 - val_loss: 0.0411\n",
      "Epoch 492/3000\n",
      "2370/2370 [==============================] - 0s 31us/step - loss: 0.0347 - val_loss: 0.0418\n",
      "Epoch 493/3000\n",
      "2370/2370 [==============================] - 0s 32us/step - loss: 0.0379 - val_loss: 0.0448\n",
      "Epoch 494/3000\n",
      "2370/2370 [==============================] - 0s 31us/step - loss: 0.0344 - val_loss: 0.0498\n",
      "Epoch 495/3000\n",
      "2370/2370 [==============================] - 0s 31us/step - loss: 0.0354 - val_loss: 0.0411\n",
      "Epoch 496/3000\n",
      "2370/2370 [==============================] - 0s 32us/step - loss: 0.0360 - val_loss: 0.0615\n",
      "Epoch 497/3000\n",
      "2370/2370 [==============================] - 0s 33us/step - loss: 0.0356 - val_loss: 0.0559\n",
      "Epoch 498/3000\n",
      "2370/2370 [==============================] - 0s 35us/step - loss: 0.0359 - val_loss: 0.0629\n",
      "Epoch 499/3000\n",
      "2370/2370 [==============================] - 0s 32us/step - loss: 0.0361 - val_loss: 0.0449\n",
      "Epoch 500/3000\n",
      "2370/2370 [==============================] - 0s 31us/step - loss: 0.0356 - val_loss: 0.0512\n",
      "Epoch 501/3000\n",
      "2370/2370 [==============================] - 0s 32us/step - loss: 0.0377 - val_loss: 0.0402\n",
      "Epoch 502/3000\n",
      "2370/2370 [==============================] - 0s 31us/step - loss: 0.0386 - val_loss: 0.0380\n",
      "Epoch 503/3000\n",
      "2370/2370 [==============================] - 0s 32us/step - loss: 0.0385 - val_loss: 0.0355\n",
      "Epoch 504/3000\n",
      "2370/2370 [==============================] - 0s 30us/step - loss: 0.0375 - val_loss: 0.0386\n",
      "Epoch 505/3000\n",
      "2370/2370 [==============================] - 0s 31us/step - loss: 0.0343 - val_loss: 0.0623\n",
      "Epoch 506/3000\n",
      "2370/2370 [==============================] - 0s 30us/step - loss: 0.0384 - val_loss: 0.0483\n",
      "Epoch 507/3000\n",
      "2370/2370 [==============================] - 0s 30us/step - loss: 0.0368 - val_loss: 0.0420\n",
      "Epoch 508/3000\n",
      "2370/2370 [==============================] - 0s 30us/step - loss: 0.0380 - val_loss: 0.0435\n",
      "Epoch 509/3000\n",
      "2370/2370 [==============================] - 0s 30us/step - loss: 0.0377 - val_loss: 0.0649\n",
      "Epoch 510/3000\n",
      "2370/2370 [==============================] - 0s 30us/step - loss: 0.0391 - val_loss: 0.0514\n",
      "Epoch 511/3000\n",
      "2370/2370 [==============================] - 0s 30us/step - loss: 0.0316 - val_loss: 0.0450\n",
      "Epoch 512/3000\n",
      "2370/2370 [==============================] - 0s 30us/step - loss: 0.0350 - val_loss: 0.0575\n",
      "Epoch 513/3000\n",
      "2370/2370 [==============================] - 0s 30us/step - loss: 0.0368 - val_loss: 0.0503\n",
      "Epoch 514/3000\n",
      "2370/2370 [==============================] - 0s 30us/step - loss: 0.0334 - val_loss: 0.0367\n",
      "Epoch 515/3000\n",
      "2370/2370 [==============================] - 0s 30us/step - loss: 0.0347 - val_loss: 0.0367\n",
      "Epoch 516/3000\n",
      "2370/2370 [==============================] - 0s 31us/step - loss: 0.0374 - val_loss: 0.0476\n",
      "Epoch 517/3000\n",
      "2370/2370 [==============================] - 0s 30us/step - loss: 0.0386 - val_loss: 0.0341\n",
      "Epoch 518/3000\n",
      "2370/2370 [==============================] - 0s 30us/step - loss: 0.0367 - val_loss: 0.0382\n",
      "Epoch 519/3000\n",
      "2370/2370 [==============================] - 0s 30us/step - loss: 0.0389 - val_loss: 0.0419\n",
      "Epoch 520/3000\n",
      "2370/2370 [==============================] - 0s 30us/step - loss: 0.0390 - val_loss: 0.0522\n",
      "Epoch 521/3000\n",
      "2370/2370 [==============================] - 0s 30us/step - loss: 0.0409 - val_loss: 0.0472\n",
      "Epoch 522/3000\n",
      "2370/2370 [==============================] - 0s 30us/step - loss: 0.0381 - val_loss: 0.0395\n",
      "Epoch 523/3000\n",
      "2370/2370 [==============================] - 0s 30us/step - loss: 0.0360 - val_loss: 0.0591\n",
      "Epoch 524/3000\n",
      "2370/2370 [==============================] - 0s 31us/step - loss: 0.0392 - val_loss: 0.0383\n",
      "Epoch 525/3000\n",
      "2370/2370 [==============================] - 0s 30us/step - loss: 0.0357 - val_loss: 0.0431\n",
      "Epoch 526/3000\n",
      "2370/2370 [==============================] - 0s 30us/step - loss: 0.0348 - val_loss: 0.0561\n",
      "Epoch 527/3000\n",
      "2370/2370 [==============================] - 0s 30us/step - loss: 0.0334 - val_loss: 0.0579\n",
      "Epoch 528/3000\n",
      "2370/2370 [==============================] - 0s 31us/step - loss: 0.0366 - val_loss: 0.0535\n",
      "Epoch 529/3000\n",
      "2370/2370 [==============================] - 0s 30us/step - loss: 0.0375 - val_loss: 0.0586\n",
      "Epoch 530/3000\n",
      "2370/2370 [==============================] - 0s 30us/step - loss: 0.0357 - val_loss: 0.0330\n",
      "Epoch 531/3000\n",
      "2370/2370 [==============================] - 0s 30us/step - loss: 0.0363 - val_loss: 0.0427\n",
      "Epoch 532/3000\n",
      "2370/2370 [==============================] - 0s 30us/step - loss: 0.0361 - val_loss: 0.0450\n",
      "Epoch 533/3000\n",
      "2370/2370 [==============================] - 0s 30us/step - loss: 0.0361 - val_loss: 0.0449\n",
      "Epoch 534/3000\n",
      "2370/2370 [==============================] - 0s 30us/step - loss: 0.0373 - val_loss: 0.0408\n",
      "Epoch 535/3000\n",
      "2370/2370 [==============================] - 0s 31us/step - loss: 0.0370 - val_loss: 0.0553\n",
      "Epoch 536/3000\n",
      "2370/2370 [==============================] - 0s 30us/step - loss: 0.0359 - val_loss: 0.0456\n",
      "Epoch 537/3000\n",
      "2370/2370 [==============================] - 0s 30us/step - loss: 0.0382 - val_loss: 0.0561\n",
      "Epoch 538/3000\n",
      "2370/2370 [==============================] - 0s 31us/step - loss: 0.0355 - val_loss: 0.0540\n",
      "Epoch 539/3000\n",
      "2370/2370 [==============================] - 0s 30us/step - loss: 0.0353 - val_loss: 0.0516\n",
      "Epoch 540/3000\n",
      "2370/2370 [==============================] - 0s 30us/step - loss: 0.0391 - val_loss: 0.0421\n",
      "Epoch 541/3000\n",
      "2370/2370 [==============================] - 0s 30us/step - loss: 0.0351 - val_loss: 0.0491\n",
      "Epoch 542/3000\n",
      "2370/2370 [==============================] - 0s 31us/step - loss: 0.0429 - val_loss: 0.0413\n",
      "Epoch 543/3000\n",
      "2370/2370 [==============================] - 0s 31us/step - loss: 0.0406 - val_loss: 0.0527\n",
      "Epoch 544/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0389 - val_loss: 0.0431\n",
      "Epoch 545/3000\n",
      "2370/2370 [==============================] - 0s 30us/step - loss: 0.0376 - val_loss: 0.0566\n",
      "Epoch 546/3000\n",
      "2370/2370 [==============================] - 0s 30us/step - loss: 0.0357 - val_loss: 0.0510\n",
      "Epoch 547/3000\n",
      "2370/2370 [==============================] - 0s 30us/step - loss: 0.0360 - val_loss: 0.0464\n",
      "Epoch 548/3000\n",
      "2370/2370 [==============================] - 0s 30us/step - loss: 0.0367 - val_loss: 0.0474\n",
      "Epoch 549/3000\n",
      "2370/2370 [==============================] - 0s 30us/step - loss: 0.0353 - val_loss: 0.0505\n",
      "Epoch 550/3000\n",
      "2370/2370 [==============================] - 0s 30us/step - loss: 0.0379 - val_loss: 0.0338\n",
      "Epoch 551/3000\n",
      "2370/2370 [==============================] - 0s 31us/step - loss: 0.0369 - val_loss: 0.0488\n",
      "Epoch 552/3000\n",
      "2370/2370 [==============================] - 0s 30us/step - loss: 0.0366 - val_loss: 0.0367\n",
      "Epoch 553/3000\n",
      "2370/2370 [==============================] - 0s 30us/step - loss: 0.0385 - val_loss: 0.0532\n",
      "Epoch 554/3000\n",
      "2370/2370 [==============================] - 0s 31us/step - loss: 0.0376 - val_loss: 0.0497\n",
      "Epoch 555/3000\n",
      "2370/2370 [==============================] - 0s 30us/step - loss: 0.0346 - val_loss: 0.0407\n",
      "Epoch 556/3000\n",
      "2370/2370 [==============================] - 0s 30us/step - loss: 0.0364 - val_loss: 0.0646\n",
      "Epoch 557/3000\n",
      "2370/2370 [==============================] - 0s 30us/step - loss: 0.0334 - val_loss: 0.0465\n",
      "Epoch 558/3000\n",
      "2370/2370 [==============================] - 0s 34us/step - loss: 0.0352 - val_loss: 0.0493\n",
      "Epoch 559/3000\n",
      "2370/2370 [==============================] - 0s 35us/step - loss: 0.0358 - val_loss: 0.0412\n",
      "Epoch 560/3000\n",
      "2370/2370 [==============================] - 0s 34us/step - loss: 0.0382 - val_loss: 0.0573\n",
      "Epoch 561/3000\n",
      "2370/2370 [==============================] - 0s 33us/step - loss: 0.0357 - val_loss: 0.0368\n",
      "Epoch 562/3000\n",
      "2370/2370 [==============================] - 0s 33us/step - loss: 0.0355 - val_loss: 0.0500\n",
      "Epoch 563/3000\n",
      "2370/2370 [==============================] - 0s 34us/step - loss: 0.0352 - val_loss: 0.0487\n",
      "Epoch 564/3000\n",
      "2370/2370 [==============================] - 0s 34us/step - loss: 0.0350 - val_loss: 0.0533\n",
      "Epoch 565/3000\n",
      "2370/2370 [==============================] - 0s 33us/step - loss: 0.0361 - val_loss: 0.0455\n",
      "Epoch 566/3000\n",
      "2370/2370 [==============================] - 0s 33us/step - loss: 0.0381 - val_loss: 0.0526\n",
      "Epoch 567/3000\n",
      "2370/2370 [==============================] - 0s 33us/step - loss: 0.0358 - val_loss: 0.0487\n",
      "Epoch 568/3000\n",
      "2370/2370 [==============================] - 0s 33us/step - loss: 0.0359 - val_loss: 0.0492\n",
      "Epoch 569/3000\n",
      "2370/2370 [==============================] - 0s 34us/step - loss: 0.0385 - val_loss: 0.0643\n",
      "Epoch 570/3000\n",
      "2370/2370 [==============================] - 0s 33us/step - loss: 0.0379 - val_loss: 0.0445\n",
      "Epoch 571/3000\n",
      "2370/2370 [==============================] - 0s 34us/step - loss: 0.0361 - val_loss: 0.0462\n",
      "Epoch 572/3000\n",
      "2370/2370 [==============================] - 0s 33us/step - loss: 0.0355 - val_loss: 0.0556\n",
      "Epoch 573/3000\n",
      "2370/2370 [==============================] - 0s 33us/step - loss: 0.0346 - val_loss: 0.0529\n",
      "Epoch 574/3000\n",
      "2370/2370 [==============================] - 0s 33us/step - loss: 0.0418 - val_loss: 0.0407\n",
      "Epoch 575/3000\n",
      "2370/2370 [==============================] - 0s 34us/step - loss: 0.0328 - val_loss: 0.0469\n",
      "Epoch 576/3000\n",
      "2370/2370 [==============================] - 0s 33us/step - loss: 0.0386 - val_loss: 0.0550\n",
      "Epoch 577/3000\n",
      "2370/2370 [==============================] - 0s 34us/step - loss: 0.0396 - val_loss: 0.0508\n",
      "Epoch 578/3000\n",
      "2370/2370 [==============================] - 0s 34us/step - loss: 0.0362 - val_loss: 0.0497\n",
      "Epoch 579/3000\n",
      "2370/2370 [==============================] - 0s 33us/step - loss: 0.0385 - val_loss: 0.0471\n",
      "Epoch 580/3000\n",
      "2370/2370 [==============================] - 0s 33us/step - loss: 0.0361 - val_loss: 0.0409\n",
      "Epoch 581/3000\n",
      "2370/2370 [==============================] - 0s 33us/step - loss: 0.0376 - val_loss: 0.0583\n",
      "Epoch 582/3000\n",
      "2370/2370 [==============================] - 0s 32us/step - loss: 0.0333 - val_loss: 0.0531\n",
      "Epoch 583/3000\n",
      "2370/2370 [==============================] - 0s 31us/step - loss: 0.0358 - val_loss: 0.0493\n",
      "Epoch 584/3000\n",
      "2370/2370 [==============================] - 0s 31us/step - loss: 0.0395 - val_loss: 0.0381\n",
      "Epoch 585/3000\n",
      "2370/2370 [==============================] - 0s 32us/step - loss: 0.0383 - val_loss: 0.0404\n",
      "Epoch 586/3000\n",
      "2370/2370 [==============================] - 0s 32us/step - loss: 0.0392 - val_loss: 0.0452\n",
      "Epoch 587/3000\n",
      "2370/2370 [==============================] - 0s 32us/step - loss: 0.0363 - val_loss: 0.0513\n",
      "Epoch 588/3000\n",
      "2370/2370 [==============================] - 0s 32us/step - loss: 0.0365 - val_loss: 0.0549\n",
      "Epoch 589/3000\n",
      "2370/2370 [==============================] - 0s 31us/step - loss: 0.0360 - val_loss: 0.0376\n",
      "Epoch 590/3000\n",
      "2370/2370 [==============================] - 0s 32us/step - loss: 0.0362 - val_loss: 0.0415\n",
      "Epoch 591/3000\n",
      "2370/2370 [==============================] - 0s 32us/step - loss: 0.0342 - val_loss: 0.0443\n",
      "Epoch 592/3000\n",
      "2370/2370 [==============================] - 0s 32us/step - loss: 0.0376 - val_loss: 0.0421\n",
      "Epoch 593/3000\n",
      "2370/2370 [==============================] - 0s 31us/step - loss: 0.0324 - val_loss: 0.0491\n",
      "Epoch 594/3000\n",
      "2370/2370 [==============================] - 0s 32us/step - loss: 0.0344 - val_loss: 0.0479\n",
      "Epoch 595/3000\n",
      "2370/2370 [==============================] - 0s 32us/step - loss: 0.0375 - val_loss: 0.0445\n",
      "Epoch 596/3000\n",
      "2370/2370 [==============================] - 0s 32us/step - loss: 0.0379 - val_loss: 0.0363\n",
      "Epoch 597/3000\n",
      "2370/2370 [==============================] - 0s 32us/step - loss: 0.0305 - val_loss: 0.0411\n",
      "Epoch 598/3000\n",
      "2370/2370 [==============================] - 0s 32us/step - loss: 0.0357 - val_loss: 0.0494\n",
      "Epoch 599/3000\n",
      "2370/2370 [==============================] - 0s 32us/step - loss: 0.0395 - val_loss: 0.0437\n",
      "Epoch 600/3000\n",
      "2370/2370 [==============================] - 0s 31us/step - loss: 0.0340 - val_loss: 0.0520\n",
      "Epoch 601/3000\n",
      "2370/2370 [==============================] - 0s 32us/step - loss: 0.0342 - val_loss: 0.0604\n",
      "Epoch 602/3000\n",
      "2370/2370 [==============================] - 0s 31us/step - loss: 0.0372 - val_loss: 0.0567\n",
      "Epoch 603/3000\n",
      "2370/2370 [==============================] - 0s 31us/step - loss: 0.0363 - val_loss: 0.0449\n",
      "Epoch 604/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0368 - val_loss: 0.0552\n",
      "Epoch 605/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0357 - val_loss: 0.0477\n",
      "Epoch 606/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0373 - val_loss: 0.0478\n",
      "Epoch 607/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0381 - val_loss: 0.0656\n",
      "Epoch 608/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0382 - val_loss: 0.0713\n",
      "Epoch 609/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0351 - val_loss: 0.0497\n",
      "Epoch 610/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0383 - val_loss: 0.0504\n",
      "Epoch 611/3000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0341 - val_loss: 0.0542\n",
      "Epoch 612/3000\n",
      "2370/2370 [==============================] - 0s 30us/step - loss: 0.0374 - val_loss: 0.0479\n",
      "Epoch 613/3000\n",
      "2370/2370 [==============================] - 0s 30us/step - loss: 0.0358 - val_loss: 0.0598\n",
      "Epoch 614/3000\n",
      "2370/2370 [==============================] - 0s 30us/step - loss: 0.0362 - val_loss: 0.0510\n",
      "Epoch 615/3000\n",
      "2370/2370 [==============================] - 0s 34us/step - loss: 0.0394 - val_loss: 0.0372\n",
      "Epoch 616/3000\n",
      "2370/2370 [==============================] - 0s 32us/step - loss: 0.0350 - val_loss: 0.0390\n",
      "Epoch 617/3000\n",
      "2370/2370 [==============================] - 0s 32us/step - loss: 0.0345 - val_loss: 0.0645\n",
      "Epoch 618/3000\n",
      "2370/2370 [==============================] - 0s 30us/step - loss: 0.0353 - val_loss: 0.0506\n",
      "Epoch 619/3000\n",
      "2370/2370 [==============================] - 0s 31us/step - loss: 0.0349 - val_loss: 0.0485\n",
      "Epoch 620/3000\n",
      "2370/2370 [==============================] - 0s 31us/step - loss: 0.0328 - val_loss: 0.0490\n",
      "Epoch 621/3000\n",
      "2370/2370 [==============================] - 0s 31us/step - loss: 0.0350 - val_loss: 0.0490\n",
      "Epoch 622/3000\n",
      "2370/2370 [==============================] - 0s 31us/step - loss: 0.0361 - val_loss: 0.0538\n",
      "Epoch 623/3000\n",
      "2370/2370 [==============================] - 0s 31us/step - loss: 0.0371 - val_loss: 0.0363\n",
      "Epoch 624/3000\n",
      "2370/2370 [==============================] - 0s 31us/step - loss: 0.0345 - val_loss: 0.0412\n",
      "Epoch 625/3000\n",
      "2370/2370 [==============================] - 0s 32us/step - loss: 0.0370 - val_loss: 0.0484\n",
      "Epoch 626/3000\n",
      "2370/2370 [==============================] - 0s 31us/step - loss: 0.0373 - val_loss: 0.0495\n",
      "Epoch 627/3000\n",
      "2370/2370 [==============================] - 0s 30us/step - loss: 0.0366 - val_loss: 0.0536\n",
      "Epoch 628/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0355 - val_loss: 0.0450\n",
      "Epoch 629/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0358 - val_loss: 0.0549\n",
      "Epoch 630/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0323 - val_loss: 0.0535\n",
      "Epoch 631/3000\n",
      "2370/2370 [==============================] - 0s 31us/step - loss: 0.0402 - val_loss: 0.0496\n",
      "Epoch 632/3000\n",
      "2370/2370 [==============================] - 0s 28us/step - loss: 0.0352 - val_loss: 0.0440\n",
      "Epoch 633/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0349 - val_loss: 0.0500\n",
      "Epoch 634/3000\n",
      "2370/2370 [==============================] - 0s 30us/step - loss: 0.0381 - val_loss: 0.0461\n",
      "Epoch 635/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0380 - val_loss: 0.0504\n",
      "Epoch 636/3000\n",
      "2370/2370 [==============================] - 0s 30us/step - loss: 0.0377 - val_loss: 0.0506\n",
      "Epoch 637/3000\n",
      "2370/2370 [==============================] - 0s 30us/step - loss: 0.0391 - val_loss: 0.0390\n",
      "Epoch 638/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0322 - val_loss: 0.0342\n",
      "Epoch 639/3000\n",
      "2370/2370 [==============================] - 0s 28us/step - loss: 0.0352 - val_loss: 0.0477\n",
      "Epoch 640/3000\n",
      "2370/2370 [==============================] - 0s 71us/step - loss: 0.0371 - val_loss: 0.0317\n",
      "Epoch 641/3000\n",
      "2370/2370 [==============================] - 0s 34us/step - loss: 0.0384 - val_loss: 0.0607\n",
      "Epoch 642/3000\n",
      "2370/2370 [==============================] - 0s 33us/step - loss: 0.0352 - val_loss: 0.0538\n",
      "Epoch 643/3000\n",
      "2370/2370 [==============================] - 0s 33us/step - loss: 0.0335 - val_loss: 0.0451\n",
      "Epoch 644/3000\n",
      "2370/2370 [==============================] - 0s 82us/step - loss: 0.0379 - val_loss: 0.0367\n",
      "Epoch 645/3000\n",
      "2370/2370 [==============================] - 0s 33us/step - loss: 0.0358 - val_loss: 0.0428\n",
      "Epoch 646/3000\n",
      "2370/2370 [==============================] - 0s 30us/step - loss: 0.0358 - val_loss: 0.0444\n",
      "Epoch 647/3000\n",
      "2370/2370 [==============================] - 0s 32us/step - loss: 0.0343 - val_loss: 0.0706\n",
      "Epoch 648/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0369 - val_loss: 0.0542\n",
      "Epoch 649/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0333 - val_loss: 0.0436\n",
      "Epoch 650/3000\n",
      "2370/2370 [==============================] - 0s 72us/step - loss: 0.0369 - val_loss: 0.0383\n",
      "Epoch 651/3000\n",
      "2370/2370 [==============================] - 0s 46us/step - loss: 0.0360 - val_loss: 0.0567\n",
      "Epoch 652/3000\n",
      "2370/2370 [==============================] - 0s 34us/step - loss: 0.0382 - val_loss: 0.0369\n",
      "Epoch 653/3000\n",
      "2370/2370 [==============================] - 0s 31us/step - loss: 0.0368 - val_loss: 0.0363\n",
      "Epoch 654/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0359 - val_loss: 0.0628\n",
      "Epoch 655/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0361 - val_loss: 0.0396\n",
      "Epoch 656/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0347 - val_loss: 0.0503\n",
      "Epoch 657/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0375 - val_loss: 0.0373\n",
      "Epoch 658/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0374 - val_loss: 0.0508\n",
      "Epoch 659/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0339 - val_loss: 0.0467\n",
      "Epoch 660/3000\n",
      "2370/2370 [==============================] - 0s 28us/step - loss: 0.0392 - val_loss: 0.0487\n",
      "Epoch 661/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0341 - val_loss: 0.0437\n",
      "Epoch 662/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0390 - val_loss: 0.0504\n",
      "Epoch 663/3000\n",
      "2370/2370 [==============================] - 0s 30us/step - loss: 0.0379 - val_loss: 0.0447\n",
      "Epoch 664/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0362 - val_loss: 0.0581\n",
      "Epoch 665/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0364 - val_loss: 0.0550\n",
      "Epoch 666/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0360 - val_loss: 0.0403\n",
      "Epoch 667/3000\n",
      "2370/2370 [==============================] - 0s 31us/step - loss: 0.0348 - val_loss: 0.0469\n",
      "Epoch 668/3000\n",
      "2370/2370 [==============================] - 0s 32us/step - loss: 0.0388 - val_loss: 0.0569\n",
      "Epoch 669/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0363 - val_loss: 0.0559\n",
      "Epoch 670/3000\n",
      "2370/2370 [==============================] - 0s 31us/step - loss: 0.0353 - val_loss: 0.0431\n",
      "Epoch 671/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0321 - val_loss: 0.0461\n",
      "Epoch 672/3000\n",
      "2370/2370 [==============================] - 0s 31us/step - loss: 0.0334 - val_loss: 0.0461\n",
      "Epoch 673/3000\n",
      "2370/2370 [==============================] - 0s 32us/step - loss: 0.0337 - val_loss: 0.0429\n",
      "Epoch 674/3000\n",
      "2370/2370 [==============================] - 0s 32us/step - loss: 0.0337 - val_loss: 0.0565\n",
      "Epoch 675/3000\n",
      "2370/2370 [==============================] - 0s 32us/step - loss: 0.0363 - val_loss: 0.0539\n",
      "Epoch 676/3000\n",
      "2370/2370 [==============================] - 0s 32us/step - loss: 0.0382 - val_loss: 0.0704\n",
      "Epoch 677/3000\n",
      "2370/2370 [==============================] - 0s 32us/step - loss: 0.0350 - val_loss: 0.0495\n",
      "Epoch 678/3000\n",
      "2370/2370 [==============================] - 0s 33us/step - loss: 0.0367 - val_loss: 0.0450\n",
      "Epoch 679/3000\n",
      "2370/2370 [==============================] - 0s 30us/step - loss: 0.0370 - val_loss: 0.0378\n",
      "Epoch 680/3000\n",
      "2370/2370 [==============================] - 0s 28us/step - loss: 0.0350 - val_loss: 0.0687\n",
      "Epoch 681/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0381 - val_loss: 0.0395\n",
      "Epoch 682/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0372 - val_loss: 0.0367\n",
      "Epoch 683/3000\n",
      "2370/2370 [==============================] - 0s 28us/step - loss: 0.0334 - val_loss: 0.0570\n",
      "Epoch 684/3000\n",
      "2370/2370 [==============================] - 0s 28us/step - loss: 0.0366 - val_loss: 0.0404\n",
      "Epoch 685/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0340 - val_loss: 0.0509\n",
      "Epoch 686/3000\n",
      "2370/2370 [==============================] - 0s 31us/step - loss: 0.0361 - val_loss: 0.0475\n",
      "Epoch 687/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0378 - val_loss: 0.0452\n",
      "Epoch 688/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0363 - val_loss: 0.0635\n",
      "Epoch 689/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0345 - val_loss: 0.0609\n",
      "Epoch 690/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0370 - val_loss: 0.0487\n",
      "Epoch 691/3000\n",
      "2370/2370 [==============================] - 0s 28us/step - loss: 0.0380 - val_loss: 0.0410\n",
      "Epoch 692/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0378 - val_loss: 0.0382\n",
      "Epoch 693/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0352 - val_loss: 0.0473\n",
      "Epoch 694/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0373 - val_loss: 0.0371\n",
      "Epoch 695/3000\n",
      "2370/2370 [==============================] - 0s 30us/step - loss: 0.0336 - val_loss: 0.0377\n",
      "Epoch 696/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0359 - val_loss: 0.0495\n",
      "Epoch 697/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0365 - val_loss: 0.0459\n",
      "Epoch 698/3000\n",
      "2370/2370 [==============================] - 0s 30us/step - loss: 0.0360 - val_loss: 0.0460\n",
      "Epoch 699/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0379 - val_loss: 0.0522\n",
      "Epoch 700/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0391 - val_loss: 0.0380\n",
      "Epoch 701/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0381 - val_loss: 0.0371\n",
      "Epoch 702/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0361 - val_loss: 0.0480\n",
      "Epoch 703/3000\n",
      "2370/2370 [==============================] - 0s 28us/step - loss: 0.0373 - val_loss: 0.0419\n",
      "Epoch 704/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0359 - val_loss: 0.0504\n",
      "Epoch 705/3000\n",
      "2370/2370 [==============================] - 0s 28us/step - loss: 0.0387 - val_loss: 0.0360\n",
      "Epoch 706/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0373 - val_loss: 0.0355\n",
      "Epoch 707/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0356 - val_loss: 0.0455\n",
      "Epoch 708/3000\n",
      "2370/2370 [==============================] - 0s 28us/step - loss: 0.0365 - val_loss: 0.0424\n",
      "Epoch 709/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0357 - val_loss: 0.0481\n",
      "Epoch 710/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0351 - val_loss: 0.0461\n",
      "Epoch 711/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0316 - val_loss: 0.0570\n",
      "Epoch 712/3000\n",
      "2370/2370 [==============================] - 0s 28us/step - loss: 0.0364 - val_loss: 0.0575\n",
      "Epoch 713/3000\n",
      "2370/2370 [==============================] - 0s 28us/step - loss: 0.0368 - val_loss: 0.0371\n",
      "Epoch 714/3000\n",
      "2370/2370 [==============================] - 0s 28us/step - loss: 0.0357 - val_loss: 0.0494\n",
      "Epoch 715/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0359 - val_loss: 0.0530\n",
      "Epoch 716/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0374 - val_loss: 0.0405\n",
      "Epoch 717/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0369 - val_loss: 0.0526\n",
      "Epoch 718/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0339 - val_loss: 0.0455\n",
      "Epoch 719/3000\n",
      "2370/2370 [==============================] - 0s 28us/step - loss: 0.0361 - val_loss: 0.0436\n",
      "Epoch 720/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0338 - val_loss: 0.0387\n",
      "Epoch 721/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0363 - val_loss: 0.0444\n",
      "Epoch 722/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0364 - val_loss: 0.0522\n",
      "Epoch 723/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0365 - val_loss: 0.0452\n",
      "Epoch 724/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0386 - val_loss: 0.0472\n",
      "Epoch 725/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0369 - val_loss: 0.0520\n",
      "Epoch 726/3000\n",
      "2370/2370 [==============================] - 0s 28us/step - loss: 0.0333 - val_loss: 0.0576\n",
      "Epoch 727/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0391 - val_loss: 0.0583\n",
      "Epoch 728/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0340 - val_loss: 0.0444\n",
      "Epoch 729/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0349 - val_loss: 0.0434\n",
      "Epoch 730/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0386 - val_loss: 0.0317\n",
      "Epoch 731/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0397 - val_loss: 0.0409\n",
      "Epoch 732/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0351 - val_loss: 0.0658\n",
      "Epoch 733/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0371 - val_loss: 0.0415\n",
      "Epoch 734/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0374 - val_loss: 0.0503\n",
      "Epoch 735/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0371 - val_loss: 0.0568\n",
      "Epoch 736/3000\n",
      "2370/2370 [==============================] - 0s 32us/step - loss: 0.0352 - val_loss: 0.0478\n",
      "Epoch 737/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0339 - val_loss: 0.0469\n",
      "Epoch 738/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0363 - val_loss: 0.0524\n",
      "Epoch 739/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0374 - val_loss: 0.0624\n",
      "Epoch 740/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0378 - val_loss: 0.0340\n",
      "Epoch 741/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0385 - val_loss: 0.0468\n",
      "Epoch 742/3000\n",
      "2370/2370 [==============================] - 0s 28us/step - loss: 0.0332 - val_loss: 0.0410\n",
      "Epoch 743/3000\n",
      "2370/2370 [==============================] - 0s 30us/step - loss: 0.0311 - val_loss: 0.0493\n",
      "Epoch 744/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0364 - val_loss: 0.0451\n",
      "Epoch 745/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0332 - val_loss: 0.0446\n",
      "Epoch 746/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0402 - val_loss: 0.0529\n",
      "Epoch 747/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0354 - val_loss: 0.0543\n",
      "Epoch 748/3000\n",
      "2370/2370 [==============================] - 0s 28us/step - loss: 0.0367 - val_loss: 0.0466\n",
      "Epoch 749/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0325 - val_loss: 0.0353\n",
      "Epoch 750/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0364 - val_loss: 0.0487\n",
      "Epoch 751/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0406 - val_loss: 0.0497\n",
      "Epoch 752/3000\n",
      "2370/2370 [==============================] - 0s 30us/step - loss: 0.0392 - val_loss: 0.0507\n",
      "Epoch 753/3000\n",
      "2370/2370 [==============================] - 0s 28us/step - loss: 0.0357 - val_loss: 0.0473\n",
      "Epoch 754/3000\n",
      "2370/2370 [==============================] - 0s 28us/step - loss: 0.0331 - val_loss: 0.0567\n",
      "Epoch 755/3000\n",
      "2370/2370 [==============================] - 0s 30us/step - loss: 0.0377 - val_loss: 0.0529\n",
      "Epoch 756/3000\n",
      "2370/2370 [==============================] - 0s 28us/step - loss: 0.0386 - val_loss: 0.0314\n",
      "Epoch 757/3000\n",
      "2370/2370 [==============================] - 0s 30us/step - loss: 0.0380 - val_loss: 0.0614\n",
      "Epoch 758/3000\n",
      "2370/2370 [==============================] - 0s 28us/step - loss: 0.0361 - val_loss: 0.0372\n",
      "Epoch 759/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0374 - val_loss: 0.0393\n",
      "Epoch 760/3000\n",
      "2370/2370 [==============================] - 0s 28us/step - loss: 0.0348 - val_loss: 0.0448\n",
      "Epoch 761/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0384 - val_loss: 0.0365\n",
      "Epoch 762/3000\n",
      "2370/2370 [==============================] - 0s 30us/step - loss: 0.0352 - val_loss: 0.0510\n",
      "Epoch 763/3000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2370/2370 [==============================] - 0s 32us/step - loss: 0.0345 - val_loss: 0.0543\n",
      "Epoch 764/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0327 - val_loss: 0.0396\n",
      "Epoch 765/3000\n",
      "2370/2370 [==============================] - 0s 30us/step - loss: 0.0347 - val_loss: 0.0561\n",
      "Epoch 766/3000\n",
      "2370/2370 [==============================] - 0s 30us/step - loss: 0.0345 - val_loss: 0.0462\n",
      "Epoch 767/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0360 - val_loss: 0.0385\n",
      "Epoch 768/3000\n",
      "2370/2370 [==============================] - 0s 28us/step - loss: 0.0369 - val_loss: 0.0431\n",
      "Epoch 769/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0360 - val_loss: 0.0748\n",
      "Epoch 770/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0352 - val_loss: 0.0538\n",
      "Epoch 771/3000\n",
      "2370/2370 [==============================] - 0s 28us/step - loss: 0.0346 - val_loss: 0.0693\n",
      "Epoch 772/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0386 - val_loss: 0.0469\n",
      "Epoch 773/3000\n",
      "2370/2370 [==============================] - 0s 66us/step - loss: 0.0386 - val_loss: 0.0492\n",
      "Epoch 774/3000\n",
      "2370/2370 [==============================] - 0s 45us/step - loss: 0.0353 - val_loss: 0.0556\n",
      "Epoch 775/3000\n",
      "2370/2370 [==============================] - 0s 92us/step - loss: 0.0366 - val_loss: 0.0554\n",
      "Epoch 776/3000\n",
      "2370/2370 [==============================] - 0s 36us/step - loss: 0.0383 - val_loss: 0.0591\n",
      "Epoch 777/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0363 - val_loss: 0.0611\n",
      "Epoch 778/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0372 - val_loss: 0.0474\n",
      "Epoch 779/3000\n",
      "2370/2370 [==============================] - 0s 28us/step - loss: 0.0373 - val_loss: 0.0559\n",
      "Epoch 780/3000\n",
      "2370/2370 [==============================] - 0s 75us/step - loss: 0.0345 - val_loss: 0.0791\n",
      "Epoch 781/3000\n",
      "2370/2370 [==============================] - 0s 35us/step - loss: 0.0361 - val_loss: 0.0516\n",
      "Epoch 782/3000\n",
      "2370/2370 [==============================] - 0s 43us/step - loss: 0.0357 - val_loss: 0.0592\n",
      "Epoch 783/3000\n",
      "2370/2370 [==============================] - 0s 71us/step - loss: 0.0336 - val_loss: 0.0460\n",
      "Epoch 784/3000\n",
      "2370/2370 [==============================] - 0s 69us/step - loss: 0.0377 - val_loss: 0.0497\n",
      "Epoch 785/3000\n",
      "2370/2370 [==============================] - 0s 67us/step - loss: 0.0359 - val_loss: 0.0494\n",
      "Epoch 786/3000\n",
      "2370/2370 [==============================] - 0s 66us/step - loss: 0.0351 - val_loss: 0.0422\n",
      "Epoch 787/3000\n",
      "2370/2370 [==============================] - 0s 63us/step - loss: 0.0333 - val_loss: 0.0459\n",
      "Epoch 788/3000\n",
      "2370/2370 [==============================] - 0s 50us/step - loss: 0.0389 - val_loss: 0.0504\n",
      "Epoch 789/3000\n",
      "2370/2370 [==============================] - 0s 33us/step - loss: 0.0354 - val_loss: 0.0388\n",
      "Epoch 790/3000\n",
      "2370/2370 [==============================] - 0s 34us/step - loss: 0.0326 - val_loss: 0.0404\n",
      "Epoch 791/3000\n",
      "2370/2370 [==============================] - 0s 34us/step - loss: 0.0337 - val_loss: 0.0515\n",
      "Epoch 792/3000\n",
      "2370/2370 [==============================] - 0s 34us/step - loss: 0.0345 - val_loss: 0.0728\n",
      "Epoch 793/3000\n",
      "2370/2370 [==============================] - 0s 33us/step - loss: 0.0399 - val_loss: 0.0407\n",
      "Epoch 794/3000\n",
      "2370/2370 [==============================] - 0s 33us/step - loss: 0.0387 - val_loss: 0.0411\n",
      "Epoch 795/3000\n",
      "2370/2370 [==============================] - 0s 34us/step - loss: 0.0364 - val_loss: 0.0576\n",
      "Epoch 796/3000\n",
      "2370/2370 [==============================] - 0s 30us/step - loss: 0.0374 - val_loss: 0.0431\n",
      "Epoch 797/3000\n",
      "2370/2370 [==============================] - 0s 30us/step - loss: 0.0359 - val_loss: 0.0338\n",
      "Epoch 798/3000\n",
      "2370/2370 [==============================] - 0s 30us/step - loss: 0.0372 - val_loss: 0.0412\n",
      "Epoch 799/3000\n",
      "2370/2370 [==============================] - 0s 30us/step - loss: 0.0366 - val_loss: 0.0480\n",
      "Epoch 800/3000\n",
      "2370/2370 [==============================] - 0s 30us/step - loss: 0.0367 - val_loss: 0.0418\n",
      "Epoch 801/3000\n",
      "2370/2370 [==============================] - 0s 30us/step - loss: 0.0354 - val_loss: 0.0673\n",
      "Epoch 802/3000\n",
      "2370/2370 [==============================] - 0s 30us/step - loss: 0.0376 - val_loss: 0.0449\n",
      "Epoch 803/3000\n",
      "2370/2370 [==============================] - 0s 30us/step - loss: 0.0365 - val_loss: 0.0651\n",
      "Epoch 804/3000\n",
      "2370/2370 [==============================] - 0s 31us/step - loss: 0.0342 - val_loss: 0.0409\n",
      "Epoch 805/3000\n",
      "2370/2370 [==============================] - 0s 31us/step - loss: 0.0392 - val_loss: 0.0678\n",
      "Epoch 806/3000\n",
      "2370/2370 [==============================] - 0s 30us/step - loss: 0.0376 - val_loss: 0.0408\n",
      "Epoch 807/3000\n",
      "2370/2370 [==============================] - 0s 31us/step - loss: 0.0334 - val_loss: 0.0387\n",
      "Epoch 808/3000\n",
      "2370/2370 [==============================] - 0s 31us/step - loss: 0.0335 - val_loss: 0.0448\n",
      "Epoch 809/3000\n",
      "2370/2370 [==============================] - 0s 30us/step - loss: 0.0367 - val_loss: 0.0485\n",
      "Epoch 810/3000\n",
      "2370/2370 [==============================] - 0s 31us/step - loss: 0.0363 - val_loss: 0.0455\n",
      "Epoch 811/3000\n",
      "2370/2370 [==============================] - 0s 30us/step - loss: 0.0355 - val_loss: 0.0445\n",
      "Epoch 812/3000\n",
      "2370/2370 [==============================] - 0s 30us/step - loss: 0.0384 - val_loss: 0.0329\n",
      "Epoch 813/3000\n",
      "2370/2370 [==============================] - 0s 30us/step - loss: 0.0352 - val_loss: 0.0391\n",
      "Epoch 814/3000\n",
      "2370/2370 [==============================] - 0s 30us/step - loss: 0.0374 - val_loss: 0.0397\n",
      "Epoch 815/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0378 - val_loss: 0.0502\n",
      "Epoch 816/3000\n",
      "2370/2370 [==============================] - 0s 28us/step - loss: 0.0363 - val_loss: 0.0395\n",
      "Epoch 817/3000\n",
      "2370/2370 [==============================] - 0s 30us/step - loss: 0.0370 - val_loss: 0.0493\n",
      "Epoch 818/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0373 - val_loss: 0.0473\n",
      "Epoch 819/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0359 - val_loss: 0.0407\n",
      "Epoch 820/3000\n",
      "2370/2370 [==============================] - 0s 30us/step - loss: 0.0358 - val_loss: 0.0532\n",
      "Epoch 821/3000\n",
      "2370/2370 [==============================] - 0s 33us/step - loss: 0.0354 - val_loss: 0.0365\n",
      "Epoch 822/3000\n",
      "2370/2370 [==============================] - 0s 30us/step - loss: 0.0350 - val_loss: 0.0440\n",
      "Epoch 823/3000\n",
      "2370/2370 [==============================] - 0s 30us/step - loss: 0.0362 - val_loss: 0.0654\n",
      "Epoch 824/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0375 - val_loss: 0.0344\n",
      "Epoch 825/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0347 - val_loss: 0.0511\n",
      "Epoch 826/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0378 - val_loss: 0.0472\n",
      "Epoch 827/3000\n",
      "2370/2370 [==============================] - 0s 30us/step - loss: 0.0352 - val_loss: 0.0627\n",
      "Epoch 828/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0384 - val_loss: 0.0507\n",
      "Epoch 829/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0380 - val_loss: 0.0389\n",
      "Epoch 830/3000\n",
      "2370/2370 [==============================] - 0s 30us/step - loss: 0.0382 - val_loss: 0.0419\n",
      "Epoch 831/3000\n",
      "2370/2370 [==============================] - 0s 30us/step - loss: 0.0364 - val_loss: 0.0508\n",
      "Epoch 832/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0337 - val_loss: 0.0504\n",
      "Epoch 833/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0370 - val_loss: 0.0503\n",
      "Epoch 834/3000\n",
      "2370/2370 [==============================] - 0s 28us/step - loss: 0.0356 - val_loss: 0.0512\n",
      "Epoch 835/3000\n",
      "2370/2370 [==============================] - 0s 28us/step - loss: 0.0363 - val_loss: 0.0524\n",
      "Epoch 836/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0358 - val_loss: 0.0335\n",
      "Epoch 837/3000\n",
      "2370/2370 [==============================] - 0s 30us/step - loss: 0.0373 - val_loss: 0.0513\n",
      "Epoch 838/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0351 - val_loss: 0.0488\n",
      "Epoch 839/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0364 - val_loss: 0.0642\n",
      "Epoch 840/3000\n",
      "2370/2370 [==============================] - 0s 32us/step - loss: 0.0377 - val_loss: 0.0582\n",
      "Epoch 841/3000\n",
      "2370/2370 [==============================] - 0s 30us/step - loss: 0.0361 - val_loss: 0.0463\n",
      "Epoch 842/3000\n",
      "2370/2370 [==============================] - 0s 30us/step - loss: 0.0369 - val_loss: 0.0522\n",
      "Epoch 843/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0374 - val_loss: 0.0407\n",
      "Epoch 844/3000\n",
      "2370/2370 [==============================] - 0s 30us/step - loss: 0.0314 - val_loss: 0.0462\n",
      "Epoch 845/3000\n",
      "2370/2370 [==============================] - 0s 30us/step - loss: 0.0346 - val_loss: 0.0584\n",
      "Epoch 846/3000\n",
      "2370/2370 [==============================] - 0s 30us/step - loss: 0.0378 - val_loss: 0.0563\n",
      "Epoch 847/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0356 - val_loss: 0.0457\n",
      "Epoch 848/3000\n",
      "2370/2370 [==============================] - 0s 30us/step - loss: 0.0364 - val_loss: 0.0438\n",
      "Epoch 849/3000\n",
      "2370/2370 [==============================] - 0s 30us/step - loss: 0.0342 - val_loss: 0.0532\n",
      "Epoch 850/3000\n",
      "2370/2370 [==============================] - 0s 30us/step - loss: 0.0345 - val_loss: 0.0458\n",
      "Epoch 851/3000\n",
      "2370/2370 [==============================] - 0s 30us/step - loss: 0.0373 - val_loss: 0.0446\n",
      "Epoch 852/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0354 - val_loss: 0.0394\n",
      "Epoch 853/3000\n",
      "2370/2370 [==============================] - 0s 30us/step - loss: 0.0324 - val_loss: 0.0427\n",
      "Epoch 854/3000\n",
      "2370/2370 [==============================] - 0s 30us/step - loss: 0.0344 - val_loss: 0.0636\n",
      "Epoch 855/3000\n",
      "2370/2370 [==============================] - 0s 30us/step - loss: 0.0383 - val_loss: 0.0366\n",
      "Epoch 856/3000\n",
      "2370/2370 [==============================] - 0s 30us/step - loss: 0.0364 - val_loss: 0.0476\n",
      "Epoch 857/3000\n",
      "2370/2370 [==============================] - 0s 30us/step - loss: 0.0357 - val_loss: 0.0392\n",
      "Epoch 858/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0345 - val_loss: 0.0449\n",
      "Epoch 859/3000\n",
      "2370/2370 [==============================] - 0s 30us/step - loss: 0.0374 - val_loss: 0.0378\n",
      "Epoch 860/3000\n",
      "2370/2370 [==============================] - 0s 30us/step - loss: 0.0360 - val_loss: 0.0471\n",
      "Epoch 861/3000\n",
      "2370/2370 [==============================] - 0s 31us/step - loss: 0.0358 - val_loss: 0.0627\n",
      "Epoch 862/3000\n",
      "2370/2370 [==============================] - 0s 30us/step - loss: 0.0401 - val_loss: 0.0539\n",
      "Epoch 863/3000\n",
      "2370/2370 [==============================] - 0s 30us/step - loss: 0.0375 - val_loss: 0.0460\n",
      "Epoch 864/3000\n",
      "2370/2370 [==============================] - 0s 30us/step - loss: 0.0358 - val_loss: 0.0550\n",
      "Epoch 865/3000\n",
      "2370/2370 [==============================] - 0s 32us/step - loss: 0.0362 - val_loss: 0.0712\n",
      "Epoch 866/3000\n",
      "2370/2370 [==============================] - 0s 30us/step - loss: 0.0354 - val_loss: 0.0463\n",
      "Epoch 867/3000\n",
      "2370/2370 [==============================] - 0s 30us/step - loss: 0.0365 - val_loss: 0.0439\n",
      "Epoch 868/3000\n",
      "2370/2370 [==============================] - 0s 30us/step - loss: 0.0375 - val_loss: 0.0431\n",
      "Epoch 869/3000\n",
      "2370/2370 [==============================] - 0s 30us/step - loss: 0.0343 - val_loss: 0.0554\n",
      "Epoch 870/3000\n",
      "2370/2370 [==============================] - 0s 32us/step - loss: 0.0389 - val_loss: 0.0400\n",
      "Epoch 871/3000\n",
      "2370/2370 [==============================] - 0s 30us/step - loss: 0.0390 - val_loss: 0.0641\n",
      "Epoch 872/3000\n",
      "2370/2370 [==============================] - 0s 30us/step - loss: 0.0415 - val_loss: 0.0361\n",
      "Epoch 873/3000\n",
      "2370/2370 [==============================] - 0s 30us/step - loss: 0.0370 - val_loss: 0.0483\n",
      "Epoch 874/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0356 - val_loss: 0.0597\n",
      "Epoch 875/3000\n",
      "2370/2370 [==============================] - 0s 30us/step - loss: 0.0356 - val_loss: 0.0442\n",
      "Epoch 876/3000\n",
      "2370/2370 [==============================] - 0s 30us/step - loss: 0.0367 - val_loss: 0.0495\n",
      "Epoch 877/3000\n",
      "2370/2370 [==============================] - 0s 30us/step - loss: 0.0377 - val_loss: 0.0385\n",
      "Epoch 878/3000\n",
      "2370/2370 [==============================] - 0s 32us/step - loss: 0.0362 - val_loss: 0.0558\n",
      "Epoch 879/3000\n",
      "2370/2370 [==============================] - 0s 30us/step - loss: 0.0342 - val_loss: 0.0441\n",
      "Epoch 880/3000\n",
      "2370/2370 [==============================] - 0s 30us/step - loss: 0.0376 - val_loss: 0.0395\n",
      "Epoch 881/3000\n",
      "2370/2370 [==============================] - 0s 30us/step - loss: 0.0358 - val_loss: 0.0620\n",
      "Epoch 882/3000\n",
      "2370/2370 [==============================] - 0s 30us/step - loss: 0.0347 - val_loss: 0.0449\n",
      "Epoch 883/3000\n",
      "2370/2370 [==============================] - 0s 30us/step - loss: 0.0361 - val_loss: 0.0477\n",
      "Epoch 884/3000\n",
      "2370/2370 [==============================] - 0s 30us/step - loss: 0.0361 - val_loss: 0.0386\n",
      "Epoch 885/3000\n",
      "2370/2370 [==============================] - 0s 30us/step - loss: 0.0346 - val_loss: 0.0722\n",
      "Epoch 886/3000\n",
      "2370/2370 [==============================] - 0s 30us/step - loss: 0.0366 - val_loss: 0.0454\n",
      "Epoch 887/3000\n",
      "2370/2370 [==============================] - 0s 30us/step - loss: 0.0350 - val_loss: 0.0451\n",
      "Epoch 888/3000\n",
      "2370/2370 [==============================] - 0s 31us/step - loss: 0.0384 - val_loss: 0.0422\n",
      "Epoch 889/3000\n",
      "2370/2370 [==============================] - 0s 31us/step - loss: 0.0354 - val_loss: 0.0438\n",
      "Epoch 890/3000\n",
      "2370/2370 [==============================] - 0s 30us/step - loss: 0.0373 - val_loss: 0.0470\n",
      "Epoch 891/3000\n",
      "2370/2370 [==============================] - 0s 30us/step - loss: 0.0355 - val_loss: 0.0482\n",
      "Epoch 892/3000\n",
      "2370/2370 [==============================] - 0s 30us/step - loss: 0.0335 - val_loss: 0.0613\n",
      "Epoch 893/3000\n",
      "2370/2370 [==============================] - 0s 30us/step - loss: 0.0362 - val_loss: 0.0517\n",
      "Epoch 894/3000\n",
      "2370/2370 [==============================] - 0s 30us/step - loss: 0.0373 - val_loss: 0.0677\n",
      "Epoch 895/3000\n",
      "2370/2370 [==============================] - 0s 30us/step - loss: 0.0407 - val_loss: 0.0532\n",
      "Epoch 896/3000\n",
      "2370/2370 [==============================] - 0s 30us/step - loss: 0.0368 - val_loss: 0.0329\n",
      "Epoch 897/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0379 - val_loss: 0.0484\n",
      "Epoch 898/3000\n",
      "2370/2370 [==============================] - 0s 30us/step - loss: 0.0377 - val_loss: 0.0529\n",
      "Epoch 899/3000\n",
      "2370/2370 [==============================] - 0s 30us/step - loss: 0.0363 - val_loss: 0.0482\n",
      "Epoch 900/3000\n",
      "2370/2370 [==============================] - 0s 30us/step - loss: 0.0333 - val_loss: 0.0392\n",
      "Epoch 901/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0325 - val_loss: 0.0564\n",
      "Epoch 902/3000\n",
      "2370/2370 [==============================] - 0s 30us/step - loss: 0.0380 - val_loss: 0.0341\n",
      "Epoch 903/3000\n",
      "2370/2370 [==============================] - 0s 30us/step - loss: 0.0376 - val_loss: 0.0427\n",
      "Epoch 904/3000\n",
      "2370/2370 [==============================] - 0s 31us/step - loss: 0.0363 - val_loss: 0.0456\n",
      "Epoch 905/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0362 - val_loss: 0.0416\n",
      "Epoch 906/3000\n",
      "2370/2370 [==============================] - 0s 30us/step - loss: 0.0325 - val_loss: 0.0463\n",
      "Epoch 907/3000\n",
      "2370/2370 [==============================] - 0s 36us/step - loss: 0.0370 - val_loss: 0.0606\n",
      "Epoch 908/3000\n",
      "2370/2370 [==============================] - 0s 30us/step - loss: 0.0369 - val_loss: 0.0598\n",
      "Epoch 909/3000\n",
      "2370/2370 [==============================] - 0s 30us/step - loss: 0.0349 - val_loss: 0.0624\n",
      "Epoch 910/3000\n",
      "2370/2370 [==============================] - 0s 32us/step - loss: 0.0371 - val_loss: 0.0463\n",
      "Epoch 911/3000\n",
      "2370/2370 [==============================] - 0s 30us/step - loss: 0.0362 - val_loss: 0.0453\n",
      "Epoch 912/3000\n",
      "2370/2370 [==============================] - 0s 30us/step - loss: 0.0342 - val_loss: 0.0452\n",
      "Epoch 913/3000\n",
      "2370/2370 [==============================] - 0s 30us/step - loss: 0.0397 - val_loss: 0.0593\n",
      "Epoch 914/3000\n",
      "2370/2370 [==============================] - 0s 30us/step - loss: 0.0345 - val_loss: 0.0422\n",
      "Epoch 915/3000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2370/2370 [==============================] - 0s 30us/step - loss: 0.0359 - val_loss: 0.0419\n",
      "Epoch 916/3000\n",
      "2370/2370 [==============================] - 0s 32us/step - loss: 0.0356 - val_loss: 0.0509\n",
      "Epoch 917/3000\n",
      "2370/2370 [==============================] - 0s 30us/step - loss: 0.0362 - val_loss: 0.0549\n",
      "Epoch 918/3000\n",
      "2370/2370 [==============================] - 0s 32us/step - loss: 0.0358 - val_loss: 0.0532\n",
      "Epoch 919/3000\n",
      "2370/2370 [==============================] - 0s 30us/step - loss: 0.0362 - val_loss: 0.0550\n",
      "Epoch 920/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0370 - val_loss: 0.0511\n",
      "Epoch 921/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0398 - val_loss: 0.0474\n",
      "Epoch 922/3000\n",
      "2370/2370 [==============================] - 0s 28us/step - loss: 0.0400 - val_loss: 0.0421\n",
      "Epoch 923/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0385 - val_loss: 0.0434\n",
      "Epoch 924/3000\n",
      "2370/2370 [==============================] - 0s 28us/step - loss: 0.0370 - val_loss: 0.0525\n",
      "Epoch 925/3000\n",
      "2370/2370 [==============================] - 0s 28us/step - loss: 0.0393 - val_loss: 0.0583\n",
      "Epoch 926/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0355 - val_loss: 0.0556\n",
      "Epoch 927/3000\n",
      "2370/2370 [==============================] - 0s 30us/step - loss: 0.0353 - val_loss: 0.0627\n",
      "Epoch 928/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0382 - val_loss: 0.0488\n",
      "Epoch 929/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0382 - val_loss: 0.0493\n",
      "Epoch 930/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0377 - val_loss: 0.0383\n",
      "Epoch 931/3000\n",
      "2370/2370 [==============================] - 0s 30us/step - loss: 0.0360 - val_loss: 0.0415\n",
      "Epoch 932/3000\n",
      "2370/2370 [==============================] - 0s 30us/step - loss: 0.0337 - val_loss: 0.0493\n",
      "Epoch 933/3000\n",
      "2370/2370 [==============================] - 0s 30us/step - loss: 0.0358 - val_loss: 0.0420\n",
      "Epoch 934/3000\n",
      "2370/2370 [==============================] - 0s 30us/step - loss: 0.0397 - val_loss: 0.0529\n",
      "Epoch 935/3000\n",
      "2370/2370 [==============================] - 0s 30us/step - loss: 0.0338 - val_loss: 0.0579\n",
      "Epoch 936/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0377 - val_loss: 0.0544\n",
      "Epoch 937/3000\n",
      "2370/2370 [==============================] - 0s 30us/step - loss: 0.0374 - val_loss: 0.0568\n",
      "Epoch 938/3000\n",
      "2370/2370 [==============================] - 0s 30us/step - loss: 0.0324 - val_loss: 0.0460\n",
      "Epoch 939/3000\n",
      "2370/2370 [==============================] - 0s 30us/step - loss: 0.0326 - val_loss: 0.0390\n",
      "Epoch 940/3000\n",
      "2370/2370 [==============================] - 0s 30us/step - loss: 0.0359 - val_loss: 0.0498\n",
      "Epoch 941/3000\n",
      "2370/2370 [==============================] - 0s 30us/step - loss: 0.0377 - val_loss: 0.0505\n",
      "Epoch 942/3000\n",
      "2370/2370 [==============================] - 0s 30us/step - loss: 0.0299 - val_loss: 0.0458\n",
      "Epoch 943/3000\n",
      "2370/2370 [==============================] - 0s 30us/step - loss: 0.0356 - val_loss: 0.0556\n",
      "Epoch 944/3000\n",
      "2370/2370 [==============================] - 0s 30us/step - loss: 0.0322 - val_loss: 0.0693\n",
      "Epoch 945/3000\n",
      "2370/2370 [==============================] - 0s 30us/step - loss: 0.0341 - val_loss: 0.0427\n",
      "Epoch 946/3000\n",
      "2370/2370 [==============================] - 0s 31us/step - loss: 0.0361 - val_loss: 0.0487\n",
      "Epoch 947/3000\n",
      "2370/2370 [==============================] - 0s 31us/step - loss: 0.0371 - val_loss: 0.0428\n",
      "Epoch 948/3000\n",
      "2370/2370 [==============================] - 0s 31us/step - loss: 0.0388 - val_loss: 0.0534\n",
      "Epoch 949/3000\n",
      "2370/2370 [==============================] - 0s 30us/step - loss: 0.0355 - val_loss: 0.0459\n",
      "Epoch 950/3000\n",
      "2370/2370 [==============================] - 0s 31us/step - loss: 0.0399 - val_loss: 0.0371\n",
      "Epoch 951/3000\n",
      "2370/2370 [==============================] - 0s 30us/step - loss: 0.0350 - val_loss: 0.0379\n",
      "Epoch 952/3000\n",
      "2370/2370 [==============================] - 0s 30us/step - loss: 0.0381 - val_loss: 0.0467\n",
      "Epoch 953/3000\n",
      "2370/2370 [==============================] - 0s 30us/step - loss: 0.0356 - val_loss: 0.0424\n",
      "Epoch 954/3000\n",
      "2370/2370 [==============================] - 0s 30us/step - loss: 0.0337 - val_loss: 0.0459\n",
      "Epoch 955/3000\n",
      "2370/2370 [==============================] - 0s 30us/step - loss: 0.0350 - val_loss: 0.0435\n",
      "Epoch 956/3000\n",
      "2370/2370 [==============================] - 0s 31us/step - loss: 0.0355 - val_loss: 0.0321\n",
      "Epoch 957/3000\n",
      "2370/2370 [==============================] - 0s 30us/step - loss: 0.0314 - val_loss: 0.0497\n",
      "Epoch 958/3000\n",
      "2370/2370 [==============================] - 0s 30us/step - loss: 0.0324 - val_loss: 0.0530\n",
      "Epoch 959/3000\n",
      "2370/2370 [==============================] - 0s 31us/step - loss: 0.0358 - val_loss: 0.0425\n",
      "Epoch 960/3000\n",
      "2370/2370 [==============================] - 0s 31us/step - loss: 0.0386 - val_loss: 0.0554\n",
      "Epoch 961/3000\n",
      "2370/2370 [==============================] - 0s 30us/step - loss: 0.0383 - val_loss: 0.0606\n",
      "Epoch 962/3000\n",
      "2370/2370 [==============================] - 0s 34us/step - loss: 0.0395 - val_loss: 0.0522\n",
      "Epoch 963/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0359 - val_loss: 0.0517\n",
      "Epoch 964/3000\n",
      "2370/2370 [==============================] - 0s 30us/step - loss: 0.0372 - val_loss: 0.0502\n",
      "Epoch 965/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0344 - val_loss: 0.0463\n",
      "Epoch 966/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0342 - val_loss: 0.0603\n",
      "Epoch 967/3000\n",
      "2370/2370 [==============================] - 0s 33us/step - loss: 0.0316 - val_loss: 0.0541\n",
      "Epoch 968/3000\n",
      "2370/2370 [==============================] - 0s 32us/step - loss: 0.0346 - val_loss: 0.0511\n",
      "Epoch 969/3000\n",
      "2370/2370 [==============================] - 0s 32us/step - loss: 0.0358 - val_loss: 0.0426\n",
      "Epoch 970/3000\n",
      "2370/2370 [==============================] - 0s 32us/step - loss: 0.0347 - val_loss: 0.0596\n",
      "Epoch 971/3000\n",
      "2370/2370 [==============================] - 0s 31us/step - loss: 0.0386 - val_loss: 0.0555\n",
      "Epoch 972/3000\n",
      "2370/2370 [==============================] - 0s 31us/step - loss: 0.0383 - val_loss: 0.0470\n",
      "Epoch 973/3000\n",
      "2370/2370 [==============================] - 0s 31us/step - loss: 0.0345 - val_loss: 0.0448\n",
      "Epoch 974/3000\n",
      "2370/2370 [==============================] - 0s 31us/step - loss: 0.0373 - val_loss: 0.0501\n",
      "Epoch 975/3000\n",
      "2370/2370 [==============================] - 0s 34us/step - loss: 0.0334 - val_loss: 0.0585\n",
      "Epoch 976/3000\n",
      "2370/2370 [==============================] - 0s 31us/step - loss: 0.0365 - val_loss: 0.0508\n",
      "Epoch 977/3000\n",
      "2370/2370 [==============================] - 0s 31us/step - loss: 0.0328 - val_loss: 0.0475\n",
      "Epoch 978/3000\n",
      "2370/2370 [==============================] - 0s 30us/step - loss: 0.0359 - val_loss: 0.0478\n",
      "Epoch 979/3000\n",
      "2370/2370 [==============================] - 0s 30us/step - loss: 0.0367 - val_loss: 0.0449\n",
      "Epoch 980/3000\n",
      "2370/2370 [==============================] - 0s 30us/step - loss: 0.0353 - val_loss: 0.0435\n",
      "Epoch 981/3000\n",
      "2370/2370 [==============================] - 0s 30us/step - loss: 0.0354 - val_loss: 0.0505\n",
      "Epoch 982/3000\n",
      "2370/2370 [==============================] - 0s 31us/step - loss: 0.0341 - val_loss: 0.0347\n",
      "Epoch 983/3000\n",
      "2370/2370 [==============================] - 0s 32us/step - loss: 0.0331 - val_loss: 0.0369\n",
      "Epoch 984/3000\n",
      "2370/2370 [==============================] - 0s 45us/step - loss: 0.0383 - val_loss: 0.0488\n",
      "Epoch 985/3000\n",
      "2370/2370 [==============================] - 0s 61us/step - loss: 0.0384 - val_loss: 0.0580\n",
      "Epoch 986/3000\n",
      "2370/2370 [==============================] - 0s 87us/step - loss: 0.0351 - val_loss: 0.0363\n",
      "Epoch 987/3000\n",
      "2370/2370 [==============================] - 0s 92us/step - loss: 0.0352 - val_loss: 0.0584\n",
      "Epoch 988/3000\n",
      "2370/2370 [==============================] - 0s 34us/step - loss: 0.0377 - val_loss: 0.0442\n",
      "Epoch 989/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0351 - val_loss: 0.0449\n",
      "Epoch 990/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0360 - val_loss: 0.0457\n",
      "Epoch 991/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0367 - val_loss: 0.0402\n",
      "Epoch 992/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0387 - val_loss: 0.0637\n",
      "Epoch 993/3000\n",
      "2370/2370 [==============================] - 0s 64us/step - loss: 0.0357 - val_loss: 0.0442\n",
      "Epoch 994/3000\n",
      "2370/2370 [==============================] - 0s 70us/step - loss: 0.0369 - val_loss: 0.0413\n",
      "Epoch 995/3000\n",
      "2370/2370 [==============================] - 0s 46us/step - loss: 0.0375 - val_loss: 0.0636\n",
      "Epoch 996/3000\n",
      "2370/2370 [==============================] - 0s 30us/step - loss: 0.0366 - val_loss: 0.0467\n",
      "Epoch 997/3000\n",
      "2370/2370 [==============================] - 0s 30us/step - loss: 0.0374 - val_loss: 0.0610\n",
      "Epoch 998/3000\n",
      "2370/2370 [==============================] - 0s 30us/step - loss: 0.0383 - val_loss: 0.0494\n",
      "Epoch 999/3000\n",
      "2370/2370 [==============================] - 0s 30us/step - loss: 0.0363 - val_loss: 0.0558\n",
      "Epoch 1000/3000\n",
      "2370/2370 [==============================] - 0s 30us/step - loss: 0.0347 - val_loss: 0.0444\n",
      "Epoch 1001/3000\n",
      "2370/2370 [==============================] - 0s 30us/step - loss: 0.0371 - val_loss: 0.0458\n",
      "Epoch 1002/3000\n",
      "2370/2370 [==============================] - 0s 30us/step - loss: 0.0342 - val_loss: 0.0474\n",
      "Epoch 1003/3000\n",
      "2370/2370 [==============================] - 0s 30us/step - loss: 0.0353 - val_loss: 0.0397\n",
      "Epoch 1004/3000\n",
      "2370/2370 [==============================] - 0s 30us/step - loss: 0.0394 - val_loss: 0.0388\n",
      "Epoch 1005/3000\n",
      "2370/2370 [==============================] - 0s 30us/step - loss: 0.0365 - val_loss: 0.0445\n",
      "Epoch 1006/3000\n",
      "2370/2370 [==============================] - 0s 31us/step - loss: 0.0347 - val_loss: 0.0499\n",
      "Epoch 1007/3000\n",
      "2370/2370 [==============================] - 0s 30us/step - loss: 0.0364 - val_loss: 0.0569\n",
      "Epoch 1008/3000\n",
      "2370/2370 [==============================] - 0s 32us/step - loss: 0.0354 - val_loss: 0.0505\n",
      "Epoch 1009/3000\n",
      "2370/2370 [==============================] - 0s 31us/step - loss: 0.0379 - val_loss: 0.0677\n",
      "Epoch 1010/3000\n",
      "2370/2370 [==============================] - 0s 31us/step - loss: 0.0359 - val_loss: 0.0382\n",
      "Epoch 1011/3000\n",
      "2370/2370 [==============================] - 0s 32us/step - loss: 0.0365 - val_loss: 0.0489\n",
      "Epoch 1012/3000\n",
      "2370/2370 [==============================] - 0s 31us/step - loss: 0.0331 - val_loss: 0.0639\n",
      "Epoch 1013/3000\n",
      "2370/2370 [==============================] - 0s 32us/step - loss: 0.0370 - val_loss: 0.0424\n",
      "Epoch 1014/3000\n",
      "2370/2370 [==============================] - 0s 31us/step - loss: 0.0378 - val_loss: 0.0557\n",
      "Epoch 1015/3000\n",
      "2370/2370 [==============================] - 0s 31us/step - loss: 0.0353 - val_loss: 0.0450\n",
      "Epoch 1016/3000\n",
      "2370/2370 [==============================] - 0s 31us/step - loss: 0.0334 - val_loss: 0.0654\n",
      "Epoch 1017/3000\n",
      "2370/2370 [==============================] - 0s 32us/step - loss: 0.0343 - val_loss: 0.0489\n",
      "Epoch 1018/3000\n",
      "2370/2370 [==============================] - 0s 31us/step - loss: 0.0392 - val_loss: 0.0484\n",
      "Epoch 1019/3000\n",
      "2370/2370 [==============================] - 0s 32us/step - loss: 0.0356 - val_loss: 0.0413\n",
      "Epoch 1020/3000\n",
      "2370/2370 [==============================] - 0s 31us/step - loss: 0.0345 - val_loss: 0.0425\n",
      "Epoch 1021/3000\n",
      "2370/2370 [==============================] - 0s 32us/step - loss: 0.0358 - val_loss: 0.0514\n",
      "Epoch 1022/3000\n",
      "2370/2370 [==============================] - 0s 32us/step - loss: 0.0350 - val_loss: 0.0540\n",
      "Epoch 1023/3000\n",
      "2370/2370 [==============================] - 0s 30us/step - loss: 0.0330 - val_loss: 0.0389\n",
      "Epoch 1024/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0337 - val_loss: 0.0558\n",
      "Epoch 1025/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0374 - val_loss: 0.0438\n",
      "Epoch 1026/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0359 - val_loss: 0.0457\n",
      "Epoch 1027/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0384 - val_loss: 0.0519\n",
      "Epoch 1028/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0389 - val_loss: 0.0806\n",
      "Epoch 1029/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0393 - val_loss: 0.0410\n",
      "Epoch 1030/3000\n",
      "2370/2370 [==============================] - 0s 28us/step - loss: 0.0359 - val_loss: 0.0433\n",
      "Epoch 1031/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0363 - val_loss: 0.0427\n",
      "Epoch 1032/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0391 - val_loss: 0.0558\n",
      "Epoch 1033/3000\n",
      "2370/2370 [==============================] - 0s 30us/step - loss: 0.0353 - val_loss: 0.0490\n",
      "Epoch 1034/3000\n",
      "2370/2370 [==============================] - 0s 30us/step - loss: 0.0343 - val_loss: 0.0584\n",
      "Epoch 1035/3000\n",
      "2370/2370 [==============================] - 0s 30us/step - loss: 0.0356 - val_loss: 0.0543\n",
      "Epoch 1036/3000\n",
      "2370/2370 [==============================] - 0s 34us/step - loss: 0.0392 - val_loss: 0.0470\n",
      "Epoch 1037/3000\n",
      "2370/2370 [==============================] - 0s 33us/step - loss: 0.0387 - val_loss: 0.0378\n",
      "Epoch 1038/3000\n",
      "2370/2370 [==============================] - 0s 33us/step - loss: 0.0370 - val_loss: 0.0552\n",
      "Epoch 1039/3000\n",
      "2370/2370 [==============================] - 0s 33us/step - loss: 0.0371 - val_loss: 0.0446\n",
      "Epoch 1040/3000\n",
      "2370/2370 [==============================] - 0s 33us/step - loss: 0.0353 - val_loss: 0.0367\n",
      "Epoch 1041/3000\n",
      "2370/2370 [==============================] - 0s 32us/step - loss: 0.0360 - val_loss: 0.0364\n",
      "Epoch 1042/3000\n",
      "2370/2370 [==============================] - 0s 33us/step - loss: 0.0366 - val_loss: 0.0513\n",
      "Epoch 1043/3000\n",
      "2370/2370 [==============================] - 0s 30us/step - loss: 0.0361 - val_loss: 0.0496\n",
      "Epoch 1044/3000\n",
      "2370/2370 [==============================] - 0s 30us/step - loss: 0.0355 - val_loss: 0.0586\n",
      "Epoch 1045/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0362 - val_loss: 0.0575\n",
      "Epoch 1046/3000\n",
      "2370/2370 [==============================] - 0s 30us/step - loss: 0.0361 - val_loss: 0.0460\n",
      "Epoch 1047/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0379 - val_loss: 0.0425\n",
      "Epoch 1048/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0342 - val_loss: 0.0373\n",
      "Epoch 1049/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0361 - val_loss: 0.0444\n",
      "Epoch 1050/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0408 - val_loss: 0.0432\n",
      "Epoch 1051/3000\n",
      "2370/2370 [==============================] - 0s 30us/step - loss: 0.0362 - val_loss: 0.0429\n",
      "Epoch 1052/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0338 - val_loss: 0.0443\n",
      "Epoch 1053/3000\n",
      "2370/2370 [==============================] - 0s 28us/step - loss: 0.0423 - val_loss: 0.0593\n",
      "Epoch 1054/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0356 - val_loss: 0.0395\n",
      "Epoch 1055/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0378 - val_loss: 0.0410\n",
      "Epoch 1056/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0316 - val_loss: 0.0625\n",
      "Epoch 1057/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0350 - val_loss: 0.0452\n",
      "Epoch 1058/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0361 - val_loss: 0.0454\n",
      "Epoch 1059/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0380 - val_loss: 0.0528\n",
      "Epoch 1060/3000\n",
      "2370/2370 [==============================] - 0s 30us/step - loss: 0.0362 - val_loss: 0.0410\n",
      "Epoch 1061/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0393 - val_loss: 0.0325\n",
      "Epoch 1062/3000\n",
      "2370/2370 [==============================] - 0s 28us/step - loss: 0.0366 - val_loss: 0.0462\n",
      "Epoch 1063/3000\n",
      "2370/2370 [==============================] - 0s 30us/step - loss: 0.0371 - val_loss: 0.0437\n",
      "Epoch 1064/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0357 - val_loss: 0.0518\n",
      "Epoch 1065/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0352 - val_loss: 0.0356\n",
      "Epoch 1066/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0355 - val_loss: 0.0502\n",
      "Epoch 1067/3000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0364 - val_loss: 0.0439\n",
      "Epoch 1068/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0341 - val_loss: 0.0543\n",
      "Epoch 1069/3000\n",
      "2370/2370 [==============================] - 0s 30us/step - loss: 0.0344 - val_loss: 0.0612\n",
      "Epoch 1070/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0337 - val_loss: 0.0525\n",
      "Epoch 1071/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0356 - val_loss: 0.0503\n",
      "Epoch 1072/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0397 - val_loss: 0.0444\n",
      "Epoch 1073/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0350 - val_loss: 0.0475\n",
      "Epoch 1074/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0345 - val_loss: 0.0350\n",
      "Epoch 1075/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0332 - val_loss: 0.0454\n",
      "Epoch 1076/3000\n",
      "2370/2370 [==============================] - 0s 28us/step - loss: 0.0332 - val_loss: 0.0420\n",
      "Epoch 1077/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0378 - val_loss: 0.0459\n",
      "Epoch 1078/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0361 - val_loss: 0.0470\n",
      "Epoch 1079/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0341 - val_loss: 0.0373\n",
      "Epoch 1080/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0356 - val_loss: 0.0398\n",
      "Epoch 1081/3000\n",
      "2370/2370 [==============================] - 0s 30us/step - loss: 0.0369 - val_loss: 0.0440\n",
      "Epoch 1082/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0391 - val_loss: 0.0638\n",
      "Epoch 1083/3000\n",
      "2370/2370 [==============================] - 0s 28us/step - loss: 0.0335 - val_loss: 0.0496\n",
      "Epoch 1084/3000\n",
      "2370/2370 [==============================] - 0s 28us/step - loss: 0.0396 - val_loss: 0.0570\n",
      "Epoch 1085/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0352 - val_loss: 0.0588\n",
      "Epoch 1086/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0380 - val_loss: 0.0387\n",
      "Epoch 1087/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0374 - val_loss: 0.0422\n",
      "Epoch 1088/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0368 - val_loss: 0.0485\n",
      "Epoch 1089/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0333 - val_loss: 0.0479\n",
      "Epoch 1090/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0356 - val_loss: 0.0383\n",
      "Epoch 1091/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0346 - val_loss: 0.0588\n",
      "Epoch 1092/3000\n",
      "2370/2370 [==============================] - 0s 28us/step - loss: 0.0363 - val_loss: 0.0469\n",
      "Epoch 1093/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0346 - val_loss: 0.0378\n",
      "Epoch 1094/3000\n",
      "2370/2370 [==============================] - 0s 28us/step - loss: 0.0400 - val_loss: 0.0495\n",
      "Epoch 1095/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0393 - val_loss: 0.0497\n",
      "Epoch 1096/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0353 - val_loss: 0.0447\n",
      "Epoch 1097/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0347 - val_loss: 0.0593\n",
      "Epoch 1098/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0392 - val_loss: 0.0417\n",
      "Epoch 1099/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0381 - val_loss: 0.0625\n",
      "Epoch 1100/3000\n",
      "2370/2370 [==============================] - 0s 28us/step - loss: 0.0391 - val_loss: 0.0396\n",
      "Epoch 1101/3000\n",
      "2370/2370 [==============================] - 0s 30us/step - loss: 0.0358 - val_loss: 0.0586\n",
      "Epoch 1102/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0365 - val_loss: 0.0492\n",
      "Epoch 1103/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0334 - val_loss: 0.0441\n",
      "Epoch 1104/3000\n",
      "2370/2370 [==============================] - 0s 30us/step - loss: 0.0372 - val_loss: 0.0369\n",
      "Epoch 1105/3000\n",
      "2370/2370 [==============================] - 0s 30us/step - loss: 0.0357 - val_loss: 0.0437\n",
      "Epoch 1106/3000\n",
      "2370/2370 [==============================] - 0s 30us/step - loss: 0.0352 - val_loss: 0.0299\n",
      "Epoch 1107/3000\n",
      "2370/2370 [==============================] - 0s 31us/step - loss: 0.0399 - val_loss: 0.0458\n",
      "Epoch 1108/3000\n",
      "2370/2370 [==============================] - 0s 30us/step - loss: 0.0367 - val_loss: 0.0414\n",
      "Epoch 1109/3000\n",
      "2370/2370 [==============================] - 0s 31us/step - loss: 0.0335 - val_loss: 0.0448\n",
      "Epoch 1110/3000\n",
      "2370/2370 [==============================] - 0s 30us/step - loss: 0.0378 - val_loss: 0.0513\n",
      "Epoch 1111/3000\n",
      "2370/2370 [==============================] - 0s 30us/step - loss: 0.0356 - val_loss: 0.0327\n",
      "Epoch 1112/3000\n",
      "2370/2370 [==============================] - 0s 30us/step - loss: 0.0404 - val_loss: 0.0459\n",
      "Epoch 1113/3000\n",
      "2370/2370 [==============================] - 0s 30us/step - loss: 0.0360 - val_loss: 0.0391\n",
      "Epoch 1114/3000\n",
      "2370/2370 [==============================] - 0s 30us/step - loss: 0.0386 - val_loss: 0.0476\n",
      "Epoch 1115/3000\n",
      "2370/2370 [==============================] - 0s 30us/step - loss: 0.0393 - val_loss: 0.0468\n",
      "Epoch 1116/3000\n",
      "2370/2370 [==============================] - 0s 30us/step - loss: 0.0346 - val_loss: 0.0471\n",
      "Epoch 1117/3000\n",
      "2370/2370 [==============================] - 0s 30us/step - loss: 0.0375 - val_loss: 0.0486\n",
      "Epoch 1118/3000\n",
      "2370/2370 [==============================] - 0s 30us/step - loss: 0.0370 - val_loss: 0.0322\n",
      "Epoch 1119/3000\n",
      "2370/2370 [==============================] - 0s 30us/step - loss: 0.0334 - val_loss: 0.0542\n",
      "Epoch 1120/3000\n",
      "2370/2370 [==============================] - 0s 31us/step - loss: 0.0370 - val_loss: 0.0575\n",
      "Epoch 1121/3000\n",
      "2370/2370 [==============================] - 0s 30us/step - loss: 0.0368 - val_loss: 0.0371\n",
      "Epoch 1122/3000\n",
      "2370/2370 [==============================] - 0s 30us/step - loss: 0.0373 - val_loss: 0.0509\n",
      "Epoch 1123/3000\n",
      "2370/2370 [==============================] - 0s 30us/step - loss: 0.0375 - val_loss: 0.0435\n",
      "Epoch 1124/3000\n",
      "2370/2370 [==============================] - 0s 31us/step - loss: 0.0361 - val_loss: 0.0448\n",
      "Epoch 1125/3000\n",
      "2370/2370 [==============================] - 0s 30us/step - loss: 0.0394 - val_loss: 0.0584\n",
      "Epoch 1126/3000\n",
      "2370/2370 [==============================] - 0s 30us/step - loss: 0.0361 - val_loss: 0.0483\n",
      "Epoch 1127/3000\n",
      "2370/2370 [==============================] - 0s 31us/step - loss: 0.0306 - val_loss: 0.0695\n",
      "Epoch 1128/3000\n",
      "2370/2370 [==============================] - 0s 31us/step - loss: 0.0335 - val_loss: 0.0562\n",
      "Epoch 1129/3000\n",
      "2370/2370 [==============================] - 0s 31us/step - loss: 0.0378 - val_loss: 0.0612\n",
      "Epoch 1130/3000\n",
      "2370/2370 [==============================] - 0s 31us/step - loss: 0.0362 - val_loss: 0.0531\n",
      "Epoch 1131/3000\n",
      "2370/2370 [==============================] - 0s 31us/step - loss: 0.0358 - val_loss: 0.0447\n",
      "Epoch 1132/3000\n",
      "2370/2370 [==============================] - 0s 30us/step - loss: 0.0374 - val_loss: 0.0497\n",
      "Epoch 1133/3000\n",
      "2370/2370 [==============================] - 0s 30us/step - loss: 0.0362 - val_loss: 0.0396\n",
      "Epoch 1134/3000\n",
      "2370/2370 [==============================] - 0s 30us/step - loss: 0.0364 - val_loss: 0.0475\n",
      "Epoch 1135/3000\n",
      "2370/2370 [==============================] - 0s 31us/step - loss: 0.0350 - val_loss: 0.0432\n",
      "Epoch 1136/3000\n",
      "2370/2370 [==============================] - 0s 31us/step - loss: 0.0357 - val_loss: 0.0503\n",
      "Epoch 1137/3000\n",
      "2370/2370 [==============================] - 0s 31us/step - loss: 0.0368 - val_loss: 0.0557\n",
      "Epoch 1138/3000\n",
      "2370/2370 [==============================] - 0s 31us/step - loss: 0.0339 - val_loss: 0.0490\n",
      "Epoch 1139/3000\n",
      "2370/2370 [==============================] - 0s 30us/step - loss: 0.0365 - val_loss: 0.0405\n",
      "Epoch 1140/3000\n",
      "2370/2370 [==============================] - 0s 30us/step - loss: 0.0343 - val_loss: 0.0445\n",
      "Epoch 1141/3000\n",
      "2370/2370 [==============================] - 0s 30us/step - loss: 0.0386 - val_loss: 0.0506\n",
      "Epoch 1142/3000\n",
      "2370/2370 [==============================] - 0s 31us/step - loss: 0.0356 - val_loss: 0.0336\n",
      "Epoch 1143/3000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2370/2370 [==============================] - 0s 31us/step - loss: 0.0355 - val_loss: 0.0408\n",
      "Epoch 1144/3000\n",
      "2370/2370 [==============================] - 0s 30us/step - loss: 0.0328 - val_loss: 0.0437\n",
      "Epoch 1145/3000\n",
      "2370/2370 [==============================] - 0s 30us/step - loss: 0.0337 - val_loss: 0.0469\n",
      "Epoch 1146/3000\n",
      "2370/2370 [==============================] - 0s 30us/step - loss: 0.0341 - val_loss: 0.0594\n",
      "Epoch 1147/3000\n",
      "2370/2370 [==============================] - 0s 30us/step - loss: 0.0375 - val_loss: 0.0464\n",
      "Epoch 1148/3000\n",
      "2370/2370 [==============================] - 0s 30us/step - loss: 0.0384 - val_loss: 0.0469\n",
      "Epoch 1149/3000\n",
      "2370/2370 [==============================] - 0s 30us/step - loss: 0.0354 - val_loss: 0.0462\n",
      "Epoch 1150/3000\n",
      "2370/2370 [==============================] - 0s 30us/step - loss: 0.0404 - val_loss: 0.0443\n",
      "Epoch 1151/3000\n",
      "2370/2370 [==============================] - 0s 30us/step - loss: 0.0339 - val_loss: 0.0589\n",
      "Epoch 1152/3000\n",
      "2370/2370 [==============================] - 0s 31us/step - loss: 0.0362 - val_loss: 0.0568\n",
      "Epoch 1153/3000\n",
      "2370/2370 [==============================] - 0s 30us/step - loss: 0.0364 - val_loss: 0.0394\n",
      "Epoch 1154/3000\n",
      "2370/2370 [==============================] - 0s 30us/step - loss: 0.0340 - val_loss: 0.0533\n",
      "Epoch 1155/3000\n",
      "2370/2370 [==============================] - 0s 31us/step - loss: 0.0395 - val_loss: 0.0712\n",
      "Epoch 1156/3000\n",
      "2370/2370 [==============================] - 0s 67us/step - loss: 0.0344 - val_loss: 0.0508\n",
      "Epoch 1157/3000\n",
      "2370/2370 [==============================] - 0s 65us/step - loss: 0.0341 - val_loss: 0.0462\n",
      "Epoch 1158/3000\n",
      "2370/2370 [==============================] - 0s 37us/step - loss: 0.0367 - val_loss: 0.0570\n",
      "Epoch 1159/3000\n",
      "2370/2370 [==============================] - 0s 79us/step - loss: 0.0361 - val_loss: 0.0314\n",
      "Epoch 1160/3000\n",
      "2370/2370 [==============================] - 0s 42us/step - loss: 0.0381 - val_loss: 0.0423\n",
      "Epoch 1161/3000\n",
      "2370/2370 [==============================] - 0s 57us/step - loss: 0.0368 - val_loss: 0.0390\n",
      "Epoch 1162/3000\n",
      "2370/2370 [==============================] - 0s 76us/step - loss: 0.0365 - val_loss: 0.0314\n",
      "Epoch 1163/3000\n",
      "2370/2370 [==============================] - 0s 60us/step - loss: 0.0344 - val_loss: 0.0533\n",
      "Epoch 1164/3000\n",
      "2370/2370 [==============================] - 0s 38us/step - loss: 0.0361 - val_loss: 0.0561\n",
      "Epoch 1165/3000\n",
      "2370/2370 [==============================] - 0s 49us/step - loss: 0.0369 - val_loss: 0.0460\n",
      "Epoch 1166/3000\n",
      "2370/2370 [==============================] - 0s 65us/step - loss: 0.0377 - val_loss: 0.0369\n",
      "Epoch 1167/3000\n",
      "2370/2370 [==============================] - 0s 45us/step - loss: 0.0344 - val_loss: 0.0469\n",
      "Epoch 1168/3000\n",
      "2370/2370 [==============================] - 0s 90us/step - loss: 0.0382 - val_loss: 0.0521\n",
      "Epoch 1169/3000\n",
      "2370/2370 [==============================] - 0s 34us/step - loss: 0.0315 - val_loss: 0.0707\n",
      "Epoch 1170/3000\n",
      "2370/2370 [==============================] - 0s 32us/step - loss: 0.0345 - val_loss: 0.0391\n",
      "Epoch 1171/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0384 - val_loss: 0.0550\n",
      "Epoch 1172/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0351 - val_loss: 0.0551\n",
      "Epoch 1173/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0378 - val_loss: 0.0629\n",
      "Epoch 1174/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0338 - val_loss: 0.0555\n",
      "Epoch 1175/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0377 - val_loss: 0.0411\n",
      "Epoch 1176/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0380 - val_loss: 0.0555\n",
      "Epoch 1177/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0349 - val_loss: 0.0554\n",
      "Epoch 1178/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0364 - val_loss: 0.0530\n",
      "Epoch 1179/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0354 - val_loss: 0.0335\n",
      "Epoch 1180/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0363 - val_loss: 0.0635\n",
      "Epoch 1181/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0342 - val_loss: 0.0522\n",
      "Epoch 1182/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0383 - val_loss: 0.0564\n",
      "Epoch 1183/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0336 - val_loss: 0.0576\n",
      "Epoch 1184/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0374 - val_loss: 0.0648\n",
      "Epoch 1185/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0341 - val_loss: 0.0411\n",
      "Epoch 1186/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0364 - val_loss: 0.0494\n",
      "Epoch 1187/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0351 - val_loss: 0.0367\n",
      "Epoch 1188/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0361 - val_loss: 0.0504\n",
      "Epoch 1189/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0371 - val_loss: 0.0386\n",
      "Epoch 1190/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0364 - val_loss: 0.0492\n",
      "Epoch 1191/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0367 - val_loss: 0.0595\n",
      "Epoch 1192/3000\n",
      "2370/2370 [==============================] - 0s 30us/step - loss: 0.0334 - val_loss: 0.0486\n",
      "Epoch 1193/3000\n",
      "2370/2370 [==============================] - 0s 30us/step - loss: 0.0391 - val_loss: 0.0543\n",
      "Epoch 1194/3000\n",
      "2370/2370 [==============================] - 0s 30us/step - loss: 0.0342 - val_loss: 0.0423\n",
      "Epoch 1195/3000\n",
      "2370/2370 [==============================] - 0s 30us/step - loss: 0.0391 - val_loss: 0.0456\n",
      "Epoch 1196/3000\n",
      "2370/2370 [==============================] - 0s 30us/step - loss: 0.0331 - val_loss: 0.0610\n",
      "Epoch 1197/3000\n",
      "2370/2370 [==============================] - 0s 30us/step - loss: 0.0349 - val_loss: 0.0489\n",
      "Epoch 1198/3000\n",
      "2370/2370 [==============================] - 0s 30us/step - loss: 0.0343 - val_loss: 0.0486\n",
      "Epoch 1199/3000\n",
      "2370/2370 [==============================] - 0s 30us/step - loss: 0.0359 - val_loss: 0.0392\n",
      "Epoch 1200/3000\n",
      "2370/2370 [==============================] - 0s 30us/step - loss: 0.0376 - val_loss: 0.0634\n",
      "Epoch 1201/3000\n",
      "2370/2370 [==============================] - 0s 30us/step - loss: 0.0358 - val_loss: 0.0597\n",
      "Epoch 1202/3000\n",
      "2370/2370 [==============================] - 0s 30us/step - loss: 0.0358 - val_loss: 0.0500\n",
      "Epoch 1203/3000\n",
      "2370/2370 [==============================] - 0s 30us/step - loss: 0.0364 - val_loss: 0.0547\n",
      "Epoch 1204/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0377 - val_loss: 0.0494\n",
      "Epoch 1205/3000\n",
      "2370/2370 [==============================] - 0s 31us/step - loss: 0.0378 - val_loss: 0.0538\n",
      "Epoch 1206/3000\n",
      "2370/2370 [==============================] - 0s 31us/step - loss: 0.0355 - val_loss: 0.0423\n",
      "Epoch 1207/3000\n",
      "2370/2370 [==============================] - 0s 31us/step - loss: 0.0360 - val_loss: 0.0437\n",
      "Epoch 1208/3000\n",
      "2370/2370 [==============================] - 0s 31us/step - loss: 0.0353 - val_loss: 0.0542\n",
      "Epoch 1209/3000\n",
      "2370/2370 [==============================] - 0s 31us/step - loss: 0.0379 - val_loss: 0.0379\n",
      "Epoch 1210/3000\n",
      "2370/2370 [==============================] - 0s 31us/step - loss: 0.0392 - val_loss: 0.0428\n",
      "Epoch 1211/3000\n",
      "2370/2370 [==============================] - 0s 31us/step - loss: 0.0356 - val_loss: 0.0475\n",
      "Epoch 1212/3000\n",
      "2370/2370 [==============================] - 0s 31us/step - loss: 0.0360 - val_loss: 0.0444\n",
      "Epoch 1213/3000\n",
      "2370/2370 [==============================] - 0s 31us/step - loss: 0.0369 - val_loss: 0.0592\n",
      "Epoch 1214/3000\n",
      "2370/2370 [==============================] - 0s 30us/step - loss: 0.0395 - val_loss: 0.0476\n",
      "Epoch 1215/3000\n",
      "2370/2370 [==============================] - 0s 31us/step - loss: 0.0333 - val_loss: 0.0433\n",
      "Epoch 1216/3000\n",
      "2370/2370 [==============================] - 0s 31us/step - loss: 0.0342 - val_loss: 0.0637\n",
      "Epoch 1217/3000\n",
      "2370/2370 [==============================] - 0s 30us/step - loss: 0.0354 - val_loss: 0.0639\n",
      "Epoch 1218/3000\n",
      "2370/2370 [==============================] - 0s 30us/step - loss: 0.0333 - val_loss: 0.0413\n",
      "Epoch 1219/3000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2370/2370 [==============================] - 0s 31us/step - loss: 0.0419 - val_loss: 0.0619\n",
      "Epoch 1220/3000\n",
      "2370/2370 [==============================] - 0s 30us/step - loss: 0.0357 - val_loss: 0.0453\n",
      "Epoch 1221/3000\n",
      "2370/2370 [==============================] - 0s 30us/step - loss: 0.0376 - val_loss: 0.0437\n",
      "Epoch 1222/3000\n",
      "2370/2370 [==============================] - 0s 32us/step - loss: 0.0335 - val_loss: 0.0478\n",
      "Epoch 1223/3000\n",
      "2370/2370 [==============================] - 0s 32us/step - loss: 0.0363 - val_loss: 0.0366\n",
      "Epoch 1224/3000\n",
      "2370/2370 [==============================] - 0s 30us/step - loss: 0.0364 - val_loss: 0.0372\n",
      "Epoch 1225/3000\n",
      "2370/2370 [==============================] - 0s 30us/step - loss: 0.0322 - val_loss: 0.0420\n",
      "Epoch 1226/3000\n",
      "2370/2370 [==============================] - 0s 31us/step - loss: 0.0358 - val_loss: 0.0545\n",
      "Epoch 1227/3000\n",
      "2370/2370 [==============================] - 0s 31us/step - loss: 0.0367 - val_loss: 0.0745\n",
      "Epoch 1228/3000\n",
      "2370/2370 [==============================] - 0s 31us/step - loss: 0.0344 - val_loss: 0.0503\n",
      "Epoch 1229/3000\n",
      "2370/2370 [==============================] - 0s 31us/step - loss: 0.0393 - val_loss: 0.0614\n",
      "Epoch 1230/3000\n",
      "2370/2370 [==============================] - 0s 30us/step - loss: 0.0372 - val_loss: 0.0451\n",
      "Epoch 1231/3000\n",
      "2370/2370 [==============================] - 0s 31us/step - loss: 0.0354 - val_loss: 0.0419\n",
      "Epoch 1232/3000\n",
      "2370/2370 [==============================] - 0s 31us/step - loss: 0.0375 - val_loss: 0.0468\n",
      "Epoch 1233/3000\n",
      "2370/2370 [==============================] - 0s 30us/step - loss: 0.0355 - val_loss: 0.0491\n",
      "Epoch 1234/3000\n",
      "2370/2370 [==============================] - 0s 30us/step - loss: 0.0374 - val_loss: 0.0598\n",
      "Epoch 1235/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0350 - val_loss: 0.0540\n",
      "Epoch 1236/3000\n",
      "2370/2370 [==============================] - 0s 30us/step - loss: 0.0332 - val_loss: 0.0492\n",
      "Epoch 1237/3000\n",
      "2370/2370 [==============================] - 0s 30us/step - loss: 0.0385 - val_loss: 0.0554\n",
      "Epoch 1238/3000\n",
      "2370/2370 [==============================] - 0s 30us/step - loss: 0.0374 - val_loss: 0.0658\n",
      "Epoch 1239/3000\n",
      "2370/2370 [==============================] - 0s 30us/step - loss: 0.0350 - val_loss: 0.0561\n",
      "Epoch 1240/3000\n",
      "2370/2370 [==============================] - 0s 30us/step - loss: 0.0338 - val_loss: 0.0408\n",
      "Epoch 1241/3000\n",
      "2370/2370 [==============================] - 0s 30us/step - loss: 0.0337 - val_loss: 0.0507\n",
      "Epoch 1242/3000\n",
      "2370/2370 [==============================] - 0s 31us/step - loss: 0.0375 - val_loss: 0.0455\n",
      "Epoch 1243/3000\n",
      "2370/2370 [==============================] - 0s 30us/step - loss: 0.0355 - val_loss: 0.0334\n",
      "Epoch 1244/3000\n",
      "2370/2370 [==============================] - 0s 30us/step - loss: 0.0356 - val_loss: 0.0443\n",
      "Epoch 1245/3000\n",
      "2370/2370 [==============================] - 0s 30us/step - loss: 0.0344 - val_loss: 0.0489\n",
      "Epoch 1246/3000\n",
      "2370/2370 [==============================] - 0s 33us/step - loss: 0.0364 - val_loss: 0.0404\n",
      "Epoch 1247/3000\n",
      "2370/2370 [==============================] - 0s 32us/step - loss: 0.0398 - val_loss: 0.0479\n",
      "Epoch 1248/3000\n",
      "2370/2370 [==============================] - 0s 30us/step - loss: 0.0388 - val_loss: 0.0414\n",
      "Epoch 1249/3000\n",
      "2370/2370 [==============================] - 0s 44us/step - loss: 0.0345 - val_loss: 0.0491\n",
      "Epoch 1250/3000\n",
      "2370/2370 [==============================] - 0s 88us/step - loss: 0.0390 - val_loss: 0.0425\n",
      "Epoch 1251/3000\n",
      "2370/2370 [==============================] - 0s 30us/step - loss: 0.0352 - val_loss: 0.0443\n",
      "Epoch 1252/3000\n",
      "2370/2370 [==============================] - 0s 31us/step - loss: 0.0363 - val_loss: 0.0472\n",
      "Epoch 1253/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0344 - val_loss: 0.0572\n",
      "Epoch 1254/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0346 - val_loss: 0.0401\n",
      "Epoch 1255/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0365 - val_loss: 0.0396\n",
      "Epoch 1256/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0352 - val_loss: 0.0425\n",
      "Epoch 1257/3000\n",
      "2370/2370 [==============================] - 0s 28us/step - loss: 0.0383 - val_loss: 0.0447\n",
      "Epoch 1258/3000\n",
      "2370/2370 [==============================] - 0s 30us/step - loss: 0.0318 - val_loss: 0.0459\n",
      "Epoch 1259/3000\n",
      "2370/2370 [==============================] - 0s 68us/step - loss: 0.0383 - val_loss: 0.0560\n",
      "Epoch 1260/3000\n",
      "2370/2370 [==============================] - 0s 87us/step - loss: 0.0308 - val_loss: 0.0463\n",
      "Epoch 1261/3000\n",
      "2370/2370 [==============================] - 0s 38us/step - loss: 0.0377 - val_loss: 0.0553\n",
      "Epoch 1262/3000\n",
      "2370/2370 [==============================] - 0s 33us/step - loss: 0.0365 - val_loss: 0.0382\n",
      "Epoch 1263/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0388 - val_loss: 0.0445\n",
      "Epoch 1264/3000\n",
      "2370/2370 [==============================] - 0s 28us/step - loss: 0.0358 - val_loss: 0.0504\n",
      "Epoch 1265/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0344 - val_loss: 0.0395\n",
      "Epoch 1266/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0386 - val_loss: 0.0460\n",
      "Epoch 1267/3000\n",
      "2370/2370 [==============================] - 0s 30us/step - loss: 0.0382 - val_loss: 0.0518\n",
      "Epoch 1268/3000\n",
      "2370/2370 [==============================] - 0s 31us/step - loss: 0.0383 - val_loss: 0.0511\n",
      "Epoch 1269/3000\n",
      "2370/2370 [==============================] - 0s 31us/step - loss: 0.0370 - val_loss: 0.0779\n",
      "Epoch 1270/3000\n",
      "2370/2370 [==============================] - 0s 30us/step - loss: 0.0358 - val_loss: 0.0430\n",
      "Epoch 1271/3000\n",
      "2370/2370 [==============================] - 0s 30us/step - loss: 0.0361 - val_loss: 0.0837\n",
      "Epoch 1272/3000\n",
      "2370/2370 [==============================] - 0s 30us/step - loss: 0.0365 - val_loss: 0.0533\n",
      "Epoch 1273/3000\n",
      "2370/2370 [==============================] - 0s 30us/step - loss: 0.0352 - val_loss: 0.0671\n",
      "Epoch 1274/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0365 - val_loss: 0.0414\n",
      "Epoch 1275/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0406 - val_loss: 0.0650\n",
      "Epoch 1276/3000\n",
      "2370/2370 [==============================] - 0s 30us/step - loss: 0.0330 - val_loss: 0.0413\n",
      "Epoch 1277/3000\n",
      "2370/2370 [==============================] - 0s 30us/step - loss: 0.0366 - val_loss: 0.0406\n",
      "Epoch 1278/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0365 - val_loss: 0.0505\n",
      "Epoch 1279/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0349 - val_loss: 0.0489\n",
      "Epoch 1280/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0348 - val_loss: 0.0491\n",
      "Epoch 1281/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0368 - val_loss: 0.0348\n",
      "Epoch 1282/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0355 - val_loss: 0.0422\n",
      "Epoch 1283/3000\n",
      "2370/2370 [==============================] - 0s 28us/step - loss: 0.0382 - val_loss: 0.0428\n",
      "Epoch 1284/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0358 - val_loss: 0.0376\n",
      "Epoch 1285/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0340 - val_loss: 0.0373\n",
      "Epoch 1286/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0374 - val_loss: 0.0399\n",
      "Epoch 1287/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0360 - val_loss: 0.0405\n",
      "Epoch 1288/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0351 - val_loss: 0.0435\n",
      "Epoch 1289/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0337 - val_loss: 0.0807\n",
      "Epoch 1290/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0375 - val_loss: 0.0552\n",
      "Epoch 1291/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0375 - val_loss: 0.0530\n",
      "Epoch 1292/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0325 - val_loss: 0.0449\n",
      "Epoch 1293/3000\n",
      "2370/2370 [==============================] - 0s 30us/step - loss: 0.0358 - val_loss: 0.0416\n",
      "Epoch 1294/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0381 - val_loss: 0.0559\n",
      "Epoch 1295/3000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0370 - val_loss: 0.0575\n",
      "Epoch 1296/3000\n",
      "2370/2370 [==============================] - 0s 30us/step - loss: 0.0403 - val_loss: 0.0361\n",
      "Epoch 1297/3000\n",
      "2370/2370 [==============================] - 0s 32us/step - loss: 0.0353 - val_loss: 0.0516\n",
      "Epoch 1298/3000\n",
      "2370/2370 [==============================] - 0s 28us/step - loss: 0.0337 - val_loss: 0.0429\n",
      "Epoch 1299/3000\n",
      "2370/2370 [==============================] - 0s 28us/step - loss: 0.0359 - val_loss: 0.0494\n",
      "Epoch 1300/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0341 - val_loss: 0.0710\n",
      "Epoch 1301/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0345 - val_loss: 0.0417\n",
      "Epoch 1302/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0351 - val_loss: 0.0367\n",
      "Epoch 1303/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0356 - val_loss: 0.0418\n",
      "Epoch 1304/3000\n",
      "2370/2370 [==============================] - 0s 30us/step - loss: 0.0367 - val_loss: 0.0381\n",
      "Epoch 1305/3000\n",
      "2370/2370 [==============================] - 0s 76us/step - loss: 0.0368 - val_loss: 0.0532\n",
      "Epoch 1306/3000\n",
      "2370/2370 [==============================] - 0s 41us/step - loss: 0.0308 - val_loss: 0.0530\n",
      "Epoch 1307/3000\n",
      "2370/2370 [==============================] - 0s 37us/step - loss: 0.0379 - val_loss: 0.0342\n",
      "Epoch 1308/3000\n",
      "2370/2370 [==============================] - 0s 30us/step - loss: 0.0344 - val_loss: 0.0360\n",
      "Epoch 1309/3000\n",
      "2370/2370 [==============================] - 0s 82us/step - loss: 0.0376 - val_loss: 0.0443\n",
      "Epoch 1310/3000\n",
      "2370/2370 [==============================] - 0s 36us/step - loss: 0.0365 - val_loss: 0.0517\n",
      "Epoch 1311/3000\n",
      "2370/2370 [==============================] - 0s 61us/step - loss: 0.0395 - val_loss: 0.0507\n",
      "Epoch 1312/3000\n",
      "2370/2370 [==============================] - 0s 73us/step - loss: 0.0360 - val_loss: 0.0429\n",
      "Epoch 1313/3000\n",
      "2370/2370 [==============================] - 0s 46us/step - loss: 0.0349 - val_loss: 0.0558\n",
      "Epoch 1314/3000\n",
      "2370/2370 [==============================] - 0s 39us/step - loss: 0.0363 - val_loss: 0.0496\n",
      "Epoch 1315/3000\n",
      "2370/2370 [==============================] - 0s 37us/step - loss: 0.0340 - val_loss: 0.0397\n",
      "Epoch 1316/3000\n",
      "2370/2370 [==============================] - 0s 32us/step - loss: 0.0345 - val_loss: 0.0598\n",
      "Epoch 1317/3000\n",
      "2370/2370 [==============================] - 0s 33us/step - loss: 0.0360 - val_loss: 0.0606\n",
      "Epoch 1318/3000\n",
      "2370/2370 [==============================] - 0s 30us/step - loss: 0.0339 - val_loss: 0.0476\n",
      "Epoch 1319/3000\n",
      "2370/2370 [==============================] - 0s 30us/step - loss: 0.0322 - val_loss: 0.0390\n",
      "Epoch 1320/3000\n",
      "2370/2370 [==============================] - 0s 30us/step - loss: 0.0358 - val_loss: 0.0408\n",
      "Epoch 1321/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0334 - val_loss: 0.0575\n",
      "Epoch 1322/3000\n",
      "2370/2370 [==============================] - 0s 30us/step - loss: 0.0336 - val_loss: 0.0448\n",
      "Epoch 1323/3000\n",
      "2370/2370 [==============================] - 0s 31us/step - loss: 0.0376 - val_loss: 0.0523\n",
      "Epoch 1324/3000\n",
      "2370/2370 [==============================] - 0s 30us/step - loss: 0.0336 - val_loss: 0.0563\n",
      "Epoch 1325/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0369 - val_loss: 0.0547\n",
      "Epoch 1326/3000\n",
      "2370/2370 [==============================] - 0s 30us/step - loss: 0.0353 - val_loss: 0.0504\n",
      "Epoch 1327/3000\n",
      "2370/2370 [==============================] - 0s 30us/step - loss: 0.0366 - val_loss: 0.0396\n",
      "Epoch 1328/3000\n",
      "2370/2370 [==============================] - 0s 30us/step - loss: 0.0343 - val_loss: 0.0626\n",
      "Epoch 1329/3000\n",
      "2370/2370 [==============================] - 0s 30us/step - loss: 0.0357 - val_loss: 0.0392\n",
      "Epoch 1330/3000\n",
      "2370/2370 [==============================] - 0s 30us/step - loss: 0.0375 - val_loss: 0.0553\n",
      "Epoch 1331/3000\n",
      "2370/2370 [==============================] - 0s 31us/step - loss: 0.0353 - val_loss: 0.0364\n",
      "Epoch 1332/3000\n",
      "2370/2370 [==============================] - 0s 30us/step - loss: 0.0362 - val_loss: 0.0546\n",
      "Epoch 1333/3000\n",
      "2370/2370 [==============================] - 0s 30us/step - loss: 0.0370 - val_loss: 0.0497\n",
      "Epoch 1334/3000\n",
      "2370/2370 [==============================] - 0s 30us/step - loss: 0.0319 - val_loss: 0.0677\n",
      "Epoch 1335/3000\n",
      "2370/2370 [==============================] - 0s 30us/step - loss: 0.0361 - val_loss: 0.0400\n",
      "Epoch 1336/3000\n",
      "2370/2370 [==============================] - 0s 32us/step - loss: 0.0358 - val_loss: 0.0367\n",
      "Epoch 1337/3000\n",
      "2370/2370 [==============================] - 0s 30us/step - loss: 0.0339 - val_loss: 0.0464\n",
      "Epoch 1338/3000\n",
      "2370/2370 [==============================] - 0s 31us/step - loss: 0.0328 - val_loss: 0.0495\n",
      "Epoch 1339/3000\n",
      "2370/2370 [==============================] - 0s 30us/step - loss: 0.0364 - val_loss: 0.0389\n",
      "Epoch 1340/3000\n",
      "2370/2370 [==============================] - 0s 31us/step - loss: 0.0305 - val_loss: 0.0599\n",
      "Epoch 1341/3000\n",
      "2370/2370 [==============================] - 0s 30us/step - loss: 0.0342 - val_loss: 0.0510\n",
      "Epoch 1342/3000\n",
      "2370/2370 [==============================] - 0s 30us/step - loss: 0.0347 - val_loss: 0.0514\n",
      "Epoch 1343/3000\n",
      "2370/2370 [==============================] - 0s 30us/step - loss: 0.0344 - val_loss: 0.0635\n",
      "Epoch 1344/3000\n",
      "2370/2370 [==============================] - 0s 30us/step - loss: 0.0347 - val_loss: 0.0409\n",
      "Epoch 1345/3000\n",
      "2370/2370 [==============================] - 0s 30us/step - loss: 0.0342 - val_loss: 0.0447\n",
      "Epoch 1346/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0337 - val_loss: 0.0487\n",
      "Epoch 1347/3000\n",
      "2370/2370 [==============================] - 0s 32us/step - loss: 0.0350 - val_loss: 0.0521\n",
      "Epoch 1348/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0340 - val_loss: 0.0415\n",
      "Epoch 1349/3000\n",
      "2370/2370 [==============================] - 0s 31us/step - loss: 0.0356 - val_loss: 0.0392\n",
      "Epoch 1350/3000\n",
      "2370/2370 [==============================] - 0s 30us/step - loss: 0.0332 - val_loss: 0.0698\n",
      "Epoch 1351/3000\n",
      "2370/2370 [==============================] - 0s 30us/step - loss: 0.0343 - val_loss: 0.0479\n",
      "Epoch 1352/3000\n",
      "2370/2370 [==============================] - 0s 28us/step - loss: 0.0358 - val_loss: 0.0403\n",
      "Epoch 1353/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0371 - val_loss: 0.0621\n",
      "Epoch 1354/3000\n",
      "2370/2370 [==============================] - 0s 30us/step - loss: 0.0386 - val_loss: 0.0593\n",
      "Epoch 1355/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0370 - val_loss: 0.0476\n",
      "Epoch 1356/3000\n",
      "2370/2370 [==============================] - 0s 74us/step - loss: 0.0336 - val_loss: 0.0343\n",
      "Epoch 1357/3000\n",
      "2370/2370 [==============================] - 0s 58us/step - loss: 0.0331 - val_loss: 0.0743\n",
      "Epoch 1358/3000\n",
      "2370/2370 [==============================] - 0s 76us/step - loss: 0.0358 - val_loss: 0.0420\n",
      "Epoch 1359/3000\n",
      "2370/2370 [==============================] - 0s 32us/step - loss: 0.0353 - val_loss: 0.0544\n",
      "Epoch 1360/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0329 - val_loss: 0.0419\n",
      "Epoch 1361/3000\n",
      "2370/2370 [==============================] - 0s 80us/step - loss: 0.0372 - val_loss: 0.0524\n",
      "Epoch 1362/3000\n",
      "2370/2370 [==============================] - 0s 34us/step - loss: 0.0394 - val_loss: 0.0719\n",
      "Epoch 1363/3000\n",
      "2370/2370 [==============================] - 0s 37us/step - loss: 0.0391 - val_loss: 0.0504\n",
      "Epoch 1364/3000\n",
      "2370/2370 [==============================] - 0s 76us/step - loss: 0.0356 - val_loss: 0.0472\n",
      "Epoch 1365/3000\n",
      "2370/2370 [==============================] - 0s 33us/step - loss: 0.0366 - val_loss: 0.0534\n",
      "Epoch 1366/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0334 - val_loss: 0.0494\n",
      "Epoch 1367/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0372 - val_loss: 0.0452\n",
      "Epoch 1368/3000\n",
      "2370/2370 [==============================] - 0s 56us/step - loss: 0.0388 - val_loss: 0.0471\n",
      "Epoch 1369/3000\n",
      "2370/2370 [==============================] - 0s 64us/step - loss: 0.0334 - val_loss: 0.0591\n",
      "Epoch 1370/3000\n",
      "2370/2370 [==============================] - 0s 30us/step - loss: 0.0333 - val_loss: 0.0483\n",
      "Epoch 1371/3000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2370/2370 [==============================] - 0s 77us/step - loss: 0.0337 - val_loss: 0.0449\n",
      "Epoch 1372/3000\n",
      "2370/2370 [==============================] - 0s 75us/step - loss: 0.0343 - val_loss: 0.0533\n",
      "Epoch 1373/3000\n",
      "2370/2370 [==============================] - 0s 58us/step - loss: 0.0325 - val_loss: 0.0469\n",
      "Epoch 1374/3000\n",
      "2370/2370 [==============================] - 0s 31us/step - loss: 0.0339 - val_loss: 0.0386\n",
      "Epoch 1375/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0345 - val_loss: 0.0459\n",
      "Epoch 1376/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0356 - val_loss: 0.0522\n",
      "Epoch 1377/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0372 - val_loss: 0.0622\n",
      "Epoch 1378/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0347 - val_loss: 0.0509\n",
      "Epoch 1379/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0361 - val_loss: 0.0466\n",
      "Epoch 1380/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0421 - val_loss: 0.0413\n",
      "Epoch 1381/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0354 - val_loss: 0.0561\n",
      "Epoch 1382/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0328 - val_loss: 0.0425\n",
      "Epoch 1383/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0345 - val_loss: 0.0508\n",
      "Epoch 1384/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0400 - val_loss: 0.0423\n",
      "Epoch 1385/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0360 - val_loss: 0.0367\n",
      "Epoch 1386/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0363 - val_loss: 0.0465\n",
      "Epoch 1387/3000\n",
      "2370/2370 [==============================] - 0s 30us/step - loss: 0.0364 - val_loss: 0.0379\n",
      "Epoch 1388/3000\n",
      "2370/2370 [==============================] - 0s 45us/step - loss: 0.0374 - val_loss: 0.0678\n",
      "Epoch 1389/3000\n",
      "2370/2370 [==============================] - 0s 80us/step - loss: 0.0358 - val_loss: 0.0380\n",
      "Epoch 1390/3000\n",
      "2370/2370 [==============================] - 0s 30us/step - loss: 0.0344 - val_loss: 0.0365\n",
      "Epoch 1391/3000\n",
      "2370/2370 [==============================] - 0s 30us/step - loss: 0.0363 - val_loss: 0.0563\n",
      "Epoch 1392/3000\n",
      "2370/2370 [==============================] - 0s 30us/step - loss: 0.0373 - val_loss: 0.0653\n",
      "Epoch 1393/3000\n",
      "2370/2370 [==============================] - 0s 87us/step - loss: 0.0335 - val_loss: 0.0503\n",
      "Epoch 1394/3000\n",
      "2370/2370 [==============================] - 0s 87us/step - loss: 0.0340 - val_loss: 0.0467\n",
      "Epoch 1395/3000\n",
      "2370/2370 [==============================] - 0s 35us/step - loss: 0.0361 - val_loss: 0.0450\n",
      "Epoch 1396/3000\n",
      "2370/2370 [==============================] - 0s 89us/step - loss: 0.0391 - val_loss: 0.0403\n",
      "Epoch 1397/3000\n",
      "2370/2370 [==============================] - 0s 47us/step - loss: 0.0347 - val_loss: 0.0453\n",
      "Epoch 1398/3000\n",
      "2370/2370 [==============================] - 0s 64us/step - loss: 0.0334 - val_loss: 0.0417\n",
      "Epoch 1399/3000\n",
      "2370/2370 [==============================] - 0s 57us/step - loss: 0.0350 - val_loss: 0.0446\n",
      "Epoch 1400/3000\n",
      "2370/2370 [==============================] - 0s 51us/step - loss: 0.0360 - val_loss: 0.0444\n",
      "Epoch 1401/3000\n",
      "2370/2370 [==============================] - 0s 71us/step - loss: 0.0367 - val_loss: 0.0387\n",
      "Epoch 1402/3000\n",
      "2370/2370 [==============================] - 0s 48us/step - loss: 0.0346 - val_loss: 0.0443\n",
      "Epoch 1403/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0312 - val_loss: 0.0469\n",
      "Epoch 1404/3000\n",
      "2370/2370 [==============================] - 0s 28us/step - loss: 0.0350 - val_loss: 0.0563\n",
      "Epoch 1405/3000\n",
      "2370/2370 [==============================] - 0s 75us/step - loss: 0.0360 - val_loss: 0.0570\n",
      "Epoch 1406/3000\n",
      "2370/2370 [==============================] - 0s 43us/step - loss: 0.0341 - val_loss: 0.0549\n",
      "Epoch 1407/3000\n",
      "2370/2370 [==============================] - 0s 41us/step - loss: 0.0320 - val_loss: 0.0463\n",
      "Epoch 1408/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0354 - val_loss: 0.0367\n",
      "Epoch 1409/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0326 - val_loss: 0.0431\n",
      "Epoch 1410/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0348 - val_loss: 0.0481\n",
      "Epoch 1411/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0334 - val_loss: 0.0451\n",
      "Epoch 1412/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0323 - val_loss: 0.0363\n",
      "Epoch 1413/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0356 - val_loss: 0.0457\n",
      "Epoch 1414/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0379 - val_loss: 0.0589\n",
      "Epoch 1415/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0344 - val_loss: 0.0608\n",
      "Epoch 1416/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0338 - val_loss: 0.0534\n",
      "Epoch 1417/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0382 - val_loss: 0.0361\n",
      "Epoch 1418/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0380 - val_loss: 0.0433\n",
      "Epoch 1419/3000\n",
      "2370/2370 [==============================] - 0s 30us/step - loss: 0.0318 - val_loss: 0.0425\n",
      "Epoch 1420/3000\n",
      "2370/2370 [==============================] - 0s 31us/step - loss: 0.0362 - val_loss: 0.0385\n",
      "Epoch 1421/3000\n",
      "2370/2370 [==============================] - 0s 32us/step - loss: 0.0362 - val_loss: 0.0543\n",
      "Epoch 1422/3000\n",
      "2370/2370 [==============================] - 0s 31us/step - loss: 0.0313 - val_loss: 0.0569\n",
      "Epoch 1423/3000\n",
      "2370/2370 [==============================] - 0s 31us/step - loss: 0.0336 - val_loss: 0.0521\n",
      "Epoch 1424/3000\n",
      "2370/2370 [==============================] - 0s 31us/step - loss: 0.0352 - val_loss: 0.0462\n",
      "Epoch 1425/3000\n",
      "2370/2370 [==============================] - 0s 31us/step - loss: 0.0361 - val_loss: 0.0432\n",
      "Epoch 1426/3000\n",
      "2370/2370 [==============================] - 0s 32us/step - loss: 0.0347 - val_loss: 0.0537\n",
      "Epoch 1427/3000\n",
      "2370/2370 [==============================] - 0s 30us/step - loss: 0.0335 - val_loss: 0.0500\n",
      "Epoch 1428/3000\n",
      "2370/2370 [==============================] - 0s 38us/step - loss: 0.0352 - val_loss: 0.0565\n",
      "Epoch 1429/3000\n",
      "2370/2370 [==============================] - 0s 38us/step - loss: 0.0347 - val_loss: 0.0477\n",
      "Epoch 1430/3000\n",
      "2370/2370 [==============================] - 0s 35us/step - loss: 0.0379 - val_loss: 0.0478\n",
      "Epoch 1431/3000\n",
      "2370/2370 [==============================] - 0s 34us/step - loss: 0.0345 - val_loss: 0.0439\n",
      "Epoch 1432/3000\n",
      "2370/2370 [==============================] - 0s 32us/step - loss: 0.0411 - val_loss: 0.0602\n",
      "Epoch 1433/3000\n",
      "2370/2370 [==============================] - 0s 80us/step - loss: 0.0370 - val_loss: 0.0518\n",
      "Epoch 1434/3000\n",
      "2370/2370 [==============================] - 0s 125us/step - loss: 0.0340 - val_loss: 0.0547\n",
      "Epoch 1435/3000\n",
      "2370/2370 [==============================] - 0s 126us/step - loss: 0.0366 - val_loss: 0.0563\n",
      "Epoch 1436/3000\n",
      "2370/2370 [==============================] - 0s 85us/step - loss: 0.0379 - val_loss: 0.0487\n",
      "Epoch 1437/3000\n",
      "2370/2370 [==============================] - 0s 44us/step - loss: 0.0336 - val_loss: 0.0453\n",
      "Epoch 1438/3000\n",
      "2370/2370 [==============================] - 0s 34us/step - loss: 0.0345 - val_loss: 0.0608\n",
      "Epoch 1439/3000\n",
      "2370/2370 [==============================] - 0s 34us/step - loss: 0.0362 - val_loss: 0.0425\n",
      "Epoch 1440/3000\n",
      "2370/2370 [==============================] - 0s 33us/step - loss: 0.0377 - val_loss: 0.0469\n",
      "Epoch 1441/3000\n",
      "2370/2370 [==============================] - 0s 33us/step - loss: 0.0381 - val_loss: 0.0483\n",
      "Epoch 1442/3000\n",
      "2370/2370 [==============================] - 0s 33us/step - loss: 0.0337 - val_loss: 0.0605\n",
      "Epoch 1443/3000\n",
      "2370/2370 [==============================] - 0s 34us/step - loss: 0.0357 - val_loss: 0.0460\n",
      "Epoch 1444/3000\n",
      "2370/2370 [==============================] - 0s 34us/step - loss: 0.0368 - val_loss: 0.0598\n",
      "Epoch 1445/3000\n",
      "2370/2370 [==============================] - 0s 33us/step - loss: 0.0352 - val_loss: 0.0444\n",
      "Epoch 1446/3000\n",
      "2370/2370 [==============================] - 0s 33us/step - loss: 0.0371 - val_loss: 0.0447\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1447/3000\n",
      "2370/2370 [==============================] - 0s 35us/step - loss: 0.0370 - val_loss: 0.0449\n",
      "Epoch 1448/3000\n",
      "2370/2370 [==============================] - 0s 34us/step - loss: 0.0341 - val_loss: 0.0465\n",
      "Epoch 1449/3000\n",
      "2370/2370 [==============================] - 0s 33us/step - loss: 0.0363 - val_loss: 0.0548\n",
      "Epoch 1450/3000\n",
      "2370/2370 [==============================] - 0s 33us/step - loss: 0.0385 - val_loss: 0.0452\n",
      "Epoch 1451/3000\n",
      "2370/2370 [==============================] - 0s 33us/step - loss: 0.0338 - val_loss: 0.0494\n",
      "Epoch 1452/3000\n",
      "2370/2370 [==============================] - 0s 33us/step - loss: 0.0351 - val_loss: 0.0499\n",
      "Epoch 1453/3000\n",
      "2370/2370 [==============================] - 0s 33us/step - loss: 0.0367 - val_loss: 0.0459\n",
      "Epoch 1454/3000\n",
      "2370/2370 [==============================] - 0s 34us/step - loss: 0.0336 - val_loss: 0.0449\n",
      "Epoch 1455/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0374 - val_loss: 0.0446\n",
      "Epoch 1456/3000\n",
      "2370/2370 [==============================] - 0s 30us/step - loss: 0.0373 - val_loss: 0.0727\n",
      "Epoch 1457/3000\n",
      "2370/2370 [==============================] - 0s 28us/step - loss: 0.0348 - val_loss: 0.0409\n",
      "Epoch 1458/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0382 - val_loss: 0.0471\n",
      "Epoch 1459/3000\n",
      "2370/2370 [==============================] - 0s 28us/step - loss: 0.0379 - val_loss: 0.0479\n",
      "Epoch 1460/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0327 - val_loss: 0.0594\n",
      "Epoch 1461/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0348 - val_loss: 0.0470\n",
      "Epoch 1462/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0376 - val_loss: 0.0567\n",
      "Epoch 1463/3000\n",
      "2370/2370 [==============================] - 0s 30us/step - loss: 0.0387 - val_loss: 0.0615\n",
      "Epoch 1464/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0365 - val_loss: 0.0360\n",
      "Epoch 1465/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0357 - val_loss: 0.0458\n",
      "Epoch 1466/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0390 - val_loss: 0.0638\n",
      "Epoch 1467/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0345 - val_loss: 0.0779\n",
      "Epoch 1468/3000\n",
      "2370/2370 [==============================] - 0s 28us/step - loss: 0.0347 - val_loss: 0.0534\n",
      "Epoch 1469/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0350 - val_loss: 0.0603\n",
      "Epoch 1470/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0376 - val_loss: 0.0572\n",
      "Epoch 1471/3000\n",
      "2370/2370 [==============================] - 0s 28us/step - loss: 0.0374 - val_loss: 0.0458\n",
      "Epoch 1472/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0376 - val_loss: 0.0431\n",
      "Epoch 1473/3000\n",
      "2370/2370 [==============================] - 0s 28us/step - loss: 0.0391 - val_loss: 0.0469\n",
      "Epoch 1474/3000\n",
      "2370/2370 [==============================] - 0s 30us/step - loss: 0.0351 - val_loss: 0.0400\n",
      "Epoch 1475/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0366 - val_loss: 0.0660\n",
      "Epoch 1476/3000\n",
      "2370/2370 [==============================] - 0s 30us/step - loss: 0.0363 - val_loss: 0.0373\n",
      "Epoch 1477/3000\n",
      "2370/2370 [==============================] - 0s 32us/step - loss: 0.0342 - val_loss: 0.0664\n",
      "Epoch 1478/3000\n",
      "2370/2370 [==============================] - 0s 30us/step - loss: 0.0378 - val_loss: 0.0561\n",
      "Epoch 1479/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0350 - val_loss: 0.0497\n",
      "Epoch 1480/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0374 - val_loss: 0.0701\n",
      "Epoch 1481/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0369 - val_loss: 0.0643\n",
      "Epoch 1482/3000\n",
      "2370/2370 [==============================] - 0s 31us/step - loss: 0.0385 - val_loss: 0.0480\n",
      "Epoch 1483/3000\n",
      "2370/2370 [==============================] - 0s 30us/step - loss: 0.0366 - val_loss: 0.0466\n",
      "Epoch 1484/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0346 - val_loss: 0.0728\n",
      "Epoch 1485/3000\n",
      "2370/2370 [==============================] - 0s 30us/step - loss: 0.0373 - val_loss: 0.0485\n",
      "Epoch 1486/3000\n",
      "2370/2370 [==============================] - 0s 37us/step - loss: 0.0334 - val_loss: 0.0453\n",
      "Epoch 1487/3000\n",
      "2370/2370 [==============================] - 0s 36us/step - loss: 0.0330 - val_loss: 0.0459\n",
      "Epoch 1488/3000\n",
      "2370/2370 [==============================] - 0s 35us/step - loss: 0.0350 - val_loss: 0.0497\n",
      "Epoch 1489/3000\n",
      "2370/2370 [==============================] - 0s 35us/step - loss: 0.0375 - val_loss: 0.0735\n",
      "Epoch 1490/3000\n",
      "2370/2370 [==============================] - 0s 36us/step - loss: 0.0356 - val_loss: 0.0444\n",
      "Epoch 1491/3000\n",
      "2370/2370 [==============================] - 0s 35us/step - loss: 0.0353 - val_loss: 0.0395\n",
      "Epoch 1492/3000\n",
      "2370/2370 [==============================] - 0s 35us/step - loss: 0.0358 - val_loss: 0.0413\n",
      "Epoch 1493/3000\n",
      "2370/2370 [==============================] - 0s 36us/step - loss: 0.0342 - val_loss: 0.0490\n",
      "Epoch 1494/3000\n",
      "2370/2370 [==============================] - 0s 35us/step - loss: 0.0368 - val_loss: 0.0478\n",
      "Epoch 1495/3000\n",
      "2370/2370 [==============================] - 0s 35us/step - loss: 0.0377 - val_loss: 0.0396\n",
      "Epoch 1496/3000\n",
      "2370/2370 [==============================] - 0s 32us/step - loss: 0.0324 - val_loss: 0.0579\n",
      "Epoch 1497/3000\n",
      "2370/2370 [==============================] - 0s 31us/step - loss: 0.0394 - val_loss: 0.0486\n",
      "Epoch 1498/3000\n",
      "2370/2370 [==============================] - 0s 30us/step - loss: 0.0344 - val_loss: 0.0517\n",
      "Epoch 1499/3000\n",
      "2370/2370 [==============================] - 0s 32us/step - loss: 0.0348 - val_loss: 0.0365\n",
      "Epoch 1500/3000\n",
      "2370/2370 [==============================] - 0s 32us/step - loss: 0.0334 - val_loss: 0.0525\n",
      "Epoch 1501/3000\n",
      "2370/2370 [==============================] - 0s 31us/step - loss: 0.0389 - val_loss: 0.0558\n",
      "Epoch 1502/3000\n",
      "2370/2370 [==============================] - 0s 30us/step - loss: 0.0332 - val_loss: 0.0459\n",
      "Epoch 1503/3000\n",
      "2370/2370 [==============================] - 0s 30us/step - loss: 0.0381 - val_loss: 0.0493\n",
      "Epoch 1504/3000\n",
      "2370/2370 [==============================] - 0s 30us/step - loss: 0.0347 - val_loss: 0.0490\n",
      "Epoch 1505/3000\n",
      "2370/2370 [==============================] - 0s 30us/step - loss: 0.0358 - val_loss: 0.0543\n",
      "Epoch 1506/3000\n",
      "2370/2370 [==============================] - 0s 32us/step - loss: 0.0374 - val_loss: 0.0465\n",
      "Epoch 1507/3000\n",
      "2370/2370 [==============================] - 0s 30us/step - loss: 0.0364 - val_loss: 0.0537\n",
      "Epoch 1508/3000\n",
      "2370/2370 [==============================] - 0s 30us/step - loss: 0.0343 - val_loss: 0.0570\n",
      "Epoch 1509/3000\n",
      "2370/2370 [==============================] - 0s 33us/step - loss: 0.0356 - val_loss: 0.0650\n",
      "Epoch 1510/3000\n",
      "2370/2370 [==============================] - 0s 82us/step - loss: 0.0349 - val_loss: 0.0447\n",
      "Epoch 1511/3000\n",
      "2370/2370 [==============================] - 0s 56us/step - loss: 0.0343 - val_loss: 0.0430\n",
      "Epoch 1512/3000\n",
      "2370/2370 [==============================] - 0s 32us/step - loss: 0.0359 - val_loss: 0.0541\n",
      "Epoch 1513/3000\n",
      "2370/2370 [==============================] - 0s 30us/step - loss: 0.0314 - val_loss: 0.0484\n",
      "Epoch 1514/3000\n",
      "2370/2370 [==============================] - 0s 30us/step - loss: 0.0373 - val_loss: 0.0462\n",
      "Epoch 1515/3000\n",
      "2370/2370 [==============================] - 0s 30us/step - loss: 0.0336 - val_loss: 0.0459\n",
      "Epoch 1516/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0372 - val_loss: 0.0475\n",
      "Epoch 1517/3000\n",
      "2370/2370 [==============================] - 0s 30us/step - loss: 0.0335 - val_loss: 0.0498\n",
      "Epoch 1518/3000\n",
      "2370/2370 [==============================] - 0s 30us/step - loss: 0.0354 - val_loss: 0.0487\n",
      "Epoch 1519/3000\n",
      "2370/2370 [==============================] - 0s 31us/step - loss: 0.0365 - val_loss: 0.0544\n",
      "Epoch 1520/3000\n",
      "2370/2370 [==============================] - 0s 30us/step - loss: 0.0375 - val_loss: 0.0532\n",
      "Epoch 1521/3000\n",
      "2370/2370 [==============================] - 0s 30us/step - loss: 0.0377 - val_loss: 0.0636\n",
      "Epoch 1522/3000\n",
      "2370/2370 [==============================] - 0s 34us/step - loss: 0.0361 - val_loss: 0.0721\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1523/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0375 - val_loss: 0.0545\n",
      "Epoch 1524/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0338 - val_loss: 0.0579\n",
      "Epoch 1525/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0396 - val_loss: 0.0366\n",
      "Epoch 1526/3000\n",
      "2370/2370 [==============================] - 0s 28us/step - loss: 0.0338 - val_loss: 0.0570\n",
      "Epoch 1527/3000\n",
      "2370/2370 [==============================] - 0s 28us/step - loss: 0.0345 - val_loss: 0.0501\n",
      "Epoch 1528/3000\n",
      "2370/2370 [==============================] - 0s 31us/step - loss: 0.0353 - val_loss: 0.0507\n",
      "Epoch 1529/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0348 - val_loss: 0.0420\n",
      "Epoch 1530/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0372 - val_loss: 0.0549\n",
      "Epoch 1531/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0393 - val_loss: 0.0619\n",
      "Epoch 1532/3000\n",
      "2370/2370 [==============================] - 0s 31us/step - loss: 0.0370 - val_loss: 0.0581\n",
      "Epoch 1533/3000\n",
      "2370/2370 [==============================] - 0s 30us/step - loss: 0.0378 - val_loss: 0.0441\n",
      "Epoch 1534/3000\n",
      "2370/2370 [==============================] - 0s 30us/step - loss: 0.0373 - val_loss: 0.0502\n",
      "Epoch 1535/3000\n",
      "2370/2370 [==============================] - 0s 30us/step - loss: 0.0334 - val_loss: 0.0403\n",
      "Epoch 1536/3000\n",
      "2370/2370 [==============================] - 0s 32us/step - loss: 0.0360 - val_loss: 0.0612\n",
      "Epoch 1537/3000\n",
      "2370/2370 [==============================] - 0s 30us/step - loss: 0.0331 - val_loss: 0.0519\n",
      "Epoch 1538/3000\n",
      "2370/2370 [==============================] - 0s 30us/step - loss: 0.0379 - val_loss: 0.0477\n",
      "Epoch 1539/3000\n",
      "2370/2370 [==============================] - 0s 30us/step - loss: 0.0359 - val_loss: 0.0516\n",
      "Epoch 1540/3000\n",
      "2370/2370 [==============================] - 0s 30us/step - loss: 0.0326 - val_loss: 0.0566\n",
      "Epoch 1541/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0338 - val_loss: 0.0497\n",
      "Epoch 1542/3000\n",
      "2370/2370 [==============================] - 0s 28us/step - loss: 0.0365 - val_loss: 0.0617\n",
      "Epoch 1543/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0363 - val_loss: 0.0569\n",
      "Epoch 1544/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0335 - val_loss: 0.0543\n",
      "Epoch 1545/3000\n",
      "2370/2370 [==============================] - 0s 31us/step - loss: 0.0360 - val_loss: 0.0470\n",
      "Epoch 1546/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0417 - val_loss: 0.0530\n",
      "Epoch 1547/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0369 - val_loss: 0.0565\n",
      "Epoch 1548/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0356 - val_loss: 0.0562\n",
      "Epoch 1549/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0339 - val_loss: 0.0648\n",
      "Epoch 1550/3000\n",
      "2370/2370 [==============================] - 0s 30us/step - loss: 0.0330 - val_loss: 0.0414\n",
      "Epoch 1551/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0320 - val_loss: 0.0533\n",
      "Epoch 1552/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0360 - val_loss: 0.0567\n",
      "Epoch 1553/3000\n",
      "2370/2370 [==============================] - 0s 30us/step - loss: 0.0357 - val_loss: 0.0677\n",
      "Epoch 1554/3000\n",
      "2370/2370 [==============================] - 0s 30us/step - loss: 0.0338 - val_loss: 0.0508\n",
      "Epoch 1555/3000\n",
      "2370/2370 [==============================] - 0s 33us/step - loss: 0.0372 - val_loss: 0.0461\n",
      "Epoch 1556/3000\n",
      "2370/2370 [==============================] - 0s 33us/step - loss: 0.0388 - val_loss: 0.0375\n",
      "Epoch 1557/3000\n",
      "2370/2370 [==============================] - 0s 32us/step - loss: 0.0346 - val_loss: 0.0655\n",
      "Epoch 1558/3000\n",
      "2370/2370 [==============================] - 0s 32us/step - loss: 0.0377 - val_loss: 0.0455\n",
      "Epoch 1559/3000\n",
      "2370/2370 [==============================] - 0s 32us/step - loss: 0.0366 - val_loss: 0.0529\n",
      "Epoch 1560/3000\n",
      "2370/2370 [==============================] - 0s 32us/step - loss: 0.0363 - val_loss: 0.0491\n",
      "Epoch 1561/3000\n",
      "2370/2370 [==============================] - 0s 32us/step - loss: 0.0400 - val_loss: 0.0475\n",
      "Epoch 1562/3000\n",
      "2370/2370 [==============================] - 0s 32us/step - loss: 0.0376 - val_loss: 0.0544\n",
      "Epoch 1563/3000\n",
      "2370/2370 [==============================] - 0s 38us/step - loss: 0.0363 - val_loss: 0.0463\n",
      "Epoch 1564/3000\n",
      "2370/2370 [==============================] - 0s 35us/step - loss: 0.0385 - val_loss: 0.0341\n",
      "Epoch 1565/3000\n",
      "2370/2370 [==============================] - 0s 30us/step - loss: 0.0372 - val_loss: 0.0456\n",
      "Epoch 1566/3000\n",
      "2370/2370 [==============================] - 0s 30us/step - loss: 0.0341 - val_loss: 0.0493\n",
      "Epoch 1567/3000\n",
      "2370/2370 [==============================] - 0s 31us/step - loss: 0.0366 - val_loss: 0.0433\n",
      "Epoch 1568/3000\n",
      "2370/2370 [==============================] - 0s 30us/step - loss: 0.0371 - val_loss: 0.0449\n",
      "Epoch 1569/3000\n",
      "2370/2370 [==============================] - 0s 30us/step - loss: 0.0349 - val_loss: 0.0584\n",
      "Epoch 1570/3000\n",
      "2370/2370 [==============================] - 0s 30us/step - loss: 0.0364 - val_loss: 0.0480\n",
      "Epoch 1571/3000\n",
      "2370/2370 [==============================] - 0s 30us/step - loss: 0.0388 - val_loss: 0.0704\n",
      "Epoch 1572/3000\n",
      "2370/2370 [==============================] - 0s 30us/step - loss: 0.0375 - val_loss: 0.0465\n",
      "Epoch 1573/3000\n",
      "2370/2370 [==============================] - 0s 32us/step - loss: 0.0345 - val_loss: 0.0565\n",
      "Epoch 1574/3000\n",
      "2370/2370 [==============================] - 0s 84us/step - loss: 0.0338 - val_loss: 0.0381\n",
      "Epoch 1575/3000\n",
      "2370/2370 [==============================] - 0s 31us/step - loss: 0.0337 - val_loss: 0.0432\n",
      "Epoch 1576/3000\n",
      "2370/2370 [==============================] - 0s 30us/step - loss: 0.0371 - val_loss: 0.0432\n",
      "Epoch 1577/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0354 - val_loss: 0.0639\n",
      "Epoch 1578/3000\n",
      "2370/2370 [==============================] - 0s 80us/step - loss: 0.0371 - val_loss: 0.0418\n",
      "Epoch 1579/3000\n",
      "2370/2370 [==============================] - 0s 80us/step - loss: 0.0329 - val_loss: 0.0676\n",
      "Epoch 1580/3000\n",
      "2370/2370 [==============================] - 0s 30us/step - loss: 0.0350 - val_loss: 0.0382\n",
      "Epoch 1581/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0363 - val_loss: 0.0628\n",
      "Epoch 1582/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0318 - val_loss: 0.0627\n",
      "Epoch 1583/3000\n",
      "2370/2370 [==============================] - 0s 86us/step - loss: 0.0349 - val_loss: 0.0371\n",
      "Epoch 1584/3000\n",
      "2370/2370 [==============================] - 0s 42us/step - loss: 0.0331 - val_loss: 0.0533\n",
      "Epoch 1585/3000\n",
      "2370/2370 [==============================] - 0s 38us/step - loss: 0.0346 - val_loss: 0.0476\n",
      "Epoch 1586/3000\n",
      "2370/2370 [==============================] - 0s 30us/step - loss: 0.0355 - val_loss: 0.0554\n",
      "Epoch 1587/3000\n",
      "2370/2370 [==============================] - 0s 28us/step - loss: 0.0335 - val_loss: 0.0484\n",
      "Epoch 1588/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0340 - val_loss: 0.0402\n",
      "Epoch 1589/3000\n",
      "2370/2370 [==============================] - 0s 28us/step - loss: 0.0395 - val_loss: 0.0499\n",
      "Epoch 1590/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0357 - val_loss: 0.0419\n",
      "Epoch 1591/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0344 - val_loss: 0.0384\n",
      "Epoch 1592/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0365 - val_loss: 0.0542\n",
      "Epoch 1593/3000\n",
      "2370/2370 [==============================] - 0s 30us/step - loss: 0.0354 - val_loss: 0.0525\n",
      "Epoch 1594/3000\n",
      "2370/2370 [==============================] - 0s 28us/step - loss: 0.0356 - val_loss: 0.0393\n",
      "Epoch 1595/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0384 - val_loss: 0.0608\n",
      "Epoch 1596/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0342 - val_loss: 0.0493\n",
      "Epoch 1597/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0360 - val_loss: 0.0619\n",
      "Epoch 1598/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0371 - val_loss: 0.0539\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1599/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0354 - val_loss: 0.0558\n",
      "Epoch 1600/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0335 - val_loss: 0.0706\n",
      "Epoch 1601/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0386 - val_loss: 0.0311\n",
      "Epoch 1602/3000\n",
      "2370/2370 [==============================] - 0s 81us/step - loss: 0.0332 - val_loss: 0.0510\n",
      "Epoch 1603/3000\n",
      "2370/2370 [==============================] - 0s 34us/step - loss: 0.0353 - val_loss: 0.0592\n",
      "Epoch 1604/3000\n",
      "2370/2370 [==============================] - 0s 34us/step - loss: 0.0349 - val_loss: 0.0472\n",
      "Epoch 1605/3000\n",
      "2370/2370 [==============================] - 0s 32us/step - loss: 0.0341 - val_loss: 0.0548\n",
      "Epoch 1606/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0349 - val_loss: 0.0438\n",
      "Epoch 1607/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0366 - val_loss: 0.0505\n",
      "Epoch 1608/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0357 - val_loss: 0.0560\n",
      "Epoch 1609/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0354 - val_loss: 0.0476\n",
      "Epoch 1610/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0355 - val_loss: 0.0394\n",
      "Epoch 1611/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0364 - val_loss: 0.0446\n",
      "Epoch 1612/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0336 - val_loss: 0.0556\n",
      "Epoch 1613/3000\n",
      "2370/2370 [==============================] - 0s 30us/step - loss: 0.0336 - val_loss: 0.0528\n",
      "Epoch 1614/3000\n",
      "2370/2370 [==============================] - 0s 30us/step - loss: 0.0378 - val_loss: 0.0501\n",
      "Epoch 1615/3000\n",
      "2370/2370 [==============================] - 0s 30us/step - loss: 0.0356 - val_loss: 0.0416\n",
      "Epoch 1616/3000\n",
      "2370/2370 [==============================] - 0s 30us/step - loss: 0.0394 - val_loss: 0.0416\n",
      "Epoch 1617/3000\n",
      "2370/2370 [==============================] - 0s 30us/step - loss: 0.0343 - val_loss: 0.0548\n",
      "Epoch 1618/3000\n",
      "2370/2370 [==============================] - 0s 30us/step - loss: 0.0335 - val_loss: 0.0406\n",
      "Epoch 1619/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0373 - val_loss: 0.0494\n",
      "Epoch 1620/3000\n",
      "2370/2370 [==============================] - 0s 30us/step - loss: 0.0377 - val_loss: 0.0398\n",
      "Epoch 1621/3000\n",
      "2370/2370 [==============================] - 0s 31us/step - loss: 0.0346 - val_loss: 0.0400\n",
      "Epoch 1622/3000\n",
      "2370/2370 [==============================] - 0s 34us/step - loss: 0.0316 - val_loss: 0.0485\n",
      "Epoch 1623/3000\n",
      "2370/2370 [==============================] - 0s 62us/step - loss: 0.0335 - val_loss: 0.0650\n",
      "Epoch 1624/3000\n",
      "2370/2370 [==============================] - 0s 40us/step - loss: 0.0361 - val_loss: 0.0571\n",
      "Epoch 1625/3000\n",
      "2370/2370 [==============================] - 0s 39us/step - loss: 0.0364 - val_loss: 0.0531\n",
      "Epoch 1626/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0343 - val_loss: 0.0671\n",
      "Epoch 1627/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0372 - val_loss: 0.0502\n",
      "Epoch 1628/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0351 - val_loss: 0.0537\n",
      "Epoch 1629/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0330 - val_loss: 0.0577\n",
      "Epoch 1630/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0367 - val_loss: 0.0589\n",
      "Epoch 1631/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0339 - val_loss: 0.0513\n",
      "Epoch 1632/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0384 - val_loss: 0.0512\n",
      "Epoch 1633/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0355 - val_loss: 0.0694\n",
      "Epoch 1634/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0360 - val_loss: 0.0553\n",
      "Epoch 1635/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0338 - val_loss: 0.0535\n",
      "Epoch 1636/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0341 - val_loss: 0.0568\n",
      "Epoch 1637/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0383 - val_loss: 0.0461\n",
      "Epoch 1638/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0324 - val_loss: 0.0392\n",
      "Epoch 1639/3000\n",
      "2370/2370 [==============================] - 0s 28us/step - loss: 0.0385 - val_loss: 0.0402\n",
      "Epoch 1640/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0334 - val_loss: 0.0437\n",
      "Epoch 1641/3000\n",
      "2370/2370 [==============================] - 0s 30us/step - loss: 0.0395 - val_loss: 0.0404\n",
      "Epoch 1642/3000\n",
      "2370/2370 [==============================] - 0s 30us/step - loss: 0.0361 - val_loss: 0.0584\n",
      "Epoch 1643/3000\n",
      "2370/2370 [==============================] - 0s 30us/step - loss: 0.0365 - val_loss: 0.0563\n",
      "Epoch 1644/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0340 - val_loss: 0.0627\n",
      "Epoch 1645/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0344 - val_loss: 0.0601\n",
      "Epoch 1646/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0355 - val_loss: 0.0608\n",
      "Epoch 1647/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0345 - val_loss: 0.0462\n",
      "Epoch 1648/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0374 - val_loss: 0.0433\n",
      "Epoch 1649/3000\n",
      "2370/2370 [==============================] - 0s 28us/step - loss: 0.0373 - val_loss: 0.0685\n",
      "Epoch 1650/3000\n",
      "2370/2370 [==============================] - 0s 34us/step - loss: 0.0374 - val_loss: 0.0628\n",
      "Epoch 1651/3000\n",
      "2370/2370 [==============================] - 0s 31us/step - loss: 0.0360 - val_loss: 0.0632\n",
      "Epoch 1652/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0372 - val_loss: 0.0640\n",
      "Epoch 1653/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0355 - val_loss: 0.0496\n",
      "Epoch 1654/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0361 - val_loss: 0.0463\n",
      "Epoch 1655/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0344 - val_loss: 0.0507\n",
      "Epoch 1656/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0334 - val_loss: 0.0903\n",
      "Epoch 1657/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0355 - val_loss: 0.0484\n",
      "Epoch 1658/3000\n",
      "2370/2370 [==============================] - 0s 30us/step - loss: 0.0347 - val_loss: 0.0393\n",
      "Epoch 1659/3000\n",
      "2370/2370 [==============================] - 0s 30us/step - loss: 0.0364 - val_loss: 0.0472\n",
      "Epoch 1660/3000\n",
      "2370/2370 [==============================] - 0s 31us/step - loss: 0.0337 - val_loss: 0.0533\n",
      "Epoch 1661/3000\n",
      "2370/2370 [==============================] - 0s 28us/step - loss: 0.0344 - val_loss: 0.0644\n",
      "Epoch 1662/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0357 - val_loss: 0.0586\n",
      "Epoch 1663/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0329 - val_loss: 0.0663\n",
      "Epoch 1664/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0365 - val_loss: 0.0450\n",
      "Epoch 1665/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0332 - val_loss: 0.0516\n",
      "Epoch 1666/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0364 - val_loss: 0.0417\n",
      "Epoch 1667/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0329 - val_loss: 0.0480\n",
      "Epoch 1668/3000\n",
      "2370/2370 [==============================] - 0s 30us/step - loss: 0.0364 - val_loss: 0.0515\n",
      "Epoch 1669/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0354 - val_loss: 0.0501\n",
      "Epoch 1670/3000\n",
      "2370/2370 [==============================] - 0s 28us/step - loss: 0.0393 - val_loss: 0.0471\n",
      "Epoch 1671/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0336 - val_loss: 0.0429\n",
      "Epoch 1672/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0366 - val_loss: 0.0506\n",
      "Epoch 1673/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0348 - val_loss: 0.0418\n",
      "Epoch 1674/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0303 - val_loss: 0.0454\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1675/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0351 - val_loss: 0.0565\n",
      "Epoch 1676/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0351 - val_loss: 0.0547\n",
      "Epoch 1677/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0350 - val_loss: 0.0592\n",
      "Epoch 1678/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0369 - val_loss: 0.0514\n",
      "Epoch 1679/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0380 - val_loss: 0.0417\n",
      "Epoch 1680/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0369 - val_loss: 0.0528\n",
      "Epoch 1681/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0375 - val_loss: 0.0581\n",
      "Epoch 1682/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0349 - val_loss: 0.0362\n",
      "Epoch 1683/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0391 - val_loss: 0.0359\n",
      "Epoch 1684/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0384 - val_loss: 0.0597\n",
      "Epoch 1685/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0365 - val_loss: 0.0514\n",
      "Epoch 1686/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0359 - val_loss: 0.0646\n",
      "Epoch 1687/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0330 - val_loss: 0.0458\n",
      "Epoch 1688/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0383 - val_loss: 0.0439\n",
      "Epoch 1689/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0371 - val_loss: 0.0592\n",
      "Epoch 1690/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0376 - val_loss: 0.0533\n",
      "Epoch 1691/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0344 - val_loss: 0.0652\n",
      "Epoch 1692/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0357 - val_loss: 0.0482\n",
      "Epoch 1693/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0343 - val_loss: 0.0510\n",
      "Epoch 1694/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0355 - val_loss: 0.0543\n",
      "Epoch 1695/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0346 - val_loss: 0.0506\n",
      "Epoch 1696/3000\n",
      "2370/2370 [==============================] - 0s 31us/step - loss: 0.0364 - val_loss: 0.0541\n",
      "Epoch 1697/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0349 - val_loss: 0.0396\n",
      "Epoch 1698/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0324 - val_loss: 0.0662\n",
      "Epoch 1699/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0371 - val_loss: 0.0500\n",
      "Epoch 1700/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0355 - val_loss: 0.0552\n",
      "Epoch 1701/3000\n",
      "2370/2370 [==============================] - 0s 28us/step - loss: 0.0316 - val_loss: 0.0429\n",
      "Epoch 1702/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0356 - val_loss: 0.0650\n",
      "Epoch 1703/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0362 - val_loss: 0.0475\n",
      "Epoch 1704/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0354 - val_loss: 0.0510\n",
      "Epoch 1705/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0347 - val_loss: 0.0369\n",
      "Epoch 1706/3000\n",
      "2370/2370 [==============================] - 0s 28us/step - loss: 0.0326 - val_loss: 0.0434\n",
      "Epoch 1707/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0326 - val_loss: 0.0521\n",
      "Epoch 1708/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0351 - val_loss: 0.0484\n",
      "Epoch 1709/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0333 - val_loss: 0.0422\n",
      "Epoch 1710/3000\n",
      "2370/2370 [==============================] - 0s 28us/step - loss: 0.0341 - val_loss: 0.0493\n",
      "Epoch 1711/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0373 - val_loss: 0.0497\n",
      "Epoch 1712/3000\n",
      "2370/2370 [==============================] - 0s 28us/step - loss: 0.0356 - val_loss: 0.0512\n",
      "Epoch 1713/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0338 - val_loss: 0.0389\n",
      "Epoch 1714/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0386 - val_loss: 0.0403\n",
      "Epoch 1715/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0370 - val_loss: 0.0509\n",
      "Epoch 1716/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0340 - val_loss: 0.0532\n",
      "Epoch 1717/3000\n",
      "2370/2370 [==============================] - 0s 28us/step - loss: 0.0320 - val_loss: 0.0482\n",
      "Epoch 1718/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0312 - val_loss: 0.0529\n",
      "Epoch 1719/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0378 - val_loss: 0.0467\n",
      "Epoch 1720/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0373 - val_loss: 0.0552\n",
      "Epoch 1721/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0347 - val_loss: 0.0429\n",
      "Epoch 1722/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0312 - val_loss: 0.0651\n",
      "Epoch 1723/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0368 - val_loss: 0.0583\n",
      "Epoch 1724/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0351 - val_loss: 0.0651\n",
      "Epoch 1725/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0373 - val_loss: 0.0541\n",
      "Epoch 1726/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0386 - val_loss: 0.0514\n",
      "Epoch 1727/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0369 - val_loss: 0.0533\n",
      "Epoch 1728/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0352 - val_loss: 0.0538\n",
      "Epoch 1729/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0375 - val_loss: 0.0600\n",
      "Epoch 1730/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0339 - val_loss: 0.0669\n",
      "Epoch 1731/3000\n",
      "2370/2370 [==============================] - 0s 30us/step - loss: 0.0389 - val_loss: 0.0512\n",
      "Epoch 1732/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0390 - val_loss: 0.0420\n",
      "Epoch 1733/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0383 - val_loss: 0.0430\n",
      "Epoch 1734/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0385 - val_loss: 0.0631\n",
      "Epoch 1735/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0330 - val_loss: 0.0493\n",
      "Epoch 1736/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0374 - val_loss: 0.0526\n",
      "Epoch 1737/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0364 - val_loss: 0.0504\n",
      "Epoch 1738/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0347 - val_loss: 0.0473\n",
      "Epoch 1739/3000\n",
      "2370/2370 [==============================] - 0s 28us/step - loss: 0.0331 - val_loss: 0.0582\n",
      "Epoch 1740/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0358 - val_loss: 0.0472\n",
      "Epoch 1741/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0362 - val_loss: 0.0606\n",
      "Epoch 1742/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0341 - val_loss: 0.0566\n",
      "Epoch 1743/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0379 - val_loss: 0.0621\n",
      "Epoch 1744/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0357 - val_loss: 0.0545\n",
      "Epoch 1745/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0299 - val_loss: 0.0464\n",
      "Epoch 1746/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0347 - val_loss: 0.0385\n",
      "Epoch 1747/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0365 - val_loss: 0.0405\n",
      "Epoch 1748/3000\n",
      "2370/2370 [==============================] - 0s 30us/step - loss: 0.0354 - val_loss: 0.0595\n",
      "Epoch 1749/3000\n",
      "2370/2370 [==============================] - 0s 30us/step - loss: 0.0359 - val_loss: 0.0455\n",
      "Epoch 1750/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0356 - val_loss: 0.0517\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1751/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0372 - val_loss: 0.0575\n",
      "Epoch 1752/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0343 - val_loss: 0.0451\n",
      "Epoch 1753/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0372 - val_loss: 0.0820\n",
      "Epoch 1754/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0344 - val_loss: 0.0530\n",
      "Epoch 1755/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0376 - val_loss: 0.0531\n",
      "Epoch 1756/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0381 - val_loss: 0.0492\n",
      "Epoch 1757/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0358 - val_loss: 0.0501\n",
      "Epoch 1758/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0372 - val_loss: 0.0785\n",
      "Epoch 1759/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0373 - val_loss: 0.0444\n",
      "Epoch 1760/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0337 - val_loss: 0.0409\n",
      "Epoch 1761/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0400 - val_loss: 0.0391\n",
      "Epoch 1762/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0377 - val_loss: 0.0537\n",
      "Epoch 1763/3000\n",
      "2370/2370 [==============================] - 0s 28us/step - loss: 0.0360 - val_loss: 0.0461\n",
      "Epoch 1764/3000\n",
      "2370/2370 [==============================] - 0s 30us/step - loss: 0.0361 - val_loss: 0.0628\n",
      "Epoch 1765/3000\n",
      "2370/2370 [==============================] - 0s 30us/step - loss: 0.0348 - val_loss: 0.0439\n",
      "Epoch 1766/3000\n",
      "2370/2370 [==============================] - 0s 30us/step - loss: 0.0388 - val_loss: 0.0502\n",
      "Epoch 1767/3000\n",
      "2370/2370 [==============================] - 0s 30us/step - loss: 0.0393 - val_loss: 0.0571\n",
      "Epoch 1768/3000\n",
      "2370/2370 [==============================] - 0s 30us/step - loss: 0.0363 - val_loss: 0.0447\n",
      "Epoch 1769/3000\n",
      "2370/2370 [==============================] - 0s 30us/step - loss: 0.0308 - val_loss: 0.0367\n",
      "Epoch 1770/3000\n",
      "2370/2370 [==============================] - 0s 30us/step - loss: 0.0383 - val_loss: 0.0484\n",
      "Epoch 1771/3000\n",
      "2370/2370 [==============================] - 0s 30us/step - loss: 0.0361 - val_loss: 0.0460\n",
      "Epoch 1772/3000\n",
      "2370/2370 [==============================] - 0s 30us/step - loss: 0.0365 - val_loss: 0.0389\n",
      "Epoch 1773/3000\n",
      "2370/2370 [==============================] - 0s 30us/step - loss: 0.0363 - val_loss: 0.0642\n",
      "Epoch 1774/3000\n",
      "2370/2370 [==============================] - 0s 31us/step - loss: 0.0337 - val_loss: 0.0579\n",
      "Epoch 1775/3000\n",
      "2370/2370 [==============================] - 0s 30us/step - loss: 0.0365 - val_loss: 0.0376\n",
      "Epoch 1776/3000\n",
      "2370/2370 [==============================] - 0s 30us/step - loss: 0.0364 - val_loss: 0.0492\n",
      "Epoch 1777/3000\n",
      "2370/2370 [==============================] - 0s 30us/step - loss: 0.0364 - val_loss: 0.0477\n",
      "Epoch 1778/3000\n",
      "2370/2370 [==============================] - 0s 30us/step - loss: 0.0351 - val_loss: 0.0575\n",
      "Epoch 1779/3000\n",
      "2370/2370 [==============================] - 0s 34us/step - loss: 0.0325 - val_loss: 0.0615\n",
      "Epoch 1780/3000\n",
      "2370/2370 [==============================] - 0s 32us/step - loss: 0.0364 - val_loss: 0.0422\n",
      "Epoch 1781/3000\n",
      "2370/2370 [==============================] - 0s 30us/step - loss: 0.0374 - val_loss: 0.0405\n",
      "Epoch 1782/3000\n",
      "2370/2370 [==============================] - 0s 30us/step - loss: 0.0348 - val_loss: 0.0460\n",
      "Epoch 1783/3000\n",
      "2370/2370 [==============================] - 0s 30us/step - loss: 0.0371 - val_loss: 0.0503\n",
      "Epoch 1784/3000\n",
      "2370/2370 [==============================] - 0s 31us/step - loss: 0.0341 - val_loss: 0.0591\n",
      "Epoch 1785/3000\n",
      "2370/2370 [==============================] - 0s 31us/step - loss: 0.0361 - val_loss: 0.0526\n",
      "Epoch 1786/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0335 - val_loss: 0.0486\n",
      "Epoch 1787/3000\n",
      "2370/2370 [==============================] - 0s 31us/step - loss: 0.0358 - val_loss: 0.0570\n",
      "Epoch 1788/3000\n",
      "2370/2370 [==============================] - 0s 32us/step - loss: 0.0391 - val_loss: 0.0537\n",
      "Epoch 1789/3000\n",
      "2370/2370 [==============================] - 0s 30us/step - loss: 0.0333 - val_loss: 0.0606\n",
      "Epoch 1790/3000\n",
      "2370/2370 [==============================] - 0s 30us/step - loss: 0.0339 - val_loss: 0.0528\n",
      "Epoch 1791/3000\n",
      "2370/2370 [==============================] - 0s 30us/step - loss: 0.0364 - val_loss: 0.0746\n",
      "Epoch 1792/3000\n",
      "2370/2370 [==============================] - 0s 30us/step - loss: 0.0350 - val_loss: 0.0459\n",
      "Epoch 1793/3000\n",
      "2370/2370 [==============================] - 0s 30us/step - loss: 0.0332 - val_loss: 0.0661\n",
      "Epoch 1794/3000\n",
      "2370/2370 [==============================] - 0s 37us/step - loss: 0.0365 - val_loss: 0.0680\n",
      "Epoch 1795/3000\n",
      "2370/2370 [==============================] - 0s 30us/step - loss: 0.0342 - val_loss: 0.0496\n",
      "Epoch 1796/3000\n",
      "2370/2370 [==============================] - 0s 30us/step - loss: 0.0376 - val_loss: 0.0500\n",
      "Epoch 1797/3000\n",
      "2370/2370 [==============================] - 0s 31us/step - loss: 0.0335 - val_loss: 0.0501\n",
      "Epoch 1798/3000\n",
      "2370/2370 [==============================] - 0s 30us/step - loss: 0.0382 - val_loss: 0.0447\n",
      "Epoch 1799/3000\n",
      "2370/2370 [==============================] - 0s 30us/step - loss: 0.0334 - val_loss: 0.0506\n",
      "Epoch 1800/3000\n",
      "2370/2370 [==============================] - 0s 30us/step - loss: 0.0363 - val_loss: 0.0423\n",
      "Epoch 1801/3000\n",
      "2370/2370 [==============================] - 0s 30us/step - loss: 0.0341 - val_loss: 0.0612\n",
      "Epoch 1802/3000\n",
      "2370/2370 [==============================] - 0s 30us/step - loss: 0.0341 - val_loss: 0.0497\n",
      "Epoch 1803/3000\n",
      "2370/2370 [==============================] - 0s 31us/step - loss: 0.0360 - val_loss: 0.0490\n",
      "Epoch 1804/3000\n",
      "2370/2370 [==============================] - 0s 30us/step - loss: 0.0373 - val_loss: 0.0447\n",
      "Epoch 1805/3000\n",
      "2370/2370 [==============================] - 0s 31us/step - loss: 0.0367 - val_loss: 0.0613\n",
      "Epoch 1806/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0349 - val_loss: 0.0593\n",
      "Epoch 1807/3000\n",
      "2370/2370 [==============================] - 0s 28us/step - loss: 0.0332 - val_loss: 0.0439\n",
      "Epoch 1808/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0329 - val_loss: 0.0448\n",
      "Epoch 1809/3000\n",
      "2370/2370 [==============================] - 0s 28us/step - loss: 0.0380 - val_loss: 0.0559\n",
      "Epoch 1810/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0353 - val_loss: 0.0595\n",
      "Epoch 1811/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0361 - val_loss: 0.0354\n",
      "Epoch 1812/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0365 - val_loss: 0.0559\n",
      "Epoch 1813/3000\n",
      "2370/2370 [==============================] - 0s 30us/step - loss: 0.0360 - val_loss: 0.0397\n",
      "Epoch 1814/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0349 - val_loss: 0.0460\n",
      "Epoch 1815/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0337 - val_loss: 0.0499\n",
      "Epoch 1816/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0358 - val_loss: 0.0544\n",
      "Epoch 1817/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0340 - val_loss: 0.0600\n",
      "Epoch 1818/3000\n",
      "2370/2370 [==============================] - 0s 28us/step - loss: 0.0379 - val_loss: 0.0445\n",
      "Epoch 1819/3000\n",
      "2370/2370 [==============================] - 0s 30us/step - loss: 0.0362 - val_loss: 0.0610\n",
      "Epoch 1820/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0341 - val_loss: 0.0595\n",
      "Epoch 1821/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0370 - val_loss: 0.0507\n",
      "Epoch 1822/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0372 - val_loss: 0.0433\n",
      "Epoch 1823/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0350 - val_loss: 0.0449\n",
      "Epoch 1824/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0352 - val_loss: 0.0641\n",
      "Epoch 1825/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0340 - val_loss: 0.0608\n",
      "Epoch 1826/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0371 - val_loss: 0.0435\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1827/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0345 - val_loss: 0.0461\n",
      "Epoch 1828/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0349 - val_loss: 0.0671\n",
      "Epoch 1829/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0367 - val_loss: 0.0617\n",
      "Epoch 1830/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0342 - val_loss: 0.0498\n",
      "Epoch 1831/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0347 - val_loss: 0.0507\n",
      "Epoch 1832/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0327 - val_loss: 0.0506\n",
      "Epoch 1833/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0349 - val_loss: 0.0483\n",
      "Epoch 1834/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0380 - val_loss: 0.0435\n",
      "Epoch 1835/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0340 - val_loss: 0.0530\n",
      "Epoch 1836/3000\n",
      "2370/2370 [==============================] - 0s 30us/step - loss: 0.0318 - val_loss: 0.0538\n",
      "Epoch 1837/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0351 - val_loss: 0.0509\n",
      "Epoch 1838/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0349 - val_loss: 0.0471\n",
      "Epoch 1839/3000\n",
      "2370/2370 [==============================] - 0s 28us/step - loss: 0.0339 - val_loss: 0.0672\n",
      "Epoch 1840/3000\n",
      "2370/2370 [==============================] - 0s 30us/step - loss: 0.0347 - val_loss: 0.0520\n",
      "Epoch 1841/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0351 - val_loss: 0.0429\n",
      "Epoch 1842/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0409 - val_loss: 0.0488\n",
      "Epoch 1843/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0368 - val_loss: 0.0510\n",
      "Epoch 1844/3000\n",
      "2370/2370 [==============================] - 0s 30us/step - loss: 0.0362 - val_loss: 0.0440\n",
      "Epoch 1845/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0370 - val_loss: 0.0457\n",
      "Epoch 1846/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0334 - val_loss: 0.0481\n",
      "Epoch 1847/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0341 - val_loss: 0.0492\n",
      "Epoch 1848/3000\n",
      "2370/2370 [==============================] - 0s 28us/step - loss: 0.0349 - val_loss: 0.0474\n",
      "Epoch 1849/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0358 - val_loss: 0.0549\n",
      "Epoch 1850/3000\n",
      "2370/2370 [==============================] - 0s 30us/step - loss: 0.0390 - val_loss: 0.0521\n",
      "Epoch 1851/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0328 - val_loss: 0.0488\n",
      "Epoch 1852/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0372 - val_loss: 0.0499\n",
      "Epoch 1853/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0372 - val_loss: 0.0476\n",
      "Epoch 1854/3000\n",
      "2370/2370 [==============================] - 0s 30us/step - loss: 0.0351 - val_loss: 0.0589\n",
      "Epoch 1855/3000\n",
      "2370/2370 [==============================] - 0s 28us/step - loss: 0.0383 - val_loss: 0.0656\n",
      "Epoch 1856/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0337 - val_loss: 0.0542\n",
      "Epoch 1857/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0349 - val_loss: 0.0640\n",
      "Epoch 1858/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0343 - val_loss: 0.0490\n",
      "Epoch 1859/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0385 - val_loss: 0.0443\n",
      "Epoch 1860/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0359 - val_loss: 0.0384\n",
      "Epoch 1861/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0368 - val_loss: 0.0457\n",
      "Epoch 1862/3000\n",
      "2370/2370 [==============================] - 0s 30us/step - loss: 0.0345 - val_loss: 0.0585\n",
      "Epoch 1863/3000\n",
      "2370/2370 [==============================] - 0s 30us/step - loss: 0.0349 - val_loss: 0.0514\n",
      "Epoch 1864/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0375 - val_loss: 0.0517\n",
      "Epoch 1865/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0348 - val_loss: 0.0619\n",
      "Epoch 1866/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0340 - val_loss: 0.0440\n",
      "Epoch 1867/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0365 - val_loss: 0.0565\n",
      "Epoch 1868/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0402 - val_loss: 0.0437\n",
      "Epoch 1869/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0371 - val_loss: 0.0541\n",
      "Epoch 1870/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0388 - val_loss: 0.0461\n",
      "Epoch 1871/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0335 - val_loss: 0.0383\n",
      "Epoch 1872/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0337 - val_loss: 0.0581\n",
      "Epoch 1873/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0396 - val_loss: 0.0573\n",
      "Epoch 1874/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0351 - val_loss: 0.0582\n",
      "Epoch 1875/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0357 - val_loss: 0.0528\n",
      "Epoch 1876/3000\n",
      "2370/2370 [==============================] - 0s 28us/step - loss: 0.0353 - val_loss: 0.0504\n",
      "Epoch 1877/3000\n",
      "2370/2370 [==============================] - 0s 28us/step - loss: 0.0364 - val_loss: 0.0454\n",
      "Epoch 1878/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0356 - val_loss: 0.0654\n",
      "Epoch 1879/3000\n",
      "2370/2370 [==============================] - 0s 30us/step - loss: 0.0356 - val_loss: 0.0498\n",
      "Epoch 1880/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0337 - val_loss: 0.0463\n",
      "Epoch 1881/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0339 - val_loss: 0.0677\n",
      "Epoch 1882/3000\n",
      "2370/2370 [==============================] - 0s 28us/step - loss: 0.0363 - val_loss: 0.0709\n",
      "Epoch 1883/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0325 - val_loss: 0.0343\n",
      "Epoch 1884/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0376 - val_loss: 0.0526\n",
      "Epoch 1885/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0357 - val_loss: 0.0464\n",
      "Epoch 1886/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0391 - val_loss: 0.0642\n",
      "Epoch 1887/3000\n",
      "2370/2370 [==============================] - 0s 28us/step - loss: 0.0374 - val_loss: 0.0665\n",
      "Epoch 1888/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0339 - val_loss: 0.0668\n",
      "Epoch 1889/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0367 - val_loss: 0.0584\n",
      "Epoch 1890/3000\n",
      "2370/2370 [==============================] - 0s 33us/step - loss: 0.0326 - val_loss: 0.0455\n",
      "Epoch 1891/3000\n",
      "2370/2370 [==============================] - 0s 69us/step - loss: 0.0364 - val_loss: 0.0636\n",
      "Epoch 1892/3000\n",
      "2370/2370 [==============================] - 0s 43us/step - loss: 0.0348 - val_loss: 0.0462\n",
      "Epoch 1893/3000\n",
      "2370/2370 [==============================] - 0s 43us/step - loss: 0.0326 - val_loss: 0.0609\n",
      "Epoch 1894/3000\n",
      "2370/2370 [==============================] - 0s 42us/step - loss: 0.0371 - val_loss: 0.0507\n",
      "Epoch 1895/3000\n",
      "2370/2370 [==============================] - 0s 44us/step - loss: 0.0367 - val_loss: 0.0539\n",
      "Epoch 1896/3000\n",
      "2370/2370 [==============================] - 0s 42us/step - loss: 0.0378 - val_loss: 0.0466\n",
      "Epoch 1897/3000\n",
      "2370/2370 [==============================] - 0s 42us/step - loss: 0.0392 - val_loss: 0.0534\n",
      "Epoch 1898/3000\n",
      "2370/2370 [==============================] - 0s 31us/step - loss: 0.0370 - val_loss: 0.0439\n",
      "Epoch 1899/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0345 - val_loss: 0.0602\n",
      "Epoch 1900/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0323 - val_loss: 0.0607\n",
      "Epoch 1901/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0379 - val_loss: 0.0323\n",
      "Epoch 1902/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0351 - val_loss: 0.0614\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1903/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0362 - val_loss: 0.0572\n",
      "Epoch 1904/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0303 - val_loss: 0.0617\n",
      "Epoch 1905/3000\n",
      "2370/2370 [==============================] - 0s 28us/step - loss: 0.0336 - val_loss: 0.0478\n",
      "Epoch 1906/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0335 - val_loss: 0.0584\n",
      "Epoch 1907/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0339 - val_loss: 0.0588\n",
      "Epoch 1908/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0350 - val_loss: 0.0563\n",
      "Epoch 1909/3000\n",
      "2370/2370 [==============================] - 0s 28us/step - loss: 0.0357 - val_loss: 0.0472\n",
      "Epoch 1910/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0353 - val_loss: 0.0565\n",
      "Epoch 1911/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0354 - val_loss: 0.0549\n",
      "Epoch 1912/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0363 - val_loss: 0.0420\n",
      "Epoch 1913/3000\n",
      "2370/2370 [==============================] - 0s 28us/step - loss: 0.0402 - val_loss: 0.0652\n",
      "Epoch 1914/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0367 - val_loss: 0.0563\n",
      "Epoch 1915/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0402 - val_loss: 0.0615\n",
      "Epoch 1916/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0372 - val_loss: 0.0511\n",
      "Epoch 1917/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0349 - val_loss: 0.0509\n",
      "Epoch 1918/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0357 - val_loss: 0.0624\n",
      "Epoch 1919/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0341 - val_loss: 0.0448\n",
      "Epoch 1920/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0374 - val_loss: 0.0643\n",
      "Epoch 1921/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0345 - val_loss: 0.0508\n",
      "Epoch 1922/3000\n",
      "2370/2370 [==============================] - 0s 30us/step - loss: 0.0371 - val_loss: 0.0453\n",
      "Epoch 1923/3000\n",
      "2370/2370 [==============================] - 0s 30us/step - loss: 0.0339 - val_loss: 0.0408\n",
      "Epoch 1924/3000\n",
      "2370/2370 [==============================] - 0s 30us/step - loss: 0.0353 - val_loss: 0.0566\n",
      "Epoch 1925/3000\n",
      "2370/2370 [==============================] - 0s 30us/step - loss: 0.0311 - val_loss: 0.0406\n",
      "Epoch 1926/3000\n",
      "2370/2370 [==============================] - 0s 30us/step - loss: 0.0343 - val_loss: 0.0496\n",
      "Epoch 1927/3000\n",
      "2370/2370 [==============================] - 0s 30us/step - loss: 0.0333 - val_loss: 0.0547\n",
      "Epoch 1928/3000\n",
      "2370/2370 [==============================] - 0s 30us/step - loss: 0.0339 - val_loss: 0.0589\n",
      "Epoch 1929/3000\n",
      "2370/2370 [==============================] - 0s 30us/step - loss: 0.0371 - val_loss: 0.0496\n",
      "Epoch 1930/3000\n",
      "2370/2370 [==============================] - 0s 30us/step - loss: 0.0359 - val_loss: 0.0588\n",
      "Epoch 1931/3000\n",
      "2370/2370 [==============================] - 0s 32us/step - loss: 0.0332 - val_loss: 0.0414\n",
      "Epoch 1932/3000\n",
      "2370/2370 [==============================] - 0s 31us/step - loss: 0.0366 - val_loss: 0.0599\n",
      "Epoch 1933/3000\n",
      "2370/2370 [==============================] - 0s 32us/step - loss: 0.0378 - val_loss: 0.0371\n",
      "Epoch 1934/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0385 - val_loss: 0.0553\n",
      "Epoch 1935/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0342 - val_loss: 0.0605\n",
      "Epoch 1936/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0337 - val_loss: 0.0428\n",
      "Epoch 1937/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0372 - val_loss: 0.0357\n",
      "Epoch 1938/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0360 - val_loss: 0.0490\n",
      "Epoch 1939/3000\n",
      "2370/2370 [==============================] - 0s 30us/step - loss: 0.0345 - val_loss: 0.0485\n",
      "Epoch 1940/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0373 - val_loss: 0.0403\n",
      "Epoch 1941/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0364 - val_loss: 0.0355\n",
      "Epoch 1942/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0369 - val_loss: 0.0465\n",
      "Epoch 1943/3000\n",
      "2370/2370 [==============================] - 0s 32us/step - loss: 0.0354 - val_loss: 0.0434\n",
      "Epoch 1944/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0375 - val_loss: 0.0527\n",
      "Epoch 1945/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0350 - val_loss: 0.0517\n",
      "Epoch 1946/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0310 - val_loss: 0.0550\n",
      "Epoch 1947/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0368 - val_loss: 0.0354\n",
      "Epoch 1948/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0331 - val_loss: 0.0640\n",
      "Epoch 1949/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0321 - val_loss: 0.0442\n",
      "Epoch 1950/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0387 - val_loss: 0.0537\n",
      "Epoch 1951/3000\n",
      "2370/2370 [==============================] - 0s 30us/step - loss: 0.0372 - val_loss: 0.0583\n",
      "Epoch 1952/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0367 - val_loss: 0.0597\n",
      "Epoch 1953/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0348 - val_loss: 0.0418\n",
      "Epoch 1954/3000\n",
      "2370/2370 [==============================] - 0s 28us/step - loss: 0.0378 - val_loss: 0.0477\n",
      "Epoch 1955/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0340 - val_loss: 0.0462\n",
      "Epoch 1956/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0353 - val_loss: 0.0415\n",
      "Epoch 1957/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0383 - val_loss: 0.0558\n",
      "Epoch 1958/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0341 - val_loss: 0.0686\n",
      "Epoch 1959/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0367 - val_loss: 0.0492\n",
      "Epoch 1960/3000\n",
      "2370/2370 [==============================] - 0s 30us/step - loss: 0.0360 - val_loss: 0.0632\n",
      "Epoch 1961/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0373 - val_loss: 0.0462\n",
      "Epoch 1962/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0369 - val_loss: 0.0594\n",
      "Epoch 1963/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0343 - val_loss: 0.0620\n",
      "Epoch 1964/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0349 - val_loss: 0.0475\n",
      "Epoch 1965/3000\n",
      "2370/2370 [==============================] - 0s 28us/step - loss: 0.0341 - val_loss: 0.0477\n",
      "Epoch 1966/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0348 - val_loss: 0.0564\n",
      "Epoch 1967/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0340 - val_loss: 0.0492\n",
      "Epoch 1968/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0387 - val_loss: 0.0539\n",
      "Epoch 1969/3000\n",
      "2370/2370 [==============================] - 0s 30us/step - loss: 0.0368 - val_loss: 0.0620\n",
      "Epoch 1970/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0360 - val_loss: 0.0523\n",
      "Epoch 1971/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0411 - val_loss: 0.0439\n",
      "Epoch 1972/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0367 - val_loss: 0.0425\n",
      "Epoch 1973/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0323 - val_loss: 0.0372\n",
      "Epoch 1974/3000\n",
      "2370/2370 [==============================] - 0s 30us/step - loss: 0.0346 - val_loss: 0.0618\n",
      "Epoch 1975/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0363 - val_loss: 0.0677\n",
      "Epoch 1976/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0353 - val_loss: 0.0541\n",
      "Epoch 1977/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0361 - val_loss: 0.0625\n",
      "Epoch 1978/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0371 - val_loss: 0.0485\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1979/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0387 - val_loss: 0.0504\n",
      "Epoch 1980/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0337 - val_loss: 0.0547\n",
      "Epoch 1981/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0412 - val_loss: 0.0627\n",
      "Epoch 1982/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0379 - val_loss: 0.0476\n",
      "Epoch 1983/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0404 - val_loss: 0.0492\n",
      "Epoch 1984/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0334 - val_loss: 0.0474\n",
      "Epoch 1985/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0351 - val_loss: 0.0392\n",
      "Epoch 1986/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0369 - val_loss: 0.0624\n",
      "Epoch 1987/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0316 - val_loss: 0.0537\n",
      "Epoch 1988/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0368 - val_loss: 0.0524\n",
      "Epoch 1989/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0328 - val_loss: 0.0467\n",
      "Epoch 1990/3000\n",
      "2370/2370 [==============================] - 0s 30us/step - loss: 0.0384 - val_loss: 0.0478\n",
      "Epoch 1991/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0324 - val_loss: 0.0426\n",
      "Epoch 1992/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0344 - val_loss: 0.0638\n",
      "Epoch 1993/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0347 - val_loss: 0.0559\n",
      "Epoch 1994/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0377 - val_loss: 0.0447\n",
      "Epoch 1995/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0367 - val_loss: 0.0400\n",
      "Epoch 1996/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0317 - val_loss: 0.0629\n",
      "Epoch 1997/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0368 - val_loss: 0.0564\n",
      "Epoch 1998/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0346 - val_loss: 0.0481\n",
      "Epoch 1999/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0354 - val_loss: 0.0467\n",
      "Epoch 2000/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0338 - val_loss: 0.0589\n",
      "Epoch 2001/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0369 - val_loss: 0.0435\n",
      "Epoch 2002/3000\n",
      "2370/2370 [==============================] - 0s 28us/step - loss: 0.0340 - val_loss: 0.0453\n",
      "Epoch 2003/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0386 - val_loss: 0.0543\n",
      "Epoch 2004/3000\n",
      "2370/2370 [==============================] - 0s 30us/step - loss: 0.0364 - val_loss: 0.0630\n",
      "Epoch 2005/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0369 - val_loss: 0.0543\n",
      "Epoch 2006/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0320 - val_loss: 0.0449\n",
      "Epoch 2007/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0365 - val_loss: 0.0555\n",
      "Epoch 2008/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0366 - val_loss: 0.0463\n",
      "Epoch 2009/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0340 - val_loss: 0.0556\n",
      "Epoch 2010/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0365 - val_loss: 0.0836\n",
      "Epoch 2011/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0345 - val_loss: 0.0542\n",
      "Epoch 2012/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0356 - val_loss: 0.0413\n",
      "Epoch 2013/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0356 - val_loss: 0.0431\n",
      "Epoch 2014/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0358 - val_loss: 0.0454\n",
      "Epoch 2015/3000\n",
      "2370/2370 [==============================] - 0s 28us/step - loss: 0.0332 - val_loss: 0.0553\n",
      "Epoch 2016/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0365 - val_loss: 0.0501\n",
      "Epoch 2017/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0341 - val_loss: 0.0462\n",
      "Epoch 2018/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0347 - val_loss: 0.0423\n",
      "Epoch 2019/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0361 - val_loss: 0.0634\n",
      "Epoch 2020/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0364 - val_loss: 0.0516\n",
      "Epoch 2021/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0376 - val_loss: 0.0407\n",
      "Epoch 2022/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0361 - val_loss: 0.0474\n",
      "Epoch 2023/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0362 - val_loss: 0.0530\n",
      "Epoch 2024/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0347 - val_loss: 0.0579\n",
      "Epoch 2025/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0331 - val_loss: 0.0612\n",
      "Epoch 2026/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0356 - val_loss: 0.0624\n",
      "Epoch 2027/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0319 - val_loss: 0.0638\n",
      "Epoch 2028/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0399 - val_loss: 0.0505\n",
      "Epoch 2029/3000\n",
      "2370/2370 [==============================] - 0s 28us/step - loss: 0.0367 - val_loss: 0.0698\n",
      "Epoch 2030/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0389 - val_loss: 0.0430\n",
      "Epoch 2031/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0342 - val_loss: 0.0599\n",
      "Epoch 2032/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0368 - val_loss: 0.0471\n",
      "Epoch 2033/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0347 - val_loss: 0.0708\n",
      "Epoch 2034/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0329 - val_loss: 0.0473\n",
      "Epoch 2035/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0361 - val_loss: 0.0712\n",
      "Epoch 2036/3000\n",
      "2370/2370 [==============================] - 0s 28us/step - loss: 0.0367 - val_loss: 0.0574\n",
      "Epoch 2037/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0356 - val_loss: 0.0526\n",
      "Epoch 2038/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0363 - val_loss: 0.0662\n",
      "Epoch 2039/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0393 - val_loss: 0.0687\n",
      "Epoch 2040/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0369 - val_loss: 0.0414\n",
      "Epoch 2041/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0392 - val_loss: 0.0471\n",
      "Epoch 2042/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0341 - val_loss: 0.0488\n",
      "Epoch 2043/3000\n",
      "2370/2370 [==============================] - 0s 28us/step - loss: 0.0354 - val_loss: 0.0676\n",
      "Epoch 2044/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0349 - val_loss: 0.0526\n",
      "Epoch 2045/3000\n",
      "2370/2370 [==============================] - 0s 28us/step - loss: 0.0360 - val_loss: 0.0462\n",
      "Epoch 2046/3000\n",
      "2370/2370 [==============================] - 0s 32us/step - loss: 0.0379 - val_loss: 0.0362\n",
      "Epoch 2047/3000\n",
      "2370/2370 [==============================] - 0s 32us/step - loss: 0.0376 - val_loss: 0.0634\n",
      "Epoch 2048/3000\n",
      "2370/2370 [==============================] - 0s 28us/step - loss: 0.0371 - val_loss: 0.0651\n",
      "Epoch 2049/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0376 - val_loss: 0.0603\n",
      "Epoch 2050/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0361 - val_loss: 0.0487\n",
      "Epoch 2051/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0365 - val_loss: 0.0531\n",
      "Epoch 2052/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0354 - val_loss: 0.0619\n",
      "Epoch 2053/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0352 - val_loss: 0.0553\n",
      "Epoch 2054/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0349 - val_loss: 0.0507\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2055/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0342 - val_loss: 0.0412\n",
      "Epoch 2056/3000\n",
      "2370/2370 [==============================] - 0s 31us/step - loss: 0.0367 - val_loss: 0.0623\n",
      "Epoch 2057/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0375 - val_loss: 0.0500\n",
      "Epoch 2058/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0350 - val_loss: 0.0441\n",
      "Epoch 2059/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0361 - val_loss: 0.0496\n",
      "Epoch 2060/3000\n",
      "2370/2370 [==============================] - 0s 28us/step - loss: 0.0350 - val_loss: 0.0607\n",
      "Epoch 2061/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0364 - val_loss: 0.0515\n",
      "Epoch 2062/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0360 - val_loss: 0.0708\n",
      "Epoch 2063/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0344 - val_loss: 0.0452\n",
      "Epoch 2064/3000\n",
      "2370/2370 [==============================] - 0s 30us/step - loss: 0.0380 - val_loss: 0.0461\n",
      "Epoch 2065/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0338 - val_loss: 0.0721\n",
      "Epoch 2066/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0343 - val_loss: 0.0460\n",
      "Epoch 2067/3000\n",
      "2370/2370 [==============================] - 0s 28us/step - loss: 0.0369 - val_loss: 0.0680\n",
      "Epoch 2068/3000\n",
      "2370/2370 [==============================] - 0s 30us/step - loss: 0.0332 - val_loss: 0.0472\n",
      "Epoch 2069/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0356 - val_loss: 0.0592\n",
      "Epoch 2070/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0333 - val_loss: 0.0534\n",
      "Epoch 2071/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0365 - val_loss: 0.0450\n",
      "Epoch 2072/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0348 - val_loss: 0.0454\n",
      "Epoch 2073/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0393 - val_loss: 0.0624\n",
      "Epoch 2074/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0384 - val_loss: 0.0750\n",
      "Epoch 2075/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0357 - val_loss: 0.0566\n",
      "Epoch 2076/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0355 - val_loss: 0.0681\n",
      "Epoch 2077/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0372 - val_loss: 0.0503\n",
      "Epoch 2078/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0396 - val_loss: 0.0491\n",
      "Epoch 2079/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0406 - val_loss: 0.0437\n",
      "Epoch 2080/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0356 - val_loss: 0.0517\n",
      "Epoch 2081/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0377 - val_loss: 0.0457\n",
      "Epoch 2082/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0345 - val_loss: 0.0463\n",
      "Epoch 2083/3000\n",
      "2370/2370 [==============================] - 0s 30us/step - loss: 0.0360 - val_loss: 0.0496\n",
      "Epoch 2084/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0348 - val_loss: 0.0609\n",
      "Epoch 2085/3000\n",
      "2370/2370 [==============================] - 0s 28us/step - loss: 0.0378 - val_loss: 0.0547\n",
      "Epoch 2086/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0355 - val_loss: 0.0423\n",
      "Epoch 2087/3000\n",
      "2370/2370 [==============================] - 0s 28us/step - loss: 0.0375 - val_loss: 0.0406\n",
      "Epoch 2088/3000\n",
      "2370/2370 [==============================] - 0s 28us/step - loss: 0.0387 - val_loss: 0.0426\n",
      "Epoch 2089/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0366 - val_loss: 0.0495\n",
      "Epoch 2090/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0363 - val_loss: 0.0458\n",
      "Epoch 2091/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0365 - val_loss: 0.0422\n",
      "Epoch 2092/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0377 - val_loss: 0.0735\n",
      "Epoch 2093/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0344 - val_loss: 0.0436\n",
      "Epoch 2094/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0399 - val_loss: 0.0554\n",
      "Epoch 2095/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0329 - val_loss: 0.0606\n",
      "Epoch 2096/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0365 - val_loss: 0.0644\n",
      "Epoch 2097/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0349 - val_loss: 0.0419\n",
      "Epoch 2098/3000\n",
      "2370/2370 [==============================] - 0s 28us/step - loss: 0.0342 - val_loss: 0.0584\n",
      "Epoch 2099/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0359 - val_loss: 0.0612\n",
      "Epoch 2100/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0364 - val_loss: 0.0462\n",
      "Epoch 2101/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0332 - val_loss: 0.0592\n",
      "Epoch 2102/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0323 - val_loss: 0.0509\n",
      "Epoch 2103/3000\n",
      "2370/2370 [==============================] - 0s 30us/step - loss: 0.0370 - val_loss: 0.0460\n",
      "Epoch 2104/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0390 - val_loss: 0.0554\n",
      "Epoch 2105/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0368 - val_loss: 0.0506\n",
      "Epoch 2106/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0347 - val_loss: 0.0515\n",
      "Epoch 2107/3000\n",
      "2370/2370 [==============================] - 0s 30us/step - loss: 0.0352 - val_loss: 0.0466\n",
      "Epoch 2108/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0352 - val_loss: 0.0534\n",
      "Epoch 2109/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0356 - val_loss: 0.0745\n",
      "Epoch 2110/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0368 - val_loss: 0.0461\n",
      "Epoch 2111/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0373 - val_loss: 0.0452\n",
      "Epoch 2112/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0358 - val_loss: 0.0495\n",
      "Epoch 2113/3000\n",
      "2370/2370 [==============================] - 0s 28us/step - loss: 0.0344 - val_loss: 0.0518\n",
      "Epoch 2114/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0361 - val_loss: 0.0439\n",
      "Epoch 2115/3000\n",
      "2370/2370 [==============================] - 0s 28us/step - loss: 0.0372 - val_loss: 0.0423\n",
      "Epoch 2116/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0363 - val_loss: 0.0533\n",
      "Epoch 2117/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0355 - val_loss: 0.0498\n",
      "Epoch 2118/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0354 - val_loss: 0.0584\n",
      "Epoch 2119/3000\n",
      "2370/2370 [==============================] - 0s 28us/step - loss: 0.0362 - val_loss: 0.0641\n",
      "Epoch 2120/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0353 - val_loss: 0.0476\n",
      "Epoch 2121/3000\n",
      "2370/2370 [==============================] - 0s 30us/step - loss: 0.0362 - val_loss: 0.0589\n",
      "Epoch 2122/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0374 - val_loss: 0.0680\n",
      "Epoch 2123/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0348 - val_loss: 0.0419\n",
      "Epoch 2124/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0353 - val_loss: 0.0631\n",
      "Epoch 2125/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0335 - val_loss: 0.0630\n",
      "Epoch 2126/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0341 - val_loss: 0.0486\n",
      "Epoch 2127/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0356 - val_loss: 0.0490\n",
      "Epoch 2128/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0357 - val_loss: 0.0415\n",
      "Epoch 2129/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0333 - val_loss: 0.0545\n",
      "Epoch 2130/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0370 - val_loss: 0.0439\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2131/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0350 - val_loss: 0.0489\n",
      "Epoch 2132/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0337 - val_loss: 0.0539\n",
      "Epoch 2133/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0364 - val_loss: 0.0658\n",
      "Epoch 2134/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0336 - val_loss: 0.0493\n",
      "Epoch 2135/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0369 - val_loss: 0.0444\n",
      "Epoch 2136/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0369 - val_loss: 0.0542\n",
      "Epoch 2137/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0362 - val_loss: 0.0452\n",
      "Epoch 2138/3000\n",
      "2370/2370 [==============================] - 0s 30us/step - loss: 0.0334 - val_loss: 0.0508\n",
      "Epoch 2139/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0360 - val_loss: 0.0440\n",
      "Epoch 2140/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0362 - val_loss: 0.0485\n",
      "Epoch 2141/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0351 - val_loss: 0.0539\n",
      "Epoch 2142/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0350 - val_loss: 0.0447\n",
      "Epoch 2143/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0355 - val_loss: 0.0481\n",
      "Epoch 2144/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0338 - val_loss: 0.0530\n",
      "Epoch 2145/3000\n",
      "2370/2370 [==============================] - 0s 28us/step - loss: 0.0366 - val_loss: 0.0408\n",
      "Epoch 2146/3000\n",
      "2370/2370 [==============================] - 0s 28us/step - loss: 0.0360 - val_loss: 0.0570\n",
      "Epoch 2147/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0337 - val_loss: 0.0685\n",
      "Epoch 2148/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0360 - val_loss: 0.0568\n",
      "Epoch 2149/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0342 - val_loss: 0.0490\n",
      "Epoch 2150/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0373 - val_loss: 0.0581\n",
      "Epoch 2151/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0355 - val_loss: 0.0458\n",
      "Epoch 2152/3000\n",
      "2370/2370 [==============================] - 0s 28us/step - loss: 0.0352 - val_loss: 0.0750\n",
      "Epoch 2153/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0385 - val_loss: 0.0567\n",
      "Epoch 2154/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0386 - val_loss: 0.0417\n",
      "Epoch 2155/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0400 - val_loss: 0.0488\n",
      "Epoch 2156/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0338 - val_loss: 0.0610\n",
      "Epoch 2157/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0356 - val_loss: 0.0478\n",
      "Epoch 2158/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0350 - val_loss: 0.0587\n",
      "Epoch 2159/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0331 - val_loss: 0.0584\n",
      "Epoch 2160/3000\n",
      "2370/2370 [==============================] - 0s 28us/step - loss: 0.0350 - val_loss: 0.0504\n",
      "Epoch 2161/3000\n",
      "2370/2370 [==============================] - 0s 30us/step - loss: 0.0345 - val_loss: 0.0686\n",
      "Epoch 2162/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0371 - val_loss: 0.0433\n",
      "Epoch 2163/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0341 - val_loss: 0.0517\n",
      "Epoch 2164/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0334 - val_loss: 0.0422\n",
      "Epoch 2165/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0355 - val_loss: 0.0413\n",
      "Epoch 2166/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0352 - val_loss: 0.0447\n",
      "Epoch 2167/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0361 - val_loss: 0.0477\n",
      "Epoch 2168/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0372 - val_loss: 0.0490\n",
      "Epoch 2169/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0368 - val_loss: 0.0422\n",
      "Epoch 2170/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0335 - val_loss: 0.0465\n",
      "Epoch 2171/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0396 - val_loss: 0.0512\n",
      "Epoch 2172/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0353 - val_loss: 0.0533\n",
      "Epoch 2173/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0355 - val_loss: 0.0505\n",
      "Epoch 2174/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0323 - val_loss: 0.0626\n",
      "Epoch 2175/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0389 - val_loss: 0.0630\n",
      "Epoch 2176/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0346 - val_loss: 0.0492\n",
      "Epoch 2177/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0348 - val_loss: 0.0476\n",
      "Epoch 2178/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0361 - val_loss: 0.0541\n",
      "Epoch 2179/3000\n",
      "2370/2370 [==============================] - 0s 28us/step - loss: 0.0366 - val_loss: 0.0449\n",
      "Epoch 2180/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0378 - val_loss: 0.0521\n",
      "Epoch 2181/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0319 - val_loss: 0.0551\n",
      "Epoch 2182/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0340 - val_loss: 0.0579\n",
      "Epoch 2183/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0343 - val_loss: 0.0650\n",
      "Epoch 2184/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0391 - val_loss: 0.0512\n",
      "Epoch 2185/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0352 - val_loss: 0.0511\n",
      "Epoch 2186/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0342 - val_loss: 0.0587\n",
      "Epoch 2187/3000\n",
      "2370/2370 [==============================] - 0s 30us/step - loss: 0.0362 - val_loss: 0.0618\n",
      "Epoch 2188/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0346 - val_loss: 0.0460\n",
      "Epoch 2189/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0360 - val_loss: 0.0609\n",
      "Epoch 2190/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0341 - val_loss: 0.0478\n",
      "Epoch 2191/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0337 - val_loss: 0.0532\n",
      "Epoch 2192/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0345 - val_loss: 0.0619\n",
      "Epoch 2193/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0350 - val_loss: 0.0474\n",
      "Epoch 2194/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0403 - val_loss: 0.0470\n",
      "Epoch 2195/3000\n",
      "2370/2370 [==============================] - 0s 28us/step - loss: 0.0356 - val_loss: 0.0492\n",
      "Epoch 2196/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0385 - val_loss: 0.0446\n",
      "Epoch 2197/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0330 - val_loss: 0.0661\n",
      "Epoch 2198/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0326 - val_loss: 0.0426\n",
      "Epoch 2199/3000\n",
      "2370/2370 [==============================] - 0s 30us/step - loss: 0.0358 - val_loss: 0.0559\n",
      "Epoch 2200/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0374 - val_loss: 0.0507\n",
      "Epoch 2201/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0353 - val_loss: 0.0402\n",
      "Epoch 2202/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0330 - val_loss: 0.0396\n",
      "Epoch 2203/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0365 - val_loss: 0.0540\n",
      "Epoch 2204/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0373 - val_loss: 0.0515\n",
      "Epoch 2205/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0334 - val_loss: 0.0607\n",
      "Epoch 2206/3000\n",
      "2370/2370 [==============================] - 0s 28us/step - loss: 0.0344 - val_loss: 0.0459\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2207/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0357 - val_loss: 0.0520\n",
      "Epoch 2208/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0370 - val_loss: 0.0545\n",
      "Epoch 2209/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0362 - val_loss: 0.0510\n",
      "Epoch 2210/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0367 - val_loss: 0.0545\n",
      "Epoch 2211/3000\n",
      "2370/2370 [==============================] - 0s 28us/step - loss: 0.0355 - val_loss: 0.0396\n",
      "Epoch 2212/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0365 - val_loss: 0.0705\n",
      "Epoch 2213/3000\n",
      "2370/2370 [==============================] - 0s 28us/step - loss: 0.0389 - val_loss: 0.0433\n",
      "Epoch 2214/3000\n",
      "2370/2370 [==============================] - 0s 28us/step - loss: 0.0352 - val_loss: 0.0494\n",
      "Epoch 2215/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0374 - val_loss: 0.0631\n",
      "Epoch 2216/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0351 - val_loss: 0.0528\n",
      "Epoch 2217/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0331 - val_loss: 0.0558\n",
      "Epoch 2218/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0301 - val_loss: 0.0503\n",
      "Epoch 2219/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0325 - val_loss: 0.0501\n",
      "Epoch 2220/3000\n",
      "2370/2370 [==============================] - 0s 28us/step - loss: 0.0345 - val_loss: 0.0498\n",
      "Epoch 2221/3000\n",
      "2370/2370 [==============================] - 0s 30us/step - loss: 0.0349 - val_loss: 0.0570\n",
      "Epoch 2222/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0355 - val_loss: 0.0590\n",
      "Epoch 2223/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0359 - val_loss: 0.0576\n",
      "Epoch 2224/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0314 - val_loss: 0.0432\n",
      "Epoch 2225/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0371 - val_loss: 0.0530\n",
      "Epoch 2226/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0370 - val_loss: 0.0602\n",
      "Epoch 2227/3000\n",
      "2370/2370 [==============================] - 0s 28us/step - loss: 0.0344 - val_loss: 0.0516\n",
      "Epoch 2228/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0328 - val_loss: 0.0526\n",
      "Epoch 2229/3000\n",
      "2370/2370 [==============================] - 0s 28us/step - loss: 0.0333 - val_loss: 0.0538\n",
      "Epoch 2230/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0345 - val_loss: 0.0432\n",
      "Epoch 2231/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0371 - val_loss: 0.0546\n",
      "Epoch 2232/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0336 - val_loss: 0.0569\n",
      "Epoch 2233/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0380 - val_loss: 0.0511\n",
      "Epoch 2234/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0368 - val_loss: 0.0485\n",
      "Epoch 2235/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0355 - val_loss: 0.0438\n",
      "Epoch 2236/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0340 - val_loss: 0.0556\n",
      "Epoch 2237/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0360 - val_loss: 0.0607\n",
      "Epoch 2238/3000\n",
      "2370/2370 [==============================] - 0s 28us/step - loss: 0.0370 - val_loss: 0.0551\n",
      "Epoch 2239/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0326 - val_loss: 0.0480\n",
      "Epoch 2240/3000\n",
      "2370/2370 [==============================] - 0s 28us/step - loss: 0.0386 - val_loss: 0.0483\n",
      "Epoch 2241/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0371 - val_loss: 0.0502\n",
      "Epoch 2242/3000\n",
      "2370/2370 [==============================] - 0s 28us/step - loss: 0.0350 - val_loss: 0.0558\n",
      "Epoch 2243/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0351 - val_loss: 0.0605\n",
      "Epoch 2244/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0349 - val_loss: 0.0475\n",
      "Epoch 2245/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0361 - val_loss: 0.0551\n",
      "Epoch 2246/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0381 - val_loss: 0.0544\n",
      "Epoch 2247/3000\n",
      "2370/2370 [==============================] - 0s 28us/step - loss: 0.0360 - val_loss: 0.0520\n",
      "Epoch 2248/3000\n",
      "2370/2370 [==============================] - 0s 31us/step - loss: 0.0360 - val_loss: 0.0517\n",
      "Epoch 2249/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0347 - val_loss: 0.0479\n",
      "Epoch 2250/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0383 - val_loss: 0.0634\n",
      "Epoch 2251/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0366 - val_loss: 0.0676\n",
      "Epoch 2252/3000\n",
      "2370/2370 [==============================] - 0s 28us/step - loss: 0.0365 - val_loss: 0.0498\n",
      "Epoch 2253/3000\n",
      "2370/2370 [==============================] - 0s 28us/step - loss: 0.0361 - val_loss: 0.0519\n",
      "Epoch 2254/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0368 - val_loss: 0.0488\n",
      "Epoch 2255/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0328 - val_loss: 0.0513\n",
      "Epoch 2256/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0314 - val_loss: 0.0451\n",
      "Epoch 2257/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0338 - val_loss: 0.0496\n",
      "Epoch 2258/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0342 - val_loss: 0.0585\n",
      "Epoch 2259/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0355 - val_loss: 0.0648\n",
      "Epoch 2260/3000\n",
      "2370/2370 [==============================] - 0s 31us/step - loss: 0.0372 - val_loss: 0.0567\n",
      "Epoch 2261/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0353 - val_loss: 0.0539\n",
      "Epoch 2262/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0342 - val_loss: 0.0510\n",
      "Epoch 2263/3000\n",
      "2370/2370 [==============================] - 0s 28us/step - loss: 0.0347 - val_loss: 0.0478\n",
      "Epoch 2264/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0384 - val_loss: 0.0482\n",
      "Epoch 2265/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0350 - val_loss: 0.0552\n",
      "Epoch 2266/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0361 - val_loss: 0.0568\n",
      "Epoch 2267/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0364 - val_loss: 0.0547\n",
      "Epoch 2268/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0311 - val_loss: 0.0451\n",
      "Epoch 2269/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0359 - val_loss: 0.0558\n",
      "Epoch 2270/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0350 - val_loss: 0.0579\n",
      "Epoch 2271/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0355 - val_loss: 0.0418\n",
      "Epoch 2272/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0360 - val_loss: 0.0489\n",
      "Epoch 2273/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0348 - val_loss: 0.0590\n",
      "Epoch 2274/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0358 - val_loss: 0.0565\n",
      "Epoch 2275/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0341 - val_loss: 0.0520\n",
      "Epoch 2276/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0356 - val_loss: 0.0488\n",
      "Epoch 2277/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0320 - val_loss: 0.0475\n",
      "Epoch 2278/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0362 - val_loss: 0.0481\n",
      "Epoch 2279/3000\n",
      "2370/2370 [==============================] - 0s 28us/step - loss: 0.0339 - val_loss: 0.0509\n",
      "Epoch 2280/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0331 - val_loss: 0.0526\n",
      "Epoch 2281/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0372 - val_loss: 0.0513\n",
      "Epoch 2282/3000\n",
      "2370/2370 [==============================] - 0s 30us/step - loss: 0.0343 - val_loss: 0.0433\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2283/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0323 - val_loss: 0.0372\n",
      "Epoch 2284/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0346 - val_loss: 0.0505\n",
      "Epoch 2285/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0323 - val_loss: 0.0537\n",
      "Epoch 2286/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0327 - val_loss: 0.0510\n",
      "Epoch 2287/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0382 - val_loss: 0.0433\n",
      "Epoch 2288/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0360 - val_loss: 0.0468\n",
      "Epoch 2289/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0366 - val_loss: 0.0597\n",
      "Epoch 2290/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0354 - val_loss: 0.0461\n",
      "Epoch 2291/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0326 - val_loss: 0.0531\n",
      "Epoch 2292/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0355 - val_loss: 0.0429\n",
      "Epoch 2293/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0339 - val_loss: 0.0453\n",
      "Epoch 2294/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0339 - val_loss: 0.0615\n",
      "Epoch 2295/3000\n",
      "2370/2370 [==============================] - 0s 28us/step - loss: 0.0352 - val_loss: 0.0542\n",
      "Epoch 2296/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0401 - val_loss: 0.0438\n",
      "Epoch 2297/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0362 - val_loss: 0.0692\n",
      "Epoch 2298/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0360 - val_loss: 0.0513\n",
      "Epoch 2299/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0343 - val_loss: 0.0530\n",
      "Epoch 2300/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0351 - val_loss: 0.0623\n",
      "Epoch 2301/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0353 - val_loss: 0.0576\n",
      "Epoch 2302/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0347 - val_loss: 0.0447\n",
      "Epoch 2303/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0385 - val_loss: 0.0445\n",
      "Epoch 2304/3000\n",
      "2370/2370 [==============================] - 0s 28us/step - loss: 0.0372 - val_loss: 0.0584\n",
      "Epoch 2305/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0357 - val_loss: 0.0623\n",
      "Epoch 2306/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0360 - val_loss: 0.0423\n",
      "Epoch 2307/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0351 - val_loss: 0.0533\n",
      "Epoch 2308/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0349 - val_loss: 0.0475\n",
      "Epoch 2309/3000\n",
      "2370/2370 [==============================] - 0s 30us/step - loss: 0.0362 - val_loss: 0.0460\n",
      "Epoch 2310/3000\n",
      "2370/2370 [==============================] - 0s 28us/step - loss: 0.0359 - val_loss: 0.0711\n",
      "Epoch 2311/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0345 - val_loss: 0.0562\n",
      "Epoch 2312/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0359 - val_loss: 0.0524\n",
      "Epoch 2313/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0373 - val_loss: 0.0451\n",
      "Epoch 2314/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0343 - val_loss: 0.0644\n",
      "Epoch 2315/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0369 - val_loss: 0.0399\n",
      "Epoch 2316/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0346 - val_loss: 0.0476\n",
      "Epoch 2317/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0337 - val_loss: 0.0334\n",
      "Epoch 2318/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0349 - val_loss: 0.0798\n",
      "Epoch 2319/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0385 - val_loss: 0.0492\n",
      "Epoch 2320/3000\n",
      "2370/2370 [==============================] - 0s 30us/step - loss: 0.0371 - val_loss: 0.0441\n",
      "Epoch 2321/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0341 - val_loss: 0.0472\n",
      "Epoch 2322/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0379 - val_loss: 0.0511\n",
      "Epoch 2323/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0364 - val_loss: 0.0501\n",
      "Epoch 2324/3000\n",
      "2370/2370 [==============================] - 0s 28us/step - loss: 0.0364 - val_loss: 0.0426\n",
      "Epoch 2325/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0321 - val_loss: 0.0538\n",
      "Epoch 2326/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0383 - val_loss: 0.0431\n",
      "Epoch 2327/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0348 - val_loss: 0.0473\n",
      "Epoch 2328/3000\n",
      "2370/2370 [==============================] - 0s 28us/step - loss: 0.0351 - val_loss: 0.0605\n",
      "Epoch 2329/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0356 - val_loss: 0.0594\n",
      "Epoch 2330/3000\n",
      "2370/2370 [==============================] - 0s 30us/step - loss: 0.0349 - val_loss: 0.0519\n",
      "Epoch 2331/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0383 - val_loss: 0.0432\n",
      "Epoch 2332/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0359 - val_loss: 0.0615\n",
      "Epoch 2333/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0325 - val_loss: 0.0459\n",
      "Epoch 2334/3000\n",
      "2370/2370 [==============================] - 0s 28us/step - loss: 0.0396 - val_loss: 0.0360\n",
      "Epoch 2335/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0332 - val_loss: 0.0567\n",
      "Epoch 2336/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0376 - val_loss: 0.0511\n",
      "Epoch 2337/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0357 - val_loss: 0.0518\n",
      "Epoch 2338/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0358 - val_loss: 0.0475\n",
      "Epoch 2339/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0330 - val_loss: 0.0565\n",
      "Epoch 2340/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0350 - val_loss: 0.0463\n",
      "Epoch 2341/3000\n",
      "2370/2370 [==============================] - 0s 28us/step - loss: 0.0353 - val_loss: 0.0555\n",
      "Epoch 2342/3000\n",
      "2370/2370 [==============================] - 0s 28us/step - loss: 0.0354 - val_loss: 0.0493\n",
      "Epoch 2343/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0362 - val_loss: 0.0426\n",
      "Epoch 2344/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0362 - val_loss: 0.0485\n",
      "Epoch 2345/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0395 - val_loss: 0.0492\n",
      "Epoch 2346/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0342 - val_loss: 0.0654\n",
      "Epoch 2347/3000\n",
      "2370/2370 [==============================] - 0s 30us/step - loss: 0.0351 - val_loss: 0.0535\n",
      "Epoch 2348/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0349 - val_loss: 0.0476\n",
      "Epoch 2349/3000\n",
      "2370/2370 [==============================] - 0s 35us/step - loss: 0.0357 - val_loss: 0.0728\n",
      "Epoch 2350/3000\n",
      "2370/2370 [==============================] - 0s 30us/step - loss: 0.0329 - val_loss: 0.0409\n",
      "Epoch 2351/3000\n",
      "2370/2370 [==============================] - 0s 30us/step - loss: 0.0385 - val_loss: 0.0607\n",
      "Epoch 2352/3000\n",
      "2370/2370 [==============================] - 0s 30us/step - loss: 0.0385 - val_loss: 0.0525\n",
      "Epoch 2353/3000\n",
      "2370/2370 [==============================] - 0s 30us/step - loss: 0.0327 - val_loss: 0.0466\n",
      "Epoch 2354/3000\n",
      "2370/2370 [==============================] - 0s 32us/step - loss: 0.0351 - val_loss: 0.0466\n",
      "Epoch 2355/3000\n",
      "2370/2370 [==============================] - 0s 30us/step - loss: 0.0394 - val_loss: 0.0658\n",
      "Epoch 2356/3000\n",
      "2370/2370 [==============================] - 0s 30us/step - loss: 0.0362 - val_loss: 0.0484\n",
      "Epoch 2357/3000\n",
      "2370/2370 [==============================] - 0s 30us/step - loss: 0.0373 - val_loss: 0.0554\n",
      "Epoch 2358/3000\n",
      "2370/2370 [==============================] - 0s 30us/step - loss: 0.0350 - val_loss: 0.0469\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2359/3000\n",
      "2370/2370 [==============================] - 0s 30us/step - loss: 0.0344 - val_loss: 0.0654\n",
      "Epoch 2360/3000\n",
      "2370/2370 [==============================] - 0s 31us/step - loss: 0.0372 - val_loss: 0.0521\n",
      "Epoch 2361/3000\n",
      "2370/2370 [==============================] - 0s 30us/step - loss: 0.0354 - val_loss: 0.0658\n",
      "Epoch 2362/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0346 - val_loss: 0.0574\n",
      "Epoch 2363/3000\n",
      "2370/2370 [==============================] - 0s 31us/step - loss: 0.0340 - val_loss: 0.0493\n",
      "Epoch 2364/3000\n",
      "2370/2370 [==============================] - 0s 28us/step - loss: 0.0365 - val_loss: 0.0553\n",
      "Epoch 2365/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0362 - val_loss: 0.0591\n",
      "Epoch 2366/3000\n",
      "2370/2370 [==============================] - 0s 28us/step - loss: 0.0340 - val_loss: 0.0606\n",
      "Epoch 2367/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0357 - val_loss: 0.0568\n",
      "Epoch 2368/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0373 - val_loss: 0.0578\n",
      "Epoch 2369/3000\n",
      "2370/2370 [==============================] - 0s 28us/step - loss: 0.0328 - val_loss: 0.0615\n",
      "Epoch 2370/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0328 - val_loss: 0.0730\n",
      "Epoch 2371/3000\n",
      "2370/2370 [==============================] - 0s 32us/step - loss: 0.0375 - val_loss: 0.0660\n",
      "Epoch 2372/3000\n",
      "2370/2370 [==============================] - 0s 30us/step - loss: 0.0340 - val_loss: 0.0531\n",
      "Epoch 2373/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0324 - val_loss: 0.0502\n",
      "Epoch 2374/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0337 - val_loss: 0.0523\n",
      "Epoch 2375/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0376 - val_loss: 0.0508\n",
      "Epoch 2376/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0375 - val_loss: 0.0546\n",
      "Epoch 2377/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0305 - val_loss: 0.0499\n",
      "Epoch 2378/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0358 - val_loss: 0.0614\n",
      "Epoch 2379/3000\n",
      "2370/2370 [==============================] - 0s 30us/step - loss: 0.0332 - val_loss: 0.0627\n",
      "Epoch 2380/3000\n",
      "2370/2370 [==============================] - 0s 30us/step - loss: 0.0362 - val_loss: 0.0500\n",
      "Epoch 2381/3000\n",
      "2370/2370 [==============================] - 0s 30us/step - loss: 0.0354 - val_loss: 0.0485\n",
      "Epoch 2382/3000\n",
      "2370/2370 [==============================] - 0s 30us/step - loss: 0.0320 - val_loss: 0.0725\n",
      "Epoch 2383/3000\n",
      "2370/2370 [==============================] - 0s 31us/step - loss: 0.0345 - val_loss: 0.0487\n",
      "Epoch 2384/3000\n",
      "2370/2370 [==============================] - 0s 30us/step - loss: 0.0368 - val_loss: 0.0696\n",
      "Epoch 2385/3000\n",
      "2370/2370 [==============================] - 0s 30us/step - loss: 0.0373 - val_loss: 0.0602\n",
      "Epoch 2386/3000\n",
      "2370/2370 [==============================] - 0s 30us/step - loss: 0.0362 - val_loss: 0.0452\n",
      "Epoch 2387/3000\n",
      "2370/2370 [==============================] - 0s 30us/step - loss: 0.0388 - val_loss: 0.0498\n",
      "Epoch 2388/3000\n",
      "2370/2370 [==============================] - 0s 31us/step - loss: 0.0367 - val_loss: 0.0530\n",
      "Epoch 2389/3000\n",
      "2370/2370 [==============================] - 0s 30us/step - loss: 0.0342 - val_loss: 0.0614\n",
      "Epoch 2390/3000\n",
      "2370/2370 [==============================] - 0s 30us/step - loss: 0.0361 - val_loss: 0.0446\n",
      "Epoch 2391/3000\n",
      "2370/2370 [==============================] - 0s 31us/step - loss: 0.0354 - val_loss: 0.0446\n",
      "Epoch 2392/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0334 - val_loss: 0.0499\n",
      "Epoch 2393/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0389 - val_loss: 0.0643\n",
      "Epoch 2394/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0358 - val_loss: 0.0507\n",
      "Epoch 2395/3000\n",
      "2370/2370 [==============================] - 0s 30us/step - loss: 0.0373 - val_loss: 0.0519\n",
      "Epoch 2396/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0363 - val_loss: 0.0480\n",
      "Epoch 2397/3000\n",
      "2370/2370 [==============================] - 0s 28us/step - loss: 0.0347 - val_loss: 0.0680\n",
      "Epoch 2398/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0371 - val_loss: 0.0551\n",
      "Epoch 2399/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0357 - val_loss: 0.0480\n",
      "Epoch 2400/3000\n",
      "2370/2370 [==============================] - 0s 28us/step - loss: 0.0335 - val_loss: 0.0469\n",
      "Epoch 2401/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0353 - val_loss: 0.0558\n",
      "Epoch 2402/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0375 - val_loss: 0.0742\n",
      "Epoch 2403/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0351 - val_loss: 0.0465\n",
      "Epoch 2404/3000\n",
      "2370/2370 [==============================] - 0s 30us/step - loss: 0.0333 - val_loss: 0.0433\n",
      "Epoch 2405/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0362 - val_loss: 0.0505\n",
      "Epoch 2406/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0389 - val_loss: 0.0634\n",
      "Epoch 2407/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0354 - val_loss: 0.0708\n",
      "Epoch 2408/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0345 - val_loss: 0.0408\n",
      "Epoch 2409/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0373 - val_loss: 0.0636\n",
      "Epoch 2410/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0368 - val_loss: 0.0493\n",
      "Epoch 2411/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0362 - val_loss: 0.0514\n",
      "Epoch 2412/3000\n",
      "2370/2370 [==============================] - 0s 28us/step - loss: 0.0367 - val_loss: 0.0555\n",
      "Epoch 2413/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0353 - val_loss: 0.0406\n",
      "Epoch 2414/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0359 - val_loss: 0.0552\n",
      "Epoch 2415/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0363 - val_loss: 0.0474\n",
      "Epoch 2416/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0388 - val_loss: 0.0651\n",
      "Epoch 2417/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0352 - val_loss: 0.0586\n",
      "Epoch 2418/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0353 - val_loss: 0.0474\n",
      "Epoch 2419/3000\n",
      "2370/2370 [==============================] - 0s 30us/step - loss: 0.0332 - val_loss: 0.0447\n",
      "Epoch 2420/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0362 - val_loss: 0.0652\n",
      "Epoch 2421/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0344 - val_loss: 0.0486\n",
      "Epoch 2422/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0327 - val_loss: 0.0456\n",
      "Epoch 2423/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0332 - val_loss: 0.0472\n",
      "Epoch 2424/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0361 - val_loss: 0.0525\n",
      "Epoch 2425/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0365 - val_loss: 0.0489\n",
      "Epoch 2426/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0380 - val_loss: 0.0573\n",
      "Epoch 2427/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0350 - val_loss: 0.0573\n",
      "Epoch 2428/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0358 - val_loss: 0.0556\n",
      "Epoch 2429/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0352 - val_loss: 0.0555\n",
      "Epoch 2430/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0381 - val_loss: 0.0436\n",
      "Epoch 2431/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0337 - val_loss: 0.0462\n",
      "Epoch 2432/3000\n",
      "2370/2370 [==============================] - 0s 28us/step - loss: 0.0395 - val_loss: 0.0362\n",
      "Epoch 2433/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0358 - val_loss: 0.0606\n",
      "Epoch 2434/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0348 - val_loss: 0.0619\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2435/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0358 - val_loss: 0.0780\n",
      "Epoch 2436/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0340 - val_loss: 0.0536\n",
      "Epoch 2437/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0345 - val_loss: 0.0504\n",
      "Epoch 2438/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0355 - val_loss: 0.0693\n",
      "Epoch 2439/3000\n",
      "2370/2370 [==============================] - 0s 30us/step - loss: 0.0362 - val_loss: 0.0472\n",
      "Epoch 2440/3000\n",
      "2370/2370 [==============================] - 0s 30us/step - loss: 0.0347 - val_loss: 0.0441\n",
      "Epoch 2441/3000\n",
      "2370/2370 [==============================] - 0s 30us/step - loss: 0.0364 - val_loss: 0.0547\n",
      "Epoch 2442/3000\n",
      "2370/2370 [==============================] - 0s 30us/step - loss: 0.0369 - val_loss: 0.0527\n",
      "Epoch 2443/3000\n",
      "2370/2370 [==============================] - 0s 30us/step - loss: 0.0332 - val_loss: 0.0592\n",
      "Epoch 2444/3000\n",
      "2370/2370 [==============================] - 0s 30us/step - loss: 0.0337 - val_loss: 0.0474\n",
      "Epoch 2445/3000\n",
      "2370/2370 [==============================] - 0s 31us/step - loss: 0.0385 - val_loss: 0.0479\n",
      "Epoch 2446/3000\n",
      "2370/2370 [==============================] - 0s 30us/step - loss: 0.0316 - val_loss: 0.0554\n",
      "Epoch 2447/3000\n",
      "2370/2370 [==============================] - 0s 30us/step - loss: 0.0324 - val_loss: 0.0643\n",
      "Epoch 2448/3000\n",
      "2370/2370 [==============================] - 0s 31us/step - loss: 0.0359 - val_loss: 0.0566\n",
      "Epoch 2449/3000\n",
      "2370/2370 [==============================] - 0s 30us/step - loss: 0.0346 - val_loss: 0.0497\n",
      "Epoch 2450/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0406 - val_loss: 0.0614\n",
      "Epoch 2451/3000\n",
      "2370/2370 [==============================] - 0s 30us/step - loss: 0.0337 - val_loss: 0.0446\n",
      "Epoch 2452/3000\n",
      "2370/2370 [==============================] - 0s 30us/step - loss: 0.0401 - val_loss: 0.0535\n",
      "Epoch 2453/3000\n",
      "2370/2370 [==============================] - 0s 31us/step - loss: 0.0374 - val_loss: 0.0363\n",
      "Epoch 2454/3000\n",
      "2370/2370 [==============================] - 0s 30us/step - loss: 0.0376 - val_loss: 0.0555\n",
      "Epoch 2455/3000\n",
      "2370/2370 [==============================] - 0s 30us/step - loss: 0.0373 - val_loss: 0.0650\n",
      "Epoch 2456/3000\n",
      "2370/2370 [==============================] - 0s 30us/step - loss: 0.0361 - val_loss: 0.0528\n",
      "Epoch 2457/3000\n",
      "2370/2370 [==============================] - 0s 30us/step - loss: 0.0364 - val_loss: 0.0573\n",
      "Epoch 2458/3000\n",
      "2370/2370 [==============================] - 0s 30us/step - loss: 0.0364 - val_loss: 0.0586\n",
      "Epoch 2459/3000\n",
      "2370/2370 [==============================] - 0s 30us/step - loss: 0.0340 - val_loss: 0.0693\n",
      "Epoch 2460/3000\n",
      "2370/2370 [==============================] - 0s 30us/step - loss: 0.0344 - val_loss: 0.0481\n",
      "Epoch 2461/3000\n",
      "2370/2370 [==============================] - 0s 30us/step - loss: 0.0328 - val_loss: 0.0498\n",
      "Epoch 2462/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0340 - val_loss: 0.0443\n",
      "Epoch 2463/3000\n",
      "2370/2370 [==============================] - 0s 30us/step - loss: 0.0335 - val_loss: 0.0589\n",
      "Epoch 2464/3000\n",
      "2370/2370 [==============================] - 0s 30us/step - loss: 0.0337 - val_loss: 0.0537\n",
      "Epoch 2465/3000\n",
      "2370/2370 [==============================] - 0s 30us/step - loss: 0.0330 - val_loss: 0.0522\n",
      "Epoch 2466/3000\n",
      "2370/2370 [==============================] - 0s 30us/step - loss: 0.0371 - val_loss: 0.0617\n",
      "Epoch 2467/3000\n",
      "2370/2370 [==============================] - 0s 30us/step - loss: 0.0374 - val_loss: 0.0542\n",
      "Epoch 2468/3000\n",
      "2370/2370 [==============================] - 0s 30us/step - loss: 0.0367 - val_loss: 0.0481\n",
      "Epoch 2469/3000\n",
      "2370/2370 [==============================] - 0s 30us/step - loss: 0.0373 - val_loss: 0.0634\n",
      "Epoch 2470/3000\n",
      "2370/2370 [==============================] - 0s 30us/step - loss: 0.0352 - val_loss: 0.0413\n",
      "Epoch 2471/3000\n",
      "2370/2370 [==============================] - 0s 30us/step - loss: 0.0325 - val_loss: 0.0607\n",
      "Epoch 2472/3000\n",
      "2370/2370 [==============================] - 0s 30us/step - loss: 0.0329 - val_loss: 0.0555\n",
      "Epoch 2473/3000\n",
      "2370/2370 [==============================] - 0s 30us/step - loss: 0.0322 - val_loss: 0.0684\n",
      "Epoch 2474/3000\n",
      "2370/2370 [==============================] - 0s 30us/step - loss: 0.0380 - val_loss: 0.0427\n",
      "Epoch 2475/3000\n",
      "2370/2370 [==============================] - 0s 30us/step - loss: 0.0329 - val_loss: 0.0595\n",
      "Epoch 2476/3000\n",
      "2370/2370 [==============================] - 0s 30us/step - loss: 0.0395 - val_loss: 0.0418\n",
      "Epoch 2477/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0340 - val_loss: 0.0564\n",
      "Epoch 2478/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0384 - val_loss: 0.0520\n",
      "Epoch 2479/3000\n",
      "2370/2370 [==============================] - 0s 30us/step - loss: 0.0330 - val_loss: 0.0535\n",
      "Epoch 2480/3000\n",
      "2370/2370 [==============================] - 0s 30us/step - loss: 0.0350 - val_loss: 0.0553\n",
      "Epoch 2481/3000\n",
      "2370/2370 [==============================] - 0s 30us/step - loss: 0.0326 - val_loss: 0.0582\n",
      "Epoch 2482/3000\n",
      "2370/2370 [==============================] - 0s 30us/step - loss: 0.0354 - val_loss: 0.0515\n",
      "Epoch 2483/3000\n",
      "2370/2370 [==============================] - 0s 30us/step - loss: 0.0350 - val_loss: 0.0456\n",
      "Epoch 2484/3000\n",
      "2370/2370 [==============================] - 0s 30us/step - loss: 0.0379 - val_loss: 0.0512\n",
      "Epoch 2485/3000\n",
      "2370/2370 [==============================] - 0s 30us/step - loss: 0.0370 - val_loss: 0.0645\n",
      "Epoch 2486/3000\n",
      "2370/2370 [==============================] - 0s 30us/step - loss: 0.0344 - val_loss: 0.0565\n",
      "Epoch 2487/3000\n",
      "2370/2370 [==============================] - 0s 30us/step - loss: 0.0358 - val_loss: 0.0544\n",
      "Epoch 2488/3000\n",
      "2370/2370 [==============================] - 0s 30us/step - loss: 0.0326 - val_loss: 0.0491\n",
      "Epoch 2489/3000\n",
      "2370/2370 [==============================] - 0s 30us/step - loss: 0.0345 - val_loss: 0.0575\n",
      "Epoch 2490/3000\n",
      "2370/2370 [==============================] - 0s 30us/step - loss: 0.0334 - val_loss: 0.0571\n",
      "Epoch 2491/3000\n",
      "2370/2370 [==============================] - 0s 30us/step - loss: 0.0348 - val_loss: 0.0411\n",
      "Epoch 2492/3000\n",
      "2370/2370 [==============================] - 0s 30us/step - loss: 0.0352 - val_loss: 0.0623\n",
      "Epoch 2493/3000\n",
      "2370/2370 [==============================] - 0s 30us/step - loss: 0.0348 - val_loss: 0.0602\n",
      "Epoch 2494/3000\n",
      "2370/2370 [==============================] - 0s 31us/step - loss: 0.0360 - val_loss: 0.0542\n",
      "Epoch 2495/3000\n",
      "2370/2370 [==============================] - 0s 31us/step - loss: 0.0393 - val_loss: 0.0551\n",
      "Epoch 2496/3000\n",
      "2370/2370 [==============================] - 0s 30us/step - loss: 0.0350 - val_loss: 0.0495\n",
      "Epoch 2497/3000\n",
      "2370/2370 [==============================] - 0s 31us/step - loss: 0.0356 - val_loss: 0.0598\n",
      "Epoch 2498/3000\n",
      "2370/2370 [==============================] - 0s 30us/step - loss: 0.0378 - val_loss: 0.0507\n",
      "Epoch 2499/3000\n",
      "2370/2370 [==============================] - 0s 51us/step - loss: 0.0340 - val_loss: 0.0602\n",
      "Epoch 2500/3000\n",
      "2370/2370 [==============================] - 0s 81us/step - loss: 0.0381 - val_loss: 0.0482\n",
      "Epoch 2501/3000\n",
      "2370/2370 [==============================] - 0s 38us/step - loss: 0.0382 - val_loss: 0.0529\n",
      "Epoch 2502/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0352 - val_loss: 0.0527\n",
      "Epoch 2503/3000\n",
      "2370/2370 [==============================] - 0s 32us/step - loss: 0.0353 - val_loss: 0.0464\n",
      "Epoch 2504/3000\n",
      "2370/2370 [==============================] - 0s 45us/step - loss: 0.0338 - val_loss: 0.0585\n",
      "Epoch 2505/3000\n",
      "2370/2370 [==============================] - 0s 65us/step - loss: 0.0364 - val_loss: 0.0727\n",
      "Epoch 2506/3000\n",
      "2370/2370 [==============================] - 0s 43us/step - loss: 0.0322 - val_loss: 0.0555\n",
      "Epoch 2507/3000\n",
      "2370/2370 [==============================] - 0s 32us/step - loss: 0.0330 - val_loss: 0.0506\n",
      "Epoch 2508/3000\n",
      "2370/2370 [==============================] - 0s 72us/step - loss: 0.0386 - val_loss: 0.0446\n",
      "Epoch 2509/3000\n",
      "2370/2370 [==============================] - 0s 58us/step - loss: 0.0360 - val_loss: 0.0573\n",
      "Epoch 2510/3000\n",
      "2370/2370 [==============================] - 0s 88us/step - loss: 0.0357 - val_loss: 0.0432\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2511/3000\n",
      "2370/2370 [==============================] - 0s 68us/step - loss: 0.0351 - val_loss: 0.0595\n",
      "Epoch 2512/3000\n",
      "2370/2370 [==============================] - 0s 78us/step - loss: 0.0321 - val_loss: 0.0538\n",
      "Epoch 2513/3000\n",
      "2370/2370 [==============================] - 0s 36us/step - loss: 0.0373 - val_loss: 0.0533\n",
      "Epoch 2514/3000\n",
      "2370/2370 [==============================] - 0s 33us/step - loss: 0.0358 - val_loss: 0.0431\n",
      "Epoch 2515/3000\n",
      "2370/2370 [==============================] - 0s 78us/step - loss: 0.0334 - val_loss: 0.0570\n",
      "Epoch 2516/3000\n",
      "2370/2370 [==============================] - 0s 59us/step - loss: 0.0393 - val_loss: 0.0605\n",
      "Epoch 2517/3000\n",
      "2370/2370 [==============================] - 0s 45us/step - loss: 0.0394 - val_loss: 0.0502\n",
      "Epoch 2518/3000\n",
      "2370/2370 [==============================] - 0s 67us/step - loss: 0.0324 - val_loss: 0.0470\n",
      "Epoch 2519/3000\n",
      "2370/2370 [==============================] - 0s 66us/step - loss: 0.0339 - val_loss: 0.0562\n",
      "Epoch 2520/3000\n",
      "2370/2370 [==============================] - 0s 45us/step - loss: 0.0354 - val_loss: 0.0557\n",
      "Epoch 2521/3000\n",
      "2370/2370 [==============================] - 0s 33us/step - loss: 0.0336 - val_loss: 0.0685\n",
      "Epoch 2522/3000\n",
      "2370/2370 [==============================] - 0s 64us/step - loss: 0.0378 - val_loss: 0.0696\n",
      "Epoch 2523/3000\n",
      "2370/2370 [==============================] - 0s 39us/step - loss: 0.0346 - val_loss: 0.0558\n",
      "Epoch 2524/3000\n",
      "2370/2370 [==============================] - 0s 51us/step - loss: 0.0354 - val_loss: 0.0497\n",
      "Epoch 2525/3000\n",
      "2370/2370 [==============================] - 0s 82us/step - loss: 0.0350 - val_loss: 0.0606\n",
      "Epoch 2526/3000\n",
      "2370/2370 [==============================] - 0s 36us/step - loss: 0.0378 - val_loss: 0.0483\n",
      "Epoch 2527/3000\n",
      "2370/2370 [==============================] - 0s 88us/step - loss: 0.0348 - val_loss: 0.0478\n",
      "Epoch 2528/3000\n",
      "2370/2370 [==============================] - 0s 39us/step - loss: 0.0381 - val_loss: 0.0534\n",
      "Epoch 2529/3000\n",
      "2370/2370 [==============================] - 0s 81us/step - loss: 0.0365 - val_loss: 0.0525\n",
      "Epoch 2530/3000\n",
      "2370/2370 [==============================] - 0s 33us/step - loss: 0.0332 - val_loss: 0.0484\n",
      "Epoch 2531/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0320 - val_loss: 0.0452\n",
      "Epoch 2532/3000\n",
      "2370/2370 [==============================] - 0s 28us/step - loss: 0.0350 - val_loss: 0.0477\n",
      "Epoch 2533/3000\n",
      "2370/2370 [==============================] - 0s 28us/step - loss: 0.0378 - val_loss: 0.0578\n",
      "Epoch 2534/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0341 - val_loss: 0.0750\n",
      "Epoch 2535/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0383 - val_loss: 0.0631\n",
      "Epoch 2536/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0326 - val_loss: 0.0541\n",
      "Epoch 2537/3000\n",
      "2370/2370 [==============================] - 0s 54us/step - loss: 0.0341 - val_loss: 0.0776\n",
      "Epoch 2538/3000\n",
      "2370/2370 [==============================] - 0s 68us/step - loss: 0.0330 - val_loss: 0.0503\n",
      "Epoch 2539/3000\n",
      "2370/2370 [==============================] - 0s 93us/step - loss: 0.0390 - val_loss: 0.0540\n",
      "Epoch 2540/3000\n",
      "2370/2370 [==============================] - 0s 44us/step - loss: 0.0386 - val_loss: 0.0555\n",
      "Epoch 2541/3000\n",
      "2370/2370 [==============================] - 0s 73us/step - loss: 0.0349 - val_loss: 0.0596\n",
      "Epoch 2542/3000\n",
      "2370/2370 [==============================] - 0s 37us/step - loss: 0.0403 - val_loss: 0.0505\n",
      "Epoch 2543/3000\n",
      "2370/2370 [==============================] - 0s 37us/step - loss: 0.0369 - val_loss: 0.0440\n",
      "Epoch 2544/3000\n",
      "2370/2370 [==============================] - 0s 81us/step - loss: 0.0350 - val_loss: 0.0595\n",
      "Epoch 2545/3000\n",
      "2370/2370 [==============================] - 0s 33us/step - loss: 0.0311 - val_loss: 0.0503\n",
      "Epoch 2546/3000\n",
      "2370/2370 [==============================] - 0s 68us/step - loss: 0.0355 - val_loss: 0.0595\n",
      "Epoch 2547/3000\n",
      "2370/2370 [==============================] - 0s 33us/step - loss: 0.0331 - val_loss: 0.0395\n",
      "Epoch 2548/3000\n",
      "2370/2370 [==============================] - 0s 75us/step - loss: 0.0343 - val_loss: 0.0488\n",
      "Epoch 2549/3000\n",
      "2370/2370 [==============================] - 0s 48us/step - loss: 0.0336 - val_loss: 0.0704\n",
      "Epoch 2550/3000\n",
      "2370/2370 [==============================] - 0s 39us/step - loss: 0.0350 - val_loss: 0.0552\n",
      "Epoch 2551/3000\n",
      "2370/2370 [==============================] - 0s 88us/step - loss: 0.0335 - val_loss: 0.0515\n",
      "Epoch 2552/3000\n",
      "2370/2370 [==============================] - 0s 53us/step - loss: 0.0367 - val_loss: 0.0585\n",
      "Epoch 2553/3000\n",
      "2370/2370 [==============================] - 0s 32us/step - loss: 0.0364 - val_loss: 0.0613\n",
      "Epoch 2554/3000\n",
      "2370/2370 [==============================] - 0s 31us/step - loss: 0.0351 - val_loss: 0.0671\n",
      "Epoch 2555/3000\n",
      "2370/2370 [==============================] - 0s 31us/step - loss: 0.0365 - val_loss: 0.0453\n",
      "Epoch 2556/3000\n",
      "2370/2370 [==============================] - 0s 31us/step - loss: 0.0330 - val_loss: 0.0700\n",
      "Epoch 2557/3000\n",
      "2370/2370 [==============================] - 0s 31us/step - loss: 0.0361 - val_loss: 0.0700\n",
      "Epoch 2558/3000\n",
      "2370/2370 [==============================] - 0s 72us/step - loss: 0.0345 - val_loss: 0.0621\n",
      "Epoch 2559/3000\n",
      "2370/2370 [==============================] - 0s 58us/step - loss: 0.0347 - val_loss: 0.0454\n",
      "Epoch 2560/3000\n",
      "2370/2370 [==============================] - 0s 45us/step - loss: 0.0346 - val_loss: 0.0573\n",
      "Epoch 2561/3000\n",
      "2370/2370 [==============================] - 0s 33us/step - loss: 0.0345 - val_loss: 0.0625\n",
      "Epoch 2562/3000\n",
      "2370/2370 [==============================] - 0s 68us/step - loss: 0.0338 - val_loss: 0.0576\n",
      "Epoch 2563/3000\n",
      "2370/2370 [==============================] - 0s 36us/step - loss: 0.0365 - val_loss: 0.0531\n",
      "Epoch 2564/3000\n",
      "2370/2370 [==============================] - 0s 43us/step - loss: 0.0340 - val_loss: 0.0482\n",
      "Epoch 2565/3000\n",
      "2370/2370 [==============================] - 0s 72us/step - loss: 0.0353 - val_loss: 0.0471\n",
      "Epoch 2566/3000\n",
      "2370/2370 [==============================] - 0s 35us/step - loss: 0.0365 - val_loss: 0.0562\n",
      "Epoch 2567/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0362 - val_loss: 0.0526\n",
      "Epoch 2568/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0333 - val_loss: 0.0447\n",
      "Epoch 2569/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0355 - val_loss: 0.0549\n",
      "Epoch 2570/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0359 - val_loss: 0.0587\n",
      "Epoch 2571/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0386 - val_loss: 0.0548\n",
      "Epoch 2572/3000\n",
      "2370/2370 [==============================] - 0s 28us/step - loss: 0.0348 - val_loss: 0.0505\n",
      "Epoch 2573/3000\n",
      "2370/2370 [==============================] - 0s 73us/step - loss: 0.0374 - val_loss: 0.0769\n",
      "Epoch 2574/3000\n",
      "2370/2370 [==============================] - 0s 32us/step - loss: 0.0346 - val_loss: 0.0468\n",
      "Epoch 2575/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0361 - val_loss: 0.0520\n",
      "Epoch 2576/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0352 - val_loss: 0.0644\n",
      "Epoch 2577/3000\n",
      "2370/2370 [==============================] - 0s 32us/step - loss: 0.0349 - val_loss: 0.0670\n",
      "Epoch 2578/3000\n",
      "2370/2370 [==============================] - 0s 32us/step - loss: 0.0370 - val_loss: 0.0726\n",
      "Epoch 2579/3000\n",
      "2370/2370 [==============================] - 0s 32us/step - loss: 0.0400 - val_loss: 0.0703\n",
      "Epoch 2580/3000\n",
      "2370/2370 [==============================] - 0s 31us/step - loss: 0.0360 - val_loss: 0.0484\n",
      "Epoch 2581/3000\n",
      "2370/2370 [==============================] - 0s 76us/step - loss: 0.0326 - val_loss: 0.0634\n",
      "Epoch 2582/3000\n",
      "2370/2370 [==============================] - 0s 64us/step - loss: 0.0369 - val_loss: 0.0582\n",
      "Epoch 2583/3000\n",
      "2370/2370 [==============================] - 0s 40us/step - loss: 0.0339 - val_loss: 0.0452\n",
      "Epoch 2584/3000\n",
      "2370/2370 [==============================] - 0s 30us/step - loss: 0.0381 - val_loss: 0.0527\n",
      "Epoch 2585/3000\n",
      "2370/2370 [==============================] - 0s 31us/step - loss: 0.0354 - val_loss: 0.0442\n",
      "Epoch 2586/3000\n",
      "2370/2370 [==============================] - 0s 30us/step - loss: 0.0346 - val_loss: 0.0416\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2587/3000\n",
      "2370/2370 [==============================] - 0s 30us/step - loss: 0.0362 - val_loss: 0.0632\n",
      "Epoch 2588/3000\n",
      "2370/2370 [==============================] - 0s 30us/step - loss: 0.0345 - val_loss: 0.0556\n",
      "Epoch 2589/3000\n",
      "2370/2370 [==============================] - 0s 30us/step - loss: 0.0379 - val_loss: 0.0505\n",
      "Epoch 2590/3000\n",
      "2370/2370 [==============================] - 0s 30us/step - loss: 0.0351 - val_loss: 0.0545\n",
      "Epoch 2591/3000\n",
      "2370/2370 [==============================] - 0s 30us/step - loss: 0.0312 - val_loss: 0.0427\n",
      "Epoch 2592/3000\n",
      "2370/2370 [==============================] - 0s 30us/step - loss: 0.0353 - val_loss: 0.0472\n",
      "Epoch 2593/3000\n",
      "2370/2370 [==============================] - 0s 30us/step - loss: 0.0354 - val_loss: 0.0615\n",
      "Epoch 2594/3000\n",
      "2370/2370 [==============================] - 0s 30us/step - loss: 0.0345 - val_loss: 0.0459\n",
      "Epoch 2595/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0368 - val_loss: 0.0584\n",
      "Epoch 2596/3000\n",
      "2370/2370 [==============================] - 0s 28us/step - loss: 0.0324 - val_loss: 0.0550\n",
      "Epoch 2597/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0349 - val_loss: 0.0559\n",
      "Epoch 2598/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0372 - val_loss: 0.0503\n",
      "Epoch 2599/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0375 - val_loss: 0.0510\n",
      "Epoch 2600/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0357 - val_loss: 0.0513\n",
      "Epoch 2601/3000\n",
      "2370/2370 [==============================] - 0s 31us/step - loss: 0.0332 - val_loss: 0.0714\n",
      "Epoch 2602/3000\n",
      "2370/2370 [==============================] - 0s 30us/step - loss: 0.0361 - val_loss: 0.0484\n",
      "Epoch 2603/3000\n",
      "2370/2370 [==============================] - 0s 31us/step - loss: 0.0387 - val_loss: 0.0694\n",
      "Epoch 2604/3000\n",
      "2370/2370 [==============================] - 0s 30us/step - loss: 0.0336 - val_loss: 0.0501\n",
      "Epoch 2605/3000\n",
      "2370/2370 [==============================] - 0s 30us/step - loss: 0.0344 - val_loss: 0.0752\n",
      "Epoch 2606/3000\n",
      "2370/2370 [==============================] - 0s 30us/step - loss: 0.0364 - val_loss: 0.0475\n",
      "Epoch 2607/3000\n",
      "2370/2370 [==============================] - 0s 31us/step - loss: 0.0351 - val_loss: 0.0451\n",
      "Epoch 2608/3000\n",
      "2370/2370 [==============================] - 0s 31us/step - loss: 0.0374 - val_loss: 0.0562\n",
      "Epoch 2609/3000\n",
      "2370/2370 [==============================] - 0s 30us/step - loss: 0.0388 - val_loss: 0.0594\n",
      "Epoch 2610/3000\n",
      "2370/2370 [==============================] - 0s 30us/step - loss: 0.0341 - val_loss: 0.0442\n",
      "Epoch 2611/3000\n",
      "2370/2370 [==============================] - 0s 30us/step - loss: 0.0385 - val_loss: 0.0592\n",
      "Epoch 2612/3000\n",
      "2370/2370 [==============================] - 0s 30us/step - loss: 0.0334 - val_loss: 0.0580\n",
      "Epoch 2613/3000\n",
      "2370/2370 [==============================] - 0s 30us/step - loss: 0.0376 - val_loss: 0.0426\n",
      "Epoch 2614/3000\n",
      "2370/2370 [==============================] - 0s 36us/step - loss: 0.0375 - val_loss: 0.0410\n",
      "Epoch 2615/3000\n",
      "2370/2370 [==============================] - 0s 90us/step - loss: 0.0328 - val_loss: 0.0505\n",
      "Epoch 2616/3000\n",
      "2370/2370 [==============================] - 0s 46us/step - loss: 0.0372 - val_loss: 0.0507\n",
      "Epoch 2617/3000\n",
      "2370/2370 [==============================] - 0s 32us/step - loss: 0.0321 - val_loss: 0.0599\n",
      "Epoch 2618/3000\n",
      "2370/2370 [==============================] - 0s 32us/step - loss: 0.0356 - val_loss: 0.0535\n",
      "Epoch 2619/3000\n",
      "2370/2370 [==============================] - 0s 32us/step - loss: 0.0360 - val_loss: 0.0558\n",
      "Epoch 2620/3000\n",
      "2370/2370 [==============================] - 0s 32us/step - loss: 0.0345 - val_loss: 0.0485\n",
      "Epoch 2621/3000\n",
      "2370/2370 [==============================] - 0s 32us/step - loss: 0.0372 - val_loss: 0.0448\n",
      "Epoch 2622/3000\n",
      "2370/2370 [==============================] - 0s 32us/step - loss: 0.0351 - val_loss: 0.0541\n",
      "Epoch 2623/3000\n",
      "2370/2370 [==============================] - 0s 32us/step - loss: 0.0319 - val_loss: 0.0465\n",
      "Epoch 2624/3000\n",
      "2370/2370 [==============================] - 0s 32us/step - loss: 0.0352 - val_loss: 0.0491\n",
      "Epoch 2625/3000\n",
      "2370/2370 [==============================] - 0s 32us/step - loss: 0.0376 - val_loss: 0.0569\n",
      "Epoch 2626/3000\n",
      "2370/2370 [==============================] - 0s 32us/step - loss: 0.0380 - val_loss: 0.0624\n",
      "Epoch 2627/3000\n",
      "2370/2370 [==============================] - 0s 30us/step - loss: 0.0351 - val_loss: 0.0463\n",
      "Epoch 2628/3000\n",
      "2370/2370 [==============================] - 0s 28us/step - loss: 0.0362 - val_loss: 0.0478\n",
      "Epoch 2629/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0307 - val_loss: 0.0392\n",
      "Epoch 2630/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0328 - val_loss: 0.0514\n",
      "Epoch 2631/3000\n",
      "2370/2370 [==============================] - 0s 28us/step - loss: 0.0396 - val_loss: 0.0503\n",
      "Epoch 2632/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0359 - val_loss: 0.0449\n",
      "Epoch 2633/3000\n",
      "2370/2370 [==============================] - 0s 28us/step - loss: 0.0345 - val_loss: 0.0840\n",
      "Epoch 2634/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0358 - val_loss: 0.0463\n",
      "Epoch 2635/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0369 - val_loss: 0.0669\n",
      "Epoch 2636/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0332 - val_loss: 0.0491\n",
      "Epoch 2637/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0396 - val_loss: 0.0473\n",
      "Epoch 2638/3000\n",
      "2370/2370 [==============================] - 0s 30us/step - loss: 0.0343 - val_loss: 0.0409\n",
      "Epoch 2639/3000\n",
      "2370/2370 [==============================] - 0s 30us/step - loss: 0.0346 - val_loss: 0.0444\n",
      "Epoch 2640/3000\n",
      "2370/2370 [==============================] - 0s 31us/step - loss: 0.0345 - val_loss: 0.0513\n",
      "Epoch 2641/3000\n",
      "2370/2370 [==============================] - 0s 30us/step - loss: 0.0354 - val_loss: 0.0526\n",
      "Epoch 2642/3000\n",
      "2370/2370 [==============================] - 0s 30us/step - loss: 0.0361 - val_loss: 0.0571\n",
      "Epoch 2643/3000\n",
      "2370/2370 [==============================] - 0s 31us/step - loss: 0.0366 - val_loss: 0.0611\n",
      "Epoch 2644/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0372 - val_loss: 0.0651\n",
      "Epoch 2645/3000\n",
      "2370/2370 [==============================] - 0s 28us/step - loss: 0.0352 - val_loss: 0.0469\n",
      "Epoch 2646/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0345 - val_loss: 0.0507\n",
      "Epoch 2647/3000\n",
      "2370/2370 [==============================] - 0s 30us/step - loss: 0.0369 - val_loss: 0.0499\n",
      "Epoch 2648/3000\n",
      "2370/2370 [==============================] - 0s 33us/step - loss: 0.0358 - val_loss: 0.0758\n",
      "Epoch 2649/3000\n",
      "2370/2370 [==============================] - 0s 32us/step - loss: 0.0332 - val_loss: 0.0520\n",
      "Epoch 2650/3000\n",
      "2370/2370 [==============================] - 0s 32us/step - loss: 0.0355 - val_loss: 0.0497\n",
      "Epoch 2651/3000\n",
      "2370/2370 [==============================] - 0s 32us/step - loss: 0.0289 - val_loss: 0.0566\n",
      "Epoch 2652/3000\n",
      "2370/2370 [==============================] - 0s 38us/step - loss: 0.0371 - val_loss: 0.0405\n",
      "Epoch 2653/3000\n",
      "2370/2370 [==============================] - 0s 68us/step - loss: 0.0343 - val_loss: 0.0597\n",
      "Epoch 2654/3000\n",
      "2370/2370 [==============================] - 0s 34us/step - loss: 0.0358 - val_loss: 0.0619\n",
      "Epoch 2655/3000\n",
      "2370/2370 [==============================] - 0s 34us/step - loss: 0.0341 - val_loss: 0.0538\n",
      "Epoch 2656/3000\n",
      "2370/2370 [==============================] - 0s 31us/step - loss: 0.0348 - val_loss: 0.0691\n",
      "Epoch 2657/3000\n",
      "2370/2370 [==============================] - 0s 30us/step - loss: 0.0371 - val_loss: 0.0427\n",
      "Epoch 2658/3000\n",
      "2370/2370 [==============================] - 0s 30us/step - loss: 0.0356 - val_loss: 0.0626\n",
      "Epoch 2659/3000\n",
      "2370/2370 [==============================] - 0s 32us/step - loss: 0.0320 - val_loss: 0.0415\n",
      "Epoch 2660/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0372 - val_loss: 0.0481\n",
      "Epoch 2661/3000\n",
      "2370/2370 [==============================] - 0s 30us/step - loss: 0.0327 - val_loss: 0.0446\n",
      "Epoch 2662/3000\n",
      "2370/2370 [==============================] - 0s 30us/step - loss: 0.0389 - val_loss: 0.0675\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2663/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0331 - val_loss: 0.0550\n",
      "Epoch 2664/3000\n",
      "2370/2370 [==============================] - 0s 30us/step - loss: 0.0334 - val_loss: 0.0590\n",
      "Epoch 2665/3000\n",
      "2370/2370 [==============================] - 0s 28us/step - loss: 0.0347 - val_loss: 0.0410\n",
      "Epoch 2666/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0386 - val_loss: 0.0486\n",
      "Epoch 2667/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0360 - val_loss: 0.0720\n",
      "Epoch 2668/3000\n",
      "2370/2370 [==============================] - 0s 46us/step - loss: 0.0357 - val_loss: 0.0461\n",
      "Epoch 2669/3000\n",
      "2370/2370 [==============================] - 0s 80us/step - loss: 0.0345 - val_loss: 0.0583\n",
      "Epoch 2670/3000\n",
      "2370/2370 [==============================] - 0s 39us/step - loss: 0.0365 - val_loss: 0.0567\n",
      "Epoch 2671/3000\n",
      "2370/2370 [==============================] - 0s 35us/step - loss: 0.0348 - val_loss: 0.0642\n",
      "Epoch 2672/3000\n",
      "2370/2370 [==============================] - 0s 34us/step - loss: 0.0386 - val_loss: 0.0591\n",
      "Epoch 2673/3000\n",
      "2370/2370 [==============================] - 0s 34us/step - loss: 0.0403 - val_loss: 0.0594\n",
      "Epoch 2674/3000\n",
      "2370/2370 [==============================] - 0s 34us/step - loss: 0.0334 - val_loss: 0.0621\n",
      "Epoch 2675/3000\n",
      "2370/2370 [==============================] - 0s 33us/step - loss: 0.0341 - val_loss: 0.0734\n",
      "Epoch 2676/3000\n",
      "2370/2370 [==============================] - 0s 33us/step - loss: 0.0361 - val_loss: 0.0464\n",
      "Epoch 2677/3000\n",
      "2370/2370 [==============================] - 0s 34us/step - loss: 0.0372 - val_loss: 0.0618\n",
      "Epoch 2678/3000\n",
      "2370/2370 [==============================] - 0s 36us/step - loss: 0.0320 - val_loss: 0.0430\n",
      "Epoch 2679/3000\n",
      "2370/2370 [==============================] - 0s 32us/step - loss: 0.0316 - val_loss: 0.0452\n",
      "Epoch 2680/3000\n",
      "2370/2370 [==============================] - 0s 31us/step - loss: 0.0367 - val_loss: 0.0495\n",
      "Epoch 2681/3000\n",
      "2370/2370 [==============================] - 0s 32us/step - loss: 0.0331 - val_loss: 0.0492\n",
      "Epoch 2682/3000\n",
      "2370/2370 [==============================] - 0s 35us/step - loss: 0.0415 - val_loss: 0.0613\n",
      "Epoch 2683/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0351 - val_loss: 0.0728\n",
      "Epoch 2684/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0362 - val_loss: 0.0405\n",
      "Epoch 2685/3000\n",
      "2370/2370 [==============================] - 0s 28us/step - loss: 0.0357 - val_loss: 0.0399\n",
      "Epoch 2686/3000\n",
      "2370/2370 [==============================] - 0s 56us/step - loss: 0.0356 - val_loss: 0.0585\n",
      "Epoch 2687/3000\n",
      "2370/2370 [==============================] - 0s 49us/step - loss: 0.0345 - val_loss: 0.0743\n",
      "Epoch 2688/3000\n",
      "2370/2370 [==============================] - 0s 40us/step - loss: 0.0332 - val_loss: 0.0499\n",
      "Epoch 2689/3000\n",
      "2370/2370 [==============================] - 0s 40us/step - loss: 0.0319 - val_loss: 0.0441\n",
      "Epoch 2690/3000\n",
      "2370/2370 [==============================] - 0s 40us/step - loss: 0.0340 - val_loss: 0.0659\n",
      "Epoch 2691/3000\n",
      "2370/2370 [==============================] - 0s 40us/step - loss: 0.0373 - val_loss: 0.0547\n",
      "Epoch 2692/3000\n",
      "2370/2370 [==============================] - 0s 32us/step - loss: 0.0348 - val_loss: 0.0481\n",
      "Epoch 2693/3000\n",
      "2370/2370 [==============================] - 0s 32us/step - loss: 0.0356 - val_loss: 0.0516\n",
      "Epoch 2694/3000\n",
      "2370/2370 [==============================] - 0s 31us/step - loss: 0.0342 - val_loss: 0.0722\n",
      "Epoch 2695/3000\n",
      "2370/2370 [==============================] - 0s 32us/step - loss: 0.0334 - val_loss: 0.0569\n",
      "Epoch 2696/3000\n",
      "2370/2370 [==============================] - 0s 32us/step - loss: 0.0361 - val_loss: 0.0596\n",
      "Epoch 2697/3000\n",
      "2370/2370 [==============================] - 0s 32us/step - loss: 0.0336 - val_loss: 0.0563\n",
      "Epoch 2698/3000\n",
      "2370/2370 [==============================] - 0s 32us/step - loss: 0.0330 - val_loss: 0.0550\n",
      "Epoch 2699/3000\n",
      "2370/2370 [==============================] - 0s 32us/step - loss: 0.0371 - val_loss: 0.0446\n",
      "Epoch 2700/3000\n",
      "2370/2370 [==============================] - 0s 31us/step - loss: 0.0315 - val_loss: 0.0473\n",
      "Epoch 2701/3000\n",
      "2370/2370 [==============================] - 0s 33us/step - loss: 0.0336 - val_loss: 0.0451\n",
      "Epoch 2702/3000\n",
      "2370/2370 [==============================] - 0s 31us/step - loss: 0.0368 - val_loss: 0.0451\n",
      "Epoch 2703/3000\n",
      "2370/2370 [==============================] - 0s 33us/step - loss: 0.0353 - val_loss: 0.0506\n",
      "Epoch 2704/3000\n",
      "2370/2370 [==============================] - 0s 32us/step - loss: 0.0356 - val_loss: 0.0560\n",
      "Epoch 2705/3000\n",
      "2370/2370 [==============================] - 0s 32us/step - loss: 0.0370 - val_loss: 0.0798\n",
      "Epoch 2706/3000\n",
      "2370/2370 [==============================] - 0s 31us/step - loss: 0.0340 - val_loss: 0.0569\n",
      "Epoch 2707/3000\n",
      "2370/2370 [==============================] - 0s 32us/step - loss: 0.0323 - val_loss: 0.0681\n",
      "Epoch 2708/3000\n",
      "2370/2370 [==============================] - 0s 35us/step - loss: 0.0327 - val_loss: 0.0412\n",
      "Epoch 2709/3000\n",
      "2370/2370 [==============================] - 0s 31us/step - loss: 0.0372 - val_loss: 0.0608\n",
      "Epoch 2710/3000\n",
      "2370/2370 [==============================] - 0s 31us/step - loss: 0.0336 - val_loss: 0.0546\n",
      "Epoch 2711/3000\n",
      "2370/2370 [==============================] - 0s 31us/step - loss: 0.0373 - val_loss: 0.0475\n",
      "Epoch 2712/3000\n",
      "2370/2370 [==============================] - 0s 31us/step - loss: 0.0368 - val_loss: 0.0667\n",
      "Epoch 2713/3000\n",
      "2370/2370 [==============================] - 0s 33us/step - loss: 0.0322 - val_loss: 0.0519\n",
      "Epoch 2714/3000\n",
      "2370/2370 [==============================] - 0s 32us/step - loss: 0.0349 - val_loss: 0.0489\n",
      "Epoch 2715/3000\n",
      "2370/2370 [==============================] - 0s 31us/step - loss: 0.0359 - val_loss: 0.0478\n",
      "Epoch 2716/3000\n",
      "2370/2370 [==============================] - 0s 32us/step - loss: 0.0370 - val_loss: 0.0598\n",
      "Epoch 2717/3000\n",
      "2370/2370 [==============================] - 0s 33us/step - loss: 0.0349 - val_loss: 0.0494\n",
      "Epoch 2718/3000\n",
      "2370/2370 [==============================] - 0s 31us/step - loss: 0.0349 - val_loss: 0.0477\n",
      "Epoch 2719/3000\n",
      "2370/2370 [==============================] - 0s 31us/step - loss: 0.0344 - val_loss: 0.0450\n",
      "Epoch 2720/3000\n",
      "2370/2370 [==============================] - 0s 32us/step - loss: 0.0355 - val_loss: 0.0637\n",
      "Epoch 2721/3000\n",
      "2370/2370 [==============================] - 0s 33us/step - loss: 0.0356 - val_loss: 0.0536\n",
      "Epoch 2722/3000\n",
      "2370/2370 [==============================] - 0s 32us/step - loss: 0.0340 - val_loss: 0.0475\n",
      "Epoch 2723/3000\n",
      "2370/2370 [==============================] - 0s 32us/step - loss: 0.0349 - val_loss: 0.0663\n",
      "Epoch 2724/3000\n",
      "2370/2370 [==============================] - 0s 32us/step - loss: 0.0381 - val_loss: 0.0561\n",
      "Epoch 2725/3000\n",
      "2370/2370 [==============================] - 0s 31us/step - loss: 0.0393 - val_loss: 0.0439\n",
      "Epoch 2726/3000\n",
      "2370/2370 [==============================] - 0s 31us/step - loss: 0.0340 - val_loss: 0.0428\n",
      "Epoch 2727/3000\n",
      "2370/2370 [==============================] - 0s 32us/step - loss: 0.0334 - val_loss: 0.0655\n",
      "Epoch 2728/3000\n",
      "2370/2370 [==============================] - 0s 31us/step - loss: 0.0354 - val_loss: 0.0542\n",
      "Epoch 2729/3000\n",
      "2370/2370 [==============================] - 0s 33us/step - loss: 0.0338 - val_loss: 0.0460\n",
      "Epoch 2730/3000\n",
      "2370/2370 [==============================] - 0s 31us/step - loss: 0.0346 - val_loss: 0.0553\n",
      "Epoch 2731/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0342 - val_loss: 0.0635\n",
      "Epoch 2732/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0343 - val_loss: 0.0667\n",
      "Epoch 2733/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0363 - val_loss: 0.0459\n",
      "Epoch 2734/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0360 - val_loss: 0.0688\n",
      "Epoch 2735/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0377 - val_loss: 0.0503\n",
      "Epoch 2736/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0342 - val_loss: 0.0634\n",
      "Epoch 2737/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0379 - val_loss: 0.0511\n",
      "Epoch 2738/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0360 - val_loss: 0.0481\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2739/3000\n",
      "2370/2370 [==============================] - 0s 30us/step - loss: 0.0363 - val_loss: 0.0625\n",
      "Epoch 2740/3000\n",
      "2370/2370 [==============================] - 0s 32us/step - loss: 0.0358 - val_loss: 0.0521\n",
      "Epoch 2741/3000\n",
      "2370/2370 [==============================] - 0s 31us/step - loss: 0.0340 - val_loss: 0.0547\n",
      "Epoch 2742/3000\n",
      "2370/2370 [==============================] - 0s 31us/step - loss: 0.0382 - val_loss: 0.0447\n",
      "Epoch 2743/3000\n",
      "2370/2370 [==============================] - 0s 31us/step - loss: 0.0373 - val_loss: 0.0643\n",
      "Epoch 2744/3000\n",
      "2370/2370 [==============================] - 0s 30us/step - loss: 0.0349 - val_loss: 0.0565\n",
      "Epoch 2745/3000\n",
      "2370/2370 [==============================] - 0s 30us/step - loss: 0.0314 - val_loss: 0.0658\n",
      "Epoch 2746/3000\n",
      "2370/2370 [==============================] - 0s 30us/step - loss: 0.0372 - val_loss: 0.0620\n",
      "Epoch 2747/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0377 - val_loss: 0.0510\n",
      "Epoch 2748/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0346 - val_loss: 0.0623\n",
      "Epoch 2749/3000\n",
      "2370/2370 [==============================] - 0s 31us/step - loss: 0.0354 - val_loss: 0.0534\n",
      "Epoch 2750/3000\n",
      "2370/2370 [==============================] - 0s 30us/step - loss: 0.0390 - val_loss: 0.0513\n",
      "Epoch 2751/3000\n",
      "2370/2370 [==============================] - 0s 31us/step - loss: 0.0356 - val_loss: 0.0495\n",
      "Epoch 2752/3000\n",
      "2370/2370 [==============================] - 0s 30us/step - loss: 0.0369 - val_loss: 0.0532\n",
      "Epoch 2753/3000\n",
      "2370/2370 [==============================] - 0s 30us/step - loss: 0.0339 - val_loss: 0.0575\n",
      "Epoch 2754/3000\n",
      "2370/2370 [==============================] - 0s 30us/step - loss: 0.0325 - val_loss: 0.0679\n",
      "Epoch 2755/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0342 - val_loss: 0.0546\n",
      "Epoch 2756/3000\n",
      "2370/2370 [==============================] - 0s 30us/step - loss: 0.0371 - val_loss: 0.0434\n",
      "Epoch 2757/3000\n",
      "2370/2370 [==============================] - 0s 30us/step - loss: 0.0347 - val_loss: 0.0489\n",
      "Epoch 2758/3000\n",
      "2370/2370 [==============================] - 0s 30us/step - loss: 0.0329 - val_loss: 0.0459\n",
      "Epoch 2759/3000\n",
      "2370/2370 [==============================] - 0s 30us/step - loss: 0.0334 - val_loss: 0.0526\n",
      "Epoch 2760/3000\n",
      "2370/2370 [==============================] - 0s 30us/step - loss: 0.0333 - val_loss: 0.0538\n",
      "Epoch 2761/3000\n",
      "2370/2370 [==============================] - 0s 30us/step - loss: 0.0317 - val_loss: 0.0635\n",
      "Epoch 2762/3000\n",
      "2370/2370 [==============================] - 0s 30us/step - loss: 0.0368 - val_loss: 0.0441\n",
      "Epoch 2763/3000\n",
      "2370/2370 [==============================] - 0s 30us/step - loss: 0.0389 - val_loss: 0.0512\n",
      "Epoch 2764/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0372 - val_loss: 0.0546\n",
      "Epoch 2765/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0351 - val_loss: 0.0541\n",
      "Epoch 2766/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0352 - val_loss: 0.0455\n",
      "Epoch 2767/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0350 - val_loss: 0.0553\n",
      "Epoch 2768/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0340 - val_loss: 0.0400\n",
      "Epoch 2769/3000\n",
      "2370/2370 [==============================] - 0s 28us/step - loss: 0.0385 - val_loss: 0.0537\n",
      "Epoch 2770/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0358 - val_loss: 0.0505\n",
      "Epoch 2771/3000\n",
      "2370/2370 [==============================] - 0s 30us/step - loss: 0.0325 - val_loss: 0.0391\n",
      "Epoch 2772/3000\n",
      "2370/2370 [==============================] - 0s 28us/step - loss: 0.0348 - val_loss: 0.0648\n",
      "Epoch 2773/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0379 - val_loss: 0.0474\n",
      "Epoch 2774/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0354 - val_loss: 0.0446\n",
      "Epoch 2775/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0361 - val_loss: 0.0720\n",
      "Epoch 2776/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0388 - val_loss: 0.0538\n",
      "Epoch 2777/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0369 - val_loss: 0.0484\n",
      "Epoch 2778/3000\n",
      "2370/2370 [==============================] - 0s 28us/step - loss: 0.0370 - val_loss: 0.0508\n",
      "Epoch 2779/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0369 - val_loss: 0.0585\n",
      "Epoch 2780/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0341 - val_loss: 0.0639\n",
      "Epoch 2781/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0376 - val_loss: 0.0563\n",
      "Epoch 2782/3000\n",
      "2370/2370 [==============================] - 0s 28us/step - loss: 0.0342 - val_loss: 0.0556\n",
      "Epoch 2783/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0373 - val_loss: 0.0586\n",
      "Epoch 2784/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0341 - val_loss: 0.0391\n",
      "Epoch 2785/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0351 - val_loss: 0.0535\n",
      "Epoch 2786/3000\n",
      "2370/2370 [==============================] - 0s 30us/step - loss: 0.0323 - val_loss: 0.0610\n",
      "Epoch 2787/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0373 - val_loss: 0.0597\n",
      "Epoch 2788/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0350 - val_loss: 0.0510\n",
      "Epoch 2789/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0330 - val_loss: 0.0530\n",
      "Epoch 2790/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0353 - val_loss: 0.0480\n",
      "Epoch 2791/3000\n",
      "2370/2370 [==============================] - 0s 28us/step - loss: 0.0359 - val_loss: 0.0576\n",
      "Epoch 2792/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0333 - val_loss: 0.0552\n",
      "Epoch 2793/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0341 - val_loss: 0.0391\n",
      "Epoch 2794/3000\n",
      "2370/2370 [==============================] - 0s 30us/step - loss: 0.0331 - val_loss: 0.0470\n",
      "Epoch 2795/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0348 - val_loss: 0.0492\n",
      "Epoch 2796/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0341 - val_loss: 0.0413\n",
      "Epoch 2797/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0340 - val_loss: 0.0485\n",
      "Epoch 2798/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0350 - val_loss: 0.0382\n",
      "Epoch 2799/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0329 - val_loss: 0.0476\n",
      "Epoch 2800/3000\n",
      "2370/2370 [==============================] - 0s 32us/step - loss: 0.0351 - val_loss: 0.0520\n",
      "Epoch 2801/3000\n",
      "2370/2370 [==============================] - 0s 33us/step - loss: 0.0350 - val_loss: 0.0565\n",
      "Epoch 2802/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0349 - val_loss: 0.0511\n",
      "Epoch 2803/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0353 - val_loss: 0.0486\n",
      "Epoch 2804/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0387 - val_loss: 0.0508\n",
      "Epoch 2805/3000\n",
      "2370/2370 [==============================] - 0s 30us/step - loss: 0.0376 - val_loss: 0.0468\n",
      "Epoch 2806/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0350 - val_loss: 0.0444\n",
      "Epoch 2807/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0368 - val_loss: 0.0586\n",
      "Epoch 2808/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0390 - val_loss: 0.0441\n",
      "Epoch 2809/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0375 - val_loss: 0.0499\n",
      "Epoch 2810/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0368 - val_loss: 0.0445\n",
      "Epoch 2811/3000\n",
      "2370/2370 [==============================] - 0s 28us/step - loss: 0.0355 - val_loss: 0.0614\n",
      "Epoch 2812/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0348 - val_loss: 0.0507\n",
      "Epoch 2813/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0373 - val_loss: 0.0484\n",
      "Epoch 2814/3000\n",
      "2370/2370 [==============================] - 0s 31us/step - loss: 0.0363 - val_loss: 0.0576\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2815/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0340 - val_loss: 0.0435\n",
      "Epoch 2816/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0320 - val_loss: 0.0754\n",
      "Epoch 2817/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0339 - val_loss: 0.0430\n",
      "Epoch 2818/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0330 - val_loss: 0.0528\n",
      "Epoch 2819/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0378 - val_loss: 0.0516\n",
      "Epoch 2820/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0377 - val_loss: 0.0461\n",
      "Epoch 2821/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0323 - val_loss: 0.0441\n",
      "Epoch 2822/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0334 - val_loss: 0.0616\n",
      "Epoch 2823/3000\n",
      "2370/2370 [==============================] - 0s 30us/step - loss: 0.0345 - val_loss: 0.0618\n",
      "Epoch 2824/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0400 - val_loss: 0.0419\n",
      "Epoch 2825/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0374 - val_loss: 0.0550\n",
      "Epoch 2826/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0361 - val_loss: 0.0445\n",
      "Epoch 2827/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0353 - val_loss: 0.0511\n",
      "Epoch 2828/3000\n",
      "2370/2370 [==============================] - 0s 28us/step - loss: 0.0342 - val_loss: 0.0536\n",
      "Epoch 2829/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0384 - val_loss: 0.0646\n",
      "Epoch 2830/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0332 - val_loss: 0.0583\n",
      "Epoch 2831/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0381 - val_loss: 0.0528\n",
      "Epoch 2832/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0336 - val_loss: 0.0518\n",
      "Epoch 2833/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0355 - val_loss: 0.0476\n",
      "Epoch 2834/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0340 - val_loss: 0.0575\n",
      "Epoch 2835/3000\n",
      "2370/2370 [==============================] - 0s 30us/step - loss: 0.0359 - val_loss: 0.0527\n",
      "Epoch 2836/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0396 - val_loss: 0.0484\n",
      "Epoch 2837/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0372 - val_loss: 0.0514\n",
      "Epoch 2838/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0368 - val_loss: 0.0445\n",
      "Epoch 2839/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0378 - val_loss: 0.0553\n",
      "Epoch 2840/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0352 - val_loss: 0.0487\n",
      "Epoch 2841/3000\n",
      "2370/2370 [==============================] - 0s 28us/step - loss: 0.0347 - val_loss: 0.0528\n",
      "Epoch 2842/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0334 - val_loss: 0.0619\n",
      "Epoch 2843/3000\n",
      "2370/2370 [==============================] - 0s 28us/step - loss: 0.0342 - val_loss: 0.0616\n",
      "Epoch 2844/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0344 - val_loss: 0.0595\n",
      "Epoch 2845/3000\n",
      "2370/2370 [==============================] - 0s 28us/step - loss: 0.0356 - val_loss: 0.0379\n",
      "Epoch 2846/3000\n",
      "2370/2370 [==============================] - 0s 28us/step - loss: 0.0374 - val_loss: 0.0517\n",
      "Epoch 2847/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0351 - val_loss: 0.0678\n",
      "Epoch 2848/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0324 - val_loss: 0.0599\n",
      "Epoch 2849/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0316 - val_loss: 0.0491\n",
      "Epoch 2850/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0357 - val_loss: 0.0488\n",
      "Epoch 2851/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0340 - val_loss: 0.0758\n",
      "Epoch 2852/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0329 - val_loss: 0.0869\n",
      "Epoch 2853/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0354 - val_loss: 0.0611\n",
      "Epoch 2854/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0299 - val_loss: 0.0550\n",
      "Epoch 2855/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0350 - val_loss: 0.0484\n",
      "Epoch 2856/3000\n",
      "2370/2370 [==============================] - 0s 30us/step - loss: 0.0337 - val_loss: 0.0560\n",
      "Epoch 2857/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0375 - val_loss: 0.0524\n",
      "Epoch 2858/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0362 - val_loss: 0.0492\n",
      "Epoch 2859/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0372 - val_loss: 0.0563\n",
      "Epoch 2860/3000\n",
      "2370/2370 [==============================] - 0s 28us/step - loss: 0.0365 - val_loss: 0.0436\n",
      "Epoch 2861/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0347 - val_loss: 0.0573\n",
      "Epoch 2862/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0343 - val_loss: 0.0493\n",
      "Epoch 2863/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0375 - val_loss: 0.0451\n",
      "Epoch 2864/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0334 - val_loss: 0.0520\n",
      "Epoch 2865/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0348 - val_loss: 0.0585\n",
      "Epoch 2866/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0388 - val_loss: 0.0469\n",
      "Epoch 2867/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0372 - val_loss: 0.0395\n",
      "Epoch 2868/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0328 - val_loss: 0.0565\n",
      "Epoch 2869/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0344 - val_loss: 0.0670\n",
      "Epoch 2870/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0339 - val_loss: 0.0674\n",
      "Epoch 2871/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0362 - val_loss: 0.0523\n",
      "Epoch 2872/3000\n",
      "2370/2370 [==============================] - 0s 30us/step - loss: 0.0352 - val_loss: 0.0758\n",
      "Epoch 2873/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0376 - val_loss: 0.0429\n",
      "Epoch 2874/3000\n",
      "2370/2370 [==============================] - 0s 28us/step - loss: 0.0316 - val_loss: 0.0655\n",
      "Epoch 2875/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0360 - val_loss: 0.0683\n",
      "Epoch 2876/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0326 - val_loss: 0.0515\n",
      "Epoch 2877/3000\n",
      "2370/2370 [==============================] - 0s 28us/step - loss: 0.0350 - val_loss: 0.0574\n",
      "Epoch 2878/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0367 - val_loss: 0.0487\n",
      "Epoch 2879/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0370 - val_loss: 0.0541\n",
      "Epoch 2880/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0356 - val_loss: 0.0454\n",
      "Epoch 2881/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0376 - val_loss: 0.0500\n",
      "Epoch 2882/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0361 - val_loss: 0.0524\n",
      "Epoch 2883/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0327 - val_loss: 0.0549\n",
      "Epoch 2884/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0371 - val_loss: 0.0536\n",
      "Epoch 2885/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0368 - val_loss: 0.0447\n",
      "Epoch 2886/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0354 - val_loss: 0.0454\n",
      "Epoch 2887/3000\n",
      "2370/2370 [==============================] - 0s 28us/step - loss: 0.0357 - val_loss: 0.0422\n",
      "Epoch 2888/3000\n",
      "2370/2370 [==============================] - 0s 28us/step - loss: 0.0338 - val_loss: 0.0425\n",
      "Epoch 2889/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0349 - val_loss: 0.0835\n",
      "Epoch 2890/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0393 - val_loss: 0.0457\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2891/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0351 - val_loss: 0.0399\n",
      "Epoch 2892/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0353 - val_loss: 0.0757\n",
      "Epoch 2893/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0361 - val_loss: 0.0505\n",
      "Epoch 2894/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0371 - val_loss: 0.0643\n",
      "Epoch 2895/3000\n",
      "2370/2370 [==============================] - 0s 28us/step - loss: 0.0339 - val_loss: 0.0508\n",
      "Epoch 2896/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0371 - val_loss: 0.0798\n",
      "Epoch 2897/3000\n",
      "2370/2370 [==============================] - 0s 28us/step - loss: 0.0350 - val_loss: 0.0480\n",
      "Epoch 2898/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0345 - val_loss: 0.0501\n",
      "Epoch 2899/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0351 - val_loss: 0.0663\n",
      "Epoch 2900/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0357 - val_loss: 0.0554\n",
      "Epoch 2901/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0339 - val_loss: 0.0471\n",
      "Epoch 2902/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0349 - val_loss: 0.0478\n",
      "Epoch 2903/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0368 - val_loss: 0.0401\n",
      "Epoch 2904/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0357 - val_loss: 0.0513\n",
      "Epoch 2905/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0350 - val_loss: 0.0471\n",
      "Epoch 2906/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0375 - val_loss: 0.0545\n",
      "Epoch 2907/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0372 - val_loss: 0.0466\n",
      "Epoch 2908/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0365 - val_loss: 0.0518\n",
      "Epoch 2909/3000\n",
      "2370/2370 [==============================] - 0s 34us/step - loss: 0.0363 - val_loss: 0.0567\n",
      "Epoch 2910/3000\n",
      "2370/2370 [==============================] - 0s 35us/step - loss: 0.0362 - val_loss: 0.0422\n",
      "Epoch 2911/3000\n",
      "2370/2370 [==============================] - 0s 30us/step - loss: 0.0335 - val_loss: 0.0641\n",
      "Epoch 2912/3000\n",
      "2370/2370 [==============================] - 0s 30us/step - loss: 0.0362 - val_loss: 0.0554\n",
      "Epoch 2913/3000\n",
      "2370/2370 [==============================] - 0s 42us/step - loss: 0.0354 - val_loss: 0.0543\n",
      "Epoch 2914/3000\n",
      "2370/2370 [==============================] - 0s 46us/step - loss: 0.0349 - val_loss: 0.0445\n",
      "Epoch 2915/3000\n",
      "2370/2370 [==============================] - 0s 111us/step - loss: 0.0370 - val_loss: 0.0515\n",
      "Epoch 2916/3000\n",
      "2370/2370 [==============================] - 0s 80us/step - loss: 0.0312 - val_loss: 0.0668\n",
      "Epoch 2917/3000\n",
      "2370/2370 [==============================] - 0s 33us/step - loss: 0.0391 - val_loss: 0.0601\n",
      "Epoch 2918/3000\n",
      "2370/2370 [==============================] - 0s 33us/step - loss: 0.0358 - val_loss: 0.0543\n",
      "Epoch 2919/3000\n",
      "2370/2370 [==============================] - 0s 33us/step - loss: 0.0324 - val_loss: 0.0929\n",
      "Epoch 2920/3000\n",
      "2370/2370 [==============================] - 0s 34us/step - loss: 0.0357 - val_loss: 0.0638\n",
      "Epoch 2921/3000\n",
      "2370/2370 [==============================] - 0s 33us/step - loss: 0.0346 - val_loss: 0.0490\n",
      "Epoch 2922/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0345 - val_loss: 0.0677\n",
      "Epoch 2923/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0338 - val_loss: 0.0471\n",
      "Epoch 2924/3000\n",
      "2370/2370 [==============================] - 0s 28us/step - loss: 0.0351 - val_loss: 0.0494\n",
      "Epoch 2925/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0357 - val_loss: 0.0703\n",
      "Epoch 2926/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0379 - val_loss: 0.0499\n",
      "Epoch 2927/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0352 - val_loss: 0.0483\n",
      "Epoch 2928/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0347 - val_loss: 0.0530\n",
      "Epoch 2929/3000\n",
      "2370/2370 [==============================] - 0s 28us/step - loss: 0.0370 - val_loss: 0.0546\n",
      "Epoch 2930/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0377 - val_loss: 0.0581\n",
      "Epoch 2931/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0365 - val_loss: 0.0432\n",
      "Epoch 2932/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0369 - val_loss: 0.0539\n",
      "Epoch 2933/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0330 - val_loss: 0.0437\n",
      "Epoch 2934/3000\n",
      "2370/2370 [==============================] - 0s 30us/step - loss: 0.0317 - val_loss: 0.0537\n",
      "Epoch 2935/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0335 - val_loss: 0.0443\n",
      "Epoch 2936/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0334 - val_loss: 0.0534\n",
      "Epoch 2937/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0348 - val_loss: 0.0501\n",
      "Epoch 2938/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0371 - val_loss: 0.0501\n",
      "Epoch 2939/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0344 - val_loss: 0.0510\n",
      "Epoch 2940/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0336 - val_loss: 0.0465\n",
      "Epoch 2941/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0382 - val_loss: 0.0583\n",
      "Epoch 2942/3000\n",
      "2370/2370 [==============================] - 0s 30us/step - loss: 0.0368 - val_loss: 0.0535\n",
      "Epoch 2943/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0367 - val_loss: 0.0522\n",
      "Epoch 2944/3000\n",
      "2370/2370 [==============================] - 0s 30us/step - loss: 0.0353 - val_loss: 0.0562\n",
      "Epoch 2945/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0343 - val_loss: 0.0475\n",
      "Epoch 2946/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0344 - val_loss: 0.0553\n",
      "Epoch 2947/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0353 - val_loss: 0.0637\n",
      "Epoch 2948/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0342 - val_loss: 0.0917\n",
      "Epoch 2949/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0357 - val_loss: 0.0486\n",
      "Epoch 2950/3000\n",
      "2370/2370 [==============================] - 0s 30us/step - loss: 0.0360 - val_loss: 0.0713\n",
      "Epoch 2951/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0367 - val_loss: 0.0609\n",
      "Epoch 2952/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0309 - val_loss: 0.0611\n",
      "Epoch 2953/3000\n",
      "2370/2370 [==============================] - 0s 30us/step - loss: 0.0362 - val_loss: 0.0397\n",
      "Epoch 2954/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0342 - val_loss: 0.0399\n",
      "Epoch 2955/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0371 - val_loss: 0.0558\n",
      "Epoch 2956/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0345 - val_loss: 0.0667\n",
      "Epoch 2957/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0340 - val_loss: 0.0546\n",
      "Epoch 2958/3000\n",
      "2370/2370 [==============================] - 0s 30us/step - loss: 0.0370 - val_loss: 0.0440\n",
      "Epoch 2959/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0336 - val_loss: 0.0560\n",
      "Epoch 2960/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0347 - val_loss: 0.0703\n",
      "Epoch 2961/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0321 - val_loss: 0.0538\n",
      "Epoch 2962/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0333 - val_loss: 0.0504\n",
      "Epoch 2963/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0353 - val_loss: 0.0654\n",
      "Epoch 2964/3000\n",
      "2370/2370 [==============================] - 0s 28us/step - loss: 0.0373 - val_loss: 0.0708\n",
      "Epoch 2965/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0392 - val_loss: 0.0516\n",
      "Epoch 2966/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0336 - val_loss: 0.0658\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2967/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0384 - val_loss: 0.0642\n",
      "Epoch 2968/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0361 - val_loss: 0.0652\n",
      "Epoch 2969/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0335 - val_loss: 0.0449\n",
      "Epoch 2970/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0352 - val_loss: 0.0506\n",
      "Epoch 2971/3000\n",
      "2370/2370 [==============================] - 0s 30us/step - loss: 0.0331 - val_loss: 0.0562\n",
      "Epoch 2972/3000\n",
      "2370/2370 [==============================] - 0s 31us/step - loss: 0.0353 - val_loss: 0.0632\n",
      "Epoch 2973/3000\n",
      "2370/2370 [==============================] - 0s 30us/step - loss: 0.0367 - val_loss: 0.0607\n",
      "Epoch 2974/3000\n",
      "2370/2370 [==============================] - 0s 30us/step - loss: 0.0336 - val_loss: 0.0645\n",
      "Epoch 2975/3000\n",
      "2370/2370 [==============================] - 0s 30us/step - loss: 0.0343 - val_loss: 0.0516\n",
      "Epoch 2976/3000\n",
      "2370/2370 [==============================] - 0s 30us/step - loss: 0.0367 - val_loss: 0.0565\n",
      "Epoch 2977/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0366 - val_loss: 0.0526\n",
      "Epoch 2978/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0353 - val_loss: 0.0587\n",
      "Epoch 2979/3000\n",
      "2370/2370 [==============================] - 0s 28us/step - loss: 0.0374 - val_loss: 0.0657\n",
      "Epoch 2980/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0351 - val_loss: 0.0452\n",
      "Epoch 2981/3000\n",
      "2370/2370 [==============================] - 0s 30us/step - loss: 0.0392 - val_loss: 0.0624\n",
      "Epoch 2982/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0357 - val_loss: 0.0474\n",
      "Epoch 2983/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0320 - val_loss: 0.0626\n",
      "Epoch 2984/3000\n",
      "2370/2370 [==============================] - 0s 28us/step - loss: 0.0395 - val_loss: 0.0622\n",
      "Epoch 2985/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0343 - val_loss: 0.0532\n",
      "Epoch 2986/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0326 - val_loss: 0.0574\n",
      "Epoch 2987/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0345 - val_loss: 0.0581\n",
      "Epoch 2988/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0369 - val_loss: 0.0541\n",
      "Epoch 2989/3000\n",
      "2370/2370 [==============================] - 0s 28us/step - loss: 0.0349 - val_loss: 0.0663\n",
      "Epoch 2990/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0380 - val_loss: 0.0465\n",
      "Epoch 2991/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0339 - val_loss: 0.0470\n",
      "Epoch 2992/3000\n",
      "2370/2370 [==============================] - 0s 28us/step - loss: 0.0328 - val_loss: 0.0546\n",
      "Epoch 2993/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0392 - val_loss: 0.0504\n",
      "Epoch 2994/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0361 - val_loss: 0.0527\n",
      "Epoch 2995/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0355 - val_loss: 0.0487\n",
      "Epoch 2996/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0388 - val_loss: 0.0468\n",
      "Epoch 2997/3000\n",
      "2370/2370 [==============================] - 0s 30us/step - loss: 0.0383 - val_loss: 0.0598\n",
      "Epoch 2998/3000\n",
      "2370/2370 [==============================] - 0s 28us/step - loss: 0.0405 - val_loss: 0.0625\n",
      "Epoch 2999/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0340 - val_loss: 0.0524\n",
      "Epoch 3000/3000\n",
      "2370/2370 [==============================] - 0s 28us/step - loss: 0.0327 - val_loss: 0.0544\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "233.48819375038147"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "start_seq = time.time()\n",
    "seq_history=seq_model.fit(x_train, y_train, epochs=3000, batch_size=50, verbose=1, validation_data=(x_test, y_test));\n",
    "end_seq = time.time()\n",
    "time_seq = end_seq - start_seq\n",
    "time_seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2370/2370 [==============================] - 0s 16us/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.006579852851209887"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seq_model.evaluate(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from matplotlib import pyplot\n",
    "plt.plot(seq_history.history['loss'], label='FF training Loss')\n",
    "plt.plot(seq_history.history['val_loss'], label='FF testing Loss')\n",
    "plt.yscale(\"log\")\n",
    "plt.grid()\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate predictions\n",
    "raw_seq_predictions = pd.DataFrame(seq_model.predict(x_test),columns = ['seq_prediction'], index = test_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Invert scaling for prediction data\n",
    "x_test = pd.DataFrame(x_test, columns = x_columns, index = test_index)\n",
    "unscaled_seq_predictions = pd.concat([x_test, raw_seq_predictions], axis=1)\n",
    "#unscaled_seq_predictions = pd.concat([unscaled_seq_predictions, test_data.google_45d], axis=1)\n",
    "unscaled_seq_predictions = pd.DataFrame(scaler.inverse_transform(unscaled_seq_predictions), columns=unscaled_seq_predictions.columns, index=unscaled_seq_predictions.index)\n",
    "\n",
    "# Invert scaling for actual data\n",
    "unscaled_data = pd.concat([x_test, y_test], axis=1)\n",
    "#unscaled_data = pd.concat([unscaled_data, test_data.google_45d], axis=1)\n",
    "unscaled_data = pd.DataFrame(scaler.inverse_transform(unscaled_data), columns=unscaled_data.columns, index=unscaled_data.index)\n",
    "#unscaled_data = unscaled_data['google_45d_sta']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "backup['unscaled_seq_predictions'] = unscaled_seq_predictions\n",
    "backup['unscaled_data'] = unscaled_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>google_open</th>\n",
       "      <th>google_high</th>\n",
       "      <th>google_low</th>\n",
       "      <th>google_close</th>\n",
       "      <th>google</th>\n",
       "      <th>google_volume</th>\n",
       "      <th>google_ra_04</th>\n",
       "      <th>google_std_04</th>\n",
       "      <th>google_ra_09</th>\n",
       "      <th>google_std_09</th>\n",
       "      <th>...</th>\n",
       "      <th>nasdaq_ra_04</th>\n",
       "      <th>nasdaq_std_04</th>\n",
       "      <th>nasdaq_ra_09</th>\n",
       "      <th>nasdaq_std_09</th>\n",
       "      <th>nasdaq_ra_18</th>\n",
       "      <th>nasdaq_std_18</th>\n",
       "      <th>google_daily_vol</th>\n",
       "      <th>s&amp;p_daily_vol</th>\n",
       "      <th>nasdaq_daily_vol</th>\n",
       "      <th>seq_prediction</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Date</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2018-07-31</th>\n",
       "      <td>1220.010010</td>\n",
       "      <td>1227.588013</td>\n",
       "      <td>1205.599976</td>\n",
       "      <td>1217.260010</td>\n",
       "      <td>1217.260010</td>\n",
       "      <td>1644700.0</td>\n",
       "      <td>1218.500000</td>\n",
       "      <td>1.753611</td>\n",
       "      <td>1237.301409</td>\n",
       "      <td>24.147664</td>\n",
       "      <td>...</td>\n",
       "      <td>7650.895020</td>\n",
       "      <td>29.550020</td>\n",
       "      <td>7786.610073</td>\n",
       "      <td>109.258547</td>\n",
       "      <td>7805.587565</td>\n",
       "      <td>85.060270</td>\n",
       "      <td>0.307801</td>\n",
       "      <td>0.089192</td>\n",
       "      <td>0.152314</td>\n",
       "      <td>1066.033145</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-08-01</th>\n",
       "      <td>1228.000000</td>\n",
       "      <td>1233.469971</td>\n",
       "      <td>1210.209961</td>\n",
       "      <td>1220.010010</td>\n",
       "      <td>1220.010010</td>\n",
       "      <td>1567200.0</td>\n",
       "      <td>1219.003337</td>\n",
       "      <td>1.515790</td>\n",
       "      <td>1239.374268</td>\n",
       "      <td>21.433011</td>\n",
       "      <td>...</td>\n",
       "      <td>7669.693359</td>\n",
       "      <td>38.687654</td>\n",
       "      <td>7767.384347</td>\n",
       "      <td>109.753646</td>\n",
       "      <td>7798.026217</td>\n",
       "      <td>85.881161</td>\n",
       "      <td>0.285090</td>\n",
       "      <td>0.083101</td>\n",
       "      <td>0.146205</td>\n",
       "      <td>1069.279489</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-08-02</th>\n",
       "      <td>1205.900024</td>\n",
       "      <td>1229.880005</td>\n",
       "      <td>1204.790039</td>\n",
       "      <td>1226.150024</td>\n",
       "      <td>1226.150024</td>\n",
       "      <td>1531300.0</td>\n",
       "      <td>1220.790009</td>\n",
       "      <td>3.781604</td>\n",
       "      <td>1236.241420</td>\n",
       "      <td>21.550840</td>\n",
       "      <td>...</td>\n",
       "      <td>7702.942505</td>\n",
       "      <td>73.619602</td>\n",
       "      <td>7761.944336</td>\n",
       "      <td>106.402575</td>\n",
       "      <td>7798.359340</td>\n",
       "      <td>82.521365</td>\n",
       "      <td>0.271257</td>\n",
       "      <td>0.086055</td>\n",
       "      <td>0.164864</td>\n",
       "      <td>1094.989721</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-08-03</th>\n",
       "      <td>1229.619995</td>\n",
       "      <td>1230.000000</td>\n",
       "      <td>1215.060059</td>\n",
       "      <td>1223.709961</td>\n",
       "      <td>1223.709961</td>\n",
       "      <td>1089600.0</td>\n",
       "      <td>1221.782501</td>\n",
       "      <td>3.932133</td>\n",
       "      <td>1230.528564</td>\n",
       "      <td>18.079647</td>\n",
       "      <td>...</td>\n",
       "      <td>7748.444946</td>\n",
       "      <td>69.648419</td>\n",
       "      <td>7744.768555</td>\n",
       "      <td>81.003961</td>\n",
       "      <td>7798.808594</td>\n",
       "      <td>82.581619</td>\n",
       "      <td>0.214106</td>\n",
       "      <td>0.085981</td>\n",
       "      <td>0.165144</td>\n",
       "      <td>1093.122379</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-08-06</th>\n",
       "      <td>1225.000000</td>\n",
       "      <td>1226.088013</td>\n",
       "      <td>1215.796997</td>\n",
       "      <td>1224.770020</td>\n",
       "      <td>1224.770020</td>\n",
       "      <td>1081700.0</td>\n",
       "      <td>1224.239991</td>\n",
       "      <td>0.749575</td>\n",
       "      <td>1221.940003</td>\n",
       "      <td>3.444802</td>\n",
       "      <td>...</td>\n",
       "      <td>7835.844971</td>\n",
       "      <td>33.708070</td>\n",
       "      <td>7747.243327</td>\n",
       "      <td>90.494779</td>\n",
       "      <td>7792.345052</td>\n",
       "      <td>87.849464</td>\n",
       "      <td>0.151643</td>\n",
       "      <td>0.065983</td>\n",
       "      <td>0.139857</td>\n",
       "      <td>1084.553800</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 40 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            google_open  google_high   google_low  google_close       google  \\\n",
       "Date                                                                           \n",
       "2018-07-31  1220.010010  1227.588013  1205.599976   1217.260010  1217.260010   \n",
       "2018-08-01  1228.000000  1233.469971  1210.209961   1220.010010  1220.010010   \n",
       "2018-08-02  1205.900024  1229.880005  1204.790039   1226.150024  1226.150024   \n",
       "2018-08-03  1229.619995  1230.000000  1215.060059   1223.709961  1223.709961   \n",
       "2018-08-06  1225.000000  1226.088013  1215.796997   1224.770020  1224.770020   \n",
       "\n",
       "            google_volume  google_ra_04  google_std_04  google_ra_09  \\\n",
       "Date                                                                   \n",
       "2018-07-31      1644700.0   1218.500000       1.753611   1237.301409   \n",
       "2018-08-01      1567200.0   1219.003337       1.515790   1239.374268   \n",
       "2018-08-02      1531300.0   1220.790009       3.781604   1236.241420   \n",
       "2018-08-03      1089600.0   1221.782501       3.932133   1230.528564   \n",
       "2018-08-06      1081700.0   1224.239991       0.749575   1221.940003   \n",
       "\n",
       "            google_std_09  ...  nasdaq_ra_04  nasdaq_std_04  nasdaq_ra_09  \\\n",
       "Date                       ...                                              \n",
       "2018-07-31      24.147664  ...   7650.895020      29.550020   7786.610073   \n",
       "2018-08-01      21.433011  ...   7669.693359      38.687654   7767.384347   \n",
       "2018-08-02      21.550840  ...   7702.942505      73.619602   7761.944336   \n",
       "2018-08-03      18.079647  ...   7748.444946      69.648419   7744.768555   \n",
       "2018-08-06       3.444802  ...   7835.844971      33.708070   7747.243327   \n",
       "\n",
       "            nasdaq_std_09  nasdaq_ra_18  nasdaq_std_18  google_daily_vol  \\\n",
       "Date                                                                       \n",
       "2018-07-31     109.258547   7805.587565      85.060270          0.307801   \n",
       "2018-08-01     109.753646   7798.026217      85.881161          0.285090   \n",
       "2018-08-02     106.402575   7798.359340      82.521365          0.271257   \n",
       "2018-08-03      81.003961   7798.808594      82.581619          0.214106   \n",
       "2018-08-06      90.494779   7792.345052      87.849464          0.151643   \n",
       "\n",
       "            s&p_daily_vol  nasdaq_daily_vol  seq_prediction  \n",
       "Date                                                         \n",
       "2018-07-31       0.089192          0.152314     1066.033145  \n",
       "2018-08-01       0.083101          0.146205     1069.279489  \n",
       "2018-08-02       0.086055          0.164864     1094.989721  \n",
       "2018-08-03       0.085981          0.165144     1093.122379  \n",
       "2018-08-06       0.065983          0.139857     1084.553800  \n",
       "\n",
       "[5 rows x 40 columns]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unscaled_seq_predictions.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>google_open</th>\n",
       "      <th>google_high</th>\n",
       "      <th>google_low</th>\n",
       "      <th>google_close</th>\n",
       "      <th>google</th>\n",
       "      <th>google_volume</th>\n",
       "      <th>google_ra_04</th>\n",
       "      <th>google_std_04</th>\n",
       "      <th>google_ra_09</th>\n",
       "      <th>google_std_09</th>\n",
       "      <th>...</th>\n",
       "      <th>nasdaq_ra_04</th>\n",
       "      <th>nasdaq_std_04</th>\n",
       "      <th>nasdaq_ra_09</th>\n",
       "      <th>nasdaq_std_09</th>\n",
       "      <th>nasdaq_ra_18</th>\n",
       "      <th>nasdaq_std_18</th>\n",
       "      <th>google_daily_vol</th>\n",
       "      <th>s&amp;p_daily_vol</th>\n",
       "      <th>nasdaq_daily_vol</th>\n",
       "      <th>google_45d</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Date</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2018-07-31</th>\n",
       "      <td>1220.010010</td>\n",
       "      <td>1227.588013</td>\n",
       "      <td>1205.599976</td>\n",
       "      <td>1217.260010</td>\n",
       "      <td>1217.260010</td>\n",
       "      <td>1644700.0</td>\n",
       "      <td>1218.500000</td>\n",
       "      <td>1.753611</td>\n",
       "      <td>1237.301409</td>\n",
       "      <td>24.147664</td>\n",
       "      <td>...</td>\n",
       "      <td>7650.895020</td>\n",
       "      <td>29.550020</td>\n",
       "      <td>7786.610073</td>\n",
       "      <td>109.258547</td>\n",
       "      <td>7805.587565</td>\n",
       "      <td>85.060270</td>\n",
       "      <td>0.307801</td>\n",
       "      <td>0.089192</td>\n",
       "      <td>0.152314</td>\n",
       "      <td>1156.050049</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-08-01</th>\n",
       "      <td>1228.000000</td>\n",
       "      <td>1233.469971</td>\n",
       "      <td>1210.209961</td>\n",
       "      <td>1220.010010</td>\n",
       "      <td>1220.010010</td>\n",
       "      <td>1567200.0</td>\n",
       "      <td>1219.003337</td>\n",
       "      <td>1.515790</td>\n",
       "      <td>1239.374268</td>\n",
       "      <td>21.433011</td>\n",
       "      <td>...</td>\n",
       "      <td>7669.693359</td>\n",
       "      <td>38.687654</td>\n",
       "      <td>7767.384347</td>\n",
       "      <td>109.753646</td>\n",
       "      <td>7798.026217</td>\n",
       "      <td>85.881161</td>\n",
       "      <td>0.285090</td>\n",
       "      <td>0.083101</td>\n",
       "      <td>0.146205</td>\n",
       "      <td>1161.219971</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-08-02</th>\n",
       "      <td>1205.900024</td>\n",
       "      <td>1229.880005</td>\n",
       "      <td>1204.790039</td>\n",
       "      <td>1226.150024</td>\n",
       "      <td>1226.150024</td>\n",
       "      <td>1531300.0</td>\n",
       "      <td>1220.790009</td>\n",
       "      <td>3.781604</td>\n",
       "      <td>1236.241420</td>\n",
       "      <td>21.550840</td>\n",
       "      <td>...</td>\n",
       "      <td>7702.942505</td>\n",
       "      <td>73.619602</td>\n",
       "      <td>7761.944336</td>\n",
       "      <td>106.402575</td>\n",
       "      <td>7798.359340</td>\n",
       "      <td>82.521365</td>\n",
       "      <td>0.271257</td>\n",
       "      <td>0.086055</td>\n",
       "      <td>0.164864</td>\n",
       "      <td>1171.089966</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-08-03</th>\n",
       "      <td>1229.619995</td>\n",
       "      <td>1230.000000</td>\n",
       "      <td>1215.060059</td>\n",
       "      <td>1223.709961</td>\n",
       "      <td>1223.709961</td>\n",
       "      <td>1089600.0</td>\n",
       "      <td>1221.782501</td>\n",
       "      <td>3.932133</td>\n",
       "      <td>1230.528564</td>\n",
       "      <td>18.079647</td>\n",
       "      <td>...</td>\n",
       "      <td>7748.444946</td>\n",
       "      <td>69.648419</td>\n",
       "      <td>7744.768555</td>\n",
       "      <td>81.003961</td>\n",
       "      <td>7798.808594</td>\n",
       "      <td>82.581619</td>\n",
       "      <td>0.214106</td>\n",
       "      <td>0.085981</td>\n",
       "      <td>0.165144</td>\n",
       "      <td>1186.869995</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-08-06</th>\n",
       "      <td>1225.000000</td>\n",
       "      <td>1226.088013</td>\n",
       "      <td>1215.796997</td>\n",
       "      <td>1224.770020</td>\n",
       "      <td>1224.770020</td>\n",
       "      <td>1081700.0</td>\n",
       "      <td>1224.239991</td>\n",
       "      <td>0.749575</td>\n",
       "      <td>1221.940003</td>\n",
       "      <td>3.444802</td>\n",
       "      <td>...</td>\n",
       "      <td>7835.844971</td>\n",
       "      <td>33.708070</td>\n",
       "      <td>7747.243327</td>\n",
       "      <td>90.494779</td>\n",
       "      <td>7792.345052</td>\n",
       "      <td>87.849464</td>\n",
       "      <td>0.151643</td>\n",
       "      <td>0.065983</td>\n",
       "      <td>0.139857</td>\n",
       "      <td>1166.089966</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 40 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            google_open  google_high   google_low  google_close       google  \\\n",
       "Date                                                                           \n",
       "2018-07-31  1220.010010  1227.588013  1205.599976   1217.260010  1217.260010   \n",
       "2018-08-01  1228.000000  1233.469971  1210.209961   1220.010010  1220.010010   \n",
       "2018-08-02  1205.900024  1229.880005  1204.790039   1226.150024  1226.150024   \n",
       "2018-08-03  1229.619995  1230.000000  1215.060059   1223.709961  1223.709961   \n",
       "2018-08-06  1225.000000  1226.088013  1215.796997   1224.770020  1224.770020   \n",
       "\n",
       "            google_volume  google_ra_04  google_std_04  google_ra_09  \\\n",
       "Date                                                                   \n",
       "2018-07-31      1644700.0   1218.500000       1.753611   1237.301409   \n",
       "2018-08-01      1567200.0   1219.003337       1.515790   1239.374268   \n",
       "2018-08-02      1531300.0   1220.790009       3.781604   1236.241420   \n",
       "2018-08-03      1089600.0   1221.782501       3.932133   1230.528564   \n",
       "2018-08-06      1081700.0   1224.239991       0.749575   1221.940003   \n",
       "\n",
       "            google_std_09  ...  nasdaq_ra_04  nasdaq_std_04  nasdaq_ra_09  \\\n",
       "Date                       ...                                              \n",
       "2018-07-31      24.147664  ...   7650.895020      29.550020   7786.610073   \n",
       "2018-08-01      21.433011  ...   7669.693359      38.687654   7767.384347   \n",
       "2018-08-02      21.550840  ...   7702.942505      73.619602   7761.944336   \n",
       "2018-08-03      18.079647  ...   7748.444946      69.648419   7744.768555   \n",
       "2018-08-06       3.444802  ...   7835.844971      33.708070   7747.243327   \n",
       "\n",
       "            nasdaq_std_09  nasdaq_ra_18  nasdaq_std_18  google_daily_vol  \\\n",
       "Date                                                                       \n",
       "2018-07-31     109.258547   7805.587565      85.060270          0.307801   \n",
       "2018-08-01     109.753646   7798.026217      85.881161          0.285090   \n",
       "2018-08-02     106.402575   7798.359340      82.521365          0.271257   \n",
       "2018-08-03      81.003961   7798.808594      82.581619          0.214106   \n",
       "2018-08-06      90.494779   7792.345052      87.849464          0.151643   \n",
       "\n",
       "            s&p_daily_vol  nasdaq_daily_vol   google_45d  \n",
       "Date                                                      \n",
       "2018-07-31       0.089192          0.152314  1156.050049  \n",
       "2018-08-01       0.083101          0.146205  1161.219971  \n",
       "2018-08-02       0.086055          0.164864  1171.089966  \n",
       "2018-08-03       0.085981          0.165144  1186.869995  \n",
       "2018-08-06       0.065983          0.139857  1166.089966  \n",
       "\n",
       "[5 rows x 40 columns]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unscaled_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "seq_accuracy=np.sqrt(metrics.mean_squared_error(unscaled_data.google_45d, unscaled_seq_predictions.seq_prediction))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/matthias/anaconda3/lib/python3.7/site-packages/pandas/plotting/_converter.py:129: FutureWarning: Using an implicitly registered datetime converter for a matplotlib plotting method. The converter was registered by pandas on import. Future versions of pandas will require you to explicitly register matplotlib converters.\n",
      "\n",
      "To register the converters:\n",
      "\t>>> from pandas.plotting import register_matplotlib_converters\n",
      "\t>>> register_matplotlib_converters()\n",
      "  warnings.warn(msg, FutureWarning)\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot prediction vs actual\n",
    "plt.plot(unscaled_data['google_45d'], label='Actual Adjusted Close')\n",
    "plt.plot(unscaled_seq_predictions['seq_prediction'], label='FF predicted Adjusted Close')\n",
    "plt.grid()\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LSTM model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Split into train and test set\n",
    "And here you have to be careful according to https://machinelearningmastery.com/time-series-forecasting-long-short-term-memory-network-python/ :\n",
    "\n",
    "\"A benefit of this type of network is that it can learn and remember over long sequences and does not rely on a pre-specified window lagged observation as input. In Keras, this is referred to as stateful, and involves setting the “stateful” argument to “True” when defining an LSTM layer. By default, an LSTM layer in Keras maintains state between data within one batch. A batch of data is a fixed-sized number of rows from the training dataset that defines how many patterns to process before updating the weights of the network. State in the LSTM layer between batches is cleared by default, therefore we must make the LSTM stateful. This gives us fine-grained control over when state of the LSTM layer is cleared, by calling the reset_states() function.\"\n",
    "\n",
    "But in order to can use \"stateful = True\" the data has to be a multiple of the batch size. Therefore I have to use modulo to get the test size correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 10\n",
    "train_test_split = 0.9\n",
    "x_train, x_test, y_train, y_test, test_data, test_index, x_columns = get_train_test_data(df_scaled, train_test_split, batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train=x_train.to_numpy()\n",
    "y_train=y_train.to_numpy()\n",
    "x_test = x_test.to_numpy()\n",
    "y_test = y_test.to_numpy()\n",
    "# Reshape data for (Sample, Timesteps, Features)\n",
    "x_train = x_train.reshape(x_train.shape[0], 1, x_train.shape[1])\n",
    "x_test = x_test.reshape(x_test.shape[0], 1, x_test.shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_1 (LSTM)                (None, 1, 100)            56000     \n",
      "_________________________________________________________________\n",
      "lstm_2 (LSTM)                (None, 100)               80400     \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 1)                 101       \n",
      "=================================================================\n",
      "Total params: 136,501\n",
      "Trainable params: 136,501\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "lstm_model = Sequential()\n",
    "lstm_model.add(LSTM(units=100, input_shape = (x_train.shape[1], x_train.shape[2]), dropout = 0.2, return_sequences = True))\n",
    "lstm_model.add(LSTM(units = 100, dropout = 0.2))\n",
    "lstm_model.add(Dense(1))\n",
    "lstm_model.compile(optimizer = 'adam', loss='mse')\n",
    "lstm_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/matthias/anaconda3/lib/python3.7/site-packages/tensorflow/python/ops/math_grad.py:1250: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "Train on 2370 samples, validate on 270 samples\n",
      "Epoch 1/100\n",
      "2370/2370 [==============================] - 4s 2ms/step - loss: 0.0237 - val_loss: 0.0215\n",
      "Epoch 2/100\n",
      "2370/2370 [==============================] - 2s 976us/step - loss: 0.0088 - val_loss: 0.0169\n",
      "Epoch 3/100\n",
      "2370/2370 [==============================] - 2s 866us/step - loss: 0.0088 - val_loss: 0.0181\n",
      "Epoch 4/100\n",
      "2370/2370 [==============================] - 2s 1ms/step - loss: 0.0085 - val_loss: 0.0164\n",
      "Epoch 5/100\n",
      "2370/2370 [==============================] - 2s 944us/step - loss: 0.0076 - val_loss: 0.0159\n",
      "Epoch 6/100\n",
      "2370/2370 [==============================] - 2s 670us/step - loss: 0.0078 - val_loss: 0.0190\n",
      "Epoch 7/100\n",
      "2370/2370 [==============================] - 3s 1ms/step - loss: 0.0071 - val_loss: 0.0390\n",
      "Epoch 8/100\n",
      "2370/2370 [==============================] - 3s 1ms/step - loss: 0.0074 - val_loss: 0.0206\n",
      "Epoch 9/100\n",
      "2370/2370 [==============================] - 2s 651us/step - loss: 0.0071 - val_loss: 0.0376\n",
      "Epoch 10/100\n",
      "2370/2370 [==============================] - 2s 701us/step - loss: 0.0065 - val_loss: 0.0174\n",
      "Epoch 11/100\n",
      "2370/2370 [==============================] - 1s 534us/step - loss: 0.0066 - val_loss: 0.0170\n",
      "Epoch 12/100\n",
      "2370/2370 [==============================] - 2s 824us/step - loss: 0.0064 - val_loss: 0.0178\n",
      "Epoch 13/100\n",
      "2370/2370 [==============================] - 2s 882us/step - loss: 0.0061 - val_loss: 0.0184\n",
      "Epoch 14/100\n",
      "2370/2370 [==============================] - 2s 823us/step - loss: 0.0061 - val_loss: 0.0192\n",
      "Epoch 15/100\n",
      "2370/2370 [==============================] - 2s 915us/step - loss: 0.0061 - val_loss: 0.0169\n",
      "Epoch 16/100\n",
      "2370/2370 [==============================] - 3s 1ms/step - loss: 0.0057 - val_loss: 0.0186\n",
      "Epoch 17/100\n",
      "2370/2370 [==============================] - 2s 862us/step - loss: 0.0059 - val_loss: 0.0174\n",
      "Epoch 18/100\n",
      "2370/2370 [==============================] - 2s 686us/step - loss: 0.0057 - val_loss: 0.0163\n",
      "Epoch 19/100\n",
      "2370/2370 [==============================] - 2s 810us/step - loss: 0.0058 - val_loss: 0.0181\n",
      "Epoch 20/100\n",
      "2370/2370 [==============================] - 2s 1ms/step - loss: 0.0055 - val_loss: 0.0236\n",
      "Epoch 21/100\n",
      "2370/2370 [==============================] - 2s 749us/step - loss: 0.0054 - val_loss: 0.0175\n",
      "Epoch 22/100\n",
      "2370/2370 [==============================] - 2s 719us/step - loss: 0.0054 - val_loss: 0.0173\n",
      "Epoch 23/100\n",
      "2370/2370 [==============================] - 2s 1ms/step - loss: 0.0053 - val_loss: 0.0174\n",
      "Epoch 24/100\n",
      "2370/2370 [==============================] - 3s 1ms/step - loss: 0.0055 - val_loss: 0.0180\n",
      "Epoch 25/100\n",
      "2370/2370 [==============================] - 2s 878us/step - loss: 0.0052 - val_loss: 0.0199\n",
      "Epoch 26/100\n",
      "2370/2370 [==============================] - 2s 765us/step - loss: 0.0053 - val_loss: 0.0169\n",
      "Epoch 27/100\n",
      "2370/2370 [==============================] - 2s 763us/step - loss: 0.0053 - val_loss: 0.0165\n",
      "Epoch 28/100\n",
      "2370/2370 [==============================] - 3s 1ms/step - loss: 0.0052 - val_loss: 0.0196\n",
      "Epoch 29/100\n",
      "2370/2370 [==============================] - 2s 870us/step - loss: 0.0052 - val_loss: 0.0177\n",
      "Epoch 30/100\n",
      "2370/2370 [==============================] - 2s 734us/step - loss: 0.0050 - val_loss: 0.0244\n",
      "Epoch 31/100\n",
      "2370/2370 [==============================] - 2s 922us/step - loss: 0.0050 - val_loss: 0.0172\n",
      "Epoch 32/100\n",
      "2370/2370 [==============================] - 2s 806us/step - loss: 0.0049 - val_loss: 0.0306\n",
      "Epoch 33/100\n",
      "2370/2370 [==============================] - 2s 914us/step - loss: 0.0050 - val_loss: 0.0236\n",
      "Epoch 34/100\n",
      "2370/2370 [==============================] - 2s 854us/step - loss: 0.0049 - val_loss: 0.0183\n",
      "Epoch 35/100\n",
      "2370/2370 [==============================] - 2s 701us/step - loss: 0.0048 - val_loss: 0.0186\n",
      "Epoch 36/100\n",
      "2370/2370 [==============================] - 2s 694us/step - loss: 0.0048 - val_loss: 0.0200\n",
      "Epoch 37/100\n",
      "2370/2370 [==============================] - 2s 826us/step - loss: 0.0048 - val_loss: 0.0168\n",
      "Epoch 38/100\n",
      "2370/2370 [==============================] - 2s 812us/step - loss: 0.0047 - val_loss: 0.0206\n",
      "Epoch 39/100\n",
      "2370/2370 [==============================] - 2s 772us/step - loss: 0.0047 - val_loss: 0.0185\n",
      "Epoch 40/100\n",
      "2370/2370 [==============================] - 2s 799us/step - loss: 0.0049 - val_loss: 0.0184\n",
      "Epoch 41/100\n",
      "2370/2370 [==============================] - 2s 817us/step - loss: 0.0048 - val_loss: 0.0192\n",
      "Epoch 42/100\n",
      "2370/2370 [==============================] - 2s 820us/step - loss: 0.0047 - val_loss: 0.0180\n",
      "Epoch 43/100\n",
      "2370/2370 [==============================] - 2s 867us/step - loss: 0.0046 - val_loss: 0.0179\n",
      "Epoch 44/100\n",
      "2370/2370 [==============================] - 2s 828us/step - loss: 0.0048 - val_loss: 0.0187\n",
      "Epoch 45/100\n",
      "2370/2370 [==============================] - 2s 818us/step - loss: 0.0044 - val_loss: 0.0174\n",
      "Epoch 46/100\n",
      "2370/2370 [==============================] - 2s 1ms/step - loss: 0.0046 - val_loss: 0.0177\n",
      "Epoch 47/100\n",
      "2370/2370 [==============================] - 3s 1ms/step - loss: 0.0044 - val_loss: 0.0182\n",
      "Epoch 48/100\n",
      "2370/2370 [==============================] - 3s 1ms/step - loss: 0.0046 - val_loss: 0.0175\n",
      "Epoch 49/100\n",
      "2370/2370 [==============================] - 2s 776us/step - loss: 0.0045 - val_loss: 0.0186\n",
      "Epoch 50/100\n",
      "2370/2370 [==============================] - 2s 769us/step - loss: 0.0043 - val_loss: 0.0173\n",
      "Epoch 51/100\n",
      "2370/2370 [==============================] - 2s 751us/step - loss: 0.0045 - val_loss: 0.0177\n",
      "Epoch 52/100\n",
      "2370/2370 [==============================] - 2s 947us/step - loss: 0.0046 - val_loss: 0.0188\n",
      "Epoch 53/100\n",
      "2370/2370 [==============================] - 2s 1ms/step - loss: 0.0046 - val_loss: 0.0182\n",
      "Epoch 54/100\n",
      "2370/2370 [==============================] - 3s 1ms/step - loss: 0.0043 - val_loss: 0.0174\n",
      "Epoch 55/100\n",
      "2370/2370 [==============================] - 3s 1ms/step - loss: 0.0044 - val_loss: 0.0178\n",
      "Epoch 56/100\n",
      "2370/2370 [==============================] - 3s 1ms/step - loss: 0.0044 - val_loss: 0.0182\n",
      "Epoch 57/100\n",
      "2370/2370 [==============================] - 2s 937us/step - loss: 0.0045 - val_loss: 0.0203\n",
      "Epoch 58/100\n",
      "2370/2370 [==============================] - 2s 859us/step - loss: 0.0044 - val_loss: 0.0233\n",
      "Epoch 59/100\n",
      "2370/2370 [==============================] - 2s 995us/step - loss: 0.0043 - val_loss: 0.0179\n",
      "Epoch 60/100\n",
      "2370/2370 [==============================] - 2s 1ms/step - loss: 0.0043 - val_loss: 0.0176\n",
      "Epoch 61/100\n",
      "2370/2370 [==============================] - 2s 779us/step - loss: 0.0043 - val_loss: 0.0184\n",
      "Epoch 62/100\n",
      "2370/2370 [==============================] - 2s 817us/step - loss: 0.0044 - val_loss: 0.0191\n",
      "Epoch 63/100\n",
      "2370/2370 [==============================] - 3s 1ms/step - loss: 0.0044 - val_loss: 0.0176\n",
      "Epoch 64/100\n",
      "2370/2370 [==============================] - 3s 1ms/step - loss: 0.0043 - val_loss: 0.0177\n",
      "Epoch 65/100\n",
      "2370/2370 [==============================] - 2s 788us/step - loss: 0.0042 - val_loss: 0.0185\n",
      "Epoch 66/100\n",
      "2370/2370 [==============================] - 2s 774us/step - loss: 0.0041 - val_loss: 0.0184\n",
      "Epoch 67/100\n",
      "2370/2370 [==============================] - 2s 793us/step - loss: 0.0041 - val_loss: 0.0174\n",
      "Epoch 68/100\n",
      "2370/2370 [==============================] - 2s 830us/step - loss: 0.0042 - val_loss: 0.0185\n",
      "Epoch 69/100\n",
      "2370/2370 [==============================] - 2s 992us/step - loss: 0.0041 - val_loss: 0.0208\n",
      "Epoch 70/100\n",
      "2370/2370 [==============================] - 2s 1ms/step - loss: 0.0041 - val_loss: 0.0183\n",
      "Epoch 71/100\n",
      "2370/2370 [==============================] - 2s 908us/step - loss: 0.0042 - val_loss: 0.0172\n",
      "Epoch 72/100\n",
      "2370/2370 [==============================] - 2s 1ms/step - loss: 0.0041 - val_loss: 0.0171\n",
      "Epoch 73/100\n",
      "2370/2370 [==============================] - 2s 974us/step - loss: 0.0041 - val_loss: 0.0185\n",
      "Epoch 74/100\n",
      "2370/2370 [==============================] - 3s 1ms/step - loss: 0.0042 - val_loss: 0.0177\n",
      "Epoch 75/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2370/2370 [==============================] - 2s 971us/step - loss: 0.0040 - val_loss: 0.0189\n",
      "Epoch 76/100\n",
      "2370/2370 [==============================] - 2s 920us/step - loss: 0.0040 - val_loss: 0.0185\n",
      "Epoch 77/100\n",
      "2370/2370 [==============================] - 2s 841us/step - loss: 0.0041 - val_loss: 0.0180\n",
      "Epoch 78/100\n",
      "2370/2370 [==============================] - 2s 877us/step - loss: 0.0042 - val_loss: 0.0179\n",
      "Epoch 79/100\n",
      "2370/2370 [==============================] - 2s 925us/step - loss: 0.0041 - val_loss: 0.0192\n",
      "Epoch 80/100\n",
      "2370/2370 [==============================] - 2s 913us/step - loss: 0.0041 - val_loss: 0.0182\n",
      "Epoch 81/100\n",
      "2370/2370 [==============================] - 2s 782us/step - loss: 0.0042 - val_loss: 0.0216\n",
      "Epoch 82/100\n",
      "2370/2370 [==============================] - 3s 1ms/step - loss: 0.0041 - val_loss: 0.0196\n",
      "Epoch 83/100\n",
      "2370/2370 [==============================] - 2s 893us/step - loss: 0.0040 - val_loss: 0.0301\n",
      "Epoch 84/100\n",
      "2370/2370 [==============================] - 3s 1ms/step - loss: 0.0041 - val_loss: 0.0204\n",
      "Epoch 85/100\n",
      "2370/2370 [==============================] - 2s 996us/step - loss: 0.0040 - val_loss: 0.0184\n",
      "Epoch 86/100\n",
      "2370/2370 [==============================] - 3s 1ms/step - loss: 0.0040 - val_loss: 0.0184\n",
      "Epoch 87/100\n",
      "2370/2370 [==============================] - 2s 869us/step - loss: 0.0039 - val_loss: 0.0186\n",
      "Epoch 88/100\n",
      "2370/2370 [==============================] - 2s 693us/step - loss: 0.0039 - val_loss: 0.0261\n",
      "Epoch 89/100\n",
      "2370/2370 [==============================] - 2s 847us/step - loss: 0.0040 - val_loss: 0.0191\n",
      "Epoch 90/100\n",
      "2370/2370 [==============================] - 2s 931us/step - loss: 0.0038 - val_loss: 0.0178\n",
      "Epoch 91/100\n",
      "2370/2370 [==============================] - 2s 722us/step - loss: 0.0041 - val_loss: 0.0186\n",
      "Epoch 92/100\n",
      "2370/2370 [==============================] - 1s 537us/step - loss: 0.0041 - val_loss: 0.0186\n",
      "Epoch 93/100\n",
      "2370/2370 [==============================] - 1s 523us/step - loss: 0.0039 - val_loss: 0.0198\n",
      "Epoch 94/100\n",
      "2370/2370 [==============================] - 1s 488us/step - loss: 0.0040 - val_loss: 0.0223\n",
      "Epoch 95/100\n",
      "2370/2370 [==============================] - 1s 510us/step - loss: 0.0038 - val_loss: 0.0177\n",
      "Epoch 96/100\n",
      "2370/2370 [==============================] - 1s 477us/step - loss: 0.0040 - val_loss: 0.0192\n",
      "Epoch 97/100\n",
      "2370/2370 [==============================] - 1s 536us/step - loss: 0.0040 - val_loss: 0.0194\n",
      "Epoch 98/100\n",
      "2370/2370 [==============================] - 1s 467us/step - loss: 0.0040 - val_loss: 0.0186\n",
      "Epoch 99/100\n",
      "2370/2370 [==============================] - 1s 484us/step - loss: 0.0039 - val_loss: 0.0197\n",
      "Epoch 100/100\n",
      "2370/2370 [==============================] - 1s 531us/step - loss: 0.0038 - val_loss: 0.0192\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "208.05039978027344"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "start_lstm = time.time()\n",
    "lstm_model.reset_states()\n",
    "history=lstm_model.fit(x_train, y_train, epochs=100, batch_size=batch_size, verbose=1, validation_data=(x_test, y_test));\n",
    "end_lstm = time.time()\n",
    "time_lstm = end_lstm - start_lstm\n",
    "time_lstm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2370/2370 [==============================] - 0s 117us/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.0039493569060341"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lstm_model.evaluate(x_train, y_train, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from matplotlib import pyplot\n",
    "plt.plot(history.history['loss'], label='LSTM training Loss')\n",
    "plt.plot(history.history['val_loss'], label='LSTM testing Loss')\n",
    "plt.yscale(\"log\")\n",
    "plt.grid()\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Get predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate predictions\n",
    "raw_lstm_predictions = pd.DataFrame(lstm_model.predict(x_test, batch_size=batch_size),columns = ['lstm_prediction'], index = test_index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Convert data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test = x_test.reshape(x_test.shape[0], x_test.shape[2]) # transform x_test back to 2D array\n",
    "x_test = pd.DataFrame(x_test, columns = x_columns, index = test_index)\n",
    "y_test = pd.DataFrame(y_test, columns = ['google_45d'], index = test_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Invert scaling for prediction data\n",
    "unscaled_lstm_predictions = pd.concat([x_test, raw_lstm_predictions], axis=1)\n",
    "#unscaled_lstm_predictions = pd.concat([unscaled_lstm_predictions, test_data.google_45d], axis=1)\n",
    "unscaled_lstm_predictions = pd.DataFrame(scaler.inverse_transform(unscaled_lstm_predictions), columns=unscaled_lstm_predictions.columns, index=unscaled_lstm_predictions.index)\n",
    "\n",
    "# Invert scaling for actual data\n",
    "test_data_unscaled = pd.concat([x_test, y_test], axis=1)\n",
    "#test_data_unscaled = pd.concat([test_data_unscaled, test_data.google_45d], axis=1)\n",
    "test_data_unscaled = pd.DataFrame(scaler.inverse_transform(test_data_unscaled), columns=test_data_unscaled.columns, index=test_data_unscaled.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "backup['unscaled_lstm_predictions'] = unscaled_lstm_predictions\n",
    "backup['test_data_unscaled'] = test_data_unscaled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of feedforward model is 109.95 \n",
      "Accuracy of LSTM model is        79.0\n"
     ]
    }
   ],
   "source": [
    "lstm_accuracy=np.sqrt(metrics.mean_squared_error(unscaled_data.google_45d, unscaled_lstm_predictions.lstm_prediction))\n",
    "print('Accuracy of feedforward model is {} \\nAccuracy of LSTM model is        {}'.format(round(seq_accuracy,2), round(lstm_accuracy)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/matthias/anaconda3/lib/python3.7/site-packages/pandas/plotting/_converter.py:129: FutureWarning: Using an implicitly registered datetime converter for a matplotlib plotting method. The converter was registered by pandas on import. Future versions of pandas will require you to explicitly register matplotlib converters.\n",
      "\n",
      "To register the converters:\n",
      "\t>>> from pandas.plotting import register_matplotlib_converters\n",
      "\t>>> register_matplotlib_converters()\n",
      "  warnings.warn(msg, FutureWarning)\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot prediction vs actual\n",
    "plt.plot(test_data_unscaled['google_45d'], label='Adjusted Close')\n",
    "plt.plot(unscaled_seq_predictions['seq_prediction'], label='FF Predicted')\n",
    "plt.plot(unscaled_lstm_predictions['lstm_prediction'], label='LSTM Predicted')\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('USD')\n",
    "plt.grid()\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run time for feedforward model is 342.3582148551941 seconds \n",
      "Run time for LSTM model is 208.05039978027344 seconds\n"
     ]
    }
   ],
   "source": [
    "print('Run time for feedforward model is {} seconds \\nRun time for LSTM model is {} seconds'.format(time_seq,time_lstm))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "backup.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
