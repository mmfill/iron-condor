{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Notebook for training for STATIONARY stock price with rolling forecast\n",
    "Forecast for last 10% of data. Data is stationary (differenced), forecast is done day by day.\n",
    "- Sequential model\n",
    "- LSTM model\n",
    "- STATIONARY\n",
    "- rolling forecast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from matplotlib import cm\n",
    "import numpy as np\n",
    "import scipy.stats as si\n",
    "from IPython.display import Image\n",
    "import datetime\n",
    "import time\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dense, Activation, Dropout\n",
    "from keras.layers import LSTM\n",
    "from sklearn import preprocessing, metrics\n",
    "from keras import optimizers\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "backup = pd.HDFStore('backup_v07.h5')\n",
    "#df = backup['df']\n",
    "#df_sta = backup['df_sta']\n",
    "#unscaled_seq_predictions = backup['unscaled_seq_predictions']\n",
    "#unscaled_data = backup['unscaled_data']\n",
    "#unscaled_lstm_predictions = backup['unscaled_lstm_predictions']\n",
    "#test_data_unscaled = backup['test_data_unscaled']\n",
    "#df_predicted = backup['df_predicted']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Iron Condor\n",
    "The iron condor is a investment strategy using four option with the same strike date. More precise, the iron condor uses two vertical spreads, one put spread and a call spread. The put spread consists at of a short put option (P_s) with a strike price below the actual stock price and a long put option (P_l) with a strike price below the short put option P_s. The call spread consists of a short call option (C_s) above the actual stock price and a long call option (C_l) above the short call option C_s. All options have the same strike date and the spread on both legs can vary but is chosen mostly the same.\n",
    "![title](Condor_strategy.png)\n",
    "\"A trader who buys an iron condor speculates that the spot price of the underlying instrument will be between the short strikes when the options expire where the position is the most profitable. Thus, the iron condor is an options strategy considered when the trader has a neutral outlook for the market. Buying iron condors are popular with traders who seek regular income from their trading capital. An iron condor buyer will attempt to construct the trade so that the short strikes are close enough that the position will earn a desirable net credit, but wide enough apart so that it is likely that the spot price of the underlying will remain between the short strikes for the duration of the options contract. The trader would typically play iron condors every month (if possible) thus generating monthly income with the strategy.\" [https://en.wikipedia.org/wiki/Iron_condor]\n",
    "\n",
    "### Goal of the project\n",
    "The goal of the project is to find a strategy to maximise the profit with an iron condor. The ideal short and long spreads should be found at any time based on data of the underlying stock and of the indices S&P500 and Nasdaq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data(var_name,name):\n",
    "    # reads in csv into DataFrame, keeps Adj Close and Volume and calculates rolling averages and \n",
    "    # rolling standard deviation of Adj Close for 4, 9 and 18 days\n",
    "    # var_name: path to csv file\n",
    "    # name: name of the column df\n",
    "    df = pd.read_csv('data/'+var_name+'.csv',index_col='Date', parse_dates=True)\n",
    "    df.rename(columns={\"Adj Close\": name, \"Volume\": name+'_volume', \"Open\": name+'_open'}, inplace = True)\n",
    "    df.rename(columns={\"High\": name+'_high', \"Low\": name+'_low', \"Close\": name+'_close'}, inplace = True)\n",
    "    #df.drop(['Open','High','Low','Close'],axis=1, inplace = True)\n",
    "    #if name == 'google':\n",
    "        #df['google_45d'] = df['google'].shift(-32) # stock price in 45 days (approx. 32 trading days)\n",
    "    df[name+'_ra_04'] = df[name].rolling('4d').mean()\n",
    "    df[name+'_std_04'] = df[name].rolling('4d').std()\n",
    "    df[name+'_ra_09'] = df[name].rolling('9d').mean()\n",
    "    df[name+'_std_09'] = df[name].rolling('9d').std()\n",
    "    df[name+'_ra_18'] = df[name].rolling('18d').mean()\n",
    "    df[name+'_std_18'] = df[name].rolling('18d').std()\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigma_fct(df, name, days = 10):\n",
    "    # For Black-Scholes I need the volatility of return of the underlying assets in the last days\n",
    "    # based on https://www.wikihow.com/Calculate-Historical-Stock-Volatility\n",
    "    # First the ln of daily returns is calculated for each element of names\n",
    "    for name in names:\n",
    "        a=pd.DataFrame()\n",
    "        a['ln_daily_return'] = np.log(df[name] / df[name].shift(1))\n",
    "        # next the rolling average (mean) over certain days is calculated\n",
    "        a['rol_avg'] =  a['ln_daily_return'].rolling(str(days)+'d').mean()\n",
    "        # next deviation from the mean is calculated\n",
    "        a['dev_mean'] = a['ln_daily_return'] - a['rol_avg']\n",
    "        # next variance is calculated for certain days\n",
    "        a['dev_mean_sq'] = a['dev_mean']**2\n",
    "        a['variance'] = a['dev_mean_sq'].rolling(str(days)+'d').sum()/(a['dev_mean_sq'].rolling(str(days)+'d').count()-1)\n",
    "        # volatility is square root of variance\n",
    "        # do you need annual volatility?\n",
    "        df[name + '_daily_vol'] = np.sqrt(a['variance'])*np.sqrt(252) # 252 trading days in a year\n",
    "        #df.drop(['ln_daily_return'])#,'rol_avg','dev_mean','dev_mean_sq','variance'])\n",
    "        #return df[name + '_daily_vol']\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stationary_timeseries(df, damned_list = ['volume','std','short','daily']):\n",
    "    # makes columns in df that are not in damned list stationary. Means it calculates the \n",
    "    # difference row by row\n",
    "    columns = df.columns\n",
    "    df_sta = pd.DataFrame()\n",
    "    for column in columns:\n",
    "        if any(elem in column for elem in damned_list):\n",
    "            df_sta[column] = df[column]\n",
    "        else:\n",
    "            df_sta[column+'_sta'] = df[column].shift(1) - df[column]\n",
    "    df_sta.dropna(inplace = True)  # to make sure we have all the data on all the days\n",
    "    return df_sta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_df(var_name_list,name_list):\n",
    "    # takes list paths of csv files and names and passes it on to get_data.\n",
    "    # Gets df from get_data and merges them on 'Date'\n",
    "    # Calls sigma_fct to get sigma of daily returns \n",
    "    # var_name_list: list of paths to csv files\n",
    "    # name: list of names the columns of df\n",
    "    i=0\n",
    "    for var_name in var_name_list:\n",
    "        if i == 0:\n",
    "            df = get_data(var_name, name_list[i])\n",
    "        else:\n",
    "            df = pd.merge(df,get_data(var_name, name_list[i]),on='Date')\n",
    "        i += 1\n",
    "    df.dropna(inplace = True)  # to make sure we have all the data on all the days\n",
    "    \n",
    "    df = sigma_fct(df,names)\n",
    "    df=df[23:] # drop first entries to get clean results\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_target_data(df, name):\n",
    "    # function to create the target data\n",
    "    # name: name of the column the target data is created of\n",
    "    df[name+'_45d'] = df[name].shift(-32) # stock price in 45 days (approx. 32 trading days)\n",
    "    df.dropna(inplace = True)  # to make sure we have all the data on all the days\n",
    "    # df['short_call'] shows the perfect short call. If you would have shorted a call option with a strike price \n",
    "    # exactly the same as the stock price 45 days later, you made maximum profit. Therefore this is the target \n",
    "    # value for the short call option. The minimum is 0, so we never short a call option with a strike price\n",
    "    # below the stock price right now. The same for the short put option\n",
    "    #df['short_call'] = [x/y-1 if x/y > 1 else 0 for x,y in df[[name+'_45d', name]].values]\n",
    "    #df['short_put'] = [1-x/y if x/y < 1 else 0 for x,y in df[[name+'_45d', name]].values]\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>google_open</th>\n",
       "      <th>google_high</th>\n",
       "      <th>google_low</th>\n",
       "      <th>google_close</th>\n",
       "      <th>google</th>\n",
       "      <th>google_volume</th>\n",
       "      <th>google_ra_04</th>\n",
       "      <th>google_std_04</th>\n",
       "      <th>google_ra_09</th>\n",
       "      <th>google_std_09</th>\n",
       "      <th>...</th>\n",
       "      <th>nasdaq_ra_04</th>\n",
       "      <th>nasdaq_std_04</th>\n",
       "      <th>nasdaq_ra_09</th>\n",
       "      <th>nasdaq_std_09</th>\n",
       "      <th>nasdaq_ra_18</th>\n",
       "      <th>nasdaq_std_18</th>\n",
       "      <th>google_daily_vol</th>\n",
       "      <th>s&amp;p_daily_vol</th>\n",
       "      <th>nasdaq_daily_vol</th>\n",
       "      <th>google_45d</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Date</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2008-11-21</th>\n",
       "      <td>130.764862</td>\n",
       "      <td>134.182053</td>\n",
       "      <td>123.188263</td>\n",
       "      <td>130.725006</td>\n",
       "      <td>130.725006</td>\n",
       "      <td>20565700</td>\n",
       "      <td>136.935478</td>\n",
       "      <td>8.750923</td>\n",
       "      <td>143.875715</td>\n",
       "      <td>10.798013</td>\n",
       "      <td>...</td>\n",
       "      <td>1392.540009</td>\n",
       "      <td>68.742234</td>\n",
       "      <td>1452.251430</td>\n",
       "      <td>95.188063</td>\n",
       "      <td>1541.462141</td>\n",
       "      <td>127.530682</td>\n",
       "      <td>0.716470</td>\n",
       "      <td>0.802275</td>\n",
       "      <td>0.738519</td>\n",
       "      <td>156.946732</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2008-11-24</th>\n",
       "      <td>134.127258</td>\n",
       "      <td>134.470963</td>\n",
       "      <td>124.040070</td>\n",
       "      <td>128.239334</td>\n",
       "      <td>128.239334</td>\n",
       "      <td>20184700</td>\n",
       "      <td>129.482170</td>\n",
       "      <td>1.757636</td>\n",
       "      <td>137.580147</td>\n",
       "      <td>9.599201</td>\n",
       "      <td>...</td>\n",
       "      <td>1428.184998</td>\n",
       "      <td>61.992083</td>\n",
       "      <td>1420.705017</td>\n",
       "      <td>68.914436</td>\n",
       "      <td>1498.502503</td>\n",
       "      <td>101.345711</td>\n",
       "      <td>0.529842</td>\n",
       "      <td>0.864400</td>\n",
       "      <td>0.777819</td>\n",
       "      <td>155.761169</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2008-11-25</th>\n",
       "      <td>133.838348</td>\n",
       "      <td>142.794769</td>\n",
       "      <td>133.160873</td>\n",
       "      <td>140.498383</td>\n",
       "      <td>140.498383</td>\n",
       "      <td>21623100</td>\n",
       "      <td>134.368858</td>\n",
       "      <td>8.668457</td>\n",
       "      <td>137.997038</td>\n",
       "      <td>8.831976</td>\n",
       "      <td>...</td>\n",
       "      <td>1468.375000</td>\n",
       "      <td>5.154837</td>\n",
       "      <td>1426.994298</td>\n",
       "      <td>65.073432</td>\n",
       "      <td>1483.279999</td>\n",
       "      <td>90.035297</td>\n",
       "      <td>0.833833</td>\n",
       "      <td>0.791313</td>\n",
       "      <td>0.710049</td>\n",
       "      <td>156.573120</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2008-11-26</th>\n",
       "      <td>139.616684</td>\n",
       "      <td>147.178345</td>\n",
       "      <td>137.584305</td>\n",
       "      <td>145.499634</td>\n",
       "      <td>145.499634</td>\n",
       "      <td>12760800</td>\n",
       "      <td>138.079117</td>\n",
       "      <td>8.880829</td>\n",
       "      <td>137.425609</td>\n",
       "      <td>8.059325</td>\n",
       "      <td>...</td>\n",
       "      <td>1489.616659</td>\n",
       "      <td>36.971751</td>\n",
       "      <td>1434.144287</td>\n",
       "      <td>74.235572</td>\n",
       "      <td>1487.035382</td>\n",
       "      <td>87.259170</td>\n",
       "      <td>0.812863</td>\n",
       "      <td>0.758392</td>\n",
       "      <td>0.707779</td>\n",
       "      <td>149.923050</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2008-11-28</th>\n",
       "      <td>144.747452</td>\n",
       "      <td>147.671494</td>\n",
       "      <td>143.601746</td>\n",
       "      <td>145.933014</td>\n",
       "      <td>145.933014</td>\n",
       "      <td>5150200</td>\n",
       "      <td>143.977010</td>\n",
       "      <td>3.020363</td>\n",
       "      <td>136.698456</td>\n",
       "      <td>8.236770</td>\n",
       "      <td>...</td>\n",
       "      <td>1510.799967</td>\n",
       "      <td>39.935485</td>\n",
       "      <td>1450.814982</td>\n",
       "      <td>86.016593</td>\n",
       "      <td>1480.791532</td>\n",
       "      <td>79.789023</td>\n",
       "      <td>0.871477</td>\n",
       "      <td>0.804582</td>\n",
       "      <td>0.756923</td>\n",
       "      <td>148.936752</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 40 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            google_open  google_high  google_low  google_close      google  \\\n",
       "Date                                                                         \n",
       "2008-11-21   130.764862   134.182053  123.188263    130.725006  130.725006   \n",
       "2008-11-24   134.127258   134.470963  124.040070    128.239334  128.239334   \n",
       "2008-11-25   133.838348   142.794769  133.160873    140.498383  140.498383   \n",
       "2008-11-26   139.616684   147.178345  137.584305    145.499634  145.499634   \n",
       "2008-11-28   144.747452   147.671494  143.601746    145.933014  145.933014   \n",
       "\n",
       "            google_volume  google_ra_04  google_std_04  google_ra_09  \\\n",
       "Date                                                                   \n",
       "2008-11-21       20565700    136.935478       8.750923    143.875715   \n",
       "2008-11-24       20184700    129.482170       1.757636    137.580147   \n",
       "2008-11-25       21623100    134.368858       8.668457    137.997038   \n",
       "2008-11-26       12760800    138.079117       8.880829    137.425609   \n",
       "2008-11-28        5150200    143.977010       3.020363    136.698456   \n",
       "\n",
       "            google_std_09  ...  nasdaq_ra_04  nasdaq_std_04  nasdaq_ra_09  \\\n",
       "Date                       ...                                              \n",
       "2008-11-21      10.798013  ...   1392.540009      68.742234   1452.251430   \n",
       "2008-11-24       9.599201  ...   1428.184998      61.992083   1420.705017   \n",
       "2008-11-25       8.831976  ...   1468.375000       5.154837   1426.994298   \n",
       "2008-11-26       8.059325  ...   1489.616659      36.971751   1434.144287   \n",
       "2008-11-28       8.236770  ...   1510.799967      39.935485   1450.814982   \n",
       "\n",
       "            nasdaq_std_09  nasdaq_ra_18  nasdaq_std_18  google_daily_vol  \\\n",
       "Date                                                                       \n",
       "2008-11-21      95.188063   1541.462141     127.530682          0.716470   \n",
       "2008-11-24      68.914436   1498.502503     101.345711          0.529842   \n",
       "2008-11-25      65.073432   1483.279999      90.035297          0.833833   \n",
       "2008-11-26      74.235572   1487.035382      87.259170          0.812863   \n",
       "2008-11-28      86.016593   1480.791532      79.789023          0.871477   \n",
       "\n",
       "            s&p_daily_vol  nasdaq_daily_vol  google_45d  \n",
       "Date                                                     \n",
       "2008-11-21       0.802275          0.738519  156.946732  \n",
       "2008-11-24       0.864400          0.777819  155.761169  \n",
       "2008-11-25       0.791313          0.710049  156.573120  \n",
       "2008-11-26       0.758392          0.707779  149.923050  \n",
       "2008-11-28       0.804582          0.756923  148.936752  \n",
       "\n",
       "[5 rows x 40 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "var_names = ['GOOG','S&P','Nasdaq2']\n",
    "names = ['google','s&p','nasdaq']\n",
    "df = get_df(var_names, names)\n",
    "df = get_target_data(df, 'google')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stationary timeseries\n",
    "It is clearly visible, that the stock price has a upward trend. But in order to make predictions easier we try to make the time series data stationary. A stationary timeseries is a timeseries with constant mean, variance and covariance. Here we only make the mean constant. We do this by a lag=1 differencing, or said in different words\n",
    "- we look at the daily change\n",
    "- we look at the offset of the timeseries\n",
    "\n",
    "But first we have to check which feature (column) shows a trend. I do not use statistical test like the Augmented Dickey Fuller Test or the Kwiatkowski-Phillips-Schmidt-Shin Test, but simply to it by visual inspection.\n",
    "\n",
    "If the stock price shows an upward trend, so does S&P 500 and Nasdaq and all of their averages. Not a trend of the mean show all standard deviations, volumens and daily volatility of annual return."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>google_open_sta</th>\n",
       "      <th>google_high_sta</th>\n",
       "      <th>google_low_sta</th>\n",
       "      <th>google_close_sta</th>\n",
       "      <th>google_sta</th>\n",
       "      <th>google_volume</th>\n",
       "      <th>google_ra_04_sta</th>\n",
       "      <th>google_std_04</th>\n",
       "      <th>google_ra_09_sta</th>\n",
       "      <th>google_std_09</th>\n",
       "      <th>...</th>\n",
       "      <th>nasdaq_std_04</th>\n",
       "      <th>nasdaq_ra_09_sta</th>\n",
       "      <th>nasdaq_std_09</th>\n",
       "      <th>nasdaq_ra_18_sta</th>\n",
       "      <th>nasdaq_std_18</th>\n",
       "      <th>google_daily_vol</th>\n",
       "      <th>s&amp;p_daily_vol</th>\n",
       "      <th>nasdaq_daily_vol</th>\n",
       "      <th>google_45d_sta</th>\n",
       "      <th>google_45d</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Date</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2008-11-24</th>\n",
       "      <td>-3.362396</td>\n",
       "      <td>-0.288910</td>\n",
       "      <td>-0.851807</td>\n",
       "      <td>2.485672</td>\n",
       "      <td>2.485672</td>\n",
       "      <td>20184700</td>\n",
       "      <td>7.453308</td>\n",
       "      <td>1.757636</td>\n",
       "      <td>6.295568</td>\n",
       "      <td>9.599201</td>\n",
       "      <td>...</td>\n",
       "      <td>61.992083</td>\n",
       "      <td>31.546413</td>\n",
       "      <td>68.914436</td>\n",
       "      <td>42.959638</td>\n",
       "      <td>101.345711</td>\n",
       "      <td>0.529842</td>\n",
       "      <td>0.864400</td>\n",
       "      <td>0.777819</td>\n",
       "      <td>1.185563</td>\n",
       "      <td>155.761169</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2008-11-25</th>\n",
       "      <td>0.288910</td>\n",
       "      <td>-8.323806</td>\n",
       "      <td>-9.120803</td>\n",
       "      <td>-12.259049</td>\n",
       "      <td>-12.259049</td>\n",
       "      <td>21623100</td>\n",
       "      <td>-4.886688</td>\n",
       "      <td>8.668457</td>\n",
       "      <td>-0.416891</td>\n",
       "      <td>8.831976</td>\n",
       "      <td>...</td>\n",
       "      <td>5.154837</td>\n",
       "      <td>-6.289280</td>\n",
       "      <td>65.073432</td>\n",
       "      <td>15.222504</td>\n",
       "      <td>90.035297</td>\n",
       "      <td>0.833833</td>\n",
       "      <td>0.791313</td>\n",
       "      <td>0.710049</td>\n",
       "      <td>-0.811951</td>\n",
       "      <td>156.573120</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2008-11-26</th>\n",
       "      <td>-5.778336</td>\n",
       "      <td>-4.383576</td>\n",
       "      <td>-4.423432</td>\n",
       "      <td>-5.001251</td>\n",
       "      <td>-5.001251</td>\n",
       "      <td>12760800</td>\n",
       "      <td>-3.710259</td>\n",
       "      <td>8.880829</td>\n",
       "      <td>0.571429</td>\n",
       "      <td>8.059325</td>\n",
       "      <td>...</td>\n",
       "      <td>36.971751</td>\n",
       "      <td>-7.149990</td>\n",
       "      <td>74.235572</td>\n",
       "      <td>-3.755383</td>\n",
       "      <td>87.259170</td>\n",
       "      <td>0.812863</td>\n",
       "      <td>0.758392</td>\n",
       "      <td>0.707779</td>\n",
       "      <td>6.650070</td>\n",
       "      <td>149.923050</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2008-11-28</th>\n",
       "      <td>-5.130768</td>\n",
       "      <td>-0.493149</td>\n",
       "      <td>-6.017441</td>\n",
       "      <td>-0.433380</td>\n",
       "      <td>-0.433380</td>\n",
       "      <td>5150200</td>\n",
       "      <td>-5.897893</td>\n",
       "      <td>3.020363</td>\n",
       "      <td>0.727153</td>\n",
       "      <td>8.236770</td>\n",
       "      <td>...</td>\n",
       "      <td>39.935485</td>\n",
       "      <td>-16.670695</td>\n",
       "      <td>86.016593</td>\n",
       "      <td>6.243850</td>\n",
       "      <td>79.789023</td>\n",
       "      <td>0.871477</td>\n",
       "      <td>0.804582</td>\n",
       "      <td>0.756923</td>\n",
       "      <td>0.986298</td>\n",
       "      <td>148.936752</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2008-12-01</th>\n",
       "      <td>1.942719</td>\n",
       "      <td>4.518067</td>\n",
       "      <td>11.108368</td>\n",
       "      <td>13.434647</td>\n",
       "      <td>13.434647</td>\n",
       "      <td>11465100</td>\n",
       "      <td>4.761320</td>\n",
       "      <td>9.499730</td>\n",
       "      <td>-1.835291</td>\n",
       "      <td>7.898311</td>\n",
       "      <td>...</td>\n",
       "      <td>97.227182</td>\n",
       "      <td>-29.682991</td>\n",
       "      <td>56.591903</td>\n",
       "      <td>28.832448</td>\n",
       "      <td>70.921223</td>\n",
       "      <td>1.198604</td>\n",
       "      <td>0.976937</td>\n",
       "      <td>0.988249</td>\n",
       "      <td>-0.338730</td>\n",
       "      <td>149.275482</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 41 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            google_open_sta  google_high_sta  google_low_sta  \\\n",
       "Date                                                           \n",
       "2008-11-24        -3.362396        -0.288910       -0.851807   \n",
       "2008-11-25         0.288910        -8.323806       -9.120803   \n",
       "2008-11-26        -5.778336        -4.383576       -4.423432   \n",
       "2008-11-28        -5.130768        -0.493149       -6.017441   \n",
       "2008-12-01         1.942719         4.518067       11.108368   \n",
       "\n",
       "            google_close_sta  google_sta  google_volume  google_ra_04_sta  \\\n",
       "Date                                                                        \n",
       "2008-11-24          2.485672    2.485672       20184700          7.453308   \n",
       "2008-11-25        -12.259049  -12.259049       21623100         -4.886688   \n",
       "2008-11-26         -5.001251   -5.001251       12760800         -3.710259   \n",
       "2008-11-28         -0.433380   -0.433380        5150200         -5.897893   \n",
       "2008-12-01         13.434647   13.434647       11465100          4.761320   \n",
       "\n",
       "            google_std_04  google_ra_09_sta  google_std_09  ...  \\\n",
       "Date                                                        ...   \n",
       "2008-11-24       1.757636          6.295568       9.599201  ...   \n",
       "2008-11-25       8.668457         -0.416891       8.831976  ...   \n",
       "2008-11-26       8.880829          0.571429       8.059325  ...   \n",
       "2008-11-28       3.020363          0.727153       8.236770  ...   \n",
       "2008-12-01       9.499730         -1.835291       7.898311  ...   \n",
       "\n",
       "            nasdaq_std_04  nasdaq_ra_09_sta  nasdaq_std_09  nasdaq_ra_18_sta  \\\n",
       "Date                                                                           \n",
       "2008-11-24      61.992083         31.546413      68.914436         42.959638   \n",
       "2008-11-25       5.154837         -6.289280      65.073432         15.222504   \n",
       "2008-11-26      36.971751         -7.149990      74.235572         -3.755383   \n",
       "2008-11-28      39.935485        -16.670695      86.016593          6.243850   \n",
       "2008-12-01      97.227182        -29.682991      56.591903         28.832448   \n",
       "\n",
       "            nasdaq_std_18  google_daily_vol  s&p_daily_vol  nasdaq_daily_vol  \\\n",
       "Date                                                                           \n",
       "2008-11-24     101.345711          0.529842       0.864400          0.777819   \n",
       "2008-11-25      90.035297          0.833833       0.791313          0.710049   \n",
       "2008-11-26      87.259170          0.812863       0.758392          0.707779   \n",
       "2008-11-28      79.789023          0.871477       0.804582          0.756923   \n",
       "2008-12-01      70.921223          1.198604       0.976937          0.988249   \n",
       "\n",
       "            google_45d_sta  google_45d  \n",
       "Date                                    \n",
       "2008-11-24        1.185563  155.761169  \n",
       "2008-11-25       -0.811951  156.573120  \n",
       "2008-11-26        6.650070  149.923050  \n",
       "2008-11-28        0.986298  148.936752  \n",
       "2008-12-01       -0.338730  149.275482  \n",
       "\n",
       "[5 rows x 41 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_sta = stationary_timeseries(df)\n",
    "df_sta['google_45d'] = df.google_45d # add it to be able to reverse differencing\n",
    "df_sta.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x7fea8d4dff98>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "df_sta.reset_index().plot( x='Date', y=['google_sta'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "backup['df'] = df\n",
    "backup['df_sta'] = df_sta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Price of options\n",
    "It is really hard to get data about historic stock options. Be it as api channel or csv download. I managed only to find sources to pay: 500 USD for every year. Because that is too expensive for this project I decided to calculate the option price with the **Black–Scholes formula**, which gives a theoretical estimate of the price of European-style options.\n",
    "\n",
    "Based on the formula for non-divident paying options I calculate the option prices and add a small random term to create a bit of noise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def black_scholes(S, K, sigma, r=0.03, T=45/365, option = 'call'):\n",
    "    \n",
    "    #S: spot price\n",
    "    #K: strike price\n",
    "    #T: time to maturity in years, 45 days as default\n",
    "    #r: risk-free interest rate, assumed to be constant between t and T\n",
    "    #sigma: standard deviation (volatility) of RETURNS of underlying asset\n",
    "    \n",
    "    d1 = (np.log(S / K) + (r + 0.5 * sigma ** 2) * T) / (sigma * np.sqrt(T))\n",
    "    #d2 = (np.log(S / K) + (r - 0.5 * sigma ** 2) * T) / (sigma * np.sqrt(T))\n",
    "    d2 = d1 - sigma * np.sqrt(T)\n",
    "    \n",
    "    if option == 'call':\n",
    "        result = (S * si.norm.cdf(d1, 0.0, 1.0) - K * np.exp(-r * T) * si.norm.cdf(d2, 0.0, 1.0))\n",
    "    if option == 'put':\n",
    "        result = (K * np.exp(-r * T) * si.norm.cdf(-d2, 0.0, 1.0) - S * si.norm.cdf(-d1, 0.0, 1.0))\n",
    "        \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def option_prices(date, short_spread, long_spread, strike_days = 45):\n",
    "    # calculates prices of options and calculates initial return (net premium)\n",
    "    date = date_fct(date)\n",
    "    P_s_strike_price, P_l_strike_price, C_s_strike_price, C_l_strike_price = strike_prices(date, short_spread, long_spread)\n",
    "    sigma = get_sigma(date)\n",
    "    stock_price = stock_price_fct(date)\n",
    "    P_s_price = black_scholes(stock_price, P_s_strike_price, sigma, T=strike_days/365, option = 'put')\n",
    "    P_l_price = black_scholes(stock_price, P_l_strike_price, sigma, T=strike_days/365, option = 'put')\n",
    "    C_s_price = black_scholes(stock_price, C_s_strike_price, sigma, T=strike_days/365, option = 'call')\n",
    "    C_l_price = black_scholes(stock_price, C_l_strike_price, sigma, T=strike_days/365, option = 'call')\n",
    "    initial_return = P_s_price + C_s_price - P_l_price - C_l_price\n",
    "    \n",
    "    return P_s_price, P_l_price, C_s_price, C_l_price, initial_return"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now it is time to calculate the strike prices and option prices of all 4 options and the initial return by setting up the iron condor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate return of options\n",
    "There are five different regimes for the return. R: return, P(C) put(call) option, s(l): short(long), ir: initial return, stock_price: stock price\n",
    "- stock price is below long put option: R = ir + P_l_strike_price - P_s_strike_price\n",
    "- stock price is between long and short put option: R = ir + stock_price - P_s_strike_price\n",
    "- stock price is between short call and short put option: R = ir\n",
    "- stock price is between short and long call option: R = ir - stock_price + C_s_strike price\n",
    "- stock price is above long call option: R = ir - C_l_strike_price + C_s_strike_price"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define return function of iron condor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ic_return(trading_date, short_spread, long_spread, strike_days = 45):\n",
    "    P_s_strike_price, P_l_strike_price, C_s_strike_price, C_l_strike_price = strike_prices(trading_date, short_spread, long_spread)\n",
    "    P_s_price, P_l_price, C_s_price, C_l_price, initial_return = option_prices(trading_date,short_spread, long_spread, strike_days)\n",
    "    strike_date = date_fct(trading_date, 45)\n",
    "    final_stock_price = stock_price_fct(strike_date)\n",
    "    if final_stock_price < P_l_strike_price:\n",
    "        final_result = initial_return + P_l_strike_price - P_s_strike_price\n",
    "    elif final_stock_price >= P_l_strike_price and final_stock_price < P_s_strike_price:\n",
    "        final_result = initial_return + stock_price - P_s_strike_price\n",
    "    elif final_stock_price >= P_s_strike_price and final_stock_price < C_s_strike_price:\n",
    "        final_result = initial_return\n",
    "    elif final_stock_price >= C_s_strike_price and final_stock_price < C_l_strike_price:\n",
    "        final_result = initial_return - final_stock_price + C_s_strike_price\n",
    "    elif final_stock_price >= C_l_strike_price:\n",
    "        final_result = initial_return - C_l_strike_price + C_s_strike_price\n",
    "    maximum_result = initial_return\n",
    "    return final_result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Scale data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/matthias/anaconda3/lib/python3.7/site-packages/sklearn/preprocessing/data.py:334: DataConversionWarning: Data with input dtype int64, float64 were all converted to float64 by MinMaxScaler.\n",
      "  return self.partial_fit(X, y)\n"
     ]
    }
   ],
   "source": [
    "scaler = preprocessing.MinMaxScaler(feature_range=(-1, 1))\n",
    "df_sta_scaled = pd.DataFrame(scaler.fit_transform(df_sta), columns=df_sta.columns, index = df_sta.index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Split into train and test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_train_test_data(data, train_test_split, batch_size):\n",
    "    # data is dataframe to get train test data out of\n",
    "    train_size_raw = data.shape[0]*train_test_split\n",
    "    train_size = int(train_size_raw - train_size_raw % batch_size) # make train_size multiple of batch_size for \"statefull = True\" in LSTM\n",
    "    train_data = data.iloc[0:train_size]\n",
    "    test_size_raw = data.shape[0] - train_size\n",
    "    test_size = int(test_size_raw - test_size_raw%batch_size) # multiple of batch_size\n",
    "    test_data = data.iloc[train_size : train_size + test_size]\n",
    "    x_train = train_data.drop(columns=['google_45d','google_45d_sta']) # google_45d is not for training, just for reversing stationarity\n",
    "    y_train = train_data['google_45d_sta']\n",
    "    x_test = test_data.drop(columns=['google_45d','google_45d_sta'])\n",
    "    y_test = test_data['google_45d_sta']\n",
    "    train_index=x_train.index\n",
    "    x_columns = x_train.columns\n",
    "    test_index = x_test.index\n",
    "    return x_train, x_test, y_train, y_test, test_data, test_index, x_columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 10\n",
    "train_test_split = 0.9\n",
    "x_train, x_test, y_train, y_test, test_data, test_index, x_columns = get_train_test_data(df_sta_scaled, train_test_split, batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sequential model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/matthias/anaconda3/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:74: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/matthias/anaconda3/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:517: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/matthias/anaconda3/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:4138: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/matthias/anaconda3/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:133: The name tf.placeholder_with_default is deprecated. Please use tf.compat.v1.placeholder_with_default instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/matthias/anaconda3/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "WARNING:tensorflow:From /home/matthias/anaconda3/lib/python3.7/site-packages/keras/optimizers.py:790: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_1 (Dense)              (None, 128)               5120      \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 16)                2064      \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 16)                0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 4)                 68        \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 4)                 0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 1)                 5         \n",
      "=================================================================\n",
      "Total params: 7,257\n",
      "Trainable params: 7,257\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Build the model architecture\n",
    "seq_model = Sequential()\n",
    "seq_model.add(Dense(128, activation='relu', input_shape=(x_train.shape[1],)))\n",
    "seq_model.add(Dropout(0.2))\n",
    "seq_model.add(Dense(16, activation='relu'))\n",
    "seq_model.add(Dropout(0.2))\n",
    "seq_model.add(Dense(4, activation='relu'))\n",
    "seq_model.add(Dropout(0.2))\n",
    "seq_model.add(Dense(1))\n",
    "\n",
    "# Compile the model using a loss function and an optimizer.\n",
    "seq_model.compile(optimizer='adam',loss='mean_squared_error')\n",
    "seq_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/matthias/anaconda3/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:986: The name tf.assign_add is deprecated. Please use tf.compat.v1.assign_add instead.\n",
      "\n",
      "Train on 2370 samples, validate on 260 samples\n",
      "Epoch 1/3000\n",
      "2370/2370 [==============================] - 1s 486us/step - loss: 0.0108 - val_loss: 0.0375\n",
      "Epoch 2/3000\n",
      "2370/2370 [==============================] - 0s 133us/step - loss: 0.0070 - val_loss: 0.0359\n",
      "Epoch 3/3000\n",
      "2370/2370 [==============================] - 0s 137us/step - loss: 0.0063 - val_loss: 0.0356\n",
      "Epoch 4/3000\n",
      "2370/2370 [==============================] - 0s 131us/step - loss: 0.0062 - val_loss: 0.0357\n",
      "Epoch 5/3000\n",
      "2370/2370 [==============================] - 0s 135us/step - loss: 0.0062 - val_loss: 0.0356\n",
      "Epoch 6/3000\n",
      "2370/2370 [==============================] - 0s 128us/step - loss: 0.0062 - val_loss: 0.0357\n",
      "Epoch 7/3000\n",
      "2370/2370 [==============================] - 0s 114us/step - loss: 0.0061 - val_loss: 0.0357\n",
      "Epoch 8/3000\n",
      "2370/2370 [==============================] - 0s 112us/step - loss: 0.0061 - val_loss: 0.0358\n",
      "Epoch 9/3000\n",
      "2370/2370 [==============================] - 0s 100us/step - loss: 0.0061 - val_loss: 0.0359\n",
      "Epoch 10/3000\n",
      "2370/2370 [==============================] - 0s 109us/step - loss: 0.0061 - val_loss: 0.0357\n",
      "Epoch 11/3000\n",
      "2370/2370 [==============================] - 0s 111us/step - loss: 0.0061 - val_loss: 0.0367\n",
      "Epoch 12/3000\n",
      "2370/2370 [==============================] - 0s 114us/step - loss: 0.0061 - val_loss: 0.0357\n",
      "Epoch 13/3000\n",
      "2370/2370 [==============================] - 0s 135us/step - loss: 0.0060 - val_loss: 0.0357\n",
      "Epoch 14/3000\n",
      "2370/2370 [==============================] - 0s 87us/step - loss: 0.0060 - val_loss: 0.0355\n",
      "Epoch 15/3000\n",
      "2370/2370 [==============================] - 0s 116us/step - loss: 0.0061 - val_loss: 0.0357\n",
      "Epoch 16/3000\n",
      "2370/2370 [==============================] - 0s 107us/step - loss: 0.0061 - val_loss: 0.0355\n",
      "Epoch 17/3000\n",
      "2370/2370 [==============================] - 0s 108us/step - loss: 0.0060 - val_loss: 0.0356\n",
      "Epoch 18/3000\n",
      "2370/2370 [==============================] - 0s 116us/step - loss: 0.0060 - val_loss: 0.0358\n",
      "Epoch 19/3000\n",
      "2370/2370 [==============================] - 0s 124us/step - loss: 0.0060 - val_loss: 0.0360\n",
      "Epoch 20/3000\n",
      "2370/2370 [==============================] - 0s 120us/step - loss: 0.0061 - val_loss: 0.0359\n",
      "Epoch 21/3000\n",
      "2370/2370 [==============================] - 0s 113us/step - loss: 0.0061 - val_loss: 0.0357\n",
      "Epoch 22/3000\n",
      "2370/2370 [==============================] - 0s 121us/step - loss: 0.0060 - val_loss: 0.0356\n",
      "Epoch 23/3000\n",
      "2370/2370 [==============================] - 0s 149us/step - loss: 0.0061 - val_loss: 0.0355\n",
      "Epoch 24/3000\n",
      "2370/2370 [==============================] - 0s 112us/step - loss: 0.0060 - val_loss: 0.0356\n",
      "Epoch 25/3000\n",
      "2370/2370 [==============================] - 0s 118us/step - loss: 0.0060 - val_loss: 0.0357\n",
      "Epoch 26/3000\n",
      "2370/2370 [==============================] - 0s 85us/step - loss: 0.0060 - val_loss: 0.0356\n",
      "Epoch 27/3000\n",
      "2370/2370 [==============================] - 0s 99us/step - loss: 0.0060 - val_loss: 0.0358\n",
      "Epoch 28/3000\n",
      "2370/2370 [==============================] - 0s 108us/step - loss: 0.0061 - val_loss: 0.0358\n",
      "Epoch 29/3000\n",
      "2370/2370 [==============================] - 0s 124us/step - loss: 0.0060 - val_loss: 0.0359\n",
      "Epoch 30/3000\n",
      "2370/2370 [==============================] - 0s 114us/step - loss: 0.0061 - val_loss: 0.0362\n",
      "Epoch 31/3000\n",
      "2370/2370 [==============================] - 0s 120us/step - loss: 0.0061 - val_loss: 0.0359\n",
      "Epoch 32/3000\n",
      "2370/2370 [==============================] - 0s 105us/step - loss: 0.0060 - val_loss: 0.0359\n",
      "Epoch 33/3000\n",
      "2370/2370 [==============================] - 0s 127us/step - loss: 0.0060 - val_loss: 0.0359\n",
      "Epoch 34/3000\n",
      "2370/2370 [==============================] - 0s 138us/step - loss: 0.0060 - val_loss: 0.0360\n",
      "Epoch 35/3000\n",
      "2370/2370 [==============================] - 0s 129us/step - loss: 0.0060 - val_loss: 0.0360\n",
      "Epoch 36/3000\n",
      "2370/2370 [==============================] - 0s 104us/step - loss: 0.0060 - val_loss: 0.0359\n",
      "Epoch 37/3000\n",
      "2370/2370 [==============================] - 0s 108us/step - loss: 0.0061 - val_loss: 0.0358\n",
      "Epoch 38/3000\n",
      "2370/2370 [==============================] - 0s 128us/step - loss: 0.0060 - val_loss: 0.0358\n",
      "Epoch 39/3000\n",
      "2370/2370 [==============================] - 0s 115us/step - loss: 0.0061 - val_loss: 0.0356\n",
      "Epoch 40/3000\n",
      "2370/2370 [==============================] - 0s 100us/step - loss: 0.0060 - val_loss: 0.0356\n",
      "Epoch 41/3000\n",
      "2370/2370 [==============================] - 0s 119us/step - loss: 0.0060 - val_loss: 0.0356\n",
      "Epoch 42/3000\n",
      "2370/2370 [==============================] - 0s 107us/step - loss: 0.0060 - val_loss: 0.0356\n",
      "Epoch 43/3000\n",
      "2370/2370 [==============================] - 0s 125us/step - loss: 0.0060 - val_loss: 0.0357\n",
      "Epoch 44/3000\n",
      "2370/2370 [==============================] - 0s 106us/step - loss: 0.0060 - val_loss: 0.0357\n",
      "Epoch 45/3000\n",
      "2370/2370 [==============================] - 0s 153us/step - loss: 0.0060 - val_loss: 0.0356\n",
      "Epoch 46/3000\n",
      "2370/2370 [==============================] - 0s 114us/step - loss: 0.0061 - val_loss: 0.0357\n",
      "Epoch 47/3000\n",
      "2370/2370 [==============================] - 0s 146us/step - loss: 0.0060 - val_loss: 0.0357\n",
      "Epoch 48/3000\n",
      "2370/2370 [==============================] - 0s 87us/step - loss: 0.0060 - val_loss: 0.0358\n",
      "Epoch 49/3000\n",
      "2370/2370 [==============================] - 0s 111us/step - loss: 0.0060 - val_loss: 0.0358\n",
      "Epoch 50/3000\n",
      "2370/2370 [==============================] - 0s 104us/step - loss: 0.0060 - val_loss: 0.0357\n",
      "Epoch 51/3000\n",
      "2370/2370 [==============================] - 0s 100us/step - loss: 0.0060 - val_loss: 0.0358\n",
      "Epoch 52/3000\n",
      "2370/2370 [==============================] - 0s 117us/step - loss: 0.0060 - val_loss: 0.0357\n",
      "Epoch 53/3000\n",
      "2370/2370 [==============================] - 0s 140us/step - loss: 0.0060 - val_loss: 0.0360\n",
      "Epoch 54/3000\n",
      "2370/2370 [==============================] - 0s 126us/step - loss: 0.0060 - val_loss: 0.0360\n",
      "Epoch 55/3000\n",
      "2370/2370 [==============================] - 0s 123us/step - loss: 0.0061 - val_loss: 0.0358\n",
      "Epoch 56/3000\n",
      "2370/2370 [==============================] - 0s 141us/step - loss: 0.0061 - val_loss: 0.0359\n",
      "Epoch 57/3000\n",
      "2370/2370 [==============================] - 0s 131us/step - loss: 0.0060 - val_loss: 0.0365\n",
      "Epoch 58/3000\n",
      "2370/2370 [==============================] - 0s 131us/step - loss: 0.0060 - val_loss: 0.0357\n",
      "Epoch 59/3000\n",
      "2370/2370 [==============================] - 0s 116us/step - loss: 0.0060 - val_loss: 0.0358\n",
      "Epoch 60/3000\n",
      "2370/2370 [==============================] - 0s 133us/step - loss: 0.0060 - val_loss: 0.0358\n",
      "Epoch 61/3000\n",
      "2370/2370 [==============================] - 0s 132us/step - loss: 0.0060 - val_loss: 0.0360\n",
      "Epoch 62/3000\n",
      "2370/2370 [==============================] - 0s 136us/step - loss: 0.0060 - val_loss: 0.0362\n",
      "Epoch 63/3000\n",
      "2370/2370 [==============================] - 0s 116us/step - loss: 0.0060 - val_loss: 0.0358\n",
      "Epoch 64/3000\n",
      "2370/2370 [==============================] - 0s 116us/step - loss: 0.0060 - val_loss: 0.0360\n",
      "Epoch 65/3000\n",
      "2370/2370 [==============================] - 0s 122us/step - loss: 0.0061 - val_loss: 0.0360\n",
      "Epoch 66/3000\n",
      "2370/2370 [==============================] - 0s 130us/step - loss: 0.0060 - val_loss: 0.0360\n",
      "Epoch 67/3000\n",
      "2370/2370 [==============================] - 0s 138us/step - loss: 0.0060 - val_loss: 0.0361\n",
      "Epoch 68/3000\n",
      "2370/2370 [==============================] - 0s 137us/step - loss: 0.0060 - val_loss: 0.0358\n",
      "Epoch 69/3000\n",
      "2370/2370 [==============================] - 0s 134us/step - loss: 0.0060 - val_loss: 0.0357\n",
      "Epoch 70/3000\n",
      "2370/2370 [==============================] - 0s 110us/step - loss: 0.0059 - val_loss: 0.0357\n",
      "Epoch 71/3000\n",
      "2370/2370 [==============================] - 0s 121us/step - loss: 0.0061 - val_loss: 0.0357\n",
      "Epoch 72/3000\n",
      "2370/2370 [==============================] - 0s 117us/step - loss: 0.0060 - val_loss: 0.0359\n",
      "Epoch 73/3000\n",
      "2370/2370 [==============================] - 0s 123us/step - loss: 0.0061 - val_loss: 0.0358\n",
      "Epoch 74/3000\n",
      "2370/2370 [==============================] - 0s 98us/step - loss: 0.0061 - val_loss: 0.0358\n",
      "Epoch 75/3000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2370/2370 [==============================] - 0s 145us/step - loss: 0.0060 - val_loss: 0.0359\n",
      "Epoch 76/3000\n",
      "2370/2370 [==============================] - 0s 148us/step - loss: 0.0060 - val_loss: 0.0357\n",
      "Epoch 77/3000\n",
      "2370/2370 [==============================] - 0s 143us/step - loss: 0.0060 - val_loss: 0.0357\n",
      "Epoch 78/3000\n",
      "2370/2370 [==============================] - 0s 156us/step - loss: 0.0060 - val_loss: 0.0357\n",
      "Epoch 79/3000\n",
      "2370/2370 [==============================] - 0s 135us/step - loss: 0.0060 - val_loss: 0.0357\n",
      "Epoch 80/3000\n",
      "2370/2370 [==============================] - 0s 70us/step - loss: 0.0061 - val_loss: 0.0357\n",
      "Epoch 81/3000\n",
      "2370/2370 [==============================] - 0s 62us/step - loss: 0.0060 - val_loss: 0.0358\n",
      "Epoch 82/3000\n",
      "2370/2370 [==============================] - 0s 66us/step - loss: 0.0061 - val_loss: 0.0360\n",
      "Epoch 83/3000\n",
      "2370/2370 [==============================] - 0s 110us/step - loss: 0.0061 - val_loss: 0.0357\n",
      "Epoch 84/3000\n",
      "2370/2370 [==============================] - 0s 104us/step - loss: 0.0060 - val_loss: 0.0357\n",
      "Epoch 85/3000\n",
      "2370/2370 [==============================] - 0s 60us/step - loss: 0.0061 - val_loss: 0.0359\n",
      "Epoch 86/3000\n",
      "2370/2370 [==============================] - 0s 49us/step - loss: 0.0060 - val_loss: 0.0358\n",
      "Epoch 87/3000\n",
      "2370/2370 [==============================] - 0s 68us/step - loss: 0.0060 - val_loss: 0.0361\n",
      "Epoch 88/3000\n",
      "2370/2370 [==============================] - 0s 77us/step - loss: 0.0061 - val_loss: 0.0359\n",
      "Epoch 89/3000\n",
      "2370/2370 [==============================] - 0s 64us/step - loss: 0.0060 - val_loss: 0.0360\n",
      "Epoch 90/3000\n",
      "2370/2370 [==============================] - 0s 74us/step - loss: 0.0061 - val_loss: 0.0358\n",
      "Epoch 91/3000\n",
      "2370/2370 [==============================] - 0s 104us/step - loss: 0.0060 - val_loss: 0.0358\n",
      "Epoch 92/3000\n",
      "2370/2370 [==============================] - 0s 131us/step - loss: 0.0060 - val_loss: 0.0357\n",
      "Epoch 93/3000\n",
      "2370/2370 [==============================] - 0s 88us/step - loss: 0.0060 - val_loss: 0.0361\n",
      "Epoch 94/3000\n",
      "2370/2370 [==============================] - 0s 136us/step - loss: 0.0060 - val_loss: 0.0358\n",
      "Epoch 95/3000\n",
      "2370/2370 [==============================] - 0s 127us/step - loss: 0.0059 - val_loss: 0.0359\n",
      "Epoch 96/3000\n",
      "2370/2370 [==============================] - 0s 138us/step - loss: 0.0059 - val_loss: 0.0360\n",
      "Epoch 97/3000\n",
      "2370/2370 [==============================] - 0s 134us/step - loss: 0.0059 - val_loss: 0.0359\n",
      "Epoch 98/3000\n",
      "2370/2370 [==============================] - 0s 145us/step - loss: 0.0060 - val_loss: 0.0360\n",
      "Epoch 99/3000\n",
      "2370/2370 [==============================] - 0s 140us/step - loss: 0.0061 - val_loss: 0.0358\n",
      "Epoch 100/3000\n",
      "2370/2370 [==============================] - 0s 153us/step - loss: 0.0060 - val_loss: 0.0359\n",
      "Epoch 101/3000\n",
      "2370/2370 [==============================] - 0s 136us/step - loss: 0.0060 - val_loss: 0.0362\n",
      "Epoch 102/3000\n",
      "2370/2370 [==============================] - 0s 132us/step - loss: 0.0060 - val_loss: 0.0359\n",
      "Epoch 103/3000\n",
      "2370/2370 [==============================] - 0s 160us/step - loss: 0.0059 - val_loss: 0.0361\n",
      "Epoch 104/3000\n",
      "2370/2370 [==============================] - 0s 153us/step - loss: 0.0060 - val_loss: 0.0359\n",
      "Epoch 105/3000\n",
      "2370/2370 [==============================] - 0s 143us/step - loss: 0.0059 - val_loss: 0.0359\n",
      "Epoch 106/3000\n",
      "2370/2370 [==============================] - 0s 109us/step - loss: 0.0060 - val_loss: 0.0357\n",
      "Epoch 107/3000\n",
      "2370/2370 [==============================] - 0s 83us/step - loss: 0.0059 - val_loss: 0.0362\n",
      "Epoch 108/3000\n",
      "2370/2370 [==============================] - 0s 82us/step - loss: 0.0060 - val_loss: 0.0362\n",
      "Epoch 109/3000\n",
      "2370/2370 [==============================] - 0s 135us/step - loss: 0.0059 - val_loss: 0.0368\n",
      "Epoch 110/3000\n",
      "2370/2370 [==============================] - 0s 138us/step - loss: 0.0060 - val_loss: 0.0363\n",
      "Epoch 111/3000\n",
      "2370/2370 [==============================] - 0s 137us/step - loss: 0.0060 - val_loss: 0.0362\n",
      "Epoch 112/3000\n",
      "2370/2370 [==============================] - 0s 161us/step - loss: 0.0060 - val_loss: 0.0363\n",
      "Epoch 113/3000\n",
      "2370/2370 [==============================] - 0s 157us/step - loss: 0.0059 - val_loss: 0.0361\n",
      "Epoch 114/3000\n",
      "2370/2370 [==============================] - 0s 158us/step - loss: 0.0060 - val_loss: 0.0362\n",
      "Epoch 115/3000\n",
      "2370/2370 [==============================] - 0s 138us/step - loss: 0.0060 - val_loss: 0.0360\n",
      "Epoch 116/3000\n",
      "2370/2370 [==============================] - 0s 111us/step - loss: 0.0060 - val_loss: 0.0364\n",
      "Epoch 117/3000\n",
      "2370/2370 [==============================] - 0s 111us/step - loss: 0.0059 - val_loss: 0.0360\n",
      "Epoch 118/3000\n",
      "2370/2370 [==============================] - 0s 124us/step - loss: 0.0059 - val_loss: 0.0367\n",
      "Epoch 119/3000\n",
      "2370/2370 [==============================] - 0s 144us/step - loss: 0.0059 - val_loss: 0.0358\n",
      "Epoch 120/3000\n",
      "2370/2370 [==============================] - 0s 125us/step - loss: 0.0060 - val_loss: 0.0360\n",
      "Epoch 121/3000\n",
      "2370/2370 [==============================] - 0s 145us/step - loss: 0.0059 - val_loss: 0.0357\n",
      "Epoch 122/3000\n",
      "2370/2370 [==============================] - 0s 152us/step - loss: 0.0059 - val_loss: 0.0356\n",
      "Epoch 123/3000\n",
      "2370/2370 [==============================] - 0s 136us/step - loss: 0.0059 - val_loss: 0.0362\n",
      "Epoch 124/3000\n",
      "2370/2370 [==============================] - 0s 140us/step - loss: 0.0060 - val_loss: 0.0361\n",
      "Epoch 125/3000\n",
      "2370/2370 [==============================] - 0s 115us/step - loss: 0.0059 - val_loss: 0.0359\n",
      "Epoch 126/3000\n",
      "2370/2370 [==============================] - 0s 127us/step - loss: 0.0060 - val_loss: 0.0358\n",
      "Epoch 127/3000\n",
      "2370/2370 [==============================] - 0s 155us/step - loss: 0.0060 - val_loss: 0.0361\n",
      "Epoch 128/3000\n",
      "2370/2370 [==============================] - 0s 128us/step - loss: 0.0059 - val_loss: 0.0359\n",
      "Epoch 129/3000\n",
      "2370/2370 [==============================] - 0s 135us/step - loss: 0.0059 - val_loss: 0.0363\n",
      "Epoch 130/3000\n",
      "2370/2370 [==============================] - 0s 148us/step - loss: 0.0058 - val_loss: 0.0363\n",
      "Epoch 131/3000\n",
      "2370/2370 [==============================] - 0s 137us/step - loss: 0.0059 - val_loss: 0.0365\n",
      "Epoch 132/3000\n",
      "2370/2370 [==============================] - 0s 129us/step - loss: 0.0060 - val_loss: 0.0363\n",
      "Epoch 133/3000\n",
      "2370/2370 [==============================] - 0s 128us/step - loss: 0.0060 - val_loss: 0.0363\n",
      "Epoch 134/3000\n",
      "2370/2370 [==============================] - 0s 146us/step - loss: 0.0060 - val_loss: 0.0359\n",
      "Epoch 135/3000\n",
      "2370/2370 [==============================] - 0s 133us/step - loss: 0.0059 - val_loss: 0.0356\n",
      "Epoch 136/3000\n",
      "2370/2370 [==============================] - 0s 130us/step - loss: 0.0059 - val_loss: 0.0364\n",
      "Epoch 137/3000\n",
      "2370/2370 [==============================] - 0s 115us/step - loss: 0.0059 - val_loss: 0.0363\n",
      "Epoch 138/3000\n",
      "2370/2370 [==============================] - 0s 139us/step - loss: 0.0060 - val_loss: 0.0363\n",
      "Epoch 139/3000\n",
      "2370/2370 [==============================] - 0s 129us/step - loss: 0.0059 - val_loss: 0.0367\n",
      "Epoch 140/3000\n",
      "2370/2370 [==============================] - 0s 135us/step - loss: 0.0059 - val_loss: 0.0362\n",
      "Epoch 141/3000\n",
      "2370/2370 [==============================] - 0s 127us/step - loss: 0.0059 - val_loss: 0.0366\n",
      "Epoch 142/3000\n",
      "2370/2370 [==============================] - 0s 76us/step - loss: 0.0059 - val_loss: 0.0364\n",
      "Epoch 143/3000\n",
      "2370/2370 [==============================] - 0s 102us/step - loss: 0.0058 - val_loss: 0.0366\n",
      "Epoch 144/3000\n",
      "2370/2370 [==============================] - 0s 111us/step - loss: 0.0059 - val_loss: 0.0364\n",
      "Epoch 145/3000\n",
      "2370/2370 [==============================] - 0s 128us/step - loss: 0.0061 - val_loss: 0.0362\n",
      "Epoch 146/3000\n",
      "2370/2370 [==============================] - 0s 145us/step - loss: 0.0060 - val_loss: 0.0362\n",
      "Epoch 147/3000\n",
      "2370/2370 [==============================] - 0s 94us/step - loss: 0.0059 - val_loss: 0.0362\n",
      "Epoch 148/3000\n",
      "2370/2370 [==============================] - 0s 94us/step - loss: 0.0059 - val_loss: 0.0363\n",
      "Epoch 149/3000\n",
      "2370/2370 [==============================] - 0s 125us/step - loss: 0.0059 - val_loss: 0.0363\n",
      "Epoch 150/3000\n",
      "2370/2370 [==============================] - 0s 115us/step - loss: 0.0058 - val_loss: 0.0364\n",
      "Epoch 151/3000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2370/2370 [==============================] - 0s 95us/step - loss: 0.0060 - val_loss: 0.0360\n",
      "Epoch 152/3000\n",
      "2370/2370 [==============================] - 0s 127us/step - loss: 0.0059 - val_loss: 0.0363\n",
      "Epoch 153/3000\n",
      "2370/2370 [==============================] - 0s 126us/step - loss: 0.0059 - val_loss: 0.0364\n",
      "Epoch 154/3000\n",
      "2370/2370 [==============================] - 0s 143us/step - loss: 0.0059 - val_loss: 0.0364\n",
      "Epoch 155/3000\n",
      "2370/2370 [==============================] - 0s 82us/step - loss: 0.0059 - val_loss: 0.0365\n",
      "Epoch 156/3000\n",
      "2370/2370 [==============================] - 0s 115us/step - loss: 0.0058 - val_loss: 0.0364\n",
      "Epoch 157/3000\n",
      "2370/2370 [==============================] - 0s 142us/step - loss: 0.0059 - val_loss: 0.0363\n",
      "Epoch 158/3000\n",
      "2370/2370 [==============================] - 0s 143us/step - loss: 0.0058 - val_loss: 0.0361\n",
      "Epoch 159/3000\n",
      "2370/2370 [==============================] - 0s 149us/step - loss: 0.0058 - val_loss: 0.0365\n",
      "Epoch 160/3000\n",
      "2370/2370 [==============================] - 0s 135us/step - loss: 0.0060 - val_loss: 0.0361\n",
      "Epoch 161/3000\n",
      "2370/2370 [==============================] - 0s 138us/step - loss: 0.0059 - val_loss: 0.0361\n",
      "Epoch 162/3000\n",
      "2370/2370 [==============================] - 0s 133us/step - loss: 0.0059 - val_loss: 0.0361\n",
      "Epoch 163/3000\n",
      "2370/2370 [==============================] - 0s 151us/step - loss: 0.0058 - val_loss: 0.0362\n",
      "Epoch 164/3000\n",
      "2370/2370 [==============================] - 0s 141us/step - loss: 0.0059 - val_loss: 0.0359\n",
      "Epoch 165/3000\n",
      "2370/2370 [==============================] - 0s 170us/step - loss: 0.0058 - val_loss: 0.0364\n",
      "Epoch 166/3000\n",
      "2370/2370 [==============================] - 0s 142us/step - loss: 0.0059 - val_loss: 0.0363\n",
      "Epoch 167/3000\n",
      "2370/2370 [==============================] - 0s 152us/step - loss: 0.0059 - val_loss: 0.0361\n",
      "Epoch 168/3000\n",
      "2370/2370 [==============================] - 0s 155us/step - loss: 0.0059 - val_loss: 0.0359\n",
      "Epoch 169/3000\n",
      "2370/2370 [==============================] - 0s 152us/step - loss: 0.0058 - val_loss: 0.0363\n",
      "Epoch 170/3000\n",
      "2370/2370 [==============================] - 0s 145us/step - loss: 0.0059 - val_loss: 0.0360\n",
      "Epoch 171/3000\n",
      "2370/2370 [==============================] - 0s 138us/step - loss: 0.0058 - val_loss: 0.0362\n",
      "Epoch 172/3000\n",
      "2370/2370 [==============================] - 0s 96us/step - loss: 0.0058 - val_loss: 0.0366\n",
      "Epoch 173/3000\n",
      "2370/2370 [==============================] - 0s 94us/step - loss: 0.0058 - val_loss: 0.0367\n",
      "Epoch 174/3000\n",
      "2370/2370 [==============================] - 0s 110us/step - loss: 0.0058 - val_loss: 0.0361\n",
      "Epoch 175/3000\n",
      "2370/2370 [==============================] - 0s 143us/step - loss: 0.0058 - val_loss: 0.0357\n",
      "Epoch 176/3000\n",
      "2370/2370 [==============================] - 0s 157us/step - loss: 0.0058 - val_loss: 0.0361\n",
      "Epoch 177/3000\n",
      "2370/2370 [==============================] - 0s 129us/step - loss: 0.0059 - val_loss: 0.0366\n",
      "Epoch 178/3000\n",
      "2370/2370 [==============================] - 0s 145us/step - loss: 0.0058 - val_loss: 0.0360\n",
      "Epoch 179/3000\n",
      "2370/2370 [==============================] - 0s 135us/step - loss: 0.0058 - val_loss: 0.0371\n",
      "Epoch 180/3000\n",
      "2370/2370 [==============================] - 0s 136us/step - loss: 0.0058 - val_loss: 0.0356\n",
      "Epoch 181/3000\n",
      "2370/2370 [==============================] - 0s 154us/step - loss: 0.0058 - val_loss: 0.0363\n",
      "Epoch 182/3000\n",
      "2370/2370 [==============================] - 0s 110us/step - loss: 0.0059 - val_loss: 0.0362\n",
      "Epoch 183/3000\n",
      "2370/2370 [==============================] - 0s 159us/step - loss: 0.0058 - val_loss: 0.0368\n",
      "Epoch 184/3000\n",
      "2370/2370 [==============================] - 0s 144us/step - loss: 0.0057 - val_loss: 0.0365\n",
      "Epoch 185/3000\n",
      "2370/2370 [==============================] - 0s 144us/step - loss: 0.0057 - val_loss: 0.0363\n",
      "Epoch 186/3000\n",
      "2370/2370 [==============================] - 0s 89us/step - loss: 0.0057 - val_loss: 0.0365\n",
      "Epoch 187/3000\n",
      "2370/2370 [==============================] - 0s 80us/step - loss: 0.0057 - val_loss: 0.0364\n",
      "Epoch 188/3000\n",
      "2370/2370 [==============================] - 0s 100us/step - loss: 0.0058 - val_loss: 0.0363\n",
      "Epoch 189/3000\n",
      "2370/2370 [==============================] - 0s 143us/step - loss: 0.0057 - val_loss: 0.0365\n",
      "Epoch 190/3000\n",
      "2370/2370 [==============================] - 0s 156us/step - loss: 0.0058 - val_loss: 0.0365\n",
      "Epoch 191/3000\n",
      "2370/2370 [==============================] - 0s 127us/step - loss: 0.0058 - val_loss: 0.0361\n",
      "Epoch 192/3000\n",
      "2370/2370 [==============================] - 0s 106us/step - loss: 0.0057 - val_loss: 0.0359\n",
      "Epoch 193/3000\n",
      "2370/2370 [==============================] - 0s 98us/step - loss: 0.0059 - val_loss: 0.0363\n",
      "Epoch 194/3000\n",
      "2370/2370 [==============================] - 0s 101us/step - loss: 0.0058 - val_loss: 0.0364\n",
      "Epoch 195/3000\n",
      "2370/2370 [==============================] - 0s 150us/step - loss: 0.0058 - val_loss: 0.0363\n",
      "Epoch 196/3000\n",
      "2370/2370 [==============================] - 0s 147us/step - loss: 0.0058 - val_loss: 0.0363\n",
      "Epoch 197/3000\n",
      "2370/2370 [==============================] - 0s 134us/step - loss: 0.0057 - val_loss: 0.0360\n",
      "Epoch 198/3000\n",
      "2370/2370 [==============================] - 0s 148us/step - loss: 0.0058 - val_loss: 0.0369\n",
      "Epoch 199/3000\n",
      "2370/2370 [==============================] - 0s 130us/step - loss: 0.0059 - val_loss: 0.0367\n",
      "Epoch 200/3000\n",
      "2370/2370 [==============================] - 0s 153us/step - loss: 0.0057 - val_loss: 0.0365\n",
      "Epoch 201/3000\n",
      "2370/2370 [==============================] - 0s 131us/step - loss: 0.0058 - val_loss: 0.0364\n",
      "Epoch 202/3000\n",
      "2370/2370 [==============================] - 0s 119us/step - loss: 0.0059 - val_loss: 0.0361\n",
      "Epoch 203/3000\n",
      "2370/2370 [==============================] - 0s 140us/step - loss: 0.0058 - val_loss: 0.0367\n",
      "Epoch 204/3000\n",
      "2370/2370 [==============================] - 0s 83us/step - loss: 0.0058 - val_loss: 0.0371\n",
      "Epoch 205/3000\n",
      "2370/2370 [==============================] - 0s 71us/step - loss: 0.0058 - val_loss: 0.0364\n",
      "Epoch 206/3000\n",
      "2370/2370 [==============================] - 0s 143us/step - loss: 0.0057 - val_loss: 0.0364\n",
      "Epoch 207/3000\n",
      "2370/2370 [==============================] - 0s 147us/step - loss: 0.0057 - val_loss: 0.0368\n",
      "Epoch 208/3000\n",
      "2370/2370 [==============================] - 0s 141us/step - loss: 0.0058 - val_loss: 0.0363\n",
      "Epoch 209/3000\n",
      "2370/2370 [==============================] - 0s 136us/step - loss: 0.0057 - val_loss: 0.0363\n",
      "Epoch 210/3000\n",
      "2370/2370 [==============================] - 0s 99us/step - loss: 0.0058 - val_loss: 0.0365\n",
      "Epoch 211/3000\n",
      "2370/2370 [==============================] - 0s 81us/step - loss: 0.0057 - val_loss: 0.0364\n",
      "Epoch 212/3000\n",
      "2370/2370 [==============================] - 0s 80us/step - loss: 0.0058 - val_loss: 0.0366\n",
      "Epoch 213/3000\n",
      "2370/2370 [==============================] - 0s 74us/step - loss: 0.0057 - val_loss: 0.0364\n",
      "Epoch 214/3000\n",
      "2370/2370 [==============================] - 0s 77us/step - loss: 0.0057 - val_loss: 0.0361\n",
      "Epoch 215/3000\n",
      "2370/2370 [==============================] - 0s 69us/step - loss: 0.0057 - val_loss: 0.0365\n",
      "Epoch 216/3000\n",
      "2370/2370 [==============================] - 0s 74us/step - loss: 0.0057 - val_loss: 0.0364\n",
      "Epoch 217/3000\n",
      "2370/2370 [==============================] - 0s 88us/step - loss: 0.0056 - val_loss: 0.0363\n",
      "Epoch 218/3000\n",
      "2370/2370 [==============================] - 0s 81us/step - loss: 0.0057 - val_loss: 0.0366\n",
      "Epoch 219/3000\n",
      "2370/2370 [==============================] - 0s 74us/step - loss: 0.0056 - val_loss: 0.0370\n",
      "Epoch 220/3000\n",
      "2370/2370 [==============================] - 0s 68us/step - loss: 0.0057 - val_loss: 0.0360\n",
      "Epoch 221/3000\n",
      "2370/2370 [==============================] - 0s 68us/step - loss: 0.0057 - val_loss: 0.0361\n",
      "Epoch 222/3000\n",
      "2370/2370 [==============================] - 0s 74us/step - loss: 0.0057 - val_loss: 0.0366\n",
      "Epoch 223/3000\n",
      "2370/2370 [==============================] - 0s 65us/step - loss: 0.0058 - val_loss: 0.0362\n",
      "Epoch 224/3000\n",
      "2370/2370 [==============================] - 0s 74us/step - loss: 0.0058 - val_loss: 0.0368\n",
      "Epoch 225/3000\n",
      "2370/2370 [==============================] - 0s 70us/step - loss: 0.0056 - val_loss: 0.0367\n",
      "Epoch 226/3000\n",
      "2370/2370 [==============================] - 0s 81us/step - loss: 0.0058 - val_loss: 0.0365\n",
      "Epoch 227/3000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2370/2370 [==============================] - 0s 70us/step - loss: 0.0057 - val_loss: 0.0367\n",
      "Epoch 228/3000\n",
      "2370/2370 [==============================] - 0s 83us/step - loss: 0.0057 - val_loss: 0.0371\n",
      "Epoch 229/3000\n",
      "2370/2370 [==============================] - 0s 83us/step - loss: 0.0057 - val_loss: 0.0371\n",
      "Epoch 230/3000\n",
      "2370/2370 [==============================] - 0s 76us/step - loss: 0.0057 - val_loss: 0.0367\n",
      "Epoch 231/3000\n",
      "2370/2370 [==============================] - 0s 84us/step - loss: 0.0057 - val_loss: 0.0371\n",
      "Epoch 232/3000\n",
      "2370/2370 [==============================] - 0s 68us/step - loss: 0.0058 - val_loss: 0.0371\n",
      "Epoch 233/3000\n",
      "2370/2370 [==============================] - 0s 77us/step - loss: 0.0057 - val_loss: 0.0365\n",
      "Epoch 234/3000\n",
      "2370/2370 [==============================] - 0s 94us/step - loss: 0.0057 - val_loss: 0.0369\n",
      "Epoch 235/3000\n",
      "2370/2370 [==============================] - 0s 69us/step - loss: 0.0057 - val_loss: 0.0369\n",
      "Epoch 236/3000\n",
      "2370/2370 [==============================] - 0s 62us/step - loss: 0.0057 - val_loss: 0.0370\n",
      "Epoch 237/3000\n",
      "2370/2370 [==============================] - 0s 65us/step - loss: 0.0057 - val_loss: 0.0369\n",
      "Epoch 238/3000\n",
      "2370/2370 [==============================] - 0s 73us/step - loss: 0.0057 - val_loss: 0.0374\n",
      "Epoch 239/3000\n",
      "2370/2370 [==============================] - 0s 68us/step - loss: 0.0057 - val_loss: 0.0364\n",
      "Epoch 240/3000\n",
      "2370/2370 [==============================] - 0s 73us/step - loss: 0.0056 - val_loss: 0.0374\n",
      "Epoch 241/3000\n",
      "2370/2370 [==============================] - 0s 56us/step - loss: 0.0057 - val_loss: 0.0364\n",
      "Epoch 242/3000\n",
      "2370/2370 [==============================] - 0s 71us/step - loss: 0.0057 - val_loss: 0.0366\n",
      "Epoch 243/3000\n",
      "2370/2370 [==============================] - 0s 87us/step - loss: 0.0056 - val_loss: 0.0371\n",
      "Epoch 244/3000\n",
      "2370/2370 [==============================] - 0s 72us/step - loss: 0.0057 - val_loss: 0.0370\n",
      "Epoch 245/3000\n",
      "2370/2370 [==============================] - 0s 61us/step - loss: 0.0058 - val_loss: 0.0370\n",
      "Epoch 246/3000\n",
      "2370/2370 [==============================] - 0s 79us/step - loss: 0.0057 - val_loss: 0.0364\n",
      "Epoch 247/3000\n",
      "2370/2370 [==============================] - 0s 84us/step - loss: 0.0057 - val_loss: 0.0370\n",
      "Epoch 248/3000\n",
      "2370/2370 [==============================] - 0s 109us/step - loss: 0.0057 - val_loss: 0.0363\n",
      "Epoch 249/3000\n",
      "2370/2370 [==============================] - 0s 91us/step - loss: 0.0056 - val_loss: 0.0368\n",
      "Epoch 250/3000\n",
      "2370/2370 [==============================] - 0s 84us/step - loss: 0.0057 - val_loss: 0.0371\n",
      "Epoch 251/3000\n",
      "2370/2370 [==============================] - 0s 79us/step - loss: 0.0056 - val_loss: 0.0367\n",
      "Epoch 252/3000\n",
      "2370/2370 [==============================] - 0s 73us/step - loss: 0.0056 - val_loss: 0.0370\n",
      "Epoch 253/3000\n",
      "2370/2370 [==============================] - 0s 66us/step - loss: 0.0057 - val_loss: 0.0370\n",
      "Epoch 254/3000\n",
      "2370/2370 [==============================] - 0s 71us/step - loss: 0.0056 - val_loss: 0.0368\n",
      "Epoch 255/3000\n",
      "2370/2370 [==============================] - 0s 83us/step - loss: 0.0056 - val_loss: 0.0370\n",
      "Epoch 256/3000\n",
      "2370/2370 [==============================] - 0s 80us/step - loss: 0.0056 - val_loss: 0.0361\n",
      "Epoch 257/3000\n",
      "2370/2370 [==============================] - 0s 91us/step - loss: 0.0056 - val_loss: 0.0368\n",
      "Epoch 258/3000\n",
      "2370/2370 [==============================] - 0s 88us/step - loss: 0.0056 - val_loss: 0.0365\n",
      "Epoch 259/3000\n",
      "2370/2370 [==============================] - 0s 80us/step - loss: 0.0056 - val_loss: 0.0361\n",
      "Epoch 260/3000\n",
      "2370/2370 [==============================] - 0s 79us/step - loss: 0.0056 - val_loss: 0.0363\n",
      "Epoch 261/3000\n",
      "2370/2370 [==============================] - 0s 73us/step - loss: 0.0057 - val_loss: 0.0360\n",
      "Epoch 262/3000\n",
      "2370/2370 [==============================] - 0s 71us/step - loss: 0.0057 - val_loss: 0.0363\n",
      "Epoch 263/3000\n",
      "2370/2370 [==============================] - 0s 76us/step - loss: 0.0057 - val_loss: 0.0356\n",
      "Epoch 264/3000\n",
      "2370/2370 [==============================] - 0s 80us/step - loss: 0.0056 - val_loss: 0.0361\n",
      "Epoch 265/3000\n",
      "2370/2370 [==============================] - 0s 78us/step - loss: 0.0057 - val_loss: 0.0362\n",
      "Epoch 266/3000\n",
      "2370/2370 [==============================] - 0s 83us/step - loss: 0.0057 - val_loss: 0.0362\n",
      "Epoch 267/3000\n",
      "2370/2370 [==============================] - 0s 88us/step - loss: 0.0057 - val_loss: 0.0358\n",
      "Epoch 268/3000\n",
      "2370/2370 [==============================] - 0s 78us/step - loss: 0.0057 - val_loss: 0.0362\n",
      "Epoch 269/3000\n",
      "2370/2370 [==============================] - 0s 84us/step - loss: 0.0056 - val_loss: 0.0361\n",
      "Epoch 270/3000\n",
      "2370/2370 [==============================] - 0s 88us/step - loss: 0.0059 - val_loss: 0.0360\n",
      "Epoch 271/3000\n",
      "2370/2370 [==============================] - 0s 78us/step - loss: 0.0055 - val_loss: 0.0364\n",
      "Epoch 272/3000\n",
      "2370/2370 [==============================] - 0s 73us/step - loss: 0.0057 - val_loss: 0.0365\n",
      "Epoch 273/3000\n",
      "2370/2370 [==============================] - 0s 81us/step - loss: 0.0056 - val_loss: 0.0362\n",
      "Epoch 274/3000\n",
      "2370/2370 [==============================] - 0s 94us/step - loss: 0.0055 - val_loss: 0.0364\n",
      "Epoch 275/3000\n",
      "2370/2370 [==============================] - 0s 88us/step - loss: 0.0056 - val_loss: 0.0362\n",
      "Epoch 276/3000\n",
      "2370/2370 [==============================] - 0s 103us/step - loss: 0.0057 - val_loss: 0.0369\n",
      "Epoch 277/3000\n",
      "2370/2370 [==============================] - 0s 97us/step - loss: 0.0056 - val_loss: 0.0367\n",
      "Epoch 278/3000\n",
      "2370/2370 [==============================] - 0s 74us/step - loss: 0.0057 - val_loss: 0.0364\n",
      "Epoch 279/3000\n",
      "2370/2370 [==============================] - 0s 91us/step - loss: 0.0056 - val_loss: 0.0363\n",
      "Epoch 280/3000\n",
      "2370/2370 [==============================] - 0s 97us/step - loss: 0.0056 - val_loss: 0.0362\n",
      "Epoch 281/3000\n",
      "2370/2370 [==============================] - 0s 87us/step - loss: 0.0056 - val_loss: 0.0361\n",
      "Epoch 282/3000\n",
      "2370/2370 [==============================] - 0s 74us/step - loss: 0.0056 - val_loss: 0.0372\n",
      "Epoch 283/3000\n",
      "2370/2370 [==============================] - 0s 86us/step - loss: 0.0056 - val_loss: 0.0367\n",
      "Epoch 284/3000\n",
      "2370/2370 [==============================] - 0s 77us/step - loss: 0.0056 - val_loss: 0.0367\n",
      "Epoch 285/3000\n",
      "2370/2370 [==============================] - 0s 70us/step - loss: 0.0056 - val_loss: 0.0360\n",
      "Epoch 286/3000\n",
      "2370/2370 [==============================] - 0s 65us/step - loss: 0.0055 - val_loss: 0.0363\n",
      "Epoch 287/3000\n",
      "2370/2370 [==============================] - 0s 83us/step - loss: 0.0056 - val_loss: 0.0367\n",
      "Epoch 288/3000\n",
      "2370/2370 [==============================] - 0s 73us/step - loss: 0.0056 - val_loss: 0.0366\n",
      "Epoch 289/3000\n",
      "2370/2370 [==============================] - 0s 66us/step - loss: 0.0056 - val_loss: 0.0360\n",
      "Epoch 290/3000\n",
      "2370/2370 [==============================] - 0s 69us/step - loss: 0.0056 - val_loss: 0.0367\n",
      "Epoch 291/3000\n",
      "2370/2370 [==============================] - 0s 75us/step - loss: 0.0056 - val_loss: 0.0366\n",
      "Epoch 292/3000\n",
      "2370/2370 [==============================] - 0s 70us/step - loss: 0.0055 - val_loss: 0.0363\n",
      "Epoch 293/3000\n",
      "2370/2370 [==============================] - 0s 70us/step - loss: 0.0055 - val_loss: 0.0363\n",
      "Epoch 294/3000\n",
      "2370/2370 [==============================] - 0s 69us/step - loss: 0.0055 - val_loss: 0.0363\n",
      "Epoch 295/3000\n",
      "2370/2370 [==============================] - 0s 65us/step - loss: 0.0056 - val_loss: 0.0368\n",
      "Epoch 296/3000\n",
      "2370/2370 [==============================] - 0s 67us/step - loss: 0.0057 - val_loss: 0.0374\n",
      "Epoch 297/3000\n",
      "2370/2370 [==============================] - 0s 77us/step - loss: 0.0056 - val_loss: 0.0385\n",
      "Epoch 298/3000\n",
      "2370/2370 [==============================] - 0s 73us/step - loss: 0.0056 - val_loss: 0.0368\n",
      "Epoch 299/3000\n",
      "2370/2370 [==============================] - 0s 69us/step - loss: 0.0055 - val_loss: 0.0363\n",
      "Epoch 300/3000\n",
      "2370/2370 [==============================] - 0s 69us/step - loss: 0.0056 - val_loss: 0.0377\n",
      "Epoch 301/3000\n",
      "2370/2370 [==============================] - 0s 60us/step - loss: 0.0055 - val_loss: 0.0369\n",
      "Epoch 302/3000\n",
      "2370/2370 [==============================] - 0s 67us/step - loss: 0.0057 - val_loss: 0.0368\n",
      "Epoch 303/3000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2370/2370 [==============================] - 0s 71us/step - loss: 0.0055 - val_loss: 0.0363\n",
      "Epoch 304/3000\n",
      "2370/2370 [==============================] - 0s 77us/step - loss: 0.0056 - val_loss: 0.0366\n",
      "Epoch 305/3000\n",
      "2370/2370 [==============================] - 0s 83us/step - loss: 0.0056 - val_loss: 0.0361\n",
      "Epoch 306/3000\n",
      "2370/2370 [==============================] - 0s 79us/step - loss: 0.0056 - val_loss: 0.0374\n",
      "Epoch 307/3000\n",
      "2370/2370 [==============================] - 0s 82us/step - loss: 0.0055 - val_loss: 0.0369\n",
      "Epoch 308/3000\n",
      "2370/2370 [==============================] - 0s 75us/step - loss: 0.0057 - val_loss: 0.0370\n",
      "Epoch 309/3000\n",
      "2370/2370 [==============================] - 0s 79us/step - loss: 0.0055 - val_loss: 0.0366\n",
      "Epoch 310/3000\n",
      "2370/2370 [==============================] - 0s 63us/step - loss: 0.0056 - val_loss: 0.0368\n",
      "Epoch 311/3000\n",
      "2370/2370 [==============================] - 0s 72us/step - loss: 0.0057 - val_loss: 0.0367\n",
      "Epoch 312/3000\n",
      "2370/2370 [==============================] - 0s 78us/step - loss: 0.0056 - val_loss: 0.0368\n",
      "Epoch 313/3000\n",
      "2370/2370 [==============================] - 0s 66us/step - loss: 0.0055 - val_loss: 0.0368\n",
      "Epoch 314/3000\n",
      "2370/2370 [==============================] - 0s 63us/step - loss: 0.0055 - val_loss: 0.0365\n",
      "Epoch 315/3000\n",
      "2370/2370 [==============================] - 0s 77us/step - loss: 0.0056 - val_loss: 0.0367\n",
      "Epoch 316/3000\n",
      "2370/2370 [==============================] - 0s 63us/step - loss: 0.0056 - val_loss: 0.0376\n",
      "Epoch 317/3000\n",
      "2370/2370 [==============================] - 0s 77us/step - loss: 0.0054 - val_loss: 0.0373\n",
      "Epoch 318/3000\n",
      "2370/2370 [==============================] - 0s 66us/step - loss: 0.0056 - val_loss: 0.0366\n",
      "Epoch 319/3000\n",
      "2370/2370 [==============================] - 0s 69us/step - loss: 0.0056 - val_loss: 0.0364\n",
      "Epoch 320/3000\n",
      "2370/2370 [==============================] - 0s 68us/step - loss: 0.0056 - val_loss: 0.0366\n",
      "Epoch 321/3000\n",
      "2370/2370 [==============================] - 0s 89us/step - loss: 0.0056 - val_loss: 0.0377\n",
      "Epoch 322/3000\n",
      "2370/2370 [==============================] - 0s 70us/step - loss: 0.0056 - val_loss: 0.0369\n",
      "Epoch 323/3000\n",
      "2370/2370 [==============================] - 0s 66us/step - loss: 0.0055 - val_loss: 0.0365\n",
      "Epoch 324/3000\n",
      "2370/2370 [==============================] - 0s 74us/step - loss: 0.0056 - val_loss: 0.0362\n",
      "Epoch 325/3000\n",
      "2370/2370 [==============================] - 0s 75us/step - loss: 0.0055 - val_loss: 0.0366\n",
      "Epoch 326/3000\n",
      "2370/2370 [==============================] - 0s 68us/step - loss: 0.0055 - val_loss: 0.0373\n",
      "Epoch 327/3000\n",
      "2370/2370 [==============================] - 0s 78us/step - loss: 0.0056 - val_loss: 0.0367\n",
      "Epoch 328/3000\n",
      "2370/2370 [==============================] - 0s 92us/step - loss: 0.0055 - val_loss: 0.0366\n",
      "Epoch 329/3000\n",
      "2370/2370 [==============================] - 0s 83us/step - loss: 0.0056 - val_loss: 0.0372\n",
      "Epoch 330/3000\n",
      "2370/2370 [==============================] - 0s 71us/step - loss: 0.0055 - val_loss: 0.0370\n",
      "Epoch 331/3000\n",
      "2370/2370 [==============================] - 0s 73us/step - loss: 0.0055 - val_loss: 0.0370\n",
      "Epoch 332/3000\n",
      "2370/2370 [==============================] - 0s 76us/step - loss: 0.0055 - val_loss: 0.0368\n",
      "Epoch 333/3000\n",
      "2370/2370 [==============================] - 0s 62us/step - loss: 0.0054 - val_loss: 0.0379\n",
      "Epoch 334/3000\n",
      "2370/2370 [==============================] - 0s 72us/step - loss: 0.0055 - val_loss: 0.0365\n",
      "Epoch 335/3000\n",
      "2370/2370 [==============================] - 0s 82us/step - loss: 0.0055 - val_loss: 0.0372\n",
      "Epoch 336/3000\n",
      "2370/2370 [==============================] - 0s 74us/step - loss: 0.0055 - val_loss: 0.0366\n",
      "Epoch 337/3000\n",
      "2370/2370 [==============================] - 0s 74us/step - loss: 0.0055 - val_loss: 0.0368\n",
      "Epoch 338/3000\n",
      "2370/2370 [==============================] - 0s 76us/step - loss: 0.0055 - val_loss: 0.0369\n",
      "Epoch 339/3000\n",
      "2370/2370 [==============================] - 0s 65us/step - loss: 0.0055 - val_loss: 0.0370\n",
      "Epoch 340/3000\n",
      "2370/2370 [==============================] - 0s 69us/step - loss: 0.0056 - val_loss: 0.0367\n",
      "Epoch 341/3000\n",
      "2370/2370 [==============================] - 0s 75us/step - loss: 0.0055 - val_loss: 0.0370\n",
      "Epoch 342/3000\n",
      "2370/2370 [==============================] - 0s 68us/step - loss: 0.0055 - val_loss: 0.0366\n",
      "Epoch 343/3000\n",
      "2370/2370 [==============================] - 0s 76us/step - loss: 0.0056 - val_loss: 0.0365\n",
      "Epoch 344/3000\n",
      "2370/2370 [==============================] - 0s 73us/step - loss: 0.0055 - val_loss: 0.0367\n",
      "Epoch 345/3000\n",
      "2370/2370 [==============================] - 0s 71us/step - loss: 0.0055 - val_loss: 0.0365\n",
      "Epoch 346/3000\n",
      "2370/2370 [==============================] - 0s 65us/step - loss: 0.0055 - val_loss: 0.0370\n",
      "Epoch 347/3000\n",
      "2370/2370 [==============================] - 0s 62us/step - loss: 0.0056 - val_loss: 0.0364\n",
      "Epoch 348/3000\n",
      "2370/2370 [==============================] - 0s 72us/step - loss: 0.0055 - val_loss: 0.0358\n",
      "Epoch 349/3000\n",
      "2370/2370 [==============================] - 0s 68us/step - loss: 0.0055 - val_loss: 0.0364\n",
      "Epoch 350/3000\n",
      "2370/2370 [==============================] - 0s 69us/step - loss: 0.0058 - val_loss: 0.0360\n",
      "Epoch 351/3000\n",
      "2370/2370 [==============================] - 0s 70us/step - loss: 0.0056 - val_loss: 0.0370\n",
      "Epoch 352/3000\n",
      "2370/2370 [==============================] - 0s 66us/step - loss: 0.0058 - val_loss: 0.0364\n",
      "Epoch 353/3000\n",
      "2370/2370 [==============================] - 0s 63us/step - loss: 0.0055 - val_loss: 0.0373\n",
      "Epoch 354/3000\n",
      "2370/2370 [==============================] - 0s 68us/step - loss: 0.0054 - val_loss: 0.0366\n",
      "Epoch 355/3000\n",
      "2370/2370 [==============================] - 0s 71us/step - loss: 0.0056 - val_loss: 0.0374\n",
      "Epoch 356/3000\n",
      "2370/2370 [==============================] - 0s 76us/step - loss: 0.0056 - val_loss: 0.0365\n",
      "Epoch 357/3000\n",
      "2370/2370 [==============================] - 0s 75us/step - loss: 0.0056 - val_loss: 0.0367\n",
      "Epoch 358/3000\n",
      "2370/2370 [==============================] - 0s 73us/step - loss: 0.0055 - val_loss: 0.0365\n",
      "Epoch 359/3000\n",
      "2370/2370 [==============================] - 0s 80us/step - loss: 0.0055 - val_loss: 0.0364\n",
      "Epoch 360/3000\n",
      "2370/2370 [==============================] - 0s 63us/step - loss: 0.0055 - val_loss: 0.0372\n",
      "Epoch 361/3000\n",
      "2370/2370 [==============================] - 0s 67us/step - loss: 0.0055 - val_loss: 0.0369\n",
      "Epoch 362/3000\n",
      "2370/2370 [==============================] - 0s 67us/step - loss: 0.0055 - val_loss: 0.0373\n",
      "Epoch 363/3000\n",
      "2370/2370 [==============================] - 0s 86us/step - loss: 0.0054 - val_loss: 0.0366\n",
      "Epoch 364/3000\n",
      "2370/2370 [==============================] - 0s 81us/step - loss: 0.0057 - val_loss: 0.0372\n",
      "Epoch 365/3000\n",
      "2370/2370 [==============================] - 0s 75us/step - loss: 0.0055 - val_loss: 0.0381\n",
      "Epoch 366/3000\n",
      "2370/2370 [==============================] - 0s 70us/step - loss: 0.0054 - val_loss: 0.0367\n",
      "Epoch 367/3000\n",
      "2370/2370 [==============================] - 0s 70us/step - loss: 0.0056 - val_loss: 0.0378\n",
      "Epoch 368/3000\n",
      "2370/2370 [==============================] - 0s 66us/step - loss: 0.0055 - val_loss: 0.0376\n",
      "Epoch 369/3000\n",
      "2370/2370 [==============================] - 0s 67us/step - loss: 0.0056 - val_loss: 0.0381\n",
      "Epoch 370/3000\n",
      "2370/2370 [==============================] - 0s 77us/step - loss: 0.0054 - val_loss: 0.0385\n",
      "Epoch 371/3000\n",
      "2370/2370 [==============================] - 0s 72us/step - loss: 0.0056 - val_loss: 0.0385\n",
      "Epoch 372/3000\n",
      "2370/2370 [==============================] - 0s 81us/step - loss: 0.0055 - val_loss: 0.0371\n",
      "Epoch 373/3000\n",
      "2370/2370 [==============================] - 0s 70us/step - loss: 0.0057 - val_loss: 0.0372\n",
      "Epoch 374/3000\n",
      "2370/2370 [==============================] - 0s 61us/step - loss: 0.0056 - val_loss: 0.0376\n",
      "Epoch 375/3000\n",
      "2370/2370 [==============================] - 0s 59us/step - loss: 0.0054 - val_loss: 0.0378\n",
      "Epoch 376/3000\n",
      "2370/2370 [==============================] - 0s 65us/step - loss: 0.0054 - val_loss: 0.0375\n",
      "Epoch 377/3000\n",
      "2370/2370 [==============================] - 0s 64us/step - loss: 0.0055 - val_loss: 0.0373\n",
      "Epoch 378/3000\n",
      "2370/2370 [==============================] - 0s 73us/step - loss: 0.0054 - val_loss: 0.0369\n",
      "Epoch 379/3000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2370/2370 [==============================] - 0s 76us/step - loss: 0.0055 - val_loss: 0.0374\n",
      "Epoch 380/3000\n",
      "2370/2370 [==============================] - 0s 82us/step - loss: 0.0056 - val_loss: 0.0369\n",
      "Epoch 381/3000\n",
      "2370/2370 [==============================] - 0s 89us/step - loss: 0.0055 - val_loss: 0.0377\n",
      "Epoch 382/3000\n",
      "2370/2370 [==============================] - 0s 90us/step - loss: 0.0055 - val_loss: 0.0371\n",
      "Epoch 383/3000\n",
      "2370/2370 [==============================] - 0s 78us/step - loss: 0.0055 - val_loss: 0.0371\n",
      "Epoch 384/3000\n",
      "2370/2370 [==============================] - 0s 69us/step - loss: 0.0055 - val_loss: 0.0377\n",
      "Epoch 385/3000\n",
      "2370/2370 [==============================] - 0s 77us/step - loss: 0.0055 - val_loss: 0.0379\n",
      "Epoch 386/3000\n",
      "2370/2370 [==============================] - 0s 69us/step - loss: 0.0055 - val_loss: 0.0372\n",
      "Epoch 387/3000\n",
      "2370/2370 [==============================] - 0s 75us/step - loss: 0.0055 - val_loss: 0.0380\n",
      "Epoch 388/3000\n",
      "2370/2370 [==============================] - 0s 68us/step - loss: 0.0055 - val_loss: 0.0371\n",
      "Epoch 389/3000\n",
      "2370/2370 [==============================] - 0s 66us/step - loss: 0.0055 - val_loss: 0.0375\n",
      "Epoch 390/3000\n",
      "2370/2370 [==============================] - 0s 65us/step - loss: 0.0053 - val_loss: 0.0376\n",
      "Epoch 391/3000\n",
      "2370/2370 [==============================] - 0s 71us/step - loss: 0.0055 - val_loss: 0.0372\n",
      "Epoch 392/3000\n",
      "2370/2370 [==============================] - 0s 69us/step - loss: 0.0056 - val_loss: 0.0376\n",
      "Epoch 393/3000\n",
      "2370/2370 [==============================] - 0s 71us/step - loss: 0.0054 - val_loss: 0.0365\n",
      "Epoch 394/3000\n",
      "2370/2370 [==============================] - 0s 66us/step - loss: 0.0054 - val_loss: 0.0370\n",
      "Epoch 395/3000\n",
      "2370/2370 [==============================] - 0s 65us/step - loss: 0.0055 - val_loss: 0.0367\n",
      "Epoch 396/3000\n",
      "2370/2370 [==============================] - 0s 74us/step - loss: 0.0055 - val_loss: 0.0371\n",
      "Epoch 397/3000\n",
      "2370/2370 [==============================] - 0s 66us/step - loss: 0.0054 - val_loss: 0.0372\n",
      "Epoch 398/3000\n",
      "2370/2370 [==============================] - 0s 68us/step - loss: 0.0055 - val_loss: 0.0370\n",
      "Epoch 399/3000\n",
      "2370/2370 [==============================] - 0s 73us/step - loss: 0.0055 - val_loss: 0.0366\n",
      "Epoch 400/3000\n",
      "2370/2370 [==============================] - 0s 69us/step - loss: 0.0053 - val_loss: 0.0370\n",
      "Epoch 401/3000\n",
      "2370/2370 [==============================] - 0s 73us/step - loss: 0.0054 - val_loss: 0.0376\n",
      "Epoch 402/3000\n",
      "2370/2370 [==============================] - 0s 78us/step - loss: 0.0054 - val_loss: 0.0372\n",
      "Epoch 403/3000\n",
      "2370/2370 [==============================] - 0s 66us/step - loss: 0.0053 - val_loss: 0.0371\n",
      "Epoch 404/3000\n",
      "2370/2370 [==============================] - 0s 70us/step - loss: 0.0054 - val_loss: 0.0370\n",
      "Epoch 405/3000\n",
      "2370/2370 [==============================] - 0s 63us/step - loss: 0.0056 - val_loss: 0.0363\n",
      "Epoch 406/3000\n",
      "2370/2370 [==============================] - 0s 69us/step - loss: 0.0054 - val_loss: 0.0362\n",
      "Epoch 407/3000\n",
      "2370/2370 [==============================] - 0s 81us/step - loss: 0.0055 - val_loss: 0.0364\n",
      "Epoch 408/3000\n",
      "2370/2370 [==============================] - 0s 83us/step - loss: 0.0054 - val_loss: 0.0362\n",
      "Epoch 409/3000\n",
      "2370/2370 [==============================] - 0s 90us/step - loss: 0.0054 - val_loss: 0.0370\n",
      "Epoch 410/3000\n",
      "2370/2370 [==============================] - 0s 76us/step - loss: 0.0055 - val_loss: 0.0366\n",
      "Epoch 411/3000\n",
      "2370/2370 [==============================] - 0s 75us/step - loss: 0.0054 - val_loss: 0.0371\n",
      "Epoch 412/3000\n",
      "2370/2370 [==============================] - 0s 64us/step - loss: 0.0054 - val_loss: 0.0377\n",
      "Epoch 413/3000\n",
      "2370/2370 [==============================] - 0s 76us/step - loss: 0.0055 - val_loss: 0.0372\n",
      "Epoch 414/3000\n",
      "2370/2370 [==============================] - 0s 70us/step - loss: 0.0055 - val_loss: 0.0372\n",
      "Epoch 415/3000\n",
      "2370/2370 [==============================] - 0s 70us/step - loss: 0.0054 - val_loss: 0.0365\n",
      "Epoch 416/3000\n",
      "2370/2370 [==============================] - 0s 68us/step - loss: 0.0055 - val_loss: 0.0366\n",
      "Epoch 417/3000\n",
      "2370/2370 [==============================] - 0s 72us/step - loss: 0.0055 - val_loss: 0.0370\n",
      "Epoch 418/3000\n",
      "2370/2370 [==============================] - 0s 68us/step - loss: 0.0055 - val_loss: 0.0364\n",
      "Epoch 419/3000\n",
      "2370/2370 [==============================] - 0s 75us/step - loss: 0.0055 - val_loss: 0.0371\n",
      "Epoch 420/3000\n",
      "2370/2370 [==============================] - 0s 76us/step - loss: 0.0054 - val_loss: 0.0364\n",
      "Epoch 421/3000\n",
      "2370/2370 [==============================] - 0s 68us/step - loss: 0.0055 - val_loss: 0.0363\n",
      "Epoch 422/3000\n",
      "2370/2370 [==============================] - 0s 80us/step - loss: 0.0053 - val_loss: 0.0363\n",
      "Epoch 423/3000\n",
      "2370/2370 [==============================] - 0s 65us/step - loss: 0.0053 - val_loss: 0.0366\n",
      "Epoch 424/3000\n",
      "2370/2370 [==============================] - 0s 68us/step - loss: 0.0053 - val_loss: 0.0377\n",
      "Epoch 425/3000\n",
      "2370/2370 [==============================] - 0s 77us/step - loss: 0.0053 - val_loss: 0.0368\n",
      "Epoch 426/3000\n",
      "2370/2370 [==============================] - 0s 63us/step - loss: 0.0055 - val_loss: 0.0372\n",
      "Epoch 427/3000\n",
      "2370/2370 [==============================] - 0s 75us/step - loss: 0.0055 - val_loss: 0.0370\n",
      "Epoch 428/3000\n",
      "2370/2370 [==============================] - 0s 78us/step - loss: 0.0054 - val_loss: 0.0370\n",
      "Epoch 429/3000\n",
      "2370/2370 [==============================] - 0s 67us/step - loss: 0.0054 - val_loss: 0.0376\n",
      "Epoch 430/3000\n",
      "2370/2370 [==============================] - 0s 75us/step - loss: 0.0054 - val_loss: 0.0372\n",
      "Epoch 431/3000\n",
      "2370/2370 [==============================] - 0s 82us/step - loss: 0.0054 - val_loss: 0.0374\n",
      "Epoch 432/3000\n",
      "2370/2370 [==============================] - 0s 89us/step - loss: 0.0054 - val_loss: 0.0375\n",
      "Epoch 433/3000\n",
      "2370/2370 [==============================] - 0s 92us/step - loss: 0.0055 - val_loss: 0.0365\n",
      "Epoch 434/3000\n",
      "2370/2370 [==============================] - 0s 76us/step - loss: 0.0054 - val_loss: 0.0371\n",
      "Epoch 435/3000\n",
      "2370/2370 [==============================] - 0s 66us/step - loss: 0.0055 - val_loss: 0.0375\n",
      "Epoch 436/3000\n",
      "2370/2370 [==============================] - 0s 77us/step - loss: 0.0055 - val_loss: 0.0368\n",
      "Epoch 437/3000\n",
      "2370/2370 [==============================] - 0s 91us/step - loss: 0.0054 - val_loss: 0.0370\n",
      "Epoch 438/3000\n",
      "2370/2370 [==============================] - 0s 75us/step - loss: 0.0052 - val_loss: 0.0370\n",
      "Epoch 439/3000\n",
      "2370/2370 [==============================] - 0s 93us/step - loss: 0.0055 - val_loss: 0.0372\n",
      "Epoch 440/3000\n",
      "2370/2370 [==============================] - 0s 67us/step - loss: 0.0054 - val_loss: 0.0372\n",
      "Epoch 441/3000\n",
      "2370/2370 [==============================] - 0s 65us/step - loss: 0.0052 - val_loss: 0.0370\n",
      "Epoch 442/3000\n",
      "2370/2370 [==============================] - 0s 79us/step - loss: 0.0053 - val_loss: 0.0373\n",
      "Epoch 443/3000\n",
      "2370/2370 [==============================] - 0s 69us/step - loss: 0.0053 - val_loss: 0.0368\n",
      "Epoch 444/3000\n",
      "2370/2370 [==============================] - 0s 59us/step - loss: 0.0053 - val_loss: 0.0369\n",
      "Epoch 445/3000\n",
      "2370/2370 [==============================] - 0s 74us/step - loss: 0.0054 - val_loss: 0.0365\n",
      "Epoch 446/3000\n",
      "2370/2370 [==============================] - 0s 82us/step - loss: 0.0053 - val_loss: 0.0378\n",
      "Epoch 447/3000\n",
      "2370/2370 [==============================] - 0s 89us/step - loss: 0.0052 - val_loss: 0.0368\n",
      "Epoch 448/3000\n",
      "2370/2370 [==============================] - 0s 80us/step - loss: 0.0053 - val_loss: 0.0369\n",
      "Epoch 449/3000\n",
      "2370/2370 [==============================] - 0s 78us/step - loss: 0.0053 - val_loss: 0.0372\n",
      "Epoch 450/3000\n",
      "2370/2370 [==============================] - 0s 75us/step - loss: 0.0054 - val_loss: 0.0372\n",
      "Epoch 451/3000\n",
      "2370/2370 [==============================] - 0s 70us/step - loss: 0.0052 - val_loss: 0.0374\n",
      "Epoch 452/3000\n",
      "2370/2370 [==============================] - 0s 78us/step - loss: 0.0052 - val_loss: 0.0366\n",
      "Epoch 453/3000\n",
      "2370/2370 [==============================] - 0s 78us/step - loss: 0.0053 - val_loss: 0.0371\n",
      "Epoch 454/3000\n",
      "2370/2370 [==============================] - 0s 86us/step - loss: 0.0054 - val_loss: 0.0380\n",
      "Epoch 455/3000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2370/2370 [==============================] - 0s 79us/step - loss: 0.0053 - val_loss: 0.0377\n",
      "Epoch 456/3000\n",
      "2370/2370 [==============================] - 0s 84us/step - loss: 0.0053 - val_loss: 0.0375\n",
      "Epoch 457/3000\n",
      "2370/2370 [==============================] - 0s 86us/step - loss: 0.0052 - val_loss: 0.0370\n",
      "Epoch 458/3000\n",
      "2370/2370 [==============================] - 0s 80us/step - loss: 0.0052 - val_loss: 0.0368\n",
      "Epoch 459/3000\n",
      "2370/2370 [==============================] - 0s 90us/step - loss: 0.0054 - val_loss: 0.0371\n",
      "Epoch 460/3000\n",
      "2370/2370 [==============================] - 0s 80us/step - loss: 0.0052 - val_loss: 0.0373\n",
      "Epoch 461/3000\n",
      "2370/2370 [==============================] - 0s 68us/step - loss: 0.0053 - val_loss: 0.0370\n",
      "Epoch 462/3000\n",
      "2370/2370 [==============================] - 0s 72us/step - loss: 0.0053 - val_loss: 0.0375\n",
      "Epoch 463/3000\n",
      "2370/2370 [==============================] - 0s 85us/step - loss: 0.0052 - val_loss: 0.0380\n",
      "Epoch 464/3000\n",
      "2370/2370 [==============================] - 0s 83us/step - loss: 0.0053 - val_loss: 0.0374\n",
      "Epoch 465/3000\n",
      "2370/2370 [==============================] - 0s 72us/step - loss: 0.0053 - val_loss: 0.0375\n",
      "Epoch 466/3000\n",
      "2370/2370 [==============================] - 0s 90us/step - loss: 0.0054 - val_loss: 0.0373\n",
      "Epoch 467/3000\n",
      "2370/2370 [==============================] - 0s 72us/step - loss: 0.0052 - val_loss: 0.0377\n",
      "Epoch 468/3000\n",
      "2370/2370 [==============================] - 0s 73us/step - loss: 0.0053 - val_loss: 0.0375\n",
      "Epoch 469/3000\n",
      "2370/2370 [==============================] - 0s 81us/step - loss: 0.0054 - val_loss: 0.0378\n",
      "Epoch 470/3000\n",
      "2370/2370 [==============================] - 0s 76us/step - loss: 0.0053 - val_loss: 0.0385\n",
      "Epoch 471/3000\n",
      "2370/2370 [==============================] - 0s 79us/step - loss: 0.0053 - val_loss: 0.0382\n",
      "Epoch 472/3000\n",
      "2370/2370 [==============================] - 0s 75us/step - loss: 0.0053 - val_loss: 0.0375\n",
      "Epoch 473/3000\n",
      "2370/2370 [==============================] - 0s 85us/step - loss: 0.0053 - val_loss: 0.0376\n",
      "Epoch 474/3000\n",
      "2370/2370 [==============================] - 0s 67us/step - loss: 0.0055 - val_loss: 0.0376\n",
      "Epoch 475/3000\n",
      "2370/2370 [==============================] - 0s 67us/step - loss: 0.0054 - val_loss: 0.0377\n",
      "Epoch 476/3000\n",
      "2370/2370 [==============================] - 0s 67us/step - loss: 0.0054 - val_loss: 0.0373\n",
      "Epoch 477/3000\n",
      "2370/2370 [==============================] - 0s 66us/step - loss: 0.0052 - val_loss: 0.0375\n",
      "Epoch 478/3000\n",
      "2370/2370 [==============================] - 0s 75us/step - loss: 0.0053 - val_loss: 0.0374\n",
      "Epoch 479/3000\n",
      "2370/2370 [==============================] - 0s 80us/step - loss: 0.0054 - val_loss: 0.0366\n",
      "Epoch 480/3000\n",
      "2370/2370 [==============================] - 0s 79us/step - loss: 0.0053 - val_loss: 0.0371\n",
      "Epoch 481/3000\n",
      "2370/2370 [==============================] - 0s 78us/step - loss: 0.0054 - val_loss: 0.0378\n",
      "Epoch 482/3000\n",
      "2370/2370 [==============================] - 0s 78us/step - loss: 0.0053 - val_loss: 0.0369\n",
      "Epoch 483/3000\n",
      "2370/2370 [==============================] - 0s 87us/step - loss: 0.0056 - val_loss: 0.0367\n",
      "Epoch 484/3000\n",
      "2370/2370 [==============================] - 0s 88us/step - loss: 0.0053 - val_loss: 0.0379\n",
      "Epoch 485/3000\n",
      "2370/2370 [==============================] - 0s 78us/step - loss: 0.0052 - val_loss: 0.0375\n",
      "Epoch 486/3000\n",
      "2370/2370 [==============================] - 0s 81us/step - loss: 0.0053 - val_loss: 0.0373\n",
      "Epoch 487/3000\n",
      "2370/2370 [==============================] - 0s 77us/step - loss: 0.0052 - val_loss: 0.0369\n",
      "Epoch 488/3000\n",
      "2370/2370 [==============================] - 0s 81us/step - loss: 0.0052 - val_loss: 0.0368\n",
      "Epoch 489/3000\n",
      "2370/2370 [==============================] - 0s 82us/step - loss: 0.0053 - val_loss: 0.0368\n",
      "Epoch 490/3000\n",
      "2370/2370 [==============================] - 0s 89us/step - loss: 0.0053 - val_loss: 0.0365\n",
      "Epoch 491/3000\n",
      "2370/2370 [==============================] - 0s 82us/step - loss: 0.0054 - val_loss: 0.0365\n",
      "Epoch 492/3000\n",
      "2370/2370 [==============================] - 0s 80us/step - loss: 0.0055 - val_loss: 0.0364\n",
      "Epoch 493/3000\n",
      "2370/2370 [==============================] - 0s 98us/step - loss: 0.0054 - val_loss: 0.0376\n",
      "Epoch 494/3000\n",
      "2370/2370 [==============================] - 0s 85us/step - loss: 0.0052 - val_loss: 0.0374\n",
      "Epoch 495/3000\n",
      "2370/2370 [==============================] - 0s 81us/step - loss: 0.0054 - val_loss: 0.0365\n",
      "Epoch 496/3000\n",
      "2370/2370 [==============================] - 0s 70us/step - loss: 0.0052 - val_loss: 0.0370\n",
      "Epoch 497/3000\n",
      "2370/2370 [==============================] - 0s 80us/step - loss: 0.0053 - val_loss: 0.0373\n",
      "Epoch 498/3000\n",
      "2370/2370 [==============================] - 0s 85us/step - loss: 0.0053 - val_loss: 0.0372\n",
      "Epoch 499/3000\n",
      "2370/2370 [==============================] - 0s 69us/step - loss: 0.0054 - val_loss: 0.0368\n",
      "Epoch 500/3000\n",
      "2370/2370 [==============================] - 0s 70us/step - loss: 0.0051 - val_loss: 0.0370\n",
      "Epoch 501/3000\n",
      "2370/2370 [==============================] - 0s 70us/step - loss: 0.0052 - val_loss: 0.0368\n",
      "Epoch 502/3000\n",
      "2370/2370 [==============================] - 0s 74us/step - loss: 0.0053 - val_loss: 0.0372\n",
      "Epoch 503/3000\n",
      "2370/2370 [==============================] - 0s 77us/step - loss: 0.0053 - val_loss: 0.0376\n",
      "Epoch 504/3000\n",
      "2370/2370 [==============================] - 0s 77us/step - loss: 0.0053 - val_loss: 0.0375\n",
      "Epoch 505/3000\n",
      "2370/2370 [==============================] - 0s 71us/step - loss: 0.0053 - val_loss: 0.0374\n",
      "Epoch 506/3000\n",
      "2370/2370 [==============================] - 0s 86us/step - loss: 0.0052 - val_loss: 0.0381\n",
      "Epoch 507/3000\n",
      "2370/2370 [==============================] - 0s 75us/step - loss: 0.0052 - val_loss: 0.0375\n",
      "Epoch 508/3000\n",
      "2370/2370 [==============================] - 0s 80us/step - loss: 0.0053 - val_loss: 0.0370\n",
      "Epoch 509/3000\n",
      "2370/2370 [==============================] - 0s 86us/step - loss: 0.0054 - val_loss: 0.0376\n",
      "Epoch 510/3000\n",
      "2370/2370 [==============================] - 0s 82us/step - loss: 0.0054 - val_loss: 0.0376\n",
      "Epoch 511/3000\n",
      "2370/2370 [==============================] - 0s 88us/step - loss: 0.0053 - val_loss: 0.0377\n",
      "Epoch 512/3000\n",
      "2370/2370 [==============================] - 0s 64us/step - loss: 0.0052 - val_loss: 0.0383\n",
      "Epoch 513/3000\n",
      "2370/2370 [==============================] - 0s 64us/step - loss: 0.0052 - val_loss: 0.0381\n",
      "Epoch 514/3000\n",
      "2370/2370 [==============================] - 0s 88us/step - loss: 0.0052 - val_loss: 0.0381\n",
      "Epoch 515/3000\n",
      "2370/2370 [==============================] - 0s 81us/step - loss: 0.0054 - val_loss: 0.0385\n",
      "Epoch 516/3000\n",
      "2370/2370 [==============================] - 0s 71us/step - loss: 0.0052 - val_loss: 0.0382\n",
      "Epoch 517/3000\n",
      "2370/2370 [==============================] - 0s 73us/step - loss: 0.0052 - val_loss: 0.0386\n",
      "Epoch 518/3000\n",
      "2370/2370 [==============================] - 0s 76us/step - loss: 0.0053 - val_loss: 0.0383\n",
      "Epoch 519/3000\n",
      "2370/2370 [==============================] - 0s 76us/step - loss: 0.0055 - val_loss: 0.0370\n",
      "Epoch 520/3000\n",
      "2370/2370 [==============================] - 0s 74us/step - loss: 0.0054 - val_loss: 0.0380\n",
      "Epoch 521/3000\n",
      "2370/2370 [==============================] - 0s 69us/step - loss: 0.0052 - val_loss: 0.0377\n",
      "Epoch 522/3000\n",
      "2370/2370 [==============================] - 0s 61us/step - loss: 0.0054 - val_loss: 0.0379\n",
      "Epoch 523/3000\n",
      "2370/2370 [==============================] - 0s 75us/step - loss: 0.0053 - val_loss: 0.0378\n",
      "Epoch 524/3000\n",
      "2370/2370 [==============================] - 0s 77us/step - loss: 0.0053 - val_loss: 0.0388\n",
      "Epoch 525/3000\n",
      "2370/2370 [==============================] - 0s 79us/step - loss: 0.0052 - val_loss: 0.0382\n",
      "Epoch 526/3000\n",
      "2370/2370 [==============================] - 0s 67us/step - loss: 0.0053 - val_loss: 0.0384\n",
      "Epoch 527/3000\n",
      "2370/2370 [==============================] - 0s 74us/step - loss: 0.0054 - val_loss: 0.0373\n",
      "Epoch 528/3000\n",
      "2370/2370 [==============================] - 0s 78us/step - loss: 0.0052 - val_loss: 0.0381\n",
      "Epoch 529/3000\n",
      "2370/2370 [==============================] - 0s 85us/step - loss: 0.0053 - val_loss: 0.0385\n",
      "Epoch 530/3000\n",
      "2370/2370 [==============================] - 0s 73us/step - loss: 0.0051 - val_loss: 0.0387\n",
      "Epoch 531/3000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2370/2370 [==============================] - 0s 78us/step - loss: 0.0054 - val_loss: 0.0382\n",
      "Epoch 532/3000\n",
      "2370/2370 [==============================] - 0s 77us/step - loss: 0.0053 - val_loss: 0.0383\n",
      "Epoch 533/3000\n",
      "2370/2370 [==============================] - 0s 81us/step - loss: 0.0054 - val_loss: 0.0373\n",
      "Epoch 534/3000\n",
      "2370/2370 [==============================] - 0s 76us/step - loss: 0.0052 - val_loss: 0.0375\n",
      "Epoch 535/3000\n",
      "2370/2370 [==============================] - 0s 76us/step - loss: 0.0052 - val_loss: 0.0378\n",
      "Epoch 536/3000\n",
      "2370/2370 [==============================] - 0s 73us/step - loss: 0.0052 - val_loss: 0.0379\n",
      "Epoch 537/3000\n",
      "2370/2370 [==============================] - 0s 83us/step - loss: 0.0054 - val_loss: 0.0372\n",
      "Epoch 538/3000\n",
      "2370/2370 [==============================] - 0s 81us/step - loss: 0.0053 - val_loss: 0.0373\n",
      "Epoch 539/3000\n",
      "2370/2370 [==============================] - 0s 81us/step - loss: 0.0054 - val_loss: 0.0374\n",
      "Epoch 540/3000\n",
      "2370/2370 [==============================] - 0s 63us/step - loss: 0.0052 - val_loss: 0.0386\n",
      "Epoch 541/3000\n",
      "2370/2370 [==============================] - 0s 81us/step - loss: 0.0051 - val_loss: 0.0378\n",
      "Epoch 542/3000\n",
      "2370/2370 [==============================] - 0s 85us/step - loss: 0.0053 - val_loss: 0.0375\n",
      "Epoch 543/3000\n",
      "2370/2370 [==============================] - 0s 84us/step - loss: 0.0051 - val_loss: 0.0385\n",
      "Epoch 544/3000\n",
      "2370/2370 [==============================] - 0s 80us/step - loss: 0.0051 - val_loss: 0.0378\n",
      "Epoch 545/3000\n",
      "2370/2370 [==============================] - 0s 72us/step - loss: 0.0053 - val_loss: 0.0384\n",
      "Epoch 546/3000\n",
      "2370/2370 [==============================] - 0s 70us/step - loss: 0.0054 - val_loss: 0.0373\n",
      "Epoch 547/3000\n",
      "2370/2370 [==============================] - 0s 86us/step - loss: 0.0052 - val_loss: 0.0381\n",
      "Epoch 548/3000\n",
      "2370/2370 [==============================] - 0s 80us/step - loss: 0.0052 - val_loss: 0.0380\n",
      "Epoch 549/3000\n",
      "2370/2370 [==============================] - 0s 86us/step - loss: 0.0053 - val_loss: 0.0382\n",
      "Epoch 550/3000\n",
      "2370/2370 [==============================] - 0s 83us/step - loss: 0.0052 - val_loss: 0.0378\n",
      "Epoch 551/3000\n",
      "2370/2370 [==============================] - 0s 73us/step - loss: 0.0051 - val_loss: 0.0383\n",
      "Epoch 552/3000\n",
      "2370/2370 [==============================] - 0s 70us/step - loss: 0.0051 - val_loss: 0.0371\n",
      "Epoch 553/3000\n",
      "2370/2370 [==============================] - 0s 67us/step - loss: 0.0052 - val_loss: 0.0376\n",
      "Epoch 554/3000\n",
      "2370/2370 [==============================] - 0s 73us/step - loss: 0.0052 - val_loss: 0.0381\n",
      "Epoch 555/3000\n",
      "2370/2370 [==============================] - 0s 73us/step - loss: 0.0051 - val_loss: 0.0376\n",
      "Epoch 556/3000\n",
      "2370/2370 [==============================] - 0s 69us/step - loss: 0.0053 - val_loss: 0.0376\n",
      "Epoch 557/3000\n",
      "2370/2370 [==============================] - 0s 70us/step - loss: 0.0053 - val_loss: 0.0376\n",
      "Epoch 558/3000\n",
      "2370/2370 [==============================] - 0s 63us/step - loss: 0.0055 - val_loss: 0.0387\n",
      "Epoch 559/3000\n",
      "2370/2370 [==============================] - 0s 80us/step - loss: 0.0052 - val_loss: 0.0377\n",
      "Epoch 560/3000\n",
      "2370/2370 [==============================] - 0s 72us/step - loss: 0.0051 - val_loss: 0.0383\n",
      "Epoch 561/3000\n",
      "2370/2370 [==============================] - 0s 68us/step - loss: 0.0053 - val_loss: 0.0369\n",
      "Epoch 562/3000\n",
      "2370/2370 [==============================] - 0s 73us/step - loss: 0.0053 - val_loss: 0.0378\n",
      "Epoch 563/3000\n",
      "2370/2370 [==============================] - 0s 78us/step - loss: 0.0053 - val_loss: 0.0379\n",
      "Epoch 564/3000\n",
      "2370/2370 [==============================] - 0s 86us/step - loss: 0.0053 - val_loss: 0.0373\n",
      "Epoch 565/3000\n",
      "2370/2370 [==============================] - 0s 81us/step - loss: 0.0054 - val_loss: 0.0384\n",
      "Epoch 566/3000\n",
      "2370/2370 [==============================] - 0s 67us/step - loss: 0.0053 - val_loss: 0.0381\n",
      "Epoch 567/3000\n",
      "2370/2370 [==============================] - 0s 72us/step - loss: 0.0053 - val_loss: 0.0386\n",
      "Epoch 568/3000\n",
      "2370/2370 [==============================] - 0s 73us/step - loss: 0.0052 - val_loss: 0.0377\n",
      "Epoch 569/3000\n",
      "2370/2370 [==============================] - 0s 79us/step - loss: 0.0054 - val_loss: 0.0383\n",
      "Epoch 570/3000\n",
      "2370/2370 [==============================] - 0s 78us/step - loss: 0.0051 - val_loss: 0.0374\n",
      "Epoch 571/3000\n",
      "2370/2370 [==============================] - 0s 82us/step - loss: 0.0052 - val_loss: 0.0380\n",
      "Epoch 572/3000\n",
      "2370/2370 [==============================] - 0s 81us/step - loss: 0.0052 - val_loss: 0.0386\n",
      "Epoch 573/3000\n",
      "2370/2370 [==============================] - 0s 81us/step - loss: 0.0053 - val_loss: 0.0376\n",
      "Epoch 574/3000\n",
      "2370/2370 [==============================] - 0s 91us/step - loss: 0.0052 - val_loss: 0.0378\n",
      "Epoch 575/3000\n",
      "2370/2370 [==============================] - 0s 80us/step - loss: 0.0052 - val_loss: 0.0379\n",
      "Epoch 576/3000\n",
      "2370/2370 [==============================] - 0s 75us/step - loss: 0.0052 - val_loss: 0.0378\n",
      "Epoch 577/3000\n",
      "2370/2370 [==============================] - 0s 69us/step - loss: 0.0052 - val_loss: 0.0374\n",
      "Epoch 578/3000\n",
      "2370/2370 [==============================] - 0s 81us/step - loss: 0.0053 - val_loss: 0.0381\n",
      "Epoch 579/3000\n",
      "2370/2370 [==============================] - 0s 66us/step - loss: 0.0053 - val_loss: 0.0368\n",
      "Epoch 580/3000\n",
      "2370/2370 [==============================] - 0s 65us/step - loss: 0.0053 - val_loss: 0.0375\n",
      "Epoch 581/3000\n",
      "2370/2370 [==============================] - 0s 61us/step - loss: 0.0053 - val_loss: 0.0374\n",
      "Epoch 582/3000\n",
      "2370/2370 [==============================] - 0s 63us/step - loss: 0.0052 - val_loss: 0.0379\n",
      "Epoch 583/3000\n",
      "2370/2370 [==============================] - 0s 76us/step - loss: 0.0052 - val_loss: 0.0379\n",
      "Epoch 584/3000\n",
      "2370/2370 [==============================] - 0s 80us/step - loss: 0.0052 - val_loss: 0.0382\n",
      "Epoch 585/3000\n",
      "2370/2370 [==============================] - 0s 70us/step - loss: 0.0052 - val_loss: 0.0377\n",
      "Epoch 586/3000\n",
      "2370/2370 [==============================] - 0s 68us/step - loss: 0.0053 - val_loss: 0.0368\n",
      "Epoch 587/3000\n",
      "2370/2370 [==============================] - 0s 63us/step - loss: 0.0055 - val_loss: 0.0375\n",
      "Epoch 588/3000\n",
      "2370/2370 [==============================] - 0s 83us/step - loss: 0.0052 - val_loss: 0.0375\n",
      "Epoch 589/3000\n",
      "2370/2370 [==============================] - 0s 69us/step - loss: 0.0052 - val_loss: 0.0372\n",
      "Epoch 590/3000\n",
      "2370/2370 [==============================] - 0s 64us/step - loss: 0.0051 - val_loss: 0.0379\n",
      "Epoch 591/3000\n",
      "2370/2370 [==============================] - 0s 67us/step - loss: 0.0052 - val_loss: 0.0383\n",
      "Epoch 592/3000\n",
      "2370/2370 [==============================] - 0s 70us/step - loss: 0.0050 - val_loss: 0.0388\n",
      "Epoch 593/3000\n",
      "2370/2370 [==============================] - 0s 63us/step - loss: 0.0050 - val_loss: 0.0380\n",
      "Epoch 594/3000\n",
      "2370/2370 [==============================] - 0s 80us/step - loss: 0.0055 - val_loss: 0.0385\n",
      "Epoch 595/3000\n",
      "2370/2370 [==============================] - 0s 78us/step - loss: 0.0051 - val_loss: 0.0380\n",
      "Epoch 596/3000\n",
      "2370/2370 [==============================] - 0s 77us/step - loss: 0.0052 - val_loss: 0.0379\n",
      "Epoch 597/3000\n",
      "2370/2370 [==============================] - 0s 69us/step - loss: 0.0052 - val_loss: 0.0377\n",
      "Epoch 598/3000\n",
      "2370/2370 [==============================] - 0s 76us/step - loss: 0.0051 - val_loss: 0.0376\n",
      "Epoch 599/3000\n",
      "2370/2370 [==============================] - 0s 74us/step - loss: 0.0051 - val_loss: 0.0379\n",
      "Epoch 600/3000\n",
      "2370/2370 [==============================] - 0s 70us/step - loss: 0.0051 - val_loss: 0.0380\n",
      "Epoch 601/3000\n",
      "2370/2370 [==============================] - 0s 85us/step - loss: 0.0052 - val_loss: 0.0375\n",
      "Epoch 602/3000\n",
      "2370/2370 [==============================] - 0s 86us/step - loss: 0.0053 - val_loss: 0.0378\n",
      "Epoch 603/3000\n",
      "2370/2370 [==============================] - 0s 79us/step - loss: 0.0051 - val_loss: 0.0379\n",
      "Epoch 604/3000\n",
      "2370/2370 [==============================] - 0s 77us/step - loss: 0.0050 - val_loss: 0.0392\n",
      "Epoch 605/3000\n",
      "2370/2370 [==============================] - 0s 78us/step - loss: 0.0052 - val_loss: 0.0387\n",
      "Epoch 606/3000\n",
      "2370/2370 [==============================] - 0s 70us/step - loss: 0.0052 - val_loss: 0.0388\n",
      "Epoch 607/3000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2370/2370 [==============================] - 0s 84us/step - loss: 0.0051 - val_loss: 0.0384\n",
      "Epoch 608/3000\n",
      "2370/2370 [==============================] - 0s 98us/step - loss: 0.0051 - val_loss: 0.0383\n",
      "Epoch 609/3000\n",
      "2370/2370 [==============================] - 0s 94us/step - loss: 0.0053 - val_loss: 0.0380\n",
      "Epoch 610/3000\n",
      "2370/2370 [==============================] - 0s 81us/step - loss: 0.0052 - val_loss: 0.0379\n",
      "Epoch 611/3000\n",
      "2370/2370 [==============================] - 0s 79us/step - loss: 0.0052 - val_loss: 0.0379\n",
      "Epoch 612/3000\n",
      "2370/2370 [==============================] - 0s 87us/step - loss: 0.0051 - val_loss: 0.0384\n",
      "Epoch 613/3000\n",
      "2370/2370 [==============================] - 0s 93us/step - loss: 0.0050 - val_loss: 0.0389\n",
      "Epoch 614/3000\n",
      "2370/2370 [==============================] - 0s 80us/step - loss: 0.0052 - val_loss: 0.0386\n",
      "Epoch 615/3000\n",
      "2370/2370 [==============================] - 0s 90us/step - loss: 0.0055 - val_loss: 0.0379\n",
      "Epoch 616/3000\n",
      "2370/2370 [==============================] - 0s 88us/step - loss: 0.0051 - val_loss: 0.0381\n",
      "Epoch 617/3000\n",
      "2370/2370 [==============================] - 0s 84us/step - loss: 0.0050 - val_loss: 0.0385\n",
      "Epoch 618/3000\n",
      "2370/2370 [==============================] - 0s 79us/step - loss: 0.0052 - val_loss: 0.0390\n",
      "Epoch 619/3000\n",
      "2370/2370 [==============================] - 0s 65us/step - loss: 0.0052 - val_loss: 0.0380\n",
      "Epoch 620/3000\n",
      "2370/2370 [==============================] - 0s 97us/step - loss: 0.0052 - val_loss: 0.0378\n",
      "Epoch 621/3000\n",
      "2370/2370 [==============================] - 0s 83us/step - loss: 0.0053 - val_loss: 0.0375\n",
      "Epoch 622/3000\n",
      "2370/2370 [==============================] - 0s 68us/step - loss: 0.0051 - val_loss: 0.0379\n",
      "Epoch 623/3000\n",
      "2370/2370 [==============================] - 0s 85us/step - loss: 0.0053 - val_loss: 0.0379\n",
      "Epoch 624/3000\n",
      "2370/2370 [==============================] - 0s 81us/step - loss: 0.0051 - val_loss: 0.0375\n",
      "Epoch 625/3000\n",
      "2370/2370 [==============================] - 0s 69us/step - loss: 0.0052 - val_loss: 0.0377\n",
      "Epoch 626/3000\n",
      "2370/2370 [==============================] - 0s 64us/step - loss: 0.0051 - val_loss: 0.0380\n",
      "Epoch 627/3000\n",
      "2370/2370 [==============================] - 0s 74us/step - loss: 0.0053 - val_loss: 0.0373\n",
      "Epoch 628/3000\n",
      "2370/2370 [==============================] - 0s 70us/step - loss: 0.0052 - val_loss: 0.0383\n",
      "Epoch 629/3000\n",
      "2370/2370 [==============================] - 0s 74us/step - loss: 0.0051 - val_loss: 0.0372\n",
      "Epoch 630/3000\n",
      "2370/2370 [==============================] - 0s 76us/step - loss: 0.0053 - val_loss: 0.0373\n",
      "Epoch 631/3000\n",
      "2370/2370 [==============================] - 0s 65us/step - loss: 0.0053 - val_loss: 0.0380\n",
      "Epoch 632/3000\n",
      "2370/2370 [==============================] - 0s 67us/step - loss: 0.0051 - val_loss: 0.0380\n",
      "Epoch 633/3000\n",
      "2370/2370 [==============================] - 0s 78us/step - loss: 0.0051 - val_loss: 0.0385\n",
      "Epoch 634/3000\n",
      "2370/2370 [==============================] - 0s 63us/step - loss: 0.0052 - val_loss: 0.0381\n",
      "Epoch 635/3000\n",
      "2370/2370 [==============================] - 0s 79us/step - loss: 0.0053 - val_loss: 0.0377\n",
      "Epoch 636/3000\n",
      "2370/2370 [==============================] - 0s 78us/step - loss: 0.0054 - val_loss: 0.0369\n",
      "Epoch 637/3000\n",
      "2370/2370 [==============================] - 0s 65us/step - loss: 0.0050 - val_loss: 0.0383\n",
      "Epoch 638/3000\n",
      "2370/2370 [==============================] - 0s 79us/step - loss: 0.0051 - val_loss: 0.0378\n",
      "Epoch 639/3000\n",
      "2370/2370 [==============================] - 0s 70us/step - loss: 0.0054 - val_loss: 0.0380\n",
      "Epoch 640/3000\n",
      "2370/2370 [==============================] - 0s 68us/step - loss: 0.0052 - val_loss: 0.0386\n",
      "Epoch 641/3000\n",
      "2370/2370 [==============================] - 0s 70us/step - loss: 0.0052 - val_loss: 0.0387\n",
      "Epoch 642/3000\n",
      "2370/2370 [==============================] - 0s 68us/step - loss: 0.0052 - val_loss: 0.0386\n",
      "Epoch 643/3000\n",
      "2370/2370 [==============================] - 0s 69us/step - loss: 0.0051 - val_loss: 0.0385\n",
      "Epoch 644/3000\n",
      "2370/2370 [==============================] - 0s 82us/step - loss: 0.0052 - val_loss: 0.0388\n",
      "Epoch 645/3000\n",
      "2370/2370 [==============================] - 0s 73us/step - loss: 0.0050 - val_loss: 0.0384\n",
      "Epoch 646/3000\n",
      "2370/2370 [==============================] - 0s 77us/step - loss: 0.0051 - val_loss: 0.0385\n",
      "Epoch 647/3000\n",
      "2370/2370 [==============================] - 0s 75us/step - loss: 0.0050 - val_loss: 0.0380\n",
      "Epoch 648/3000\n",
      "2370/2370 [==============================] - 0s 64us/step - loss: 0.0051 - val_loss: 0.0385\n",
      "Epoch 649/3000\n",
      "2370/2370 [==============================] - 0s 69us/step - loss: 0.0052 - val_loss: 0.0373\n",
      "Epoch 650/3000\n",
      "2370/2370 [==============================] - 0s 77us/step - loss: 0.0052 - val_loss: 0.0384\n",
      "Epoch 651/3000\n",
      "2370/2370 [==============================] - 0s 65us/step - loss: 0.0052 - val_loss: 0.0385\n",
      "Epoch 652/3000\n",
      "2370/2370 [==============================] - 0s 72us/step - loss: 0.0051 - val_loss: 0.0380\n",
      "Epoch 653/3000\n",
      "2370/2370 [==============================] - 0s 73us/step - loss: 0.0051 - val_loss: 0.0384\n",
      "Epoch 654/3000\n",
      "2370/2370 [==============================] - 0s 68us/step - loss: 0.0053 - val_loss: 0.0375\n",
      "Epoch 655/3000\n",
      "2370/2370 [==============================] - 0s 80us/step - loss: 0.0052 - val_loss: 0.0369\n",
      "Epoch 656/3000\n",
      "2370/2370 [==============================] - 0s 76us/step - loss: 0.0050 - val_loss: 0.0379\n",
      "Epoch 657/3000\n",
      "2370/2370 [==============================] - 0s 81us/step - loss: 0.0050 - val_loss: 0.0373\n",
      "Epoch 658/3000\n",
      "2370/2370 [==============================] - 0s 64us/step - loss: 0.0052 - val_loss: 0.0377\n",
      "Epoch 659/3000\n",
      "2370/2370 [==============================] - 0s 73us/step - loss: 0.0052 - val_loss: 0.0382\n",
      "Epoch 660/3000\n",
      "2370/2370 [==============================] - 0s 77us/step - loss: 0.0051 - val_loss: 0.0371\n",
      "Epoch 661/3000\n",
      "2370/2370 [==============================] - 0s 74us/step - loss: 0.0052 - val_loss: 0.0373\n",
      "Epoch 662/3000\n",
      "2370/2370 [==============================] - 0s 70us/step - loss: 0.0051 - val_loss: 0.0380\n",
      "Epoch 663/3000\n",
      "2370/2370 [==============================] - 0s 65us/step - loss: 0.0052 - val_loss: 0.0377\n",
      "Epoch 664/3000\n",
      "2370/2370 [==============================] - 0s 69us/step - loss: 0.0053 - val_loss: 0.0375\n",
      "Epoch 665/3000\n",
      "2370/2370 [==============================] - 0s 73us/step - loss: 0.0051 - val_loss: 0.0381\n",
      "Epoch 666/3000\n",
      "2370/2370 [==============================] - 0s 78us/step - loss: 0.0051 - val_loss: 0.0376\n",
      "Epoch 667/3000\n",
      "2370/2370 [==============================] - 0s 70us/step - loss: 0.0050 - val_loss: 0.0374\n",
      "Epoch 668/3000\n",
      "2370/2370 [==============================] - 0s 73us/step - loss: 0.0052 - val_loss: 0.0372\n",
      "Epoch 669/3000\n",
      "2370/2370 [==============================] - 0s 72us/step - loss: 0.0052 - val_loss: 0.0377\n",
      "Epoch 670/3000\n",
      "2370/2370 [==============================] - 0s 70us/step - loss: 0.0051 - val_loss: 0.0371\n",
      "Epoch 671/3000\n",
      "2370/2370 [==============================] - 0s 70us/step - loss: 0.0051 - val_loss: 0.0376\n",
      "Epoch 672/3000\n",
      "2370/2370 [==============================] - 0s 69us/step - loss: 0.0051 - val_loss: 0.0380\n",
      "Epoch 673/3000\n",
      "2370/2370 [==============================] - 0s 67us/step - loss: 0.0052 - val_loss: 0.0378\n",
      "Epoch 674/3000\n",
      "2370/2370 [==============================] - 0s 73us/step - loss: 0.0052 - val_loss: 0.0381\n",
      "Epoch 675/3000\n",
      "2370/2370 [==============================] - 0s 84us/step - loss: 0.0052 - val_loss: 0.0377\n",
      "Epoch 676/3000\n",
      "2370/2370 [==============================] - 0s 70us/step - loss: 0.0053 - val_loss: 0.0376\n",
      "Epoch 677/3000\n",
      "2370/2370 [==============================] - 0s 81us/step - loss: 0.0052 - val_loss: 0.0375\n",
      "Epoch 678/3000\n",
      "2370/2370 [==============================] - 0s 85us/step - loss: 0.0052 - val_loss: 0.0373\n",
      "Epoch 679/3000\n",
      "2370/2370 [==============================] - 0s 78us/step - loss: 0.0052 - val_loss: 0.0377\n",
      "Epoch 680/3000\n",
      "2370/2370 [==============================] - 0s 63us/step - loss: 0.0052 - val_loss: 0.0377\n",
      "Epoch 681/3000\n",
      "2370/2370 [==============================] - 0s 68us/step - loss: 0.0051 - val_loss: 0.0370\n",
      "Epoch 682/3000\n",
      "2370/2370 [==============================] - 0s 88us/step - loss: 0.0051 - val_loss: 0.0374\n",
      "Epoch 683/3000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2370/2370 [==============================] - 0s 99us/step - loss: 0.0050 - val_loss: 0.0381\n",
      "Epoch 684/3000\n",
      "2370/2370 [==============================] - 0s 77us/step - loss: 0.0052 - val_loss: 0.0372\n",
      "Epoch 685/3000\n",
      "2370/2370 [==============================] - 0s 76us/step - loss: 0.0050 - val_loss: 0.0374\n",
      "Epoch 686/3000\n",
      "2370/2370 [==============================] - 0s 74us/step - loss: 0.0052 - val_loss: 0.0373\n",
      "Epoch 687/3000\n",
      "2370/2370 [==============================] - 0s 92us/step - loss: 0.0052 - val_loss: 0.0372\n",
      "Epoch 688/3000\n",
      "2370/2370 [==============================] - 0s 83us/step - loss: 0.0053 - val_loss: 0.0374\n",
      "Epoch 689/3000\n",
      "2370/2370 [==============================] - 0s 83us/step - loss: 0.0051 - val_loss: 0.0381\n",
      "Epoch 690/3000\n",
      "2370/2370 [==============================] - 0s 78us/step - loss: 0.0051 - val_loss: 0.0376\n",
      "Epoch 691/3000\n",
      "2370/2370 [==============================] - 0s 79us/step - loss: 0.0051 - val_loss: 0.0379\n",
      "Epoch 692/3000\n",
      "2370/2370 [==============================] - 0s 78us/step - loss: 0.0051 - val_loss: 0.0379\n",
      "Epoch 693/3000\n",
      "2370/2370 [==============================] - 0s 71us/step - loss: 0.0052 - val_loss: 0.0383\n",
      "Epoch 694/3000\n",
      "2370/2370 [==============================] - 0s 69us/step - loss: 0.0050 - val_loss: 0.0382\n",
      "Epoch 695/3000\n",
      "2370/2370 [==============================] - 0s 82us/step - loss: 0.0049 - val_loss: 0.0373\n",
      "Epoch 696/3000\n",
      "2370/2370 [==============================] - 0s 105us/step - loss: 0.0050 - val_loss: 0.0384\n",
      "Epoch 697/3000\n",
      "2370/2370 [==============================] - 0s 87us/step - loss: 0.0051 - val_loss: 0.0391\n",
      "Epoch 698/3000\n",
      "2370/2370 [==============================] - 0s 70us/step - loss: 0.0051 - val_loss: 0.0386\n",
      "Epoch 699/3000\n",
      "2370/2370 [==============================] - 0s 68us/step - loss: 0.0052 - val_loss: 0.0385\n",
      "Epoch 700/3000\n",
      "2370/2370 [==============================] - 0s 90us/step - loss: 0.0049 - val_loss: 0.0382\n",
      "Epoch 701/3000\n",
      "2370/2370 [==============================] - 0s 92us/step - loss: 0.0049 - val_loss: 0.0383\n",
      "Epoch 702/3000\n",
      "2370/2370 [==============================] - 0s 94us/step - loss: 0.0052 - val_loss: 0.0376\n",
      "Epoch 703/3000\n",
      "2370/2370 [==============================] - 0s 98us/step - loss: 0.0051 - val_loss: 0.0372\n",
      "Epoch 704/3000\n",
      "2370/2370 [==============================] - 0s 94us/step - loss: 0.0051 - val_loss: 0.0374\n",
      "Epoch 705/3000\n",
      "2370/2370 [==============================] - 0s 104us/step - loss: 0.0050 - val_loss: 0.0383\n",
      "Epoch 706/3000\n",
      "2370/2370 [==============================] - 0s 104us/step - loss: 0.0051 - val_loss: 0.0386\n",
      "Epoch 707/3000\n",
      "2370/2370 [==============================] - 0s 98us/step - loss: 0.0049 - val_loss: 0.0381\n",
      "Epoch 708/3000\n",
      "2370/2370 [==============================] - 0s 98us/step - loss: 0.0050 - val_loss: 0.0384\n",
      "Epoch 709/3000\n",
      "2370/2370 [==============================] - 0s 86us/step - loss: 0.0051 - val_loss: 0.0380\n",
      "Epoch 710/3000\n",
      "2370/2370 [==============================] - 0s 96us/step - loss: 0.0049 - val_loss: 0.0386\n",
      "Epoch 711/3000\n",
      "2370/2370 [==============================] - 0s 91us/step - loss: 0.0052 - val_loss: 0.0380\n",
      "Epoch 712/3000\n",
      "2370/2370 [==============================] - 0s 85us/step - loss: 0.0050 - val_loss: 0.0381\n",
      "Epoch 713/3000\n",
      "2370/2370 [==============================] - 0s 85us/step - loss: 0.0051 - val_loss: 0.0382\n",
      "Epoch 714/3000\n",
      "2370/2370 [==============================] - 0s 77us/step - loss: 0.0050 - val_loss: 0.0381\n",
      "Epoch 715/3000\n",
      "2370/2370 [==============================] - 0s 67us/step - loss: 0.0051 - val_loss: 0.0374\n",
      "Epoch 716/3000\n",
      "2370/2370 [==============================] - 0s 84us/step - loss: 0.0052 - val_loss: 0.0375\n",
      "Epoch 717/3000\n",
      "2370/2370 [==============================] - 0s 92us/step - loss: 0.0050 - val_loss: 0.0379\n",
      "Epoch 718/3000\n",
      "2370/2370 [==============================] - 0s 83us/step - loss: 0.0050 - val_loss: 0.0376\n",
      "Epoch 719/3000\n",
      "2370/2370 [==============================] - 0s 93us/step - loss: 0.0051 - val_loss: 0.0381\n",
      "Epoch 720/3000\n",
      "2370/2370 [==============================] - 0s 69us/step - loss: 0.0050 - val_loss: 0.0388\n",
      "Epoch 721/3000\n",
      "2370/2370 [==============================] - 0s 71us/step - loss: 0.0051 - val_loss: 0.0381\n",
      "Epoch 722/3000\n",
      "2370/2370 [==============================] - 0s 68us/step - loss: 0.0049 - val_loss: 0.0383\n",
      "Epoch 723/3000\n",
      "2370/2370 [==============================] - 0s 78us/step - loss: 0.0051 - val_loss: 0.0379\n",
      "Epoch 724/3000\n",
      "2370/2370 [==============================] - 0s 72us/step - loss: 0.0051 - val_loss: 0.0384\n",
      "Epoch 725/3000\n",
      "2370/2370 [==============================] - 0s 63us/step - loss: 0.0051 - val_loss: 0.0373\n",
      "Epoch 726/3000\n",
      "2370/2370 [==============================] - 0s 63us/step - loss: 0.0052 - val_loss: 0.0375\n",
      "Epoch 727/3000\n",
      "2370/2370 [==============================] - 0s 73us/step - loss: 0.0052 - val_loss: 0.0382\n",
      "Epoch 728/3000\n",
      "2370/2370 [==============================] - 0s 84us/step - loss: 0.0052 - val_loss: 0.0400\n",
      "Epoch 729/3000\n",
      "2370/2370 [==============================] - 0s 75us/step - loss: 0.0051 - val_loss: 0.0376\n",
      "Epoch 730/3000\n",
      "2370/2370 [==============================] - 0s 74us/step - loss: 0.0050 - val_loss: 0.0386\n",
      "Epoch 731/3000\n",
      "2370/2370 [==============================] - 0s 83us/step - loss: 0.0052 - val_loss: 0.0381\n",
      "Epoch 732/3000\n",
      "2370/2370 [==============================] - 0s 85us/step - loss: 0.0050 - val_loss: 0.0386\n",
      "Epoch 733/3000\n",
      "2370/2370 [==============================] - 0s 74us/step - loss: 0.0050 - val_loss: 0.0374\n",
      "Epoch 734/3000\n",
      "2370/2370 [==============================] - 0s 72us/step - loss: 0.0053 - val_loss: 0.0377\n",
      "Epoch 735/3000\n",
      "2370/2370 [==============================] - 0s 76us/step - loss: 0.0050 - val_loss: 0.0370\n",
      "Epoch 736/3000\n",
      "2370/2370 [==============================] - 0s 66us/step - loss: 0.0051 - val_loss: 0.0383\n",
      "Epoch 737/3000\n",
      "2370/2370 [==============================] - 0s 68us/step - loss: 0.0051 - val_loss: 0.0382\n",
      "Epoch 738/3000\n",
      "2370/2370 [==============================] - 0s 81us/step - loss: 0.0052 - val_loss: 0.0376\n",
      "Epoch 739/3000\n",
      "2370/2370 [==============================] - 0s 78us/step - loss: 0.0052 - val_loss: 0.0374\n",
      "Epoch 740/3000\n",
      "2370/2370 [==============================] - 0s 84us/step - loss: 0.0051 - val_loss: 0.0377\n",
      "Epoch 741/3000\n",
      "2370/2370 [==============================] - 0s 77us/step - loss: 0.0050 - val_loss: 0.0380\n",
      "Epoch 742/3000\n",
      "2370/2370 [==============================] - 0s 84us/step - loss: 0.0050 - val_loss: 0.0379\n",
      "Epoch 743/3000\n",
      "2370/2370 [==============================] - 0s 81us/step - loss: 0.0050 - val_loss: 0.0385\n",
      "Epoch 744/3000\n",
      "2370/2370 [==============================] - 0s 67us/step - loss: 0.0050 - val_loss: 0.0377\n",
      "Epoch 745/3000\n",
      "2370/2370 [==============================] - 0s 76us/step - loss: 0.0052 - val_loss: 0.0376\n",
      "Epoch 746/3000\n",
      "2370/2370 [==============================] - 0s 96us/step - loss: 0.0051 - val_loss: 0.0380\n",
      "Epoch 747/3000\n",
      "2370/2370 [==============================] - 0s 105us/step - loss: 0.0050 - val_loss: 0.0374\n",
      "Epoch 748/3000\n",
      "2370/2370 [==============================] - 0s 111us/step - loss: 0.0050 - val_loss: 0.0380\n",
      "Epoch 749/3000\n",
      "2370/2370 [==============================] - 0s 94us/step - loss: 0.0050 - val_loss: 0.0382\n",
      "Epoch 750/3000\n",
      "2370/2370 [==============================] - 0s 86us/step - loss: 0.0049 - val_loss: 0.0376\n",
      "Epoch 751/3000\n",
      "2370/2370 [==============================] - 0s 75us/step - loss: 0.0049 - val_loss: 0.0376\n",
      "Epoch 752/3000\n",
      "2370/2370 [==============================] - 0s 69us/step - loss: 0.0050 - val_loss: 0.0377\n",
      "Epoch 753/3000\n",
      "2370/2370 [==============================] - 0s 88us/step - loss: 0.0051 - val_loss: 0.0372\n",
      "Epoch 754/3000\n",
      "2370/2370 [==============================] - 0s 83us/step - loss: 0.0050 - val_loss: 0.0367\n",
      "Epoch 755/3000\n",
      "2370/2370 [==============================] - 0s 82us/step - loss: 0.0051 - val_loss: 0.0374\n",
      "Epoch 756/3000\n",
      "2370/2370 [==============================] - 0s 68us/step - loss: 0.0052 - val_loss: 0.0379\n",
      "Epoch 757/3000\n",
      "2370/2370 [==============================] - 0s 81us/step - loss: 0.0050 - val_loss: 0.0379\n",
      "Epoch 758/3000\n",
      "2370/2370 [==============================] - 0s 100us/step - loss: 0.0053 - val_loss: 0.0370\n",
      "Epoch 759/3000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2370/2370 [==============================] - 0s 81us/step - loss: 0.0050 - val_loss: 0.0380\n",
      "Epoch 760/3000\n",
      "2370/2370 [==============================] - 0s 73us/step - loss: 0.0050 - val_loss: 0.0375\n",
      "Epoch 761/3000\n",
      "2370/2370 [==============================] - 0s 60us/step - loss: 0.0049 - val_loss: 0.0381\n",
      "Epoch 762/3000\n",
      "2370/2370 [==============================] - 0s 66us/step - loss: 0.0051 - val_loss: 0.0373\n",
      "Epoch 763/3000\n",
      "2370/2370 [==============================] - 0s 72us/step - loss: 0.0049 - val_loss: 0.0384\n",
      "Epoch 764/3000\n",
      "2370/2370 [==============================] - 0s 100us/step - loss: 0.0051 - val_loss: 0.0381\n",
      "Epoch 765/3000\n",
      "2370/2370 [==============================] - 0s 85us/step - loss: 0.0051 - val_loss: 0.0384\n",
      "Epoch 766/3000\n",
      "2370/2370 [==============================] - 0s 83us/step - loss: 0.0050 - val_loss: 0.0381\n",
      "Epoch 767/3000\n",
      "2370/2370 [==============================] - 0s 73us/step - loss: 0.0051 - val_loss: 0.0376\n",
      "Epoch 768/3000\n",
      "2370/2370 [==============================] - 0s 83us/step - loss: 0.0050 - val_loss: 0.0376\n",
      "Epoch 769/3000\n",
      "2370/2370 [==============================] - 0s 89us/step - loss: 0.0051 - val_loss: 0.0377\n",
      "Epoch 770/3000\n",
      "2370/2370 [==============================] - 0s 79us/step - loss: 0.0050 - val_loss: 0.0381\n",
      "Epoch 771/3000\n",
      "2370/2370 [==============================] - 0s 81us/step - loss: 0.0051 - val_loss: 0.0385\n",
      "Epoch 772/3000\n",
      "2370/2370 [==============================] - 0s 86us/step - loss: 0.0050 - val_loss: 0.0382\n",
      "Epoch 773/3000\n",
      "2370/2370 [==============================] - 0s 79us/step - loss: 0.0052 - val_loss: 0.0378\n",
      "Epoch 774/3000\n",
      "2370/2370 [==============================] - 0s 77us/step - loss: 0.0050 - val_loss: 0.0377\n",
      "Epoch 775/3000\n",
      "2370/2370 [==============================] - 0s 78us/step - loss: 0.0049 - val_loss: 0.0386\n",
      "Epoch 776/3000\n",
      "2370/2370 [==============================] - 0s 77us/step - loss: 0.0048 - val_loss: 0.0385\n",
      "Epoch 777/3000\n",
      "2370/2370 [==============================] - 0s 71us/step - loss: 0.0050 - val_loss: 0.0376\n",
      "Epoch 778/3000\n",
      "2370/2370 [==============================] - 0s 81us/step - loss: 0.0049 - val_loss: 0.0384\n",
      "Epoch 779/3000\n",
      "2370/2370 [==============================] - 0s 82us/step - loss: 0.0051 - val_loss: 0.0381\n",
      "Epoch 780/3000\n",
      "2370/2370 [==============================] - 0s 80us/step - loss: 0.0050 - val_loss: 0.0379\n",
      "Epoch 781/3000\n",
      "2370/2370 [==============================] - 0s 70us/step - loss: 0.0047 - val_loss: 0.0380\n",
      "Epoch 782/3000\n",
      "2370/2370 [==============================] - 0s 68us/step - loss: 0.0050 - val_loss: 0.0382\n",
      "Epoch 783/3000\n",
      "2370/2370 [==============================] - 0s 83us/step - loss: 0.0049 - val_loss: 0.0387\n",
      "Epoch 784/3000\n",
      "2370/2370 [==============================] - 0s 91us/step - loss: 0.0052 - val_loss: 0.0379\n",
      "Epoch 785/3000\n",
      "2370/2370 [==============================] - 0s 95us/step - loss: 0.0051 - val_loss: 0.0378\n",
      "Epoch 786/3000\n",
      "2370/2370 [==============================] - 0s 82us/step - loss: 0.0053 - val_loss: 0.0374\n",
      "Epoch 787/3000\n",
      "2370/2370 [==============================] - 0s 71us/step - loss: 0.0052 - val_loss: 0.0381\n",
      "Epoch 788/3000\n",
      "2370/2370 [==============================] - 0s 74us/step - loss: 0.0051 - val_loss: 0.0386\n",
      "Epoch 789/3000\n",
      "2370/2370 [==============================] - 0s 71us/step - loss: 0.0051 - val_loss: 0.0386\n",
      "Epoch 790/3000\n",
      "2370/2370 [==============================] - 0s 67us/step - loss: 0.0050 - val_loss: 0.0382\n",
      "Epoch 791/3000\n",
      "2370/2370 [==============================] - 0s 72us/step - loss: 0.0051 - val_loss: 0.0371\n",
      "Epoch 792/3000\n",
      "2370/2370 [==============================] - 0s 75us/step - loss: 0.0051 - val_loss: 0.0372\n",
      "Epoch 793/3000\n",
      "2370/2370 [==============================] - 0s 73us/step - loss: 0.0051 - val_loss: 0.0381\n",
      "Epoch 794/3000\n",
      "2370/2370 [==============================] - 0s 61us/step - loss: 0.0052 - val_loss: 0.0377\n",
      "Epoch 795/3000\n",
      "2370/2370 [==============================] - 0s 78us/step - loss: 0.0052 - val_loss: 0.0390\n",
      "Epoch 796/3000\n",
      "2370/2370 [==============================] - 0s 80us/step - loss: 0.0050 - val_loss: 0.0377\n",
      "Epoch 797/3000\n",
      "2370/2370 [==============================] - 0s 83us/step - loss: 0.0049 - val_loss: 0.0375\n",
      "Epoch 798/3000\n",
      "2370/2370 [==============================] - 0s 75us/step - loss: 0.0049 - val_loss: 0.0387\n",
      "Epoch 799/3000\n",
      "2370/2370 [==============================] - 0s 79us/step - loss: 0.0047 - val_loss: 0.0386\n",
      "Epoch 800/3000\n",
      "2370/2370 [==============================] - 0s 85us/step - loss: 0.0049 - val_loss: 0.0391\n",
      "Epoch 801/3000\n",
      "2370/2370 [==============================] - 0s 68us/step - loss: 0.0049 - val_loss: 0.0378\n",
      "Epoch 802/3000\n",
      "2370/2370 [==============================] - 0s 70us/step - loss: 0.0050 - val_loss: 0.0389\n",
      "Epoch 803/3000\n",
      "2370/2370 [==============================] - 0s 79us/step - loss: 0.0050 - val_loss: 0.0378\n",
      "Epoch 804/3000\n",
      "2370/2370 [==============================] - 0s 79us/step - loss: 0.0050 - val_loss: 0.0383\n",
      "Epoch 805/3000\n",
      "2370/2370 [==============================] - 0s 70us/step - loss: 0.0052 - val_loss: 0.0381\n",
      "Epoch 806/3000\n",
      "2370/2370 [==============================] - 0s 80us/step - loss: 0.0049 - val_loss: 0.0382\n",
      "Epoch 807/3000\n",
      "2370/2370 [==============================] - 0s 90us/step - loss: 0.0051 - val_loss: 0.0383\n",
      "Epoch 808/3000\n",
      "2370/2370 [==============================] - 0s 99us/step - loss: 0.0049 - val_loss: 0.0384\n",
      "Epoch 809/3000\n",
      "2370/2370 [==============================] - 0s 84us/step - loss: 0.0048 - val_loss: 0.0380\n",
      "Epoch 810/3000\n",
      "2370/2370 [==============================] - 0s 80us/step - loss: 0.0050 - val_loss: 0.0387\n",
      "Epoch 811/3000\n",
      "2370/2370 [==============================] - 0s 79us/step - loss: 0.0049 - val_loss: 0.0380\n",
      "Epoch 812/3000\n",
      "2370/2370 [==============================] - 0s 79us/step - loss: 0.0052 - val_loss: 0.0382\n",
      "Epoch 813/3000\n",
      "2370/2370 [==============================] - 0s 83us/step - loss: 0.0049 - val_loss: 0.0384\n",
      "Epoch 814/3000\n",
      "2370/2370 [==============================] - 0s 81us/step - loss: 0.0050 - val_loss: 0.0381\n",
      "Epoch 815/3000\n",
      "2370/2370 [==============================] - 0s 78us/step - loss: 0.0048 - val_loss: 0.0390\n",
      "Epoch 816/3000\n",
      "2370/2370 [==============================] - 0s 71us/step - loss: 0.0049 - val_loss: 0.0385\n",
      "Epoch 817/3000\n",
      "2370/2370 [==============================] - 0s 79us/step - loss: 0.0050 - val_loss: 0.0387\n",
      "Epoch 818/3000\n",
      "2370/2370 [==============================] - 0s 83us/step - loss: 0.0051 - val_loss: 0.0390\n",
      "Epoch 819/3000\n",
      "2370/2370 [==============================] - 0s 74us/step - loss: 0.0051 - val_loss: 0.0394\n",
      "Epoch 820/3000\n",
      "2370/2370 [==============================] - 0s 73us/step - loss: 0.0050 - val_loss: 0.0385\n",
      "Epoch 821/3000\n",
      "2370/2370 [==============================] - 0s 65us/step - loss: 0.0051 - val_loss: 0.0379\n",
      "Epoch 822/3000\n",
      "2370/2370 [==============================] - 0s 77us/step - loss: 0.0048 - val_loss: 0.0389\n",
      "Epoch 823/3000\n",
      "2370/2370 [==============================] - 0s 71us/step - loss: 0.0050 - val_loss: 0.0400\n",
      "Epoch 824/3000\n",
      "2370/2370 [==============================] - 0s 73us/step - loss: 0.0049 - val_loss: 0.0395\n",
      "Epoch 825/3000\n",
      "2370/2370 [==============================] - 0s 70us/step - loss: 0.0049 - val_loss: 0.0387\n",
      "Epoch 826/3000\n",
      "2370/2370 [==============================] - 0s 69us/step - loss: 0.0049 - val_loss: 0.0393\n",
      "Epoch 827/3000\n",
      "2370/2370 [==============================] - 0s 66us/step - loss: 0.0049 - val_loss: 0.0399\n",
      "Epoch 828/3000\n",
      "2370/2370 [==============================] - 0s 66us/step - loss: 0.0049 - val_loss: 0.0391\n",
      "Epoch 829/3000\n",
      "2370/2370 [==============================] - 0s 68us/step - loss: 0.0049 - val_loss: 0.0390\n",
      "Epoch 830/3000\n",
      "2370/2370 [==============================] - 0s 71us/step - loss: 0.0049 - val_loss: 0.0376\n",
      "Epoch 831/3000\n",
      "2370/2370 [==============================] - 0s 76us/step - loss: 0.0050 - val_loss: 0.0375\n",
      "Epoch 832/3000\n",
      "2370/2370 [==============================] - 0s 77us/step - loss: 0.0048 - val_loss: 0.0381\n",
      "Epoch 833/3000\n",
      "2370/2370 [==============================] - 0s 93us/step - loss: 0.0050 - val_loss: 0.0381\n",
      "Epoch 834/3000\n",
      "2370/2370 [==============================] - 0s 102us/step - loss: 0.0049 - val_loss: 0.0376\n",
      "Epoch 835/3000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2370/2370 [==============================] - 0s 98us/step - loss: 0.0049 - val_loss: 0.0383\n",
      "Epoch 836/3000\n",
      "2370/2370 [==============================] - 0s 79us/step - loss: 0.0050 - val_loss: 0.0384\n",
      "Epoch 837/3000\n",
      "2370/2370 [==============================] - 0s 78us/step - loss: 0.0049 - val_loss: 0.0378\n",
      "Epoch 838/3000\n",
      "2370/2370 [==============================] - 0s 69us/step - loss: 0.0048 - val_loss: 0.0385\n",
      "Epoch 839/3000\n",
      "2370/2370 [==============================] - 0s 81us/step - loss: 0.0047 - val_loss: 0.0378\n",
      "Epoch 840/3000\n",
      "2370/2370 [==============================] - 0s 67us/step - loss: 0.0049 - val_loss: 0.0380\n",
      "Epoch 841/3000\n",
      "2370/2370 [==============================] - 0s 75us/step - loss: 0.0048 - val_loss: 0.0383\n",
      "Epoch 842/3000\n",
      "2370/2370 [==============================] - 0s 87us/step - loss: 0.0049 - val_loss: 0.0393\n",
      "Epoch 843/3000\n",
      "2370/2370 [==============================] - 0s 73us/step - loss: 0.0049 - val_loss: 0.0383\n",
      "Epoch 844/3000\n",
      "2370/2370 [==============================] - 0s 66us/step - loss: 0.0049 - val_loss: 0.0388\n",
      "Epoch 845/3000\n",
      "2370/2370 [==============================] - 0s 70us/step - loss: 0.0049 - val_loss: 0.0383\n",
      "Epoch 846/3000\n",
      "2370/2370 [==============================] - 0s 77us/step - loss: 0.0050 - val_loss: 0.0380\n",
      "Epoch 847/3000\n",
      "2370/2370 [==============================] - 0s 68us/step - loss: 0.0049 - val_loss: 0.0372\n",
      "Epoch 848/3000\n",
      "2370/2370 [==============================] - 0s 75us/step - loss: 0.0050 - val_loss: 0.0385\n",
      "Epoch 849/3000\n",
      "2370/2370 [==============================] - 0s 78us/step - loss: 0.0048 - val_loss: 0.0384\n",
      "Epoch 850/3000\n",
      "2370/2370 [==============================] - 0s 86us/step - loss: 0.0050 - val_loss: 0.0381\n",
      "Epoch 851/3000\n",
      "2370/2370 [==============================] - 0s 81us/step - loss: 0.0049 - val_loss: 0.0383\n",
      "Epoch 852/3000\n",
      "2370/2370 [==============================] - 0s 83us/step - loss: 0.0050 - val_loss: 0.0384\n",
      "Epoch 853/3000\n",
      "2370/2370 [==============================] - 0s 78us/step - loss: 0.0050 - val_loss: 0.0384\n",
      "Epoch 854/3000\n",
      "2370/2370 [==============================] - 0s 75us/step - loss: 0.0048 - val_loss: 0.0383\n",
      "Epoch 855/3000\n",
      "2370/2370 [==============================] - 0s 72us/step - loss: 0.0050 - val_loss: 0.0386\n",
      "Epoch 856/3000\n",
      "2370/2370 [==============================] - 0s 69us/step - loss: 0.0050 - val_loss: 0.0382\n",
      "Epoch 857/3000\n",
      "2370/2370 [==============================] - 0s 59us/step - loss: 0.0049 - val_loss: 0.0378\n",
      "Epoch 858/3000\n",
      "2370/2370 [==============================] - 0s 76us/step - loss: 0.0051 - val_loss: 0.0380\n",
      "Epoch 859/3000\n",
      "2370/2370 [==============================] - 0s 67us/step - loss: 0.0050 - val_loss: 0.0374\n",
      "Epoch 860/3000\n",
      "2370/2370 [==============================] - 0s 69us/step - loss: 0.0049 - val_loss: 0.0377\n",
      "Epoch 861/3000\n",
      "2370/2370 [==============================] - 0s 79us/step - loss: 0.0050 - val_loss: 0.0384\n",
      "Epoch 862/3000\n",
      "2370/2370 [==============================] - 0s 71us/step - loss: 0.0048 - val_loss: 0.0385\n",
      "Epoch 863/3000\n",
      "2370/2370 [==============================] - 0s 77us/step - loss: 0.0048 - val_loss: 0.0392\n",
      "Epoch 864/3000\n",
      "2370/2370 [==============================] - 0s 76us/step - loss: 0.0050 - val_loss: 0.0385\n",
      "Epoch 865/3000\n",
      "2370/2370 [==============================] - 0s 73us/step - loss: 0.0050 - val_loss: 0.0390\n",
      "Epoch 866/3000\n",
      "2370/2370 [==============================] - 0s 76us/step - loss: 0.0049 - val_loss: 0.0380\n",
      "Epoch 867/3000\n",
      "2370/2370 [==============================] - 0s 76us/step - loss: 0.0049 - val_loss: 0.0390\n",
      "Epoch 868/3000\n",
      "2370/2370 [==============================] - 0s 68us/step - loss: 0.0050 - val_loss: 0.0381\n",
      "Epoch 869/3000\n",
      "2370/2370 [==============================] - 0s 66us/step - loss: 0.0049 - val_loss: 0.0385\n",
      "Epoch 870/3000\n",
      "2370/2370 [==============================] - 0s 80us/step - loss: 0.0049 - val_loss: 0.0391\n",
      "Epoch 871/3000\n",
      "2370/2370 [==============================] - 0s 77us/step - loss: 0.0050 - val_loss: 0.0386\n",
      "Epoch 872/3000\n",
      "2370/2370 [==============================] - 0s 72us/step - loss: 0.0049 - val_loss: 0.0391\n",
      "Epoch 873/3000\n",
      "2370/2370 [==============================] - 0s 76us/step - loss: 0.0050 - val_loss: 0.0387\n",
      "Epoch 874/3000\n",
      "2370/2370 [==============================] - 0s 69us/step - loss: 0.0048 - val_loss: 0.0389\n",
      "Epoch 875/3000\n",
      "2370/2370 [==============================] - 0s 77us/step - loss: 0.0050 - val_loss: 0.0392\n",
      "Epoch 876/3000\n",
      "2370/2370 [==============================] - 0s 66us/step - loss: 0.0049 - val_loss: 0.0389\n",
      "Epoch 877/3000\n",
      "2370/2370 [==============================] - 0s 77us/step - loss: 0.0050 - val_loss: 0.0388\n",
      "Epoch 878/3000\n",
      "2370/2370 [==============================] - 0s 71us/step - loss: 0.0049 - val_loss: 0.0381\n",
      "Epoch 879/3000\n",
      "2370/2370 [==============================] - 0s 64us/step - loss: 0.0050 - val_loss: 0.0383\n",
      "Epoch 880/3000\n",
      "2370/2370 [==============================] - 0s 77us/step - loss: 0.0048 - val_loss: 0.0389\n",
      "Epoch 881/3000\n",
      "2370/2370 [==============================] - 0s 82us/step - loss: 0.0048 - val_loss: 0.0389\n",
      "Epoch 882/3000\n",
      "2370/2370 [==============================] - 0s 73us/step - loss: 0.0049 - val_loss: 0.0392\n",
      "Epoch 883/3000\n",
      "2370/2370 [==============================] - 0s 72us/step - loss: 0.0048 - val_loss: 0.0394\n",
      "Epoch 884/3000\n",
      "2370/2370 [==============================] - 0s 78us/step - loss: 0.0048 - val_loss: 0.0384\n",
      "Epoch 885/3000\n",
      "2370/2370 [==============================] - 0s 65us/step - loss: 0.0048 - val_loss: 0.0384\n",
      "Epoch 886/3000\n",
      "2370/2370 [==============================] - 0s 71us/step - loss: 0.0048 - val_loss: 0.0381\n",
      "Epoch 887/3000\n",
      "2370/2370 [==============================] - 0s 86us/step - loss: 0.0050 - val_loss: 0.0382\n",
      "Epoch 888/3000\n",
      "2370/2370 [==============================] - 0s 64us/step - loss: 0.0047 - val_loss: 0.0386\n",
      "Epoch 889/3000\n",
      "2370/2370 [==============================] - 0s 76us/step - loss: 0.0048 - val_loss: 0.0384\n",
      "Epoch 890/3000\n",
      "2370/2370 [==============================] - 0s 60us/step - loss: 0.0049 - val_loss: 0.0377\n",
      "Epoch 891/3000\n",
      "2370/2370 [==============================] - 0s 87us/step - loss: 0.0048 - val_loss: 0.0387\n",
      "Epoch 892/3000\n",
      "2370/2370 [==============================] - 0s 106us/step - loss: 0.0049 - val_loss: 0.0389\n",
      "Epoch 893/3000\n",
      "2370/2370 [==============================] - 0s 51us/step - loss: 0.0048 - val_loss: 0.0388\n",
      "Epoch 894/3000\n",
      "2370/2370 [==============================] - 0s 63us/step - loss: 0.0049 - val_loss: 0.0395\n",
      "Epoch 895/3000\n",
      "2370/2370 [==============================] - 0s 75us/step - loss: 0.0048 - val_loss: 0.0386\n",
      "Epoch 896/3000\n",
      "2370/2370 [==============================] - 0s 69us/step - loss: 0.0050 - val_loss: 0.0387\n",
      "Epoch 897/3000\n",
      "2370/2370 [==============================] - 0s 70us/step - loss: 0.0048 - val_loss: 0.0383\n",
      "Epoch 898/3000\n",
      "2370/2370 [==============================] - 0s 72us/step - loss: 0.0049 - val_loss: 0.0387\n",
      "Epoch 899/3000\n",
      "2370/2370 [==============================] - 0s 75us/step - loss: 0.0050 - val_loss: 0.0378\n",
      "Epoch 900/3000\n",
      "2370/2370 [==============================] - 0s 73us/step - loss: 0.0048 - val_loss: 0.0382\n",
      "Epoch 901/3000\n",
      "2370/2370 [==============================] - 0s 84us/step - loss: 0.0047 - val_loss: 0.0377\n",
      "Epoch 902/3000\n",
      "2370/2370 [==============================] - 0s 73us/step - loss: 0.0046 - val_loss: 0.0385\n",
      "Epoch 903/3000\n",
      "2370/2370 [==============================] - 0s 75us/step - loss: 0.0048 - val_loss: 0.0390\n",
      "Epoch 904/3000\n",
      "2370/2370 [==============================] - 0s 79us/step - loss: 0.0049 - val_loss: 0.0387\n",
      "Epoch 905/3000\n",
      "2370/2370 [==============================] - 0s 72us/step - loss: 0.0049 - val_loss: 0.0393\n",
      "Epoch 906/3000\n",
      "2370/2370 [==============================] - 0s 63us/step - loss: 0.0050 - val_loss: 0.0381\n",
      "Epoch 907/3000\n",
      "2370/2370 [==============================] - 0s 77us/step - loss: 0.0048 - val_loss: 0.0379\n",
      "Epoch 908/3000\n",
      "2370/2370 [==============================] - 0s 78us/step - loss: 0.0048 - val_loss: 0.0381\n",
      "Epoch 909/3000\n",
      "2370/2370 [==============================] - 0s 65us/step - loss: 0.0049 - val_loss: 0.0378\n",
      "Epoch 910/3000\n",
      "2370/2370 [==============================] - 0s 67us/step - loss: 0.0047 - val_loss: 0.0388\n",
      "Epoch 911/3000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2370/2370 [==============================] - 0s 79us/step - loss: 0.0049 - val_loss: 0.0384\n",
      "Epoch 912/3000\n",
      "2370/2370 [==============================] - 0s 68us/step - loss: 0.0049 - val_loss: 0.0389\n",
      "Epoch 913/3000\n",
      "2370/2370 [==============================] - 0s 67us/step - loss: 0.0049 - val_loss: 0.0373\n",
      "Epoch 914/3000\n",
      "2370/2370 [==============================] - 0s 62us/step - loss: 0.0052 - val_loss: 0.0384\n",
      "Epoch 915/3000\n",
      "2370/2370 [==============================] - 0s 82us/step - loss: 0.0049 - val_loss: 0.0377\n",
      "Epoch 916/3000\n",
      "2370/2370 [==============================] - 0s 73us/step - loss: 0.0051 - val_loss: 0.0378\n",
      "Epoch 917/3000\n",
      "2370/2370 [==============================] - 0s 64us/step - loss: 0.0048 - val_loss: 0.0384\n",
      "Epoch 918/3000\n",
      "2370/2370 [==============================] - 0s 66us/step - loss: 0.0050 - val_loss: 0.0383\n",
      "Epoch 919/3000\n",
      "2370/2370 [==============================] - 0s 65us/step - loss: 0.0048 - val_loss: 0.0385\n",
      "Epoch 920/3000\n",
      "2370/2370 [==============================] - 0s 67us/step - loss: 0.0048 - val_loss: 0.0388\n",
      "Epoch 921/3000\n",
      "2370/2370 [==============================] - 0s 64us/step - loss: 0.0048 - val_loss: 0.0382\n",
      "Epoch 922/3000\n",
      "2370/2370 [==============================] - 0s 78us/step - loss: 0.0049 - val_loss: 0.0382\n",
      "Epoch 923/3000\n",
      "2370/2370 [==============================] - 0s 77us/step - loss: 0.0050 - val_loss: 0.0383\n",
      "Epoch 924/3000\n",
      "2370/2370 [==============================] - 0s 73us/step - loss: 0.0049 - val_loss: 0.0379\n",
      "Epoch 925/3000\n",
      "2370/2370 [==============================] - 0s 77us/step - loss: 0.0051 - val_loss: 0.0378\n",
      "Epoch 926/3000\n",
      "2370/2370 [==============================] - 0s 76us/step - loss: 0.0049 - val_loss: 0.0397\n",
      "Epoch 927/3000\n",
      "2370/2370 [==============================] - 0s 74us/step - loss: 0.0049 - val_loss: 0.0394\n",
      "Epoch 928/3000\n",
      "2370/2370 [==============================] - 0s 77us/step - loss: 0.0048 - val_loss: 0.0395\n",
      "Epoch 929/3000\n",
      "2370/2370 [==============================] - 0s 70us/step - loss: 0.0048 - val_loss: 0.0383\n",
      "Epoch 930/3000\n",
      "2370/2370 [==============================] - 0s 69us/step - loss: 0.0047 - val_loss: 0.0389\n",
      "Epoch 931/3000\n",
      "2370/2370 [==============================] - 0s 68us/step - loss: 0.0047 - val_loss: 0.0375\n",
      "Epoch 932/3000\n",
      "2370/2370 [==============================] - 0s 82us/step - loss: 0.0048 - val_loss: 0.0389\n",
      "Epoch 933/3000\n",
      "2370/2370 [==============================] - 0s 81us/step - loss: 0.0049 - val_loss: 0.0386\n",
      "Epoch 934/3000\n",
      "2370/2370 [==============================] - 0s 78us/step - loss: 0.0049 - val_loss: 0.0389\n",
      "Epoch 935/3000\n",
      "2370/2370 [==============================] - 0s 75us/step - loss: 0.0049 - val_loss: 0.0389\n",
      "Epoch 936/3000\n",
      "2370/2370 [==============================] - 0s 85us/step - loss: 0.0046 - val_loss: 0.0393\n",
      "Epoch 937/3000\n",
      "2370/2370 [==============================] - 0s 75us/step - loss: 0.0050 - val_loss: 0.0392\n",
      "Epoch 938/3000\n",
      "2370/2370 [==============================] - 0s 105us/step - loss: 0.0047 - val_loss: 0.0391\n",
      "Epoch 939/3000\n",
      "2370/2370 [==============================] - 0s 92us/step - loss: 0.0047 - val_loss: 0.0400\n",
      "Epoch 940/3000\n",
      "2370/2370 [==============================] - 0s 92us/step - loss: 0.0049 - val_loss: 0.0386\n",
      "Epoch 941/3000\n",
      "2370/2370 [==============================] - 0s 95us/step - loss: 0.0048 - val_loss: 0.0383\n",
      "Epoch 942/3000\n",
      "2370/2370 [==============================] - 0s 99us/step - loss: 0.0047 - val_loss: 0.0388\n",
      "Epoch 943/3000\n",
      "2370/2370 [==============================] - 0s 73us/step - loss: 0.0048 - val_loss: 0.0385\n",
      "Epoch 944/3000\n",
      "2370/2370 [==============================] - 0s 73us/step - loss: 0.0047 - val_loss: 0.0382\n",
      "Epoch 945/3000\n",
      "2370/2370 [==============================] - 0s 85us/step - loss: 0.0050 - val_loss: 0.0385\n",
      "Epoch 946/3000\n",
      "2370/2370 [==============================] - 0s 79us/step - loss: 0.0047 - val_loss: 0.0387\n",
      "Epoch 947/3000\n",
      "2370/2370 [==============================] - 0s 90us/step - loss: 0.0049 - val_loss: 0.0387\n",
      "Epoch 948/3000\n",
      "2370/2370 [==============================] - 0s 74us/step - loss: 0.0050 - val_loss: 0.0394\n",
      "Epoch 949/3000\n",
      "2370/2370 [==============================] - 0s 92us/step - loss: 0.0047 - val_loss: 0.0398\n",
      "Epoch 950/3000\n",
      "2370/2370 [==============================] - 0s 94us/step - loss: 0.0048 - val_loss: 0.0386\n",
      "Epoch 951/3000\n",
      "2370/2370 [==============================] - 0s 98us/step - loss: 0.0049 - val_loss: 0.0387\n",
      "Epoch 952/3000\n",
      "2370/2370 [==============================] - 0s 81us/step - loss: 0.0049 - val_loss: 0.0380\n",
      "Epoch 953/3000\n",
      "2370/2370 [==============================] - 0s 78us/step - loss: 0.0048 - val_loss: 0.0381\n",
      "Epoch 954/3000\n",
      "2370/2370 [==============================] - 0s 85us/step - loss: 0.0047 - val_loss: 0.0380\n",
      "Epoch 955/3000\n",
      "2370/2370 [==============================] - 0s 98us/step - loss: 0.0048 - val_loss: 0.0382\n",
      "Epoch 956/3000\n",
      "2370/2370 [==============================] - 0s 70us/step - loss: 0.0046 - val_loss: 0.0381\n",
      "Epoch 957/3000\n",
      "2370/2370 [==============================] - 0s 72us/step - loss: 0.0047 - val_loss: 0.0384\n",
      "Epoch 958/3000\n",
      "2370/2370 [==============================] - 0s 75us/step - loss: 0.0049 - val_loss: 0.0380\n",
      "Epoch 959/3000\n",
      "2370/2370 [==============================] - 0s 71us/step - loss: 0.0047 - val_loss: 0.0387\n",
      "Epoch 960/3000\n",
      "2370/2370 [==============================] - 0s 71us/step - loss: 0.0048 - val_loss: 0.0401\n",
      "Epoch 961/3000\n",
      "2370/2370 [==============================] - 0s 79us/step - loss: 0.0048 - val_loss: 0.0391\n",
      "Epoch 962/3000\n",
      "2370/2370 [==============================] - 0s 75us/step - loss: 0.0049 - val_loss: 0.0389\n",
      "Epoch 963/3000\n",
      "2370/2370 [==============================] - 0s 79us/step - loss: 0.0050 - val_loss: 0.0394\n",
      "Epoch 964/3000\n",
      "2370/2370 [==============================] - 0s 77us/step - loss: 0.0046 - val_loss: 0.0389\n",
      "Epoch 965/3000\n",
      "2370/2370 [==============================] - 0s 65us/step - loss: 0.0048 - val_loss: 0.0396\n",
      "Epoch 966/3000\n",
      "2370/2370 [==============================] - 0s 83us/step - loss: 0.0046 - val_loss: 0.0384\n",
      "Epoch 967/3000\n",
      "2370/2370 [==============================] - 0s 77us/step - loss: 0.0047 - val_loss: 0.0387\n",
      "Epoch 968/3000\n",
      "2370/2370 [==============================] - 0s 79us/step - loss: 0.0048 - val_loss: 0.0392\n",
      "Epoch 969/3000\n",
      "2370/2370 [==============================] - 0s 91us/step - loss: 0.0047 - val_loss: 0.0382\n",
      "Epoch 970/3000\n",
      "2370/2370 [==============================] - 0s 77us/step - loss: 0.0049 - val_loss: 0.0389\n",
      "Epoch 971/3000\n",
      "2370/2370 [==============================] - 0s 76us/step - loss: 0.0046 - val_loss: 0.0385\n",
      "Epoch 972/3000\n",
      "2370/2370 [==============================] - 0s 86us/step - loss: 0.0047 - val_loss: 0.0380\n",
      "Epoch 973/3000\n",
      "2370/2370 [==============================] - 0s 90us/step - loss: 0.0046 - val_loss: 0.0393\n",
      "Epoch 974/3000\n",
      "2370/2370 [==============================] - 0s 78us/step - loss: 0.0048 - val_loss: 0.0391\n",
      "Epoch 975/3000\n",
      "2370/2370 [==============================] - 0s 73us/step - loss: 0.0048 - val_loss: 0.0387\n",
      "Epoch 976/3000\n",
      "2370/2370 [==============================] - 0s 91us/step - loss: 0.0047 - val_loss: 0.0383\n",
      "Epoch 977/3000\n",
      "2370/2370 [==============================] - 0s 81us/step - loss: 0.0048 - val_loss: 0.0384\n",
      "Epoch 978/3000\n",
      "2370/2370 [==============================] - 0s 81us/step - loss: 0.0046 - val_loss: 0.0389\n",
      "Epoch 979/3000\n",
      "2370/2370 [==============================] - 0s 93us/step - loss: 0.0047 - val_loss: 0.0389\n",
      "Epoch 980/3000\n",
      "2370/2370 [==============================] - 0s 75us/step - loss: 0.0047 - val_loss: 0.0394\n",
      "Epoch 981/3000\n",
      "2370/2370 [==============================] - 0s 72us/step - loss: 0.0047 - val_loss: 0.0394\n",
      "Epoch 982/3000\n",
      "2370/2370 [==============================] - 0s 78us/step - loss: 0.0048 - val_loss: 0.0394\n",
      "Epoch 983/3000\n",
      "2370/2370 [==============================] - 0s 94us/step - loss: 0.0048 - val_loss: 0.0394\n",
      "Epoch 984/3000\n",
      "2370/2370 [==============================] - 0s 92us/step - loss: 0.0048 - val_loss: 0.0379\n",
      "Epoch 985/3000\n",
      "2370/2370 [==============================] - 0s 90us/step - loss: 0.0048 - val_loss: 0.0380\n",
      "Epoch 986/3000\n",
      "2370/2370 [==============================] - 0s 91us/step - loss: 0.0048 - val_loss: 0.0392\n",
      "Epoch 987/3000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2370/2370 [==============================] - 0s 87us/step - loss: 0.0048 - val_loss: 0.0391\n",
      "Epoch 988/3000\n",
      "2370/2370 [==============================] - 0s 79us/step - loss: 0.0050 - val_loss: 0.0377\n",
      "Epoch 989/3000\n",
      "2370/2370 [==============================] - 0s 85us/step - loss: 0.0048 - val_loss: 0.0390\n",
      "Epoch 990/3000\n",
      "2370/2370 [==============================] - 0s 79us/step - loss: 0.0046 - val_loss: 0.0388\n",
      "Epoch 991/3000\n",
      "2370/2370 [==============================] - 0s 82us/step - loss: 0.0048 - val_loss: 0.0383\n",
      "Epoch 992/3000\n",
      "2370/2370 [==============================] - 0s 89us/step - loss: 0.0050 - val_loss: 0.0384\n",
      "Epoch 993/3000\n",
      "2370/2370 [==============================] - 0s 88us/step - loss: 0.0049 - val_loss: 0.0379\n",
      "Epoch 994/3000\n",
      "2370/2370 [==============================] - 0s 89us/step - loss: 0.0048 - val_loss: 0.0383\n",
      "Epoch 995/3000\n",
      "2370/2370 [==============================] - 0s 102us/step - loss: 0.0047 - val_loss: 0.0375\n",
      "Epoch 996/3000\n",
      "2370/2370 [==============================] - 0s 101us/step - loss: 0.0047 - val_loss: 0.0377\n",
      "Epoch 997/3000\n",
      "2370/2370 [==============================] - 0s 99us/step - loss: 0.0047 - val_loss: 0.0387\n",
      "Epoch 998/3000\n",
      "2370/2370 [==============================] - 0s 75us/step - loss: 0.0049 - val_loss: 0.0382\n",
      "Epoch 999/3000\n",
      "2370/2370 [==============================] - 0s 73us/step - loss: 0.0047 - val_loss: 0.0382\n",
      "Epoch 1000/3000\n",
      "2370/2370 [==============================] - 0s 76us/step - loss: 0.0045 - val_loss: 0.0387\n",
      "Epoch 1001/3000\n",
      "2370/2370 [==============================] - 0s 77us/step - loss: 0.0048 - val_loss: 0.0382\n",
      "Epoch 1002/3000\n",
      "2370/2370 [==============================] - 0s 63us/step - loss: 0.0047 - val_loss: 0.0392\n",
      "Epoch 1003/3000\n",
      "2370/2370 [==============================] - 0s 67us/step - loss: 0.0047 - val_loss: 0.0383\n",
      "Epoch 1004/3000\n",
      "2370/2370 [==============================] - 0s 60us/step - loss: 0.0047 - val_loss: 0.0382\n",
      "Epoch 1005/3000\n",
      "2370/2370 [==============================] - 0s 72us/step - loss: 0.0047 - val_loss: 0.0387\n",
      "Epoch 1006/3000\n",
      "2370/2370 [==============================] - 0s 68us/step - loss: 0.0048 - val_loss: 0.0403\n",
      "Epoch 1007/3000\n",
      "2370/2370 [==============================] - 0s 71us/step - loss: 0.0050 - val_loss: 0.0388\n",
      "Epoch 1008/3000\n",
      "2370/2370 [==============================] - 0s 73us/step - loss: 0.0047 - val_loss: 0.0387\n",
      "Epoch 1009/3000\n",
      "2370/2370 [==============================] - 0s 74us/step - loss: 0.0047 - val_loss: 0.0382\n",
      "Epoch 1010/3000\n",
      "2370/2370 [==============================] - 0s 65us/step - loss: 0.0049 - val_loss: 0.0375\n",
      "Epoch 1011/3000\n",
      "2370/2370 [==============================] - 0s 69us/step - loss: 0.0048 - val_loss: 0.0375\n",
      "Epoch 1012/3000\n",
      "2370/2370 [==============================] - 0s 75us/step - loss: 0.0047 - val_loss: 0.0396\n",
      "Epoch 1013/3000\n",
      "2370/2370 [==============================] - 0s 75us/step - loss: 0.0048 - val_loss: 0.0392\n",
      "Epoch 1014/3000\n",
      "2370/2370 [==============================] - 0s 67us/step - loss: 0.0046 - val_loss: 0.0380\n",
      "Epoch 1015/3000\n",
      "2370/2370 [==============================] - 0s 60us/step - loss: 0.0047 - val_loss: 0.0395\n",
      "Epoch 1016/3000\n",
      "2370/2370 [==============================] - 0s 76us/step - loss: 0.0046 - val_loss: 0.0401\n",
      "Epoch 1017/3000\n",
      "2370/2370 [==============================] - 0s 60us/step - loss: 0.0050 - val_loss: 0.0378\n",
      "Epoch 1018/3000\n",
      "2370/2370 [==============================] - 0s 66us/step - loss: 0.0048 - val_loss: 0.0378\n",
      "Epoch 1019/3000\n",
      "2370/2370 [==============================] - 0s 69us/step - loss: 0.0049 - val_loss: 0.0379\n",
      "Epoch 1020/3000\n",
      "2370/2370 [==============================] - 0s 63us/step - loss: 0.0049 - val_loss: 0.0381\n",
      "Epoch 1021/3000\n",
      "2370/2370 [==============================] - 0s 68us/step - loss: 0.0049 - val_loss: 0.0378\n",
      "Epoch 1022/3000\n",
      "2370/2370 [==============================] - 0s 74us/step - loss: 0.0045 - val_loss: 0.0379\n",
      "Epoch 1023/3000\n",
      "2370/2370 [==============================] - 0s 61us/step - loss: 0.0045 - val_loss: 0.0384\n",
      "Epoch 1024/3000\n",
      "2370/2370 [==============================] - 0s 65us/step - loss: 0.0045 - val_loss: 0.0390\n",
      "Epoch 1025/3000\n",
      "2370/2370 [==============================] - 0s 77us/step - loss: 0.0047 - val_loss: 0.0387\n",
      "Epoch 1026/3000\n",
      "2370/2370 [==============================] - 0s 75us/step - loss: 0.0047 - val_loss: 0.0384\n",
      "Epoch 1027/3000\n",
      "2370/2370 [==============================] - 0s 73us/step - loss: 0.0048 - val_loss: 0.0400\n",
      "Epoch 1028/3000\n",
      "2370/2370 [==============================] - 0s 80us/step - loss: 0.0045 - val_loss: 0.0383\n",
      "Epoch 1029/3000\n",
      "2370/2370 [==============================] - 0s 67us/step - loss: 0.0048 - val_loss: 0.0384\n",
      "Epoch 1030/3000\n",
      "2370/2370 [==============================] - 0s 86us/step - loss: 0.0048 - val_loss: 0.0385\n",
      "Epoch 1031/3000\n",
      "2370/2370 [==============================] - 0s 100us/step - loss: 0.0047 - val_loss: 0.0388\n",
      "Epoch 1032/3000\n",
      "2370/2370 [==============================] - 0s 75us/step - loss: 0.0046 - val_loss: 0.0399\n",
      "Epoch 1033/3000\n",
      "2370/2370 [==============================] - 0s 82us/step - loss: 0.0045 - val_loss: 0.0386\n",
      "Epoch 1034/3000\n",
      "2370/2370 [==============================] - 0s 79us/step - loss: 0.0048 - val_loss: 0.0383\n",
      "Epoch 1035/3000\n",
      "2370/2370 [==============================] - 0s 77us/step - loss: 0.0048 - val_loss: 0.0389\n",
      "Epoch 1036/3000\n",
      "2370/2370 [==============================] - 0s 82us/step - loss: 0.0047 - val_loss: 0.0388\n",
      "Epoch 1037/3000\n",
      "2370/2370 [==============================] - 0s 69us/step - loss: 0.0047 - val_loss: 0.0388\n",
      "Epoch 1038/3000\n",
      "2370/2370 [==============================] - 0s 86us/step - loss: 0.0045 - val_loss: 0.0386\n",
      "Epoch 1039/3000\n",
      "2370/2370 [==============================] - 0s 71us/step - loss: 0.0047 - val_loss: 0.0379\n",
      "Epoch 1040/3000\n",
      "2370/2370 [==============================] - 0s 104us/step - loss: 0.0048 - val_loss: 0.0382\n",
      "Epoch 1041/3000\n",
      "2370/2370 [==============================] - 0s 80us/step - loss: 0.0047 - val_loss: 0.0377\n",
      "Epoch 1042/3000\n",
      "2370/2370 [==============================] - 0s 65us/step - loss: 0.0047 - val_loss: 0.0381\n",
      "Epoch 1043/3000\n",
      "2370/2370 [==============================] - 0s 90us/step - loss: 0.0046 - val_loss: 0.0380\n",
      "Epoch 1044/3000\n",
      "2370/2370 [==============================] - 0s 70us/step - loss: 0.0046 - val_loss: 0.0389\n",
      "Epoch 1045/3000\n",
      "2370/2370 [==============================] - 0s 78us/step - loss: 0.0047 - val_loss: 0.0395\n",
      "Epoch 1046/3000\n",
      "2370/2370 [==============================] - 0s 87us/step - loss: 0.0047 - val_loss: 0.0395\n",
      "Epoch 1047/3000\n",
      "2370/2370 [==============================] - 0s 80us/step - loss: 0.0048 - val_loss: 0.0380\n",
      "Epoch 1048/3000\n",
      "2370/2370 [==============================] - 0s 69us/step - loss: 0.0048 - val_loss: 0.0384\n",
      "Epoch 1049/3000\n",
      "2370/2370 [==============================] - 0s 63us/step - loss: 0.0047 - val_loss: 0.0383\n",
      "Epoch 1050/3000\n",
      "2370/2370 [==============================] - 0s 72us/step - loss: 0.0045 - val_loss: 0.0381\n",
      "Epoch 1051/3000\n",
      "2370/2370 [==============================] - 0s 70us/step - loss: 0.0045 - val_loss: 0.0391\n",
      "Epoch 1052/3000\n",
      "2370/2370 [==============================] - 0s 77us/step - loss: 0.0048 - val_loss: 0.0384\n",
      "Epoch 1053/3000\n",
      "2370/2370 [==============================] - 0s 78us/step - loss: 0.0048 - val_loss: 0.0376\n",
      "Epoch 1054/3000\n",
      "2370/2370 [==============================] - 0s 78us/step - loss: 0.0047 - val_loss: 0.0385\n",
      "Epoch 1055/3000\n",
      "2370/2370 [==============================] - 0s 70us/step - loss: 0.0048 - val_loss: 0.0382\n",
      "Epoch 1056/3000\n",
      "2370/2370 [==============================] - 0s 57us/step - loss: 0.0048 - val_loss: 0.0384\n",
      "Epoch 1057/3000\n",
      "2370/2370 [==============================] - 0s 74us/step - loss: 0.0047 - val_loss: 0.0382\n",
      "Epoch 1058/3000\n",
      "2370/2370 [==============================] - 0s 63us/step - loss: 0.0046 - val_loss: 0.0386\n",
      "Epoch 1059/3000\n",
      "2370/2370 [==============================] - 0s 72us/step - loss: 0.0046 - val_loss: 0.0383\n",
      "Epoch 1060/3000\n",
      "2370/2370 [==============================] - 0s 67us/step - loss: 0.0046 - val_loss: 0.0390\n",
      "Epoch 1061/3000\n",
      "2370/2370 [==============================] - 0s 67us/step - loss: 0.0046 - val_loss: 0.0391\n",
      "Epoch 1062/3000\n",
      "2370/2370 [==============================] - 0s 61us/step - loss: 0.0046 - val_loss: 0.0382\n",
      "Epoch 1063/3000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2370/2370 [==============================] - 0s 63us/step - loss: 0.0047 - val_loss: 0.0384\n",
      "Epoch 1064/3000\n",
      "2370/2370 [==============================] - 0s 69us/step - loss: 0.0047 - val_loss: 0.0379\n",
      "Epoch 1065/3000\n",
      "2370/2370 [==============================] - 0s 64us/step - loss: 0.0048 - val_loss: 0.0383\n",
      "Epoch 1066/3000\n",
      "2370/2370 [==============================] - 0s 72us/step - loss: 0.0046 - val_loss: 0.0386\n",
      "Epoch 1067/3000\n",
      "2370/2370 [==============================] - 0s 76us/step - loss: 0.0047 - val_loss: 0.0385\n",
      "Epoch 1068/3000\n",
      "2370/2370 [==============================] - 0s 65us/step - loss: 0.0047 - val_loss: 0.0379\n",
      "Epoch 1069/3000\n",
      "2370/2370 [==============================] - 0s 65us/step - loss: 0.0048 - val_loss: 0.0379\n",
      "Epoch 1070/3000\n",
      "2370/2370 [==============================] - 0s 69us/step - loss: 0.0048 - val_loss: 0.0385\n",
      "Epoch 1071/3000\n",
      "2370/2370 [==============================] - 0s 66us/step - loss: 0.0047 - val_loss: 0.0382\n",
      "Epoch 1072/3000\n",
      "2370/2370 [==============================] - 0s 65us/step - loss: 0.0048 - val_loss: 0.0381\n",
      "Epoch 1073/3000\n",
      "2370/2370 [==============================] - 0s 68us/step - loss: 0.0047 - val_loss: 0.0387\n",
      "Epoch 1074/3000\n",
      "2370/2370 [==============================] - 0s 63us/step - loss: 0.0045 - val_loss: 0.0378\n",
      "Epoch 1075/3000\n",
      "2370/2370 [==============================] - 0s 73us/step - loss: 0.0048 - val_loss: 0.0384\n",
      "Epoch 1076/3000\n",
      "2370/2370 [==============================] - 0s 69us/step - loss: 0.0045 - val_loss: 0.0381\n",
      "Epoch 1077/3000\n",
      "2370/2370 [==============================] - 0s 67us/step - loss: 0.0049 - val_loss: 0.0380\n",
      "Epoch 1078/3000\n",
      "2370/2370 [==============================] - 0s 70us/step - loss: 0.0048 - val_loss: 0.0385\n",
      "Epoch 1079/3000\n",
      "2370/2370 [==============================] - 0s 64us/step - loss: 0.0046 - val_loss: 0.0382\n",
      "Epoch 1080/3000\n",
      "2370/2370 [==============================] - 0s 75us/step - loss: 0.0046 - val_loss: 0.0381\n",
      "Epoch 1081/3000\n",
      "2370/2370 [==============================] - 0s 70us/step - loss: 0.0046 - val_loss: 0.0381\n",
      "Epoch 1082/3000\n",
      "2370/2370 [==============================] - 0s 65us/step - loss: 0.0047 - val_loss: 0.0382\n",
      "Epoch 1083/3000\n",
      "2370/2370 [==============================] - 0s 78us/step - loss: 0.0049 - val_loss: 0.0388\n",
      "Epoch 1084/3000\n",
      "2370/2370 [==============================] - 0s 63us/step - loss: 0.0048 - val_loss: 0.0388\n",
      "Epoch 1085/3000\n",
      "2370/2370 [==============================] - 0s 78us/step - loss: 0.0046 - val_loss: 0.0391\n",
      "Epoch 1086/3000\n",
      "2370/2370 [==============================] - 0s 64us/step - loss: 0.0045 - val_loss: 0.0390\n",
      "Epoch 1087/3000\n",
      "2370/2370 [==============================] - 0s 68us/step - loss: 0.0045 - val_loss: 0.0391\n",
      "Epoch 1088/3000\n",
      "2370/2370 [==============================] - 0s 75us/step - loss: 0.0047 - val_loss: 0.0378\n",
      "Epoch 1089/3000\n",
      "2370/2370 [==============================] - 0s 73us/step - loss: 0.0049 - val_loss: 0.0392\n",
      "Epoch 1090/3000\n",
      "2370/2370 [==============================] - 0s 68us/step - loss: 0.0046 - val_loss: 0.0379\n",
      "Epoch 1091/3000\n",
      "2370/2370 [==============================] - 0s 74us/step - loss: 0.0046 - val_loss: 0.0392\n",
      "Epoch 1092/3000\n",
      "2370/2370 [==============================] - 0s 64us/step - loss: 0.0047 - val_loss: 0.0382\n",
      "Epoch 1093/3000\n",
      "2370/2370 [==============================] - 0s 76us/step - loss: 0.0046 - val_loss: 0.0380\n",
      "Epoch 1094/3000\n",
      "2370/2370 [==============================] - 0s 75us/step - loss: 0.0046 - val_loss: 0.0387\n",
      "Epoch 1095/3000\n",
      "2370/2370 [==============================] - 0s 82us/step - loss: 0.0047 - val_loss: 0.0383\n",
      "Epoch 1096/3000\n",
      "2370/2370 [==============================] - 0s 68us/step - loss: 0.0047 - val_loss: 0.0387\n",
      "Epoch 1097/3000\n",
      "2370/2370 [==============================] - 0s 75us/step - loss: 0.0048 - val_loss: 0.0382\n",
      "Epoch 1098/3000\n",
      "2370/2370 [==============================] - 0s 74us/step - loss: 0.0046 - val_loss: 0.0383\n",
      "Epoch 1099/3000\n",
      "2370/2370 [==============================] - 0s 68us/step - loss: 0.0046 - val_loss: 0.0380\n",
      "Epoch 1100/3000\n",
      "2370/2370 [==============================] - 0s 66us/step - loss: 0.0047 - val_loss: 0.0389\n",
      "Epoch 1101/3000\n",
      "2370/2370 [==============================] - 0s 69us/step - loss: 0.0045 - val_loss: 0.0380\n",
      "Epoch 1102/3000\n",
      "2370/2370 [==============================] - 0s 71us/step - loss: 0.0046 - val_loss: 0.0390\n",
      "Epoch 1103/3000\n",
      "2370/2370 [==============================] - 0s 68us/step - loss: 0.0046 - val_loss: 0.0393\n",
      "Epoch 1104/3000\n",
      "2370/2370 [==============================] - 0s 63us/step - loss: 0.0048 - val_loss: 0.0382\n",
      "Epoch 1105/3000\n",
      "2370/2370 [==============================] - 0s 62us/step - loss: 0.0046 - val_loss: 0.0396\n",
      "Epoch 1106/3000\n",
      "2370/2370 [==============================] - 0s 62us/step - loss: 0.0046 - val_loss: 0.0391\n",
      "Epoch 1107/3000\n",
      "2370/2370 [==============================] - 0s 77us/step - loss: 0.0046 - val_loss: 0.0377\n",
      "Epoch 1108/3000\n",
      "2370/2370 [==============================] - 0s 73us/step - loss: 0.0044 - val_loss: 0.0380\n",
      "Epoch 1109/3000\n",
      "2370/2370 [==============================] - 0s 70us/step - loss: 0.0044 - val_loss: 0.0378\n",
      "Epoch 1110/3000\n",
      "2370/2370 [==============================] - 0s 69us/step - loss: 0.0045 - val_loss: 0.0379\n",
      "Epoch 1111/3000\n",
      "2370/2370 [==============================] - 0s 70us/step - loss: 0.0048 - val_loss: 0.0389\n",
      "Epoch 1112/3000\n",
      "2370/2370 [==============================] - 0s 81us/step - loss: 0.0044 - val_loss: 0.0393\n",
      "Epoch 1113/3000\n",
      "2370/2370 [==============================] - 0s 68us/step - loss: 0.0045 - val_loss: 0.0385\n",
      "Epoch 1114/3000\n",
      "2370/2370 [==============================] - 0s 70us/step - loss: 0.0046 - val_loss: 0.0389\n",
      "Epoch 1115/3000\n",
      "2370/2370 [==============================] - 0s 72us/step - loss: 0.0046 - val_loss: 0.0385\n",
      "Epoch 1116/3000\n",
      "2370/2370 [==============================] - 0s 69us/step - loss: 0.0046 - val_loss: 0.0386\n",
      "Epoch 1117/3000\n",
      "2370/2370 [==============================] - 0s 61us/step - loss: 0.0047 - val_loss: 0.0384\n",
      "Epoch 1118/3000\n",
      "2370/2370 [==============================] - 0s 72us/step - loss: 0.0044 - val_loss: 0.0387\n",
      "Epoch 1119/3000\n",
      "2370/2370 [==============================] - 0s 68us/step - loss: 0.0046 - val_loss: 0.0392\n",
      "Epoch 1120/3000\n",
      "2370/2370 [==============================] - 0s 56us/step - loss: 0.0046 - val_loss: 0.0380\n",
      "Epoch 1121/3000\n",
      "2370/2370 [==============================] - 0s 73us/step - loss: 0.0047 - val_loss: 0.0396\n",
      "Epoch 1122/3000\n",
      "2370/2370 [==============================] - 0s 78us/step - loss: 0.0046 - val_loss: 0.0394\n",
      "Epoch 1123/3000\n",
      "2370/2370 [==============================] - 0s 66us/step - loss: 0.0047 - val_loss: 0.0389\n",
      "Epoch 1124/3000\n",
      "2370/2370 [==============================] - 0s 67us/step - loss: 0.0046 - val_loss: 0.0397\n",
      "Epoch 1125/3000\n",
      "2370/2370 [==============================] - 0s 66us/step - loss: 0.0046 - val_loss: 0.0396\n",
      "Epoch 1126/3000\n",
      "2370/2370 [==============================] - 0s 70us/step - loss: 0.0049 - val_loss: 0.0405\n",
      "Epoch 1127/3000\n",
      "2370/2370 [==============================] - 0s 70us/step - loss: 0.0048 - val_loss: 0.0388\n",
      "Epoch 1128/3000\n",
      "2370/2370 [==============================] - 0s 64us/step - loss: 0.0046 - val_loss: 0.0383\n",
      "Epoch 1129/3000\n",
      "2370/2370 [==============================] - 0s 73us/step - loss: 0.0047 - val_loss: 0.0384\n",
      "Epoch 1130/3000\n",
      "2370/2370 [==============================] - 0s 82us/step - loss: 0.0046 - val_loss: 0.0369\n",
      "Epoch 1131/3000\n",
      "2370/2370 [==============================] - 0s 74us/step - loss: 0.0046 - val_loss: 0.0383\n",
      "Epoch 1132/3000\n",
      "2370/2370 [==============================] - 0s 74us/step - loss: 0.0044 - val_loss: 0.0376\n",
      "Epoch 1133/3000\n",
      "2370/2370 [==============================] - 0s 77us/step - loss: 0.0044 - val_loss: 0.0375\n",
      "Epoch 1134/3000\n",
      "2370/2370 [==============================] - 0s 69us/step - loss: 0.0047 - val_loss: 0.0384\n",
      "Epoch 1135/3000\n",
      "2370/2370 [==============================] - 0s 61us/step - loss: 0.0046 - val_loss: 0.0394\n",
      "Epoch 1136/3000\n",
      "2370/2370 [==============================] - 0s 91us/step - loss: 0.0046 - val_loss: 0.0391\n",
      "Epoch 1137/3000\n",
      "2370/2370 [==============================] - 0s 79us/step - loss: 0.0049 - val_loss: 0.0398\n",
      "Epoch 1138/3000\n",
      "2370/2370 [==============================] - 0s 64us/step - loss: 0.0049 - val_loss: 0.0398\n",
      "Epoch 1139/3000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2370/2370 [==============================] - 0s 65us/step - loss: 0.0048 - val_loss: 0.0388\n",
      "Epoch 1140/3000\n",
      "2370/2370 [==============================] - 0s 62us/step - loss: 0.0044 - val_loss: 0.0382\n",
      "Epoch 1141/3000\n",
      "2370/2370 [==============================] - 0s 72us/step - loss: 0.0045 - val_loss: 0.0393\n",
      "Epoch 1142/3000\n",
      "2370/2370 [==============================] - 0s 80us/step - loss: 0.0045 - val_loss: 0.0394\n",
      "Epoch 1143/3000\n",
      "2370/2370 [==============================] - 0s 82us/step - loss: 0.0046 - val_loss: 0.0387\n",
      "Epoch 1144/3000\n",
      "2370/2370 [==============================] - 0s 98us/step - loss: 0.0046 - val_loss: 0.0381\n",
      "Epoch 1145/3000\n",
      "2370/2370 [==============================] - 0s 92us/step - loss: 0.0047 - val_loss: 0.0385\n",
      "Epoch 1146/3000\n",
      "2370/2370 [==============================] - 0s 85us/step - loss: 0.0047 - val_loss: 0.0385\n",
      "Epoch 1147/3000\n",
      "2370/2370 [==============================] - 0s 95us/step - loss: 0.0046 - val_loss: 0.0393\n",
      "Epoch 1148/3000\n",
      "2370/2370 [==============================] - 0s 94us/step - loss: 0.0046 - val_loss: 0.0389\n",
      "Epoch 1149/3000\n",
      "2370/2370 [==============================] - 0s 87us/step - loss: 0.0046 - val_loss: 0.0385\n",
      "Epoch 1150/3000\n",
      "2370/2370 [==============================] - 0s 80us/step - loss: 0.0044 - val_loss: 0.0389\n",
      "Epoch 1151/3000\n",
      "2370/2370 [==============================] - 0s 91us/step - loss: 0.0044 - val_loss: 0.0385\n",
      "Epoch 1152/3000\n",
      "2370/2370 [==============================] - 0s 80us/step - loss: 0.0046 - val_loss: 0.0379\n",
      "Epoch 1153/3000\n",
      "2370/2370 [==============================] - 0s 65us/step - loss: 0.0047 - val_loss: 0.0382\n",
      "Epoch 1154/3000\n",
      "2370/2370 [==============================] - 0s 93us/step - loss: 0.0043 - val_loss: 0.0382\n",
      "Epoch 1155/3000\n",
      "2370/2370 [==============================] - 0s 85us/step - loss: 0.0044 - val_loss: 0.0378\n",
      "Epoch 1156/3000\n",
      "2370/2370 [==============================] - 0s 77us/step - loss: 0.0043 - val_loss: 0.0376\n",
      "Epoch 1157/3000\n",
      "2370/2370 [==============================] - 0s 85us/step - loss: 0.0046 - val_loss: 0.0378\n",
      "Epoch 1158/3000\n",
      "2370/2370 [==============================] - 0s 93us/step - loss: 0.0046 - val_loss: 0.0384\n",
      "Epoch 1159/3000\n",
      "2370/2370 [==============================] - 0s 89us/step - loss: 0.0045 - val_loss: 0.0377\n",
      "Epoch 1160/3000\n",
      "2370/2370 [==============================] - 0s 74us/step - loss: 0.0047 - val_loss: 0.0377\n",
      "Epoch 1161/3000\n",
      "2370/2370 [==============================] - 0s 84us/step - loss: 0.0046 - val_loss: 0.0376\n",
      "Epoch 1162/3000\n",
      "2370/2370 [==============================] - 0s 100us/step - loss: 0.0047 - val_loss: 0.0384\n",
      "Epoch 1163/3000\n",
      "2370/2370 [==============================] - 0s 76us/step - loss: 0.0044 - val_loss: 0.0385\n",
      "Epoch 1164/3000\n",
      "2370/2370 [==============================] - 0s 78us/step - loss: 0.0046 - val_loss: 0.0383\n",
      "Epoch 1165/3000\n",
      "2370/2370 [==============================] - 0s 68us/step - loss: 0.0046 - val_loss: 0.0391\n",
      "Epoch 1166/3000\n",
      "2370/2370 [==============================] - 0s 77us/step - loss: 0.0045 - val_loss: 0.0390\n",
      "Epoch 1167/3000\n",
      "2370/2370 [==============================] - 0s 100us/step - loss: 0.0046 - val_loss: 0.0393\n",
      "Epoch 1168/3000\n",
      "2370/2370 [==============================] - 0s 75us/step - loss: 0.0046 - val_loss: 0.0383\n",
      "Epoch 1169/3000\n",
      "2370/2370 [==============================] - 0s 73us/step - loss: 0.0047 - val_loss: 0.0379\n",
      "Epoch 1170/3000\n",
      "2370/2370 [==============================] - 0s 76us/step - loss: 0.0045 - val_loss: 0.0380\n",
      "Epoch 1171/3000\n",
      "2370/2370 [==============================] - 0s 100us/step - loss: 0.0044 - val_loss: 0.0375\n",
      "Epoch 1172/3000\n",
      "2370/2370 [==============================] - 0s 79us/step - loss: 0.0046 - val_loss: 0.0380\n",
      "Epoch 1173/3000\n",
      "2370/2370 [==============================] - 0s 61us/step - loss: 0.0045 - val_loss: 0.0390\n",
      "Epoch 1174/3000\n",
      "2370/2370 [==============================] - 0s 66us/step - loss: 0.0046 - val_loss: 0.0380\n",
      "Epoch 1175/3000\n",
      "2370/2370 [==============================] - 0s 66us/step - loss: 0.0045 - val_loss: 0.0383\n",
      "Epoch 1176/3000\n",
      "2370/2370 [==============================] - 0s 67us/step - loss: 0.0047 - val_loss: 0.0382\n",
      "Epoch 1177/3000\n",
      "2370/2370 [==============================] - 0s 63us/step - loss: 0.0044 - val_loss: 0.0386\n",
      "Epoch 1178/3000\n",
      "2370/2370 [==============================] - 0s 84us/step - loss: 0.0046 - val_loss: 0.0380\n",
      "Epoch 1179/3000\n",
      "2370/2370 [==============================] - 0s 85us/step - loss: 0.0048 - val_loss: 0.0376\n",
      "Epoch 1180/3000\n",
      "2370/2370 [==============================] - 0s 74us/step - loss: 0.0046 - val_loss: 0.0377\n",
      "Epoch 1181/3000\n",
      "2370/2370 [==============================] - 0s 64us/step - loss: 0.0047 - val_loss: 0.0376\n",
      "Epoch 1182/3000\n",
      "2370/2370 [==============================] - 0s 65us/step - loss: 0.0045 - val_loss: 0.0380\n",
      "Epoch 1183/3000\n",
      "2370/2370 [==============================] - 0s 67us/step - loss: 0.0046 - val_loss: 0.0374\n",
      "Epoch 1184/3000\n",
      "2370/2370 [==============================] - 0s 87us/step - loss: 0.0045 - val_loss: 0.0382\n",
      "Epoch 1185/3000\n",
      "2370/2370 [==============================] - 0s 104us/step - loss: 0.0043 - val_loss: 0.0383\n",
      "Epoch 1186/3000\n",
      "2370/2370 [==============================] - 0s 84us/step - loss: 0.0047 - val_loss: 0.0380\n",
      "Epoch 1187/3000\n",
      "2370/2370 [==============================] - 0s 67us/step - loss: 0.0046 - val_loss: 0.0393\n",
      "Epoch 1188/3000\n",
      "2370/2370 [==============================] - 0s 69us/step - loss: 0.0046 - val_loss: 0.0378\n",
      "Epoch 1189/3000\n",
      "2370/2370 [==============================] - 0s 65us/step - loss: 0.0044 - val_loss: 0.0390\n",
      "Epoch 1190/3000\n",
      "2370/2370 [==============================] - 0s 68us/step - loss: 0.0045 - val_loss: 0.0392\n",
      "Epoch 1191/3000\n",
      "2370/2370 [==============================] - 0s 77us/step - loss: 0.0045 - val_loss: 0.0400\n",
      "Epoch 1192/3000\n",
      "2370/2370 [==============================] - 0s 78us/step - loss: 0.0043 - val_loss: 0.0379\n",
      "Epoch 1193/3000\n",
      "2370/2370 [==============================] - 0s 69us/step - loss: 0.0047 - val_loss: 0.0389\n",
      "Epoch 1194/3000\n",
      "2370/2370 [==============================] - 0s 79us/step - loss: 0.0044 - val_loss: 0.0383\n",
      "Epoch 1195/3000\n",
      "2370/2370 [==============================] - 0s 78us/step - loss: 0.0047 - val_loss: 0.0387\n",
      "Epoch 1196/3000\n",
      "2370/2370 [==============================] - 0s 65us/step - loss: 0.0046 - val_loss: 0.0376\n",
      "Epoch 1197/3000\n",
      "2370/2370 [==============================] - 0s 75us/step - loss: 0.0048 - val_loss: 0.0393\n",
      "Epoch 1198/3000\n",
      "2370/2370 [==============================] - 0s 78us/step - loss: 0.0044 - val_loss: 0.0390\n",
      "Epoch 1199/3000\n",
      "2370/2370 [==============================] - 0s 89us/step - loss: 0.0047 - val_loss: 0.0380\n",
      "Epoch 1200/3000\n",
      "2370/2370 [==============================] - 0s 94us/step - loss: 0.0047 - val_loss: 0.0380\n",
      "Epoch 1201/3000\n",
      "2370/2370 [==============================] - 0s 93us/step - loss: 0.0047 - val_loss: 0.0378\n",
      "Epoch 1202/3000\n",
      "2370/2370 [==============================] - 0s 93us/step - loss: 0.0045 - val_loss: 0.0383\n",
      "Epoch 1203/3000\n",
      "2370/2370 [==============================] - 0s 83us/step - loss: 0.0047 - val_loss: 0.0385\n",
      "Epoch 1204/3000\n",
      "2370/2370 [==============================] - 0s 83us/step - loss: 0.0042 - val_loss: 0.0388\n",
      "Epoch 1205/3000\n",
      "2370/2370 [==============================] - 0s 84us/step - loss: 0.0044 - val_loss: 0.0388\n",
      "Epoch 1206/3000\n",
      "2370/2370 [==============================] - 0s 91us/step - loss: 0.0047 - val_loss: 0.0378\n",
      "Epoch 1207/3000\n",
      "2370/2370 [==============================] - 0s 82us/step - loss: 0.0049 - val_loss: 0.0378\n",
      "Epoch 1208/3000\n",
      "2370/2370 [==============================] - 0s 74us/step - loss: 0.0044 - val_loss: 0.0384\n",
      "Epoch 1209/3000\n",
      "2370/2370 [==============================] - 0s 89us/step - loss: 0.0046 - val_loss: 0.0390\n",
      "Epoch 1210/3000\n",
      "2370/2370 [==============================] - 0s 74us/step - loss: 0.0047 - val_loss: 0.0382\n",
      "Epoch 1211/3000\n",
      "2370/2370 [==============================] - 0s 82us/step - loss: 0.0046 - val_loss: 0.0396\n",
      "Epoch 1212/3000\n",
      "2370/2370 [==============================] - 0s 91us/step - loss: 0.0045 - val_loss: 0.0389\n",
      "Epoch 1213/3000\n",
      "2370/2370 [==============================] - 0s 74us/step - loss: 0.0047 - val_loss: 0.0382\n",
      "Epoch 1214/3000\n",
      "2370/2370 [==============================] - 0s 84us/step - loss: 0.0048 - val_loss: 0.0392\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1215/3000\n",
      "2370/2370 [==============================] - 0s 79us/step - loss: 0.0047 - val_loss: 0.0389\n",
      "Epoch 1216/3000\n",
      "2370/2370 [==============================] - 0s 94us/step - loss: 0.0047 - val_loss: 0.0395\n",
      "Epoch 1217/3000\n",
      "2370/2370 [==============================] - 0s 87us/step - loss: 0.0047 - val_loss: 0.0385\n",
      "Epoch 1218/3000\n",
      "2370/2370 [==============================] - 0s 86us/step - loss: 0.0046 - val_loss: 0.0397\n",
      "Epoch 1219/3000\n",
      "2370/2370 [==============================] - 0s 73us/step - loss: 0.0046 - val_loss: 0.0394\n",
      "Epoch 1220/3000\n",
      "2370/2370 [==============================] - 0s 73us/step - loss: 0.0045 - val_loss: 0.0409\n",
      "Epoch 1221/3000\n",
      "2370/2370 [==============================] - 0s 64us/step - loss: 0.0045 - val_loss: 0.0389\n",
      "Epoch 1222/3000\n",
      "2370/2370 [==============================] - 0s 71us/step - loss: 0.0046 - val_loss: 0.0389\n",
      "Epoch 1223/3000\n",
      "2370/2370 [==============================] - 0s 74us/step - loss: 0.0044 - val_loss: 0.0391\n",
      "Epoch 1224/3000\n",
      "2370/2370 [==============================] - 0s 77us/step - loss: 0.0045 - val_loss: 0.0399\n",
      "Epoch 1225/3000\n",
      "2370/2370 [==============================] - 0s 71us/step - loss: 0.0047 - val_loss: 0.0387\n",
      "Epoch 1226/3000\n",
      "2370/2370 [==============================] - 0s 85us/step - loss: 0.0048 - val_loss: 0.0386\n",
      "Epoch 1227/3000\n",
      "2370/2370 [==============================] - 0s 89us/step - loss: 0.0044 - val_loss: 0.0386\n",
      "Epoch 1228/3000\n",
      "2370/2370 [==============================] - 0s 83us/step - loss: 0.0043 - val_loss: 0.0391\n",
      "Epoch 1229/3000\n",
      "2370/2370 [==============================] - 0s 72us/step - loss: 0.0044 - val_loss: 0.0387\n",
      "Epoch 1230/3000\n",
      "2370/2370 [==============================] - 0s 72us/step - loss: 0.0045 - val_loss: 0.0381\n",
      "Epoch 1231/3000\n",
      "2370/2370 [==============================] - 0s 62us/step - loss: 0.0047 - val_loss: 0.0387\n",
      "Epoch 1232/3000\n",
      "2370/2370 [==============================] - 0s 78us/step - loss: 0.0045 - val_loss: 0.0392\n",
      "Epoch 1233/3000\n",
      "2370/2370 [==============================] - 0s 85us/step - loss: 0.0046 - val_loss: 0.0401\n",
      "Epoch 1234/3000\n",
      "2370/2370 [==============================] - 0s 65us/step - loss: 0.0046 - val_loss: 0.0388\n",
      "Epoch 1235/3000\n",
      "2370/2370 [==============================] - 0s 71us/step - loss: 0.0046 - val_loss: 0.0389\n",
      "Epoch 1236/3000\n",
      "2370/2370 [==============================] - 0s 83us/step - loss: 0.0047 - val_loss: 0.0379\n",
      "Epoch 1237/3000\n",
      "2370/2370 [==============================] - 0s 78us/step - loss: 0.0043 - val_loss: 0.0385\n",
      "Epoch 1238/3000\n",
      "2370/2370 [==============================] - 0s 70us/step - loss: 0.0046 - val_loss: 0.0391\n",
      "Epoch 1239/3000\n",
      "2370/2370 [==============================] - 0s 88us/step - loss: 0.0044 - val_loss: 0.0388\n",
      "Epoch 1240/3000\n",
      "2370/2370 [==============================] - 0s 79us/step - loss: 0.0046 - val_loss: 0.0387\n",
      "Epoch 1241/3000\n",
      "2370/2370 [==============================] - 0s 75us/step - loss: 0.0043 - val_loss: 0.0383\n",
      "Epoch 1242/3000\n",
      "2370/2370 [==============================] - 0s 83us/step - loss: 0.0046 - val_loss: 0.0388\n",
      "Epoch 1243/3000\n",
      "2370/2370 [==============================] - 0s 82us/step - loss: 0.0047 - val_loss: 0.0389\n",
      "Epoch 1244/3000\n",
      "2370/2370 [==============================] - 0s 80us/step - loss: 0.0045 - val_loss: 0.0382\n",
      "Epoch 1245/3000\n",
      "2370/2370 [==============================] - 0s 89us/step - loss: 0.0046 - val_loss: 0.0388\n",
      "Epoch 1246/3000\n",
      "2370/2370 [==============================] - 0s 81us/step - loss: 0.0042 - val_loss: 0.0384\n",
      "Epoch 1247/3000\n",
      "2370/2370 [==============================] - 0s 80us/step - loss: 0.0046 - val_loss: 0.0382\n",
      "Epoch 1248/3000\n",
      "2370/2370 [==============================] - 0s 84us/step - loss: 0.0045 - val_loss: 0.0389\n",
      "Epoch 1249/3000\n",
      "2370/2370 [==============================] - 0s 100us/step - loss: 0.0047 - val_loss: 0.0391\n",
      "Epoch 1250/3000\n",
      "2370/2370 [==============================] - 0s 82us/step - loss: 0.0046 - val_loss: 0.0380\n",
      "Epoch 1251/3000\n",
      "2370/2370 [==============================] - 0s 82us/step - loss: 0.0046 - val_loss: 0.0379\n",
      "Epoch 1252/3000\n",
      "2370/2370 [==============================] - 0s 92us/step - loss: 0.0043 - val_loss: 0.0379\n",
      "Epoch 1253/3000\n",
      "2370/2370 [==============================] - 0s 92us/step - loss: 0.0047 - val_loss: 0.0379\n",
      "Epoch 1254/3000\n",
      "2370/2370 [==============================] - 0s 72us/step - loss: 0.0044 - val_loss: 0.0387\n",
      "Epoch 1255/3000\n",
      "2370/2370 [==============================] - 0s 76us/step - loss: 0.0045 - val_loss: 0.0385\n",
      "Epoch 1256/3000\n",
      "2370/2370 [==============================] - 0s 78us/step - loss: 0.0045 - val_loss: 0.0383\n",
      "Epoch 1257/3000\n",
      "2370/2370 [==============================] - 0s 84us/step - loss: 0.0044 - val_loss: 0.0389\n",
      "Epoch 1258/3000\n",
      "2370/2370 [==============================] - 0s 75us/step - loss: 0.0044 - val_loss: 0.0392\n",
      "Epoch 1259/3000\n",
      "2370/2370 [==============================] - 0s 105us/step - loss: 0.0044 - val_loss: 0.0395\n",
      "Epoch 1260/3000\n",
      "2370/2370 [==============================] - 0s 100us/step - loss: 0.0044 - val_loss: 0.0392\n",
      "Epoch 1261/3000\n",
      "2370/2370 [==============================] - 0s 103us/step - loss: 0.0044 - val_loss: 0.0386\n",
      "Epoch 1262/3000\n",
      "2370/2370 [==============================] - 0s 78us/step - loss: 0.0043 - val_loss: 0.0388\n",
      "Epoch 1263/3000\n",
      "2370/2370 [==============================] - 0s 80us/step - loss: 0.0046 - val_loss: 0.0380\n",
      "Epoch 1264/3000\n",
      "2370/2370 [==============================] - 0s 83us/step - loss: 0.0044 - val_loss: 0.0379\n",
      "Epoch 1265/3000\n",
      "2370/2370 [==============================] - 0s 87us/step - loss: 0.0044 - val_loss: 0.0381\n",
      "Epoch 1266/3000\n",
      "2370/2370 [==============================] - 0s 84us/step - loss: 0.0044 - val_loss: 0.0383\n",
      "Epoch 1267/3000\n",
      "2370/2370 [==============================] - 0s 78us/step - loss: 0.0045 - val_loss: 0.0381\n",
      "Epoch 1268/3000\n",
      "2370/2370 [==============================] - 0s 85us/step - loss: 0.0047 - val_loss: 0.0384\n",
      "Epoch 1269/3000\n",
      "2370/2370 [==============================] - 0s 84us/step - loss: 0.0046 - val_loss: 0.0387\n",
      "Epoch 1270/3000\n",
      "2370/2370 [==============================] - 0s 75us/step - loss: 0.0046 - val_loss: 0.0383\n",
      "Epoch 1271/3000\n",
      "2370/2370 [==============================] - 0s 83us/step - loss: 0.0044 - val_loss: 0.0378\n",
      "Epoch 1272/3000\n",
      "2370/2370 [==============================] - 0s 74us/step - loss: 0.0043 - val_loss: 0.0383\n",
      "Epoch 1273/3000\n",
      "2370/2370 [==============================] - 0s 73us/step - loss: 0.0044 - val_loss: 0.0383\n",
      "Epoch 1274/3000\n",
      "2370/2370 [==============================] - 0s 84us/step - loss: 0.0045 - val_loss: 0.0386\n",
      "Epoch 1275/3000\n",
      "2370/2370 [==============================] - 0s 91us/step - loss: 0.0044 - val_loss: 0.0387\n",
      "Epoch 1276/3000\n",
      "2370/2370 [==============================] - 0s 76us/step - loss: 0.0045 - val_loss: 0.0384\n",
      "Epoch 1277/3000\n",
      "2370/2370 [==============================] - 0s 89us/step - loss: 0.0044 - val_loss: 0.0391\n",
      "Epoch 1278/3000\n",
      "2370/2370 [==============================] - 0s 77us/step - loss: 0.0047 - val_loss: 0.0388\n",
      "Epoch 1279/3000\n",
      "2370/2370 [==============================] - 0s 89us/step - loss: 0.0047 - val_loss: 0.0380\n",
      "Epoch 1280/3000\n",
      "2370/2370 [==============================] - 0s 99us/step - loss: 0.0047 - val_loss: 0.0384\n",
      "Epoch 1281/3000\n",
      "2370/2370 [==============================] - 0s 78us/step - loss: 0.0045 - val_loss: 0.0380\n",
      "Epoch 1282/3000\n",
      "2370/2370 [==============================] - 0s 77us/step - loss: 0.0045 - val_loss: 0.0371\n",
      "Epoch 1283/3000\n",
      "2370/2370 [==============================] - 0s 80us/step - loss: 0.0045 - val_loss: 0.0373\n",
      "Epoch 1284/3000\n",
      "2370/2370 [==============================] - 0s 82us/step - loss: 0.0044 - val_loss: 0.0382\n",
      "Epoch 1285/3000\n",
      "2370/2370 [==============================] - 0s 76us/step - loss: 0.0045 - val_loss: 0.0382\n",
      "Epoch 1286/3000\n",
      "2370/2370 [==============================] - 0s 75us/step - loss: 0.0047 - val_loss: 0.0380\n",
      "Epoch 1287/3000\n",
      "2370/2370 [==============================] - 0s 88us/step - loss: 0.0044 - val_loss: 0.0391\n",
      "Epoch 1288/3000\n",
      "2370/2370 [==============================] - 0s 100us/step - loss: 0.0043 - val_loss: 0.0387\n",
      "Epoch 1289/3000\n",
      "2370/2370 [==============================] - 0s 80us/step - loss: 0.0047 - val_loss: 0.0388\n",
      "Epoch 1290/3000\n",
      "2370/2370 [==============================] - 0s 75us/step - loss: 0.0046 - val_loss: 0.0386\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1291/3000\n",
      "2370/2370 [==============================] - 0s 86us/step - loss: 0.0047 - val_loss: 0.0373\n",
      "Epoch 1292/3000\n",
      "2370/2370 [==============================] - 0s 73us/step - loss: 0.0048 - val_loss: 0.0382\n",
      "Epoch 1293/3000\n",
      "2370/2370 [==============================] - 0s 80us/step - loss: 0.0049 - val_loss: 0.0375\n",
      "Epoch 1294/3000\n",
      "2370/2370 [==============================] - 0s 68us/step - loss: 0.0047 - val_loss: 0.0365\n",
      "Epoch 1295/3000\n",
      "2370/2370 [==============================] - 0s 67us/step - loss: 0.0047 - val_loss: 0.0370\n",
      "Epoch 1296/3000\n",
      "2370/2370 [==============================] - 0s 80us/step - loss: 0.0043 - val_loss: 0.0374\n",
      "Epoch 1297/3000\n",
      "2370/2370 [==============================] - 0s 74us/step - loss: 0.0046 - val_loss: 0.0382\n",
      "Epoch 1298/3000\n",
      "2370/2370 [==============================] - 0s 83us/step - loss: 0.0045 - val_loss: 0.0367\n",
      "Epoch 1299/3000\n",
      "2370/2370 [==============================] - 0s 70us/step - loss: 0.0046 - val_loss: 0.0371\n",
      "Epoch 1300/3000\n",
      "2370/2370 [==============================] - 0s 75us/step - loss: 0.0046 - val_loss: 0.0374\n",
      "Epoch 1301/3000\n",
      "2370/2370 [==============================] - 0s 76us/step - loss: 0.0045 - val_loss: 0.0388\n",
      "Epoch 1302/3000\n",
      "2370/2370 [==============================] - 0s 84us/step - loss: 0.0046 - val_loss: 0.0382\n",
      "Epoch 1303/3000\n",
      "2370/2370 [==============================] - 0s 81us/step - loss: 0.0048 - val_loss: 0.0376\n",
      "Epoch 1304/3000\n",
      "2370/2370 [==============================] - 0s 77us/step - loss: 0.0045 - val_loss: 0.0386\n",
      "Epoch 1305/3000\n",
      "2370/2370 [==============================] - 0s 71us/step - loss: 0.0045 - val_loss: 0.0386\n",
      "Epoch 1306/3000\n",
      "2370/2370 [==============================] - 0s 64us/step - loss: 0.0048 - val_loss: 0.0372\n",
      "Epoch 1307/3000\n",
      "2370/2370 [==============================] - 0s 65us/step - loss: 0.0046 - val_loss: 0.0381\n",
      "Epoch 1308/3000\n",
      "2370/2370 [==============================] - 0s 71us/step - loss: 0.0046 - val_loss: 0.0375\n",
      "Epoch 1309/3000\n",
      "2370/2370 [==============================] - 0s 78us/step - loss: 0.0045 - val_loss: 0.0385\n",
      "Epoch 1310/3000\n",
      "2370/2370 [==============================] - 0s 66us/step - loss: 0.0046 - val_loss: 0.0381\n",
      "Epoch 1311/3000\n",
      "2370/2370 [==============================] - 0s 88us/step - loss: 0.0044 - val_loss: 0.0383\n",
      "Epoch 1312/3000\n",
      "2370/2370 [==============================] - 0s 76us/step - loss: 0.0042 - val_loss: 0.0384\n",
      "Epoch 1313/3000\n",
      "2370/2370 [==============================] - 0s 85us/step - loss: 0.0045 - val_loss: 0.0379\n",
      "Epoch 1314/3000\n",
      "2370/2370 [==============================] - 0s 83us/step - loss: 0.0045 - val_loss: 0.0385\n",
      "Epoch 1315/3000\n",
      "2370/2370 [==============================] - 0s 79us/step - loss: 0.0046 - val_loss: 0.0384\n",
      "Epoch 1316/3000\n",
      "2370/2370 [==============================] - 0s 79us/step - loss: 0.0044 - val_loss: 0.0395\n",
      "Epoch 1317/3000\n",
      "2370/2370 [==============================] - 0s 72us/step - loss: 0.0044 - val_loss: 0.0392\n",
      "Epoch 1318/3000\n",
      "2370/2370 [==============================] - 0s 79us/step - loss: 0.0042 - val_loss: 0.0385\n",
      "Epoch 1319/3000\n",
      "2370/2370 [==============================] - 0s 86us/step - loss: 0.0044 - val_loss: 0.0379\n",
      "Epoch 1320/3000\n",
      "2370/2370 [==============================] - 0s 76us/step - loss: 0.0043 - val_loss: 0.0386\n",
      "Epoch 1321/3000\n",
      "2370/2370 [==============================] - 0s 68us/step - loss: 0.0044 - val_loss: 0.0383\n",
      "Epoch 1322/3000\n",
      "2370/2370 [==============================] - 0s 79us/step - loss: 0.0046 - val_loss: 0.0387\n",
      "Epoch 1323/3000\n",
      "2370/2370 [==============================] - 0s 74us/step - loss: 0.0043 - val_loss: 0.0384\n",
      "Epoch 1324/3000\n",
      "2370/2370 [==============================] - 0s 62us/step - loss: 0.0044 - val_loss: 0.0382\n",
      "Epoch 1325/3000\n",
      "2370/2370 [==============================] - 0s 81us/step - loss: 0.0044 - val_loss: 0.0378\n",
      "Epoch 1326/3000\n",
      "2370/2370 [==============================] - 0s 63us/step - loss: 0.0047 - val_loss: 0.0379\n",
      "Epoch 1327/3000\n",
      "2370/2370 [==============================] - 0s 73us/step - loss: 0.0046 - val_loss: 0.0387\n",
      "Epoch 1328/3000\n",
      "2370/2370 [==============================] - 0s 72us/step - loss: 0.0044 - val_loss: 0.0383\n",
      "Epoch 1329/3000\n",
      "2370/2370 [==============================] - 0s 74us/step - loss: 0.0044 - val_loss: 0.0382\n",
      "Epoch 1330/3000\n",
      "2370/2370 [==============================] - 0s 84us/step - loss: 0.0044 - val_loss: 0.0382\n",
      "Epoch 1331/3000\n",
      "2370/2370 [==============================] - 0s 87us/step - loss: 0.0044 - val_loss: 0.0394\n",
      "Epoch 1332/3000\n",
      "2370/2370 [==============================] - 0s 109us/step - loss: 0.0046 - val_loss: 0.0380\n",
      "Epoch 1333/3000\n",
      "2370/2370 [==============================] - 0s 95us/step - loss: 0.0043 - val_loss: 0.0377\n",
      "Epoch 1334/3000\n",
      "2370/2370 [==============================] - 0s 79us/step - loss: 0.0045 - val_loss: 0.0379\n",
      "Epoch 1335/3000\n",
      "2370/2370 [==============================] - 0s 79us/step - loss: 0.0047 - val_loss: 0.0379\n",
      "Epoch 1336/3000\n",
      "2370/2370 [==============================] - 0s 66us/step - loss: 0.0045 - val_loss: 0.0379\n",
      "Epoch 1337/3000\n",
      "2370/2370 [==============================] - 0s 66us/step - loss: 0.0043 - val_loss: 0.0374\n",
      "Epoch 1338/3000\n",
      "2370/2370 [==============================] - 0s 81us/step - loss: 0.0045 - val_loss: 0.0377\n",
      "Epoch 1339/3000\n",
      "2370/2370 [==============================] - 0s 67us/step - loss: 0.0047 - val_loss: 0.0368\n",
      "Epoch 1340/3000\n",
      "2370/2370 [==============================] - 0s 80us/step - loss: 0.0045 - val_loss: 0.0377\n",
      "Epoch 1341/3000\n",
      "2370/2370 [==============================] - 0s 78us/step - loss: 0.0043 - val_loss: 0.0380\n",
      "Epoch 1342/3000\n",
      "2370/2370 [==============================] - 0s 68us/step - loss: 0.0047 - val_loss: 0.0384\n",
      "Epoch 1343/3000\n",
      "2370/2370 [==============================] - 0s 65us/step - loss: 0.0045 - val_loss: 0.0379\n",
      "Epoch 1344/3000\n",
      "2370/2370 [==============================] - 0s 78us/step - loss: 0.0043 - val_loss: 0.0379\n",
      "Epoch 1345/3000\n",
      "2370/2370 [==============================] - 0s 67us/step - loss: 0.0045 - val_loss: 0.0378\n",
      "Epoch 1346/3000\n",
      "2370/2370 [==============================] - 0s 80us/step - loss: 0.0043 - val_loss: 0.0382\n",
      "Epoch 1347/3000\n",
      "2370/2370 [==============================] - 0s 64us/step - loss: 0.0045 - val_loss: 0.0377\n",
      "Epoch 1348/3000\n",
      "2370/2370 [==============================] - 0s 69us/step - loss: 0.0043 - val_loss: 0.0377\n",
      "Epoch 1349/3000\n",
      "2370/2370 [==============================] - 0s 72us/step - loss: 0.0045 - val_loss: 0.0394\n",
      "Epoch 1350/3000\n",
      "2370/2370 [==============================] - 0s 73us/step - loss: 0.0046 - val_loss: 0.0380\n",
      "Epoch 1351/3000\n",
      "2370/2370 [==============================] - 0s 71us/step - loss: 0.0042 - val_loss: 0.0393\n",
      "Epoch 1352/3000\n",
      "2370/2370 [==============================] - 0s 66us/step - loss: 0.0042 - val_loss: 0.0376\n",
      "Epoch 1353/3000\n",
      "2370/2370 [==============================] - 0s 80us/step - loss: 0.0045 - val_loss: 0.0378\n",
      "Epoch 1354/3000\n",
      "2370/2370 [==============================] - 0s 70us/step - loss: 0.0044 - val_loss: 0.0375\n",
      "Epoch 1355/3000\n",
      "2370/2370 [==============================] - 0s 73us/step - loss: 0.0045 - val_loss: 0.0380\n",
      "Epoch 1356/3000\n",
      "2370/2370 [==============================] - 0s 82us/step - loss: 0.0047 - val_loss: 0.0386\n",
      "Epoch 1357/3000\n",
      "2370/2370 [==============================] - 0s 87us/step - loss: 0.0044 - val_loss: 0.0384\n",
      "Epoch 1358/3000\n",
      "2370/2370 [==============================] - 0s 75us/step - loss: 0.0044 - val_loss: 0.0381\n",
      "Epoch 1359/3000\n",
      "2370/2370 [==============================] - 0s 82us/step - loss: 0.0045 - val_loss: 0.0374\n",
      "Epoch 1360/3000\n",
      "2370/2370 [==============================] - 0s 65us/step - loss: 0.0046 - val_loss: 0.0397\n",
      "Epoch 1361/3000\n",
      "2370/2370 [==============================] - 0s 69us/step - loss: 0.0044 - val_loss: 0.0380\n",
      "Epoch 1362/3000\n",
      "2370/2370 [==============================] - 0s 65us/step - loss: 0.0044 - val_loss: 0.0385\n",
      "Epoch 1363/3000\n",
      "2370/2370 [==============================] - 0s 79us/step - loss: 0.0046 - val_loss: 0.0387\n",
      "Epoch 1364/3000\n",
      "2370/2370 [==============================] - 0s 74us/step - loss: 0.0043 - val_loss: 0.0388\n",
      "Epoch 1365/3000\n",
      "2370/2370 [==============================] - 0s 69us/step - loss: 0.0043 - val_loss: 0.0385\n",
      "Epoch 1366/3000\n",
      "2370/2370 [==============================] - 0s 71us/step - loss: 0.0045 - val_loss: 0.0382\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1367/3000\n",
      "2370/2370 [==============================] - 0s 72us/step - loss: 0.0046 - val_loss: 0.0374\n",
      "Epoch 1368/3000\n",
      "2370/2370 [==============================] - 0s 59us/step - loss: 0.0045 - val_loss: 0.0389\n",
      "Epoch 1369/3000\n",
      "2370/2370 [==============================] - 0s 64us/step - loss: 0.0045 - val_loss: 0.0382\n",
      "Epoch 1370/3000\n",
      "2370/2370 [==============================] - 0s 80us/step - loss: 0.0048 - val_loss: 0.0378\n",
      "Epoch 1371/3000\n",
      "2370/2370 [==============================] - 0s 67us/step - loss: 0.0044 - val_loss: 0.0376\n",
      "Epoch 1372/3000\n",
      "2370/2370 [==============================] - 0s 77us/step - loss: 0.0045 - val_loss: 0.0385\n",
      "Epoch 1373/3000\n",
      "2370/2370 [==============================] - 0s 75us/step - loss: 0.0046 - val_loss: 0.0381\n",
      "Epoch 1374/3000\n",
      "2370/2370 [==============================] - 0s 78us/step - loss: 0.0044 - val_loss: 0.0380\n",
      "Epoch 1375/3000\n",
      "2370/2370 [==============================] - 0s 68us/step - loss: 0.0044 - val_loss: 0.0386\n",
      "Epoch 1376/3000\n",
      "2370/2370 [==============================] - 0s 68us/step - loss: 0.0045 - val_loss: 0.0388\n",
      "Epoch 1377/3000\n",
      "2370/2370 [==============================] - 0s 75us/step - loss: 0.0043 - val_loss: 0.0385\n",
      "Epoch 1378/3000\n",
      "2370/2370 [==============================] - 0s 78us/step - loss: 0.0044 - val_loss: 0.0380\n",
      "Epoch 1379/3000\n",
      "2370/2370 [==============================] - 0s 71us/step - loss: 0.0043 - val_loss: 0.0392\n",
      "Epoch 1380/3000\n",
      "2370/2370 [==============================] - 0s 70us/step - loss: 0.0044 - val_loss: 0.0379\n",
      "Epoch 1381/3000\n",
      "2370/2370 [==============================] - 0s 77us/step - loss: 0.0048 - val_loss: 0.0385\n",
      "Epoch 1382/3000\n",
      "2370/2370 [==============================] - 0s 76us/step - loss: 0.0048 - val_loss: 0.0379\n",
      "Epoch 1383/3000\n",
      "2370/2370 [==============================] - 0s 71us/step - loss: 0.0045 - val_loss: 0.0389\n",
      "Epoch 1384/3000\n",
      "2370/2370 [==============================] - 0s 81us/step - loss: 0.0045 - val_loss: 0.0388\n",
      "Epoch 1385/3000\n",
      "2370/2370 [==============================] - 0s 111us/step - loss: 0.0046 - val_loss: 0.0387\n",
      "Epoch 1386/3000\n",
      "2370/2370 [==============================] - 0s 83us/step - loss: 0.0045 - val_loss: 0.0392\n",
      "Epoch 1387/3000\n",
      "2370/2370 [==============================] - 0s 65us/step - loss: 0.0045 - val_loss: 0.0389\n",
      "Epoch 1388/3000\n",
      "2370/2370 [==============================] - 0s 79us/step - loss: 0.0044 - val_loss: 0.0383\n",
      "Epoch 1389/3000\n",
      "2370/2370 [==============================] - 0s 86us/step - loss: 0.0045 - val_loss: 0.0384\n",
      "Epoch 1390/3000\n",
      "2370/2370 [==============================] - 0s 82us/step - loss: 0.0044 - val_loss: 0.0390\n",
      "Epoch 1391/3000\n",
      "2370/2370 [==============================] - 0s 86us/step - loss: 0.0044 - val_loss: 0.0385\n",
      "Epoch 1392/3000\n",
      "2370/2370 [==============================] - 0s 75us/step - loss: 0.0045 - val_loss: 0.0385\n",
      "Epoch 1393/3000\n",
      "2370/2370 [==============================] - 0s 60us/step - loss: 0.0045 - val_loss: 0.0385\n",
      "Epoch 1394/3000\n",
      "2370/2370 [==============================] - 0s 83us/step - loss: 0.0045 - val_loss: 0.0390\n",
      "Epoch 1395/3000\n",
      "2370/2370 [==============================] - 0s 89us/step - loss: 0.0043 - val_loss: 0.0387\n",
      "Epoch 1396/3000\n",
      "2370/2370 [==============================] - 0s 98us/step - loss: 0.0045 - val_loss: 0.0382\n",
      "Epoch 1397/3000\n",
      "2370/2370 [==============================] - 0s 88us/step - loss: 0.0045 - val_loss: 0.0387\n",
      "Epoch 1398/3000\n",
      "2370/2370 [==============================] - 0s 99us/step - loss: 0.0045 - val_loss: 0.0388\n",
      "Epoch 1399/3000\n",
      "2370/2370 [==============================] - 0s 97us/step - loss: 0.0046 - val_loss: 0.0384\n",
      "Epoch 1400/3000\n",
      "2370/2370 [==============================] - 0s 83us/step - loss: 0.0045 - val_loss: 0.0375\n",
      "Epoch 1401/3000\n",
      "2370/2370 [==============================] - 0s 83us/step - loss: 0.0045 - val_loss: 0.0383\n",
      "Epoch 1402/3000\n",
      "2370/2370 [==============================] - 0s 85us/step - loss: 0.0045 - val_loss: 0.0379\n",
      "Epoch 1403/3000\n",
      "2370/2370 [==============================] - 0s 73us/step - loss: 0.0044 - val_loss: 0.0384\n",
      "Epoch 1404/3000\n",
      "2370/2370 [==============================] - 0s 81us/step - loss: 0.0043 - val_loss: 0.0391\n",
      "Epoch 1405/3000\n",
      "2370/2370 [==============================] - 0s 81us/step - loss: 0.0044 - val_loss: 0.0386\n",
      "Epoch 1406/3000\n",
      "2370/2370 [==============================] - 0s 83us/step - loss: 0.0043 - val_loss: 0.0386\n",
      "Epoch 1407/3000\n",
      "2370/2370 [==============================] - 0s 90us/step - loss: 0.0045 - val_loss: 0.0385\n",
      "Epoch 1408/3000\n",
      "2370/2370 [==============================] - 0s 95us/step - loss: 0.0046 - val_loss: 0.0392\n",
      "Epoch 1409/3000\n",
      "2370/2370 [==============================] - 0s 71us/step - loss: 0.0046 - val_loss: 0.0382\n",
      "Epoch 1410/3000\n",
      "2370/2370 [==============================] - 0s 74us/step - loss: 0.0044 - val_loss: 0.0382\n",
      "Epoch 1411/3000\n",
      "2370/2370 [==============================] - 0s 92us/step - loss: 0.0043 - val_loss: 0.0383\n",
      "Epoch 1412/3000\n",
      "2370/2370 [==============================] - 0s 95us/step - loss: 0.0045 - val_loss: 0.0393\n",
      "Epoch 1413/3000\n",
      "2370/2370 [==============================] - 0s 81us/step - loss: 0.0045 - val_loss: 0.0386\n",
      "Epoch 1414/3000\n",
      "2370/2370 [==============================] - 0s 89us/step - loss: 0.0046 - val_loss: 0.0389\n",
      "Epoch 1415/3000\n",
      "2370/2370 [==============================] - 0s 85us/step - loss: 0.0046 - val_loss: 0.0385\n",
      "Epoch 1416/3000\n",
      "2370/2370 [==============================] - 0s 87us/step - loss: 0.0044 - val_loss: 0.0385\n",
      "Epoch 1417/3000\n",
      "2370/2370 [==============================] - 0s 77us/step - loss: 0.0044 - val_loss: 0.0393\n",
      "Epoch 1418/3000\n",
      "2370/2370 [==============================] - 0s 73us/step - loss: 0.0045 - val_loss: 0.0383\n",
      "Epoch 1419/3000\n",
      "2370/2370 [==============================] - 0s 97us/step - loss: 0.0043 - val_loss: 0.0389\n",
      "Epoch 1420/3000\n",
      "2370/2370 [==============================] - 0s 81us/step - loss: 0.0044 - val_loss: 0.0377\n",
      "Epoch 1421/3000\n",
      "2370/2370 [==============================] - 0s 75us/step - loss: 0.0042 - val_loss: 0.0387\n",
      "Epoch 1422/3000\n",
      "2370/2370 [==============================] - 0s 98us/step - loss: 0.0044 - val_loss: 0.0379\n",
      "Epoch 1423/3000\n",
      "2370/2370 [==============================] - 0s 102us/step - loss: 0.0046 - val_loss: 0.0384\n",
      "Epoch 1424/3000\n",
      "2370/2370 [==============================] - 0s 97us/step - loss: 0.0044 - val_loss: 0.0382\n",
      "Epoch 1425/3000\n",
      "2370/2370 [==============================] - 0s 93us/step - loss: 0.0045 - val_loss: 0.0386\n",
      "Epoch 1426/3000\n",
      "2370/2370 [==============================] - 0s 82us/step - loss: 0.0046 - val_loss: 0.0382\n",
      "Epoch 1427/3000\n",
      "2370/2370 [==============================] - 0s 78us/step - loss: 0.0044 - val_loss: 0.0380\n",
      "Epoch 1428/3000\n",
      "2370/2370 [==============================] - 0s 88us/step - loss: 0.0043 - val_loss: 0.0379\n",
      "Epoch 1429/3000\n",
      "2370/2370 [==============================] - 0s 78us/step - loss: 0.0042 - val_loss: 0.0386\n",
      "Epoch 1430/3000\n",
      "2370/2370 [==============================] - 0s 70us/step - loss: 0.0042 - val_loss: 0.0381\n",
      "Epoch 1431/3000\n",
      "2370/2370 [==============================] - 0s 94us/step - loss: 0.0045 - val_loss: 0.0378\n",
      "Epoch 1432/3000\n",
      "2370/2370 [==============================] - 0s 85us/step - loss: 0.0042 - val_loss: 0.0377\n",
      "Epoch 1433/3000\n",
      "2370/2370 [==============================] - 0s 78us/step - loss: 0.0045 - val_loss: 0.0379\n",
      "Epoch 1434/3000\n",
      "2370/2370 [==============================] - 0s 77us/step - loss: 0.0046 - val_loss: 0.0370\n",
      "Epoch 1435/3000\n",
      "2370/2370 [==============================] - 0s 103us/step - loss: 0.0046 - val_loss: 0.0379\n",
      "Epoch 1436/3000\n",
      "2370/2370 [==============================] - 0s 104us/step - loss: 0.0044 - val_loss: 0.0386\n",
      "Epoch 1437/3000\n",
      "2370/2370 [==============================] - 0s 89us/step - loss: 0.0042 - val_loss: 0.0375\n",
      "Epoch 1438/3000\n",
      "2370/2370 [==============================] - 0s 80us/step - loss: 0.0046 - val_loss: 0.0398\n",
      "Epoch 1439/3000\n",
      "2370/2370 [==============================] - 0s 68us/step - loss: 0.0045 - val_loss: 0.0380\n",
      "Epoch 1440/3000\n",
      "2370/2370 [==============================] - 0s 82us/step - loss: 0.0045 - val_loss: 0.0392\n",
      "Epoch 1441/3000\n",
      "2370/2370 [==============================] - 0s 87us/step - loss: 0.0044 - val_loss: 0.0373\n",
      "Epoch 1442/3000\n",
      "2370/2370 [==============================] - 0s 70us/step - loss: 0.0044 - val_loss: 0.0371\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1443/3000\n",
      "2370/2370 [==============================] - 0s 83us/step - loss: 0.0045 - val_loss: 0.0376\n",
      "Epoch 1444/3000\n",
      "2370/2370 [==============================] - 0s 78us/step - loss: 0.0043 - val_loss: 0.0383\n",
      "Epoch 1445/3000\n",
      "2370/2370 [==============================] - 0s 79us/step - loss: 0.0042 - val_loss: 0.0385\n",
      "Epoch 1446/3000\n",
      "2370/2370 [==============================] - 0s 77us/step - loss: 0.0044 - val_loss: 0.0386\n",
      "Epoch 1447/3000\n",
      "2370/2370 [==============================] - 0s 66us/step - loss: 0.0045 - val_loss: 0.0398\n",
      "Epoch 1448/3000\n",
      "2370/2370 [==============================] - 0s 78us/step - loss: 0.0043 - val_loss: 0.0393\n",
      "Epoch 1449/3000\n",
      "2370/2370 [==============================] - 0s 80us/step - loss: 0.0046 - val_loss: 0.0388\n",
      "Epoch 1450/3000\n",
      "2370/2370 [==============================] - 0s 70us/step - loss: 0.0046 - val_loss: 0.0387\n",
      "Epoch 1451/3000\n",
      "2370/2370 [==============================] - 0s 99us/step - loss: 0.0042 - val_loss: 0.0393\n",
      "Epoch 1452/3000\n",
      "2370/2370 [==============================] - 0s 90us/step - loss: 0.0045 - val_loss: 0.0387\n",
      "Epoch 1453/3000\n",
      "2370/2370 [==============================] - 0s 91us/step - loss: 0.0046 - val_loss: 0.0376\n",
      "Epoch 1454/3000\n",
      "2370/2370 [==============================] - 0s 80us/step - loss: 0.0045 - val_loss: 0.0382\n",
      "Epoch 1455/3000\n",
      "2370/2370 [==============================] - 0s 74us/step - loss: 0.0044 - val_loss: 0.0382\n",
      "Epoch 1456/3000\n",
      "2370/2370 [==============================] - 0s 89us/step - loss: 0.0045 - val_loss: 0.0379\n",
      "Epoch 1457/3000\n",
      "2370/2370 [==============================] - 0s 78us/step - loss: 0.0044 - val_loss: 0.0375\n",
      "Epoch 1458/3000\n",
      "2370/2370 [==============================] - 0s 67us/step - loss: 0.0044 - val_loss: 0.0384\n",
      "Epoch 1459/3000\n",
      "2370/2370 [==============================] - 0s 77us/step - loss: 0.0044 - val_loss: 0.0386\n",
      "Epoch 1460/3000\n",
      "2370/2370 [==============================] - 0s 68us/step - loss: 0.0043 - val_loss: 0.0382\n",
      "Epoch 1461/3000\n",
      "2370/2370 [==============================] - 0s 91us/step - loss: 0.0044 - val_loss: 0.0386\n",
      "Epoch 1462/3000\n",
      "2370/2370 [==============================] - 0s 85us/step - loss: 0.0043 - val_loss: 0.0381\n",
      "Epoch 1463/3000\n",
      "2370/2370 [==============================] - 0s 90us/step - loss: 0.0043 - val_loss: 0.0385\n",
      "Epoch 1464/3000\n",
      "2370/2370 [==============================] - 0s 85us/step - loss: 0.0044 - val_loss: 0.0375\n",
      "Epoch 1465/3000\n",
      "2370/2370 [==============================] - 0s 87us/step - loss: 0.0044 - val_loss: 0.0386\n",
      "Epoch 1466/3000\n",
      "2370/2370 [==============================] - 0s 81us/step - loss: 0.0044 - val_loss: 0.0392\n",
      "Epoch 1467/3000\n",
      "2370/2370 [==============================] - 0s 74us/step - loss: 0.0043 - val_loss: 0.0382\n",
      "Epoch 1468/3000\n",
      "2370/2370 [==============================] - 0s 85us/step - loss: 0.0044 - val_loss: 0.0392\n",
      "Epoch 1469/3000\n",
      "2370/2370 [==============================] - 0s 84us/step - loss: 0.0043 - val_loss: 0.0388\n",
      "Epoch 1470/3000\n",
      "2370/2370 [==============================] - 0s 78us/step - loss: 0.0045 - val_loss: 0.0388\n",
      "Epoch 1471/3000\n",
      "2370/2370 [==============================] - 0s 95us/step - loss: 0.0043 - val_loss: 0.0392\n",
      "Epoch 1472/3000\n",
      "2370/2370 [==============================] - 0s 100us/step - loss: 0.0044 - val_loss: 0.0384\n",
      "Epoch 1473/3000\n",
      "2370/2370 [==============================] - 0s 89us/step - loss: 0.0046 - val_loss: 0.0392\n",
      "Epoch 1474/3000\n",
      "2370/2370 [==============================] - 0s 92us/step - loss: 0.0042 - val_loss: 0.0378\n",
      "Epoch 1475/3000\n",
      "2370/2370 [==============================] - 0s 74us/step - loss: 0.0045 - val_loss: 0.0384\n",
      "Epoch 1476/3000\n",
      "2370/2370 [==============================] - 0s 83us/step - loss: 0.0044 - val_loss: 0.0386\n",
      "Epoch 1477/3000\n",
      "2370/2370 [==============================] - 0s 75us/step - loss: 0.0044 - val_loss: 0.0382\n",
      "Epoch 1478/3000\n",
      "2370/2370 [==============================] - 0s 77us/step - loss: 0.0042 - val_loss: 0.0379\n",
      "Epoch 1479/3000\n",
      "2370/2370 [==============================] - 0s 73us/step - loss: 0.0046 - val_loss: 0.0382\n",
      "Epoch 1480/3000\n",
      "2370/2370 [==============================] - 0s 72us/step - loss: 0.0042 - val_loss: 0.0381\n",
      "Epoch 1481/3000\n",
      "2370/2370 [==============================] - 0s 67us/step - loss: 0.0045 - val_loss: 0.0380\n",
      "Epoch 1482/3000\n",
      "2370/2370 [==============================] - 0s 81us/step - loss: 0.0044 - val_loss: 0.0383\n",
      "Epoch 1483/3000\n",
      "2370/2370 [==============================] - 0s 77us/step - loss: 0.0044 - val_loss: 0.0386\n",
      "Epoch 1484/3000\n",
      "2370/2370 [==============================] - 0s 71us/step - loss: 0.0044 - val_loss: 0.0386\n",
      "Epoch 1485/3000\n",
      "2370/2370 [==============================] - 0s 72us/step - loss: 0.0045 - val_loss: 0.0384\n",
      "Epoch 1486/3000\n",
      "2370/2370 [==============================] - 0s 78us/step - loss: 0.0042 - val_loss: 0.0378\n",
      "Epoch 1487/3000\n",
      "2370/2370 [==============================] - 0s 71us/step - loss: 0.0043 - val_loss: 0.0380\n",
      "Epoch 1488/3000\n",
      "2370/2370 [==============================] - 0s 72us/step - loss: 0.0045 - val_loss: 0.0383\n",
      "Epoch 1489/3000\n",
      "2370/2370 [==============================] - 0s 78us/step - loss: 0.0043 - val_loss: 0.0382\n",
      "Epoch 1490/3000\n",
      "2370/2370 [==============================] - 0s 75us/step - loss: 0.0046 - val_loss: 0.0378\n",
      "Epoch 1491/3000\n",
      "2370/2370 [==============================] - 0s 67us/step - loss: 0.0045 - val_loss: 0.0383\n",
      "Epoch 1492/3000\n",
      "2370/2370 [==============================] - 0s 76us/step - loss: 0.0043 - val_loss: 0.0391\n",
      "Epoch 1493/3000\n",
      "2370/2370 [==============================] - 0s 75us/step - loss: 0.0042 - val_loss: 0.0385\n",
      "Epoch 1494/3000\n",
      "2370/2370 [==============================] - 0s 74us/step - loss: 0.0043 - val_loss: 0.0394\n",
      "Epoch 1495/3000\n",
      "2370/2370 [==============================] - 0s 70us/step - loss: 0.0045 - val_loss: 0.0395\n",
      "Epoch 1496/3000\n",
      "2370/2370 [==============================] - 0s 65us/step - loss: 0.0043 - val_loss: 0.0383\n",
      "Epoch 1497/3000\n",
      "2370/2370 [==============================] - 0s 75us/step - loss: 0.0044 - val_loss: 0.0380\n",
      "Epoch 1498/3000\n",
      "2370/2370 [==============================] - 0s 71us/step - loss: 0.0046 - val_loss: 0.0384\n",
      "Epoch 1499/3000\n",
      "2370/2370 [==============================] - 0s 80us/step - loss: 0.0043 - val_loss: 0.0382\n",
      "Epoch 1500/3000\n",
      "2370/2370 [==============================] - 0s 69us/step - loss: 0.0045 - val_loss: 0.0380\n",
      "Epoch 1501/3000\n",
      "2370/2370 [==============================] - 0s 56us/step - loss: 0.0042 - val_loss: 0.0376\n",
      "Epoch 1502/3000\n",
      "2370/2370 [==============================] - 0s 73us/step - loss: 0.0044 - val_loss: 0.0375\n",
      "Epoch 1503/3000\n",
      "2370/2370 [==============================] - 0s 68us/step - loss: 0.0046 - val_loss: 0.0374\n",
      "Epoch 1504/3000\n",
      "2370/2370 [==============================] - 0s 68us/step - loss: 0.0045 - val_loss: 0.0375\n",
      "Epoch 1505/3000\n",
      "2370/2370 [==============================] - 0s 76us/step - loss: 0.0042 - val_loss: 0.0391\n",
      "Epoch 1506/3000\n",
      "2370/2370 [==============================] - 0s 75us/step - loss: 0.0045 - val_loss: 0.0386\n",
      "Epoch 1507/3000\n",
      "2370/2370 [==============================] - 0s 74us/step - loss: 0.0044 - val_loss: 0.0386\n",
      "Epoch 1508/3000\n",
      "2370/2370 [==============================] - 0s 77us/step - loss: 0.0044 - val_loss: 0.0378\n",
      "Epoch 1509/3000\n",
      "2370/2370 [==============================] - 0s 74us/step - loss: 0.0041 - val_loss: 0.0377\n",
      "Epoch 1510/3000\n",
      "2370/2370 [==============================] - 0s 67us/step - loss: 0.0045 - val_loss: 0.0377\n",
      "Epoch 1511/3000\n",
      "2370/2370 [==============================] - 0s 75us/step - loss: 0.0044 - val_loss: 0.0379\n",
      "Epoch 1512/3000\n",
      "2370/2370 [==============================] - 0s 69us/step - loss: 0.0044 - val_loss: 0.0375\n",
      "Epoch 1513/3000\n",
      "2370/2370 [==============================] - 0s 67us/step - loss: 0.0043 - val_loss: 0.0386\n",
      "Epoch 1514/3000\n",
      "2370/2370 [==============================] - 0s 77us/step - loss: 0.0043 - val_loss: 0.0385\n",
      "Epoch 1515/3000\n",
      "2370/2370 [==============================] - 0s 72us/step - loss: 0.0045 - val_loss: 0.0383\n",
      "Epoch 1516/3000\n",
      "2370/2370 [==============================] - 0s 74us/step - loss: 0.0045 - val_loss: 0.0387\n",
      "Epoch 1517/3000\n",
      "2370/2370 [==============================] - 0s 80us/step - loss: 0.0046 - val_loss: 0.0385\n",
      "Epoch 1518/3000\n",
      "2370/2370 [==============================] - 0s 68us/step - loss: 0.0042 - val_loss: 0.0388\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1519/3000\n",
      "2370/2370 [==============================] - 0s 68us/step - loss: 0.0043 - val_loss: 0.0390\n",
      "Epoch 1520/3000\n",
      "2370/2370 [==============================] - 0s 67us/step - loss: 0.0043 - val_loss: 0.0383\n",
      "Epoch 1521/3000\n",
      "2370/2370 [==============================] - 0s 73us/step - loss: 0.0046 - val_loss: 0.0382\n",
      "Epoch 1522/3000\n",
      "2370/2370 [==============================] - 0s 102us/step - loss: 0.0044 - val_loss: 0.0383\n",
      "Epoch 1523/3000\n",
      "2370/2370 [==============================] - 0s 92us/step - loss: 0.0045 - val_loss: 0.0387\n",
      "Epoch 1524/3000\n",
      "2370/2370 [==============================] - 0s 81us/step - loss: 0.0043 - val_loss: 0.0378\n",
      "Epoch 1525/3000\n",
      "2370/2370 [==============================] - 0s 79us/step - loss: 0.0044 - val_loss: 0.0387\n",
      "Epoch 1526/3000\n",
      "2370/2370 [==============================] - 0s 97us/step - loss: 0.0043 - val_loss: 0.0382\n",
      "Epoch 1527/3000\n",
      "2370/2370 [==============================] - 0s 93us/step - loss: 0.0043 - val_loss: 0.0382\n",
      "Epoch 1528/3000\n",
      "2370/2370 [==============================] - 0s 91us/step - loss: 0.0044 - val_loss: 0.0383\n",
      "Epoch 1529/3000\n",
      "2370/2370 [==============================] - 0s 89us/step - loss: 0.0044 - val_loss: 0.0383\n",
      "Epoch 1530/3000\n",
      "2370/2370 [==============================] - 0s 92us/step - loss: 0.0041 - val_loss: 0.0386\n",
      "Epoch 1531/3000\n",
      "2370/2370 [==============================] - 0s 103us/step - loss: 0.0044 - val_loss: 0.0386\n",
      "Epoch 1532/3000\n",
      "2370/2370 [==============================] - 0s 113us/step - loss: 0.0043 - val_loss: 0.0388\n",
      "Epoch 1533/3000\n",
      "2370/2370 [==============================] - 0s 90us/step - loss: 0.0044 - val_loss: 0.0387\n",
      "Epoch 1534/3000\n",
      "2370/2370 [==============================] - 0s 81us/step - loss: 0.0044 - val_loss: 0.0383\n",
      "Epoch 1535/3000\n",
      "2370/2370 [==============================] - 0s 84us/step - loss: 0.0044 - val_loss: 0.0387\n",
      "Epoch 1536/3000\n",
      "2370/2370 [==============================] - 0s 108us/step - loss: 0.0043 - val_loss: 0.0391\n",
      "Epoch 1537/3000\n",
      "2370/2370 [==============================] - 0s 76us/step - loss: 0.0044 - val_loss: 0.0383\n",
      "Epoch 1538/3000\n",
      "2370/2370 [==============================] - 0s 82us/step - loss: 0.0044 - val_loss: 0.0382\n",
      "Epoch 1539/3000\n",
      "2370/2370 [==============================] - 0s 69us/step - loss: 0.0045 - val_loss: 0.0388\n",
      "Epoch 1540/3000\n",
      "2370/2370 [==============================] - 0s 78us/step - loss: 0.0045 - val_loss: 0.0392\n",
      "Epoch 1541/3000\n",
      "2370/2370 [==============================] - 0s 81us/step - loss: 0.0042 - val_loss: 0.0381\n",
      "Epoch 1542/3000\n",
      "2370/2370 [==============================] - 0s 73us/step - loss: 0.0045 - val_loss: 0.0388\n",
      "Epoch 1543/3000\n",
      "2370/2370 [==============================] - 0s 80us/step - loss: 0.0043 - val_loss: 0.0384\n",
      "Epoch 1544/3000\n",
      "2370/2370 [==============================] - 0s 79us/step - loss: 0.0045 - val_loss: 0.0393\n",
      "Epoch 1545/3000\n",
      "2370/2370 [==============================] - 0s 69us/step - loss: 0.0045 - val_loss: 0.0380\n",
      "Epoch 1546/3000\n",
      "2370/2370 [==============================] - 0s 77us/step - loss: 0.0044 - val_loss: 0.0377\n",
      "Epoch 1547/3000\n",
      "2370/2370 [==============================] - 0s 83us/step - loss: 0.0045 - val_loss: 0.0385\n",
      "Epoch 1548/3000\n",
      "2370/2370 [==============================] - 0s 69us/step - loss: 0.0045 - val_loss: 0.0379\n",
      "Epoch 1549/3000\n",
      "2370/2370 [==============================] - 0s 69us/step - loss: 0.0047 - val_loss: 0.0393\n",
      "Epoch 1550/3000\n",
      "2370/2370 [==============================] - 0s 66us/step - loss: 0.0045 - val_loss: 0.0380\n",
      "Epoch 1551/3000\n",
      "2370/2370 [==============================] - 0s 69us/step - loss: 0.0043 - val_loss: 0.0381\n",
      "Epoch 1552/3000\n",
      "2370/2370 [==============================] - 0s 75us/step - loss: 0.0045 - val_loss: 0.0378\n",
      "Epoch 1553/3000\n",
      "2370/2370 [==============================] - 0s 77us/step - loss: 0.0045 - val_loss: 0.0383\n",
      "Epoch 1554/3000\n",
      "2370/2370 [==============================] - 0s 59us/step - loss: 0.0046 - val_loss: 0.0385\n",
      "Epoch 1555/3000\n",
      "2370/2370 [==============================] - 0s 70us/step - loss: 0.0045 - val_loss: 0.0394\n",
      "Epoch 1556/3000\n",
      "2370/2370 [==============================] - 0s 71us/step - loss: 0.0044 - val_loss: 0.0381\n",
      "Epoch 1557/3000\n",
      "2370/2370 [==============================] - 0s 70us/step - loss: 0.0043 - val_loss: 0.0390\n",
      "Epoch 1558/3000\n",
      "2370/2370 [==============================] - 0s 77us/step - loss: 0.0044 - val_loss: 0.0378\n",
      "Epoch 1559/3000\n",
      "2370/2370 [==============================] - 0s 71us/step - loss: 0.0043 - val_loss: 0.0380\n",
      "Epoch 1560/3000\n",
      "2370/2370 [==============================] - 0s 73us/step - loss: 0.0043 - val_loss: 0.0383\n",
      "Epoch 1561/3000\n",
      "2370/2370 [==============================] - 0s 76us/step - loss: 0.0043 - val_loss: 0.0378\n",
      "Epoch 1562/3000\n",
      "2370/2370 [==============================] - 0s 72us/step - loss: 0.0043 - val_loss: 0.0378\n",
      "Epoch 1563/3000\n",
      "2370/2370 [==============================] - 0s 62us/step - loss: 0.0041 - val_loss: 0.0389\n",
      "Epoch 1564/3000\n",
      "2370/2370 [==============================] - 0s 76us/step - loss: 0.0046 - val_loss: 0.0390\n",
      "Epoch 1565/3000\n",
      "2370/2370 [==============================] - 0s 65us/step - loss: 0.0044 - val_loss: 0.0385\n",
      "Epoch 1566/3000\n",
      "2370/2370 [==============================] - 0s 76us/step - loss: 0.0043 - val_loss: 0.0374\n",
      "Epoch 1567/3000\n",
      "2370/2370 [==============================] - 0s 76us/step - loss: 0.0044 - val_loss: 0.0384\n",
      "Epoch 1568/3000\n",
      "2370/2370 [==============================] - 0s 69us/step - loss: 0.0042 - val_loss: 0.0382\n",
      "Epoch 1569/3000\n",
      "2370/2370 [==============================] - 0s 68us/step - loss: 0.0046 - val_loss: 0.0385\n",
      "Epoch 1570/3000\n",
      "2370/2370 [==============================] - 0s 73us/step - loss: 0.0044 - val_loss: 0.0381\n",
      "Epoch 1571/3000\n",
      "2370/2370 [==============================] - 0s 60us/step - loss: 0.0046 - val_loss: 0.0374\n",
      "Epoch 1572/3000\n",
      "2370/2370 [==============================] - 0s 78us/step - loss: 0.0043 - val_loss: 0.0374\n",
      "Epoch 1573/3000\n",
      "2370/2370 [==============================] - 0s 79us/step - loss: 0.0045 - val_loss: 0.0381\n",
      "Epoch 1574/3000\n",
      "2370/2370 [==============================] - 0s 65us/step - loss: 0.0043 - val_loss: 0.0383\n",
      "Epoch 1575/3000\n",
      "2370/2370 [==============================] - 0s 73us/step - loss: 0.0044 - val_loss: 0.0382\n",
      "Epoch 1576/3000\n",
      "2370/2370 [==============================] - 0s 75us/step - loss: 0.0045 - val_loss: 0.0376\n",
      "Epoch 1577/3000\n",
      "2370/2370 [==============================] - 0s 67us/step - loss: 0.0047 - val_loss: 0.0391\n",
      "Epoch 1578/3000\n",
      "2370/2370 [==============================] - 0s 63us/step - loss: 0.0045 - val_loss: 0.0382\n",
      "Epoch 1579/3000\n",
      "2370/2370 [==============================] - 0s 62us/step - loss: 0.0044 - val_loss: 0.0381\n",
      "Epoch 1580/3000\n",
      "2370/2370 [==============================] - 0s 63us/step - loss: 0.0042 - val_loss: 0.0381\n",
      "Epoch 1581/3000\n",
      "2370/2370 [==============================] - 0s 77us/step - loss: 0.0045 - val_loss: 0.0384\n",
      "Epoch 1582/3000\n",
      "2370/2370 [==============================] - 0s 66us/step - loss: 0.0044 - val_loss: 0.0381\n",
      "Epoch 1583/3000\n",
      "2370/2370 [==============================] - 0s 58us/step - loss: 0.0043 - val_loss: 0.0385\n",
      "Epoch 1584/3000\n",
      "2370/2370 [==============================] - 0s 68us/step - loss: 0.0043 - val_loss: 0.0386\n",
      "Epoch 1585/3000\n",
      "2370/2370 [==============================] - 0s 76us/step - loss: 0.0045 - val_loss: 0.0378\n",
      "Epoch 1586/3000\n",
      "2370/2370 [==============================] - 0s 76us/step - loss: 0.0045 - val_loss: 0.0381\n",
      "Epoch 1587/3000\n",
      "2370/2370 [==============================] - 0s 79us/step - loss: 0.0045 - val_loss: 0.0389\n",
      "Epoch 1588/3000\n",
      "2370/2370 [==============================] - 0s 69us/step - loss: 0.0043 - val_loss: 0.0383\n",
      "Epoch 1589/3000\n",
      "2370/2370 [==============================] - 0s 68us/step - loss: 0.0044 - val_loss: 0.0381\n",
      "Epoch 1590/3000\n",
      "2370/2370 [==============================] - 0s 61us/step - loss: 0.0042 - val_loss: 0.0377\n",
      "Epoch 1591/3000\n",
      "2370/2370 [==============================] - 0s 80us/step - loss: 0.0043 - val_loss: 0.0379\n",
      "Epoch 1592/3000\n",
      "2370/2370 [==============================] - 0s 65us/step - loss: 0.0045 - val_loss: 0.0375\n",
      "Epoch 1593/3000\n",
      "2370/2370 [==============================] - 0s 62us/step - loss: 0.0046 - val_loss: 0.0387\n",
      "Epoch 1594/3000\n",
      "2370/2370 [==============================] - 0s 68us/step - loss: 0.0044 - val_loss: 0.0384\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1595/3000\n",
      "2370/2370 [==============================] - 0s 61us/step - loss: 0.0043 - val_loss: 0.0383\n",
      "Epoch 1596/3000\n",
      "2370/2370 [==============================] - 0s 63us/step - loss: 0.0044 - val_loss: 0.0377\n",
      "Epoch 1597/3000\n",
      "2370/2370 [==============================] - 0s 78us/step - loss: 0.0045 - val_loss: 0.0387\n",
      "Epoch 1598/3000\n",
      "2370/2370 [==============================] - 0s 63us/step - loss: 0.0045 - val_loss: 0.0378\n",
      "Epoch 1599/3000\n",
      "2370/2370 [==============================] - 0s 78us/step - loss: 0.0044 - val_loss: 0.0382\n",
      "Epoch 1600/3000\n",
      "2370/2370 [==============================] - 0s 80us/step - loss: 0.0042 - val_loss: 0.0369\n",
      "Epoch 1601/3000\n",
      "2370/2370 [==============================] - 0s 63us/step - loss: 0.0043 - val_loss: 0.0376\n",
      "Epoch 1602/3000\n",
      "2370/2370 [==============================] - 0s 64us/step - loss: 0.0044 - val_loss: 0.0387\n",
      "Epoch 1603/3000\n",
      "2370/2370 [==============================] - 0s 74us/step - loss: 0.0045 - val_loss: 0.0383\n",
      "Epoch 1604/3000\n",
      "2370/2370 [==============================] - 0s 80us/step - loss: 0.0045 - val_loss: 0.0379\n",
      "Epoch 1605/3000\n",
      "2370/2370 [==============================] - 0s 68us/step - loss: 0.0043 - val_loss: 0.0386\n",
      "Epoch 1606/3000\n",
      "2370/2370 [==============================] - 0s 64us/step - loss: 0.0045 - val_loss: 0.0381\n",
      "Epoch 1607/3000\n",
      "2370/2370 [==============================] - 0s 71us/step - loss: 0.0042 - val_loss: 0.0379\n",
      "Epoch 1608/3000\n",
      "2370/2370 [==============================] - 0s 58us/step - loss: 0.0045 - val_loss: 0.0383\n",
      "Epoch 1609/3000\n",
      "2370/2370 [==============================] - 0s 77us/step - loss: 0.0043 - val_loss: 0.0382\n",
      "Epoch 1610/3000\n",
      "2370/2370 [==============================] - 0s 64us/step - loss: 0.0045 - val_loss: 0.0382\n",
      "Epoch 1611/3000\n",
      "2370/2370 [==============================] - 0s 69us/step - loss: 0.0045 - val_loss: 0.0383\n",
      "Epoch 1612/3000\n",
      "2370/2370 [==============================] - 0s 69us/step - loss: 0.0045 - val_loss: 0.0379\n",
      "Epoch 1613/3000\n",
      "2370/2370 [==============================] - 0s 71us/step - loss: 0.0044 - val_loss: 0.0379\n",
      "Epoch 1614/3000\n",
      "2370/2370 [==============================] - 0s 68us/step - loss: 0.0043 - val_loss: 0.0383\n",
      "Epoch 1615/3000\n",
      "2370/2370 [==============================] - 0s 79us/step - loss: 0.0042 - val_loss: 0.0379\n",
      "Epoch 1616/3000\n",
      "2370/2370 [==============================] - 0s 70us/step - loss: 0.0043 - val_loss: 0.0384\n",
      "Epoch 1617/3000\n",
      "2370/2370 [==============================] - 0s 72us/step - loss: 0.0044 - val_loss: 0.0384\n",
      "Epoch 1618/3000\n",
      "2370/2370 [==============================] - 0s 89us/step - loss: 0.0043 - val_loss: 0.0379\n",
      "Epoch 1619/3000\n",
      "2370/2370 [==============================] - 0s 72us/step - loss: 0.0042 - val_loss: 0.0380\n",
      "Epoch 1620/3000\n",
      "2370/2370 [==============================] - 0s 73us/step - loss: 0.0042 - val_loss: 0.0375\n",
      "Epoch 1621/3000\n",
      "2370/2370 [==============================] - 0s 78us/step - loss: 0.0044 - val_loss: 0.0384\n",
      "Epoch 1622/3000\n",
      "2370/2370 [==============================] - 0s 77us/step - loss: 0.0043 - val_loss: 0.0384\n",
      "Epoch 1623/3000\n",
      "2370/2370 [==============================] - 0s 77us/step - loss: 0.0046 - val_loss: 0.0389\n",
      "Epoch 1624/3000\n",
      "2370/2370 [==============================] - 0s 78us/step - loss: 0.0041 - val_loss: 0.0383\n",
      "Epoch 1625/3000\n",
      "2370/2370 [==============================] - 0s 68us/step - loss: 0.0042 - val_loss: 0.0386\n",
      "Epoch 1626/3000\n",
      "2370/2370 [==============================] - 0s 69us/step - loss: 0.0044 - val_loss: 0.0379\n",
      "Epoch 1627/3000\n",
      "2370/2370 [==============================] - 0s 72us/step - loss: 0.0044 - val_loss: 0.0381\n",
      "Epoch 1628/3000\n",
      "2370/2370 [==============================] - 0s 62us/step - loss: 0.0043 - val_loss: 0.0381\n",
      "Epoch 1629/3000\n",
      "2370/2370 [==============================] - 0s 66us/step - loss: 0.0045 - val_loss: 0.0388\n",
      "Epoch 1630/3000\n",
      "2370/2370 [==============================] - 0s 69us/step - loss: 0.0043 - val_loss: 0.0387\n",
      "Epoch 1631/3000\n",
      "2370/2370 [==============================] - 0s 61us/step - loss: 0.0042 - val_loss: 0.0386\n",
      "Epoch 1632/3000\n",
      "2370/2370 [==============================] - 0s 73us/step - loss: 0.0042 - val_loss: 0.0384\n",
      "Epoch 1633/3000\n",
      "2370/2370 [==============================] - 0s 80us/step - loss: 0.0042 - val_loss: 0.0383\n",
      "Epoch 1634/3000\n",
      "2370/2370 [==============================] - 0s 75us/step - loss: 0.0042 - val_loss: 0.0383\n",
      "Epoch 1635/3000\n",
      "2370/2370 [==============================] - 0s 77us/step - loss: 0.0042 - val_loss: 0.0386\n",
      "Epoch 1636/3000\n",
      "2370/2370 [==============================] - 0s 61us/step - loss: 0.0044 - val_loss: 0.0391\n",
      "Epoch 1637/3000\n",
      "2370/2370 [==============================] - 0s 56us/step - loss: 0.0044 - val_loss: 0.0376\n",
      "Epoch 1638/3000\n",
      "2370/2370 [==============================] - 0s 94us/step - loss: 0.0046 - val_loss: 0.0388\n",
      "Epoch 1639/3000\n",
      "2370/2370 [==============================] - 0s 66us/step - loss: 0.0043 - val_loss: 0.0389\n",
      "Epoch 1640/3000\n",
      "2370/2370 [==============================] - 0s 74us/step - loss: 0.0044 - val_loss: 0.0392\n",
      "Epoch 1641/3000\n",
      "2370/2370 [==============================] - 0s 83us/step - loss: 0.0046 - val_loss: 0.0386\n",
      "Epoch 1642/3000\n",
      "2370/2370 [==============================] - 0s 76us/step - loss: 0.0044 - val_loss: 0.0380\n",
      "Epoch 1643/3000\n",
      "2370/2370 [==============================] - 0s 90us/step - loss: 0.0043 - val_loss: 0.0386\n",
      "Epoch 1644/3000\n",
      "2370/2370 [==============================] - 0s 80us/step - loss: 0.0045 - val_loss: 0.0389\n",
      "Epoch 1645/3000\n",
      "2370/2370 [==============================] - 0s 88us/step - loss: 0.0045 - val_loss: 0.0379\n",
      "Epoch 1646/3000\n",
      "2370/2370 [==============================] - 0s 88us/step - loss: 0.0045 - val_loss: 0.0374\n",
      "Epoch 1647/3000\n",
      "2370/2370 [==============================] - 0s 91us/step - loss: 0.0043 - val_loss: 0.0376\n",
      "Epoch 1648/3000\n",
      "2370/2370 [==============================] - 0s 93us/step - loss: 0.0043 - val_loss: 0.0381\n",
      "Epoch 1649/3000\n",
      "2370/2370 [==============================] - 0s 66us/step - loss: 0.0041 - val_loss: 0.0376\n",
      "Epoch 1650/3000\n",
      "2370/2370 [==============================] - 0s 65us/step - loss: 0.0042 - val_loss: 0.0383\n",
      "Epoch 1651/3000\n",
      "2370/2370 [==============================] - 0s 83us/step - loss: 0.0042 - val_loss: 0.0383\n",
      "Epoch 1652/3000\n",
      "2370/2370 [==============================] - 0s 66us/step - loss: 0.0041 - val_loss: 0.0386\n",
      "Epoch 1653/3000\n",
      "2370/2370 [==============================] - 0s 82us/step - loss: 0.0043 - val_loss: 0.0388\n",
      "Epoch 1654/3000\n",
      "2370/2370 [==============================] - 0s 70us/step - loss: 0.0041 - val_loss: 0.0390\n",
      "Epoch 1655/3000\n",
      "2370/2370 [==============================] - 0s 41us/step - loss: 0.0042 - val_loss: 0.0382\n",
      "Epoch 1656/3000\n",
      "2370/2370 [==============================] - 0s 46us/step - loss: 0.0044 - val_loss: 0.0391\n",
      "Epoch 1657/3000\n",
      "2370/2370 [==============================] - 0s 45us/step - loss: 0.0043 - val_loss: 0.0383\n",
      "Epoch 1658/3000\n",
      "2370/2370 [==============================] - 0s 40us/step - loss: 0.0043 - val_loss: 0.0385\n",
      "Epoch 1659/3000\n",
      "2370/2370 [==============================] - 0s 32us/step - loss: 0.0044 - val_loss: 0.0377\n",
      "Epoch 1660/3000\n",
      "2370/2370 [==============================] - 0s 31us/step - loss: 0.0045 - val_loss: 0.0377\n",
      "Epoch 1661/3000\n",
      "2370/2370 [==============================] - 0s 32us/step - loss: 0.0045 - val_loss: 0.0379\n",
      "Epoch 1662/3000\n",
      "2370/2370 [==============================] - 0s 32us/step - loss: 0.0044 - val_loss: 0.0379\n",
      "Epoch 1663/3000\n",
      "2370/2370 [==============================] - 0s 32us/step - loss: 0.0044 - val_loss: 0.0382\n",
      "Epoch 1664/3000\n",
      "2370/2370 [==============================] - 0s 30us/step - loss: 0.0045 - val_loss: 0.0383\n",
      "Epoch 1665/3000\n",
      "2370/2370 [==============================] - 0s 32us/step - loss: 0.0043 - val_loss: 0.0383\n",
      "Epoch 1666/3000\n",
      "2370/2370 [==============================] - 0s 35us/step - loss: 0.0045 - val_loss: 0.0386\n",
      "Epoch 1667/3000\n",
      "2370/2370 [==============================] - 0s 126us/step - loss: 0.0043 - val_loss: 0.0375\n",
      "Epoch 1668/3000\n",
      "2370/2370 [==============================] - 0s 95us/step - loss: 0.0043 - val_loss: 0.0376\n",
      "Epoch 1669/3000\n",
      "2370/2370 [==============================] - 0s 102us/step - loss: 0.0045 - val_loss: 0.0385\n",
      "Epoch 1670/3000\n",
      "2370/2370 [==============================] - 0s 68us/step - loss: 0.0045 - val_loss: 0.0379\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1671/3000\n",
      "2370/2370 [==============================] - 0s 77us/step - loss: 0.0044 - val_loss: 0.0377\n",
      "Epoch 1672/3000\n",
      "2370/2370 [==============================] - 0s 86us/step - loss: 0.0042 - val_loss: 0.0390\n",
      "Epoch 1673/3000\n",
      "2370/2370 [==============================] - 0s 37us/step - loss: 0.0046 - val_loss: 0.0380\n",
      "Epoch 1674/3000\n",
      "2370/2370 [==============================] - 0s 28us/step - loss: 0.0044 - val_loss: 0.0381\n",
      "Epoch 1675/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0046 - val_loss: 0.0374\n",
      "Epoch 1676/3000\n",
      "2370/2370 [==============================] - 0s 28us/step - loss: 0.0044 - val_loss: 0.0385\n",
      "Epoch 1677/3000\n",
      "2370/2370 [==============================] - 0s 78us/step - loss: 0.0046 - val_loss: 0.0385\n",
      "Epoch 1678/3000\n",
      "2370/2370 [==============================] - 0s 102us/step - loss: 0.0041 - val_loss: 0.0400\n",
      "Epoch 1679/3000\n",
      "2370/2370 [==============================] - 0s 88us/step - loss: 0.0044 - val_loss: 0.0376\n",
      "Epoch 1680/3000\n",
      "2370/2370 [==============================] - 0s 80us/step - loss: 0.0044 - val_loss: 0.0379\n",
      "Epoch 1681/3000\n",
      "2370/2370 [==============================] - 0s 92us/step - loss: 0.0043 - val_loss: 0.0373\n",
      "Epoch 1682/3000\n",
      "2370/2370 [==============================] - 0s 151us/step - loss: 0.0044 - val_loss: 0.0384\n",
      "Epoch 1683/3000\n",
      "2370/2370 [==============================] - 0s 87us/step - loss: 0.0045 - val_loss: 0.0382\n",
      "Epoch 1684/3000\n",
      "2370/2370 [==============================] - 0s 111us/step - loss: 0.0046 - val_loss: 0.0378\n",
      "Epoch 1685/3000\n",
      "2370/2370 [==============================] - 0s 138us/step - loss: 0.0043 - val_loss: 0.0385\n",
      "Epoch 1686/3000\n",
      "2370/2370 [==============================] - 0s 125us/step - loss: 0.0043 - val_loss: 0.0381\n",
      "Epoch 1687/3000\n",
      "2370/2370 [==============================] - 0s 138us/step - loss: 0.0044 - val_loss: 0.0384\n",
      "Epoch 1688/3000\n",
      "2370/2370 [==============================] - 0s 89us/step - loss: 0.0041 - val_loss: 0.0382\n",
      "Epoch 1689/3000\n",
      "2370/2370 [==============================] - 0s 57us/step - loss: 0.0042 - val_loss: 0.0379\n",
      "Epoch 1690/3000\n",
      "2370/2370 [==============================] - 0s 92us/step - loss: 0.0047 - val_loss: 0.0382\n",
      "Epoch 1691/3000\n",
      "2370/2370 [==============================] - 0s 121us/step - loss: 0.0046 - val_loss: 0.0379\n",
      "Epoch 1692/3000\n",
      "2370/2370 [==============================] - 0s 85us/step - loss: 0.0042 - val_loss: 0.0385\n",
      "Epoch 1693/3000\n",
      "2370/2370 [==============================] - 0s 111us/step - loss: 0.0043 - val_loss: 0.0376\n",
      "Epoch 1694/3000\n",
      "2370/2370 [==============================] - 0s 114us/step - loss: 0.0045 - val_loss: 0.0379\n",
      "Epoch 1695/3000\n",
      "2370/2370 [==============================] - 0s 132us/step - loss: 0.0042 - val_loss: 0.0385\n",
      "Epoch 1696/3000\n",
      "2370/2370 [==============================] - 0s 117us/step - loss: 0.0046 - val_loss: 0.0385\n",
      "Epoch 1697/3000\n",
      "2370/2370 [==============================] - 0s 138us/step - loss: 0.0044 - val_loss: 0.0381\n",
      "Epoch 1698/3000\n",
      "2370/2370 [==============================] - 0s 98us/step - loss: 0.0043 - val_loss: 0.0386\n",
      "Epoch 1699/3000\n",
      "2370/2370 [==============================] - 0s 133us/step - loss: 0.0044 - val_loss: 0.0394\n",
      "Epoch 1700/3000\n",
      "2370/2370 [==============================] - 0s 86us/step - loss: 0.0043 - val_loss: 0.0377\n",
      "Epoch 1701/3000\n",
      "2370/2370 [==============================] - 0s 61us/step - loss: 0.0044 - val_loss: 0.0387\n",
      "Epoch 1702/3000\n",
      "2370/2370 [==============================] - 0s 92us/step - loss: 0.0043 - val_loss: 0.0382\n",
      "Epoch 1703/3000\n",
      "2370/2370 [==============================] - 0s 49us/step - loss: 0.0044 - val_loss: 0.0384\n",
      "Epoch 1704/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0042 - val_loss: 0.0379\n",
      "Epoch 1705/3000\n",
      "2370/2370 [==============================] - 0s 36us/step - loss: 0.0042 - val_loss: 0.0377\n",
      "Epoch 1706/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0043 - val_loss: 0.0375\n",
      "Epoch 1707/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0044 - val_loss: 0.0379\n",
      "Epoch 1708/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0043 - val_loss: 0.0382\n",
      "Epoch 1709/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0044 - val_loss: 0.0383\n",
      "Epoch 1710/3000\n",
      "2370/2370 [==============================] - 0s 30us/step - loss: 0.0040 - val_loss: 0.0385\n",
      "Epoch 1711/3000\n",
      "2370/2370 [==============================] - 0s 30us/step - loss: 0.0042 - val_loss: 0.0378\n",
      "Epoch 1712/3000\n",
      "2370/2370 [==============================] - 0s 34us/step - loss: 0.0042 - val_loss: 0.0377\n",
      "Epoch 1713/3000\n",
      "2370/2370 [==============================] - 0s 28us/step - loss: 0.0043 - val_loss: 0.0381\n",
      "Epoch 1714/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0044 - val_loss: 0.0373\n",
      "Epoch 1715/3000\n",
      "2370/2370 [==============================] - 0s 32us/step - loss: 0.0043 - val_loss: 0.0383\n",
      "Epoch 1716/3000\n",
      "2370/2370 [==============================] - 0s 28us/step - loss: 0.0046 - val_loss: 0.0372\n",
      "Epoch 1717/3000\n",
      "2370/2370 [==============================] - 0s 28us/step - loss: 0.0045 - val_loss: 0.0377\n",
      "Epoch 1718/3000\n",
      "2370/2370 [==============================] - 0s 28us/step - loss: 0.0044 - val_loss: 0.0385\n",
      "Epoch 1719/3000\n",
      "2370/2370 [==============================] - 0s 34us/step - loss: 0.0042 - val_loss: 0.0378\n",
      "Epoch 1720/3000\n",
      "2370/2370 [==============================] - 0s 28us/step - loss: 0.0043 - val_loss: 0.0385\n",
      "Epoch 1721/3000\n",
      "2370/2370 [==============================] - 0s 30us/step - loss: 0.0042 - val_loss: 0.0389\n",
      "Epoch 1722/3000\n",
      "2370/2370 [==============================] - 0s 30us/step - loss: 0.0044 - val_loss: 0.0384\n",
      "Epoch 1723/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0041 - val_loss: 0.0387\n",
      "Epoch 1724/3000\n",
      "2370/2370 [==============================] - 0s 36us/step - loss: 0.0043 - val_loss: 0.0380\n",
      "Epoch 1725/3000\n",
      "2370/2370 [==============================] - 0s 147us/step - loss: 0.0044 - val_loss: 0.0374\n",
      "Epoch 1726/3000\n",
      "2370/2370 [==============================] - 0s 99us/step - loss: 0.0042 - val_loss: 0.0378\n",
      "Epoch 1727/3000\n",
      "2370/2370 [==============================] - 0s 83us/step - loss: 0.0042 - val_loss: 0.0380\n",
      "Epoch 1728/3000\n",
      "2370/2370 [==============================] - 0s 120us/step - loss: 0.0042 - val_loss: 0.0380\n",
      "Epoch 1729/3000\n",
      "2370/2370 [==============================] - 0s 140us/step - loss: 0.0044 - val_loss: 0.0373\n",
      "Epoch 1730/3000\n",
      "2370/2370 [==============================] - 0s 135us/step - loss: 0.0046 - val_loss: 0.0380\n",
      "Epoch 1731/3000\n",
      "2370/2370 [==============================] - 0s 108us/step - loss: 0.0044 - val_loss: 0.0378\n",
      "Epoch 1732/3000\n",
      "2370/2370 [==============================] - 0s 116us/step - loss: 0.0042 - val_loss: 0.0383\n",
      "Epoch 1733/3000\n",
      "2370/2370 [==============================] - 0s 85us/step - loss: 0.0043 - val_loss: 0.0385\n",
      "Epoch 1734/3000\n",
      "2370/2370 [==============================] - 0s 34us/step - loss: 0.0043 - val_loss: 0.0373\n",
      "Epoch 1735/3000\n",
      "2370/2370 [==============================] - 0s 35us/step - loss: 0.0042 - val_loss: 0.0384\n",
      "Epoch 1736/3000\n",
      "2370/2370 [==============================] - 0s 28us/step - loss: 0.0045 - val_loss: 0.0379\n",
      "Epoch 1737/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0043 - val_loss: 0.0381\n",
      "Epoch 1738/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0044 - val_loss: 0.0380\n",
      "Epoch 1739/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0042 - val_loss: 0.0384\n",
      "Epoch 1740/3000\n",
      "2370/2370 [==============================] - 0s 28us/step - loss: 0.0043 - val_loss: 0.0381\n",
      "Epoch 1741/3000\n",
      "2370/2370 [==============================] - 0s 27us/step - loss: 0.0042 - val_loss: 0.0380\n",
      "Epoch 1742/3000\n",
      "2370/2370 [==============================] - 0s 72us/step - loss: 0.0044 - val_loss: 0.0376\n",
      "Epoch 1743/3000\n",
      "2370/2370 [==============================] - 0s 162us/step - loss: 0.0042 - val_loss: 0.0381\n",
      "Epoch 1744/3000\n",
      "2370/2370 [==============================] - 0s 92us/step - loss: 0.0043 - val_loss: 0.0382\n",
      "Epoch 1745/3000\n",
      "2370/2370 [==============================] - 0s 97us/step - loss: 0.0044 - val_loss: 0.0381\n",
      "Epoch 1746/3000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2370/2370 [==============================] - 0s 93us/step - loss: 0.0045 - val_loss: 0.0384\n",
      "Epoch 1747/3000\n",
      "2370/2370 [==============================] - 0s 99us/step - loss: 0.0043 - val_loss: 0.0385\n",
      "Epoch 1748/3000\n",
      "2370/2370 [==============================] - 0s 39us/step - loss: 0.0044 - val_loss: 0.0382\n",
      "Epoch 1749/3000\n",
      "2370/2370 [==============================] - 0s 34us/step - loss: 0.0042 - val_loss: 0.0386\n",
      "Epoch 1750/3000\n",
      "2370/2370 [==============================] - 0s 33us/step - loss: 0.0044 - val_loss: 0.0386\n",
      "Epoch 1751/3000\n",
      "2370/2370 [==============================] - 0s 33us/step - loss: 0.0043 - val_loss: 0.0385\n",
      "Epoch 1752/3000\n",
      "2370/2370 [==============================] - 0s 33us/step - loss: 0.0045 - val_loss: 0.0379\n",
      "Epoch 1753/3000\n",
      "2370/2370 [==============================] - 0s 28us/step - loss: 0.0045 - val_loss: 0.0378\n",
      "Epoch 1754/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0045 - val_loss: 0.0387\n",
      "Epoch 1755/3000\n",
      "2370/2370 [==============================] - 0s 31us/step - loss: 0.0043 - val_loss: 0.0380\n",
      "Epoch 1756/3000\n",
      "2370/2370 [==============================] - 0s 30us/step - loss: 0.0042 - val_loss: 0.0382\n",
      "Epoch 1757/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0042 - val_loss: 0.0392\n",
      "Epoch 1758/3000\n",
      "2370/2370 [==============================] - 0s 31us/step - loss: 0.0043 - val_loss: 0.0386\n",
      "Epoch 1759/3000\n",
      "2370/2370 [==============================] - 0s 30us/step - loss: 0.0042 - val_loss: 0.0385\n",
      "Epoch 1760/3000\n",
      "2370/2370 [==============================] - 0s 31us/step - loss: 0.0044 - val_loss: 0.0382\n",
      "Epoch 1761/3000\n",
      "2370/2370 [==============================] - 0s 30us/step - loss: 0.0042 - val_loss: 0.0381\n",
      "Epoch 1762/3000\n",
      "2370/2370 [==============================] - 0s 31us/step - loss: 0.0044 - val_loss: 0.0381\n",
      "Epoch 1763/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0042 - val_loss: 0.0386\n",
      "Epoch 1764/3000\n",
      "2370/2370 [==============================] - 0s 30us/step - loss: 0.0042 - val_loss: 0.0388\n",
      "Epoch 1765/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0042 - val_loss: 0.0385\n",
      "Epoch 1766/3000\n",
      "2370/2370 [==============================] - 0s 27us/step - loss: 0.0043 - val_loss: 0.0380\n",
      "Epoch 1767/3000\n",
      "2370/2370 [==============================] - 0s 35us/step - loss: 0.0044 - val_loss: 0.0389\n",
      "Epoch 1768/3000\n",
      "2370/2370 [==============================] - 0s 39us/step - loss: 0.0044 - val_loss: 0.0376\n",
      "Epoch 1769/3000\n",
      "2370/2370 [==============================] - 0s 36us/step - loss: 0.0044 - val_loss: 0.0384\n",
      "Epoch 1770/3000\n",
      "2370/2370 [==============================] - 0s 32us/step - loss: 0.0043 - val_loss: 0.0387\n",
      "Epoch 1771/3000\n",
      "2370/2370 [==============================] - 0s 32us/step - loss: 0.0040 - val_loss: 0.0387\n",
      "Epoch 1772/3000\n",
      "2370/2370 [==============================] - 0s 33us/step - loss: 0.0041 - val_loss: 0.0379\n",
      "Epoch 1773/3000\n",
      "2370/2370 [==============================] - 0s 33us/step - loss: 0.0043 - val_loss: 0.0379\n",
      "Epoch 1774/3000\n",
      "2370/2370 [==============================] - 0s 31us/step - loss: 0.0041 - val_loss: 0.0377\n",
      "Epoch 1775/3000\n",
      "2370/2370 [==============================] - 0s 51us/step - loss: 0.0042 - val_loss: 0.0374\n",
      "Epoch 1776/3000\n",
      "2370/2370 [==============================] - 0s 79us/step - loss: 0.0043 - val_loss: 0.0381\n",
      "Epoch 1777/3000\n",
      "2370/2370 [==============================] - 0s 56us/step - loss: 0.0043 - val_loss: 0.0381\n",
      "Epoch 1778/3000\n",
      "2370/2370 [==============================] - 0s 36us/step - loss: 0.0042 - val_loss: 0.0378\n",
      "Epoch 1779/3000\n",
      "2370/2370 [==============================] - 0s 78us/step - loss: 0.0041 - val_loss: 0.0384\n",
      "Epoch 1780/3000\n",
      "2370/2370 [==============================] - 0s 69us/step - loss: 0.0044 - val_loss: 0.0392\n",
      "Epoch 1781/3000\n",
      "2370/2370 [==============================] - 0s 37us/step - loss: 0.0042 - val_loss: 0.0380\n",
      "Epoch 1782/3000\n",
      "2370/2370 [==============================] - 0s 42us/step - loss: 0.0045 - val_loss: 0.0378\n",
      "Epoch 1783/3000\n",
      "2370/2370 [==============================] - 0s 30us/step - loss: 0.0044 - val_loss: 0.0388\n",
      "Epoch 1784/3000\n",
      "2370/2370 [==============================] - 0s 36us/step - loss: 0.0043 - val_loss: 0.0384\n",
      "Epoch 1785/3000\n",
      "2370/2370 [==============================] - 0s 36us/step - loss: 0.0044 - val_loss: 0.0374\n",
      "Epoch 1786/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0043 - val_loss: 0.0390\n",
      "Epoch 1787/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0041 - val_loss: 0.0384\n",
      "Epoch 1788/3000\n",
      "2370/2370 [==============================] - 0s 32us/step - loss: 0.0041 - val_loss: 0.0391\n",
      "Epoch 1789/3000\n",
      "2370/2370 [==============================] - 0s 30us/step - loss: 0.0043 - val_loss: 0.0383\n",
      "Epoch 1790/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0042 - val_loss: 0.0392\n",
      "Epoch 1791/3000\n",
      "2370/2370 [==============================] - 0s 36us/step - loss: 0.0043 - val_loss: 0.0381\n",
      "Epoch 1792/3000\n",
      "2370/2370 [==============================] - 0s 31us/step - loss: 0.0042 - val_loss: 0.0382\n",
      "Epoch 1793/3000\n",
      "2370/2370 [==============================] - 0s 36us/step - loss: 0.0043 - val_loss: 0.0375\n",
      "Epoch 1794/3000\n",
      "2370/2370 [==============================] - 0s 36us/step - loss: 0.0042 - val_loss: 0.0379\n",
      "Epoch 1795/3000\n",
      "2370/2370 [==============================] - 0s 35us/step - loss: 0.0041 - val_loss: 0.0385\n",
      "Epoch 1796/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0043 - val_loss: 0.0378\n",
      "Epoch 1797/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0044 - val_loss: 0.0378\n",
      "Epoch 1798/3000\n",
      "2370/2370 [==============================] - 0s 35us/step - loss: 0.0042 - val_loss: 0.0375\n",
      "Epoch 1799/3000\n",
      "2370/2370 [==============================] - 0s 31us/step - loss: 0.0043 - val_loss: 0.0381\n",
      "Epoch 1800/3000\n",
      "2370/2370 [==============================] - 0s 30us/step - loss: 0.0043 - val_loss: 0.0384\n",
      "Epoch 1801/3000\n",
      "2370/2370 [==============================] - 0s 40us/step - loss: 0.0043 - val_loss: 0.0382\n",
      "Epoch 1802/3000\n",
      "2370/2370 [==============================] - 0s 42us/step - loss: 0.0043 - val_loss: 0.0383\n",
      "Epoch 1803/3000\n",
      "2370/2370 [==============================] - 0s 33us/step - loss: 0.0042 - val_loss: 0.0376\n",
      "Epoch 1804/3000\n",
      "2370/2370 [==============================] - 0s 38us/step - loss: 0.0043 - val_loss: 0.0376\n",
      "Epoch 1805/3000\n",
      "2370/2370 [==============================] - 0s 37us/step - loss: 0.0042 - val_loss: 0.0382\n",
      "Epoch 1806/3000\n",
      "2370/2370 [==============================] - 0s 28us/step - loss: 0.0044 - val_loss: 0.0383\n",
      "Epoch 1807/3000\n",
      "2370/2370 [==============================] - 0s 35us/step - loss: 0.0043 - val_loss: 0.0390\n",
      "Epoch 1808/3000\n",
      "2370/2370 [==============================] - 0s 34us/step - loss: 0.0044 - val_loss: 0.0387\n",
      "Epoch 1809/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0043 - val_loss: 0.0394\n",
      "Epoch 1810/3000\n",
      "2370/2370 [==============================] - 0s 31us/step - loss: 0.0042 - val_loss: 0.0386\n",
      "Epoch 1811/3000\n",
      "2370/2370 [==============================] - 0s 35us/step - loss: 0.0043 - val_loss: 0.0384\n",
      "Epoch 1812/3000\n",
      "2370/2370 [==============================] - 0s 28us/step - loss: 0.0046 - val_loss: 0.0378\n",
      "Epoch 1813/3000\n",
      "2370/2370 [==============================] - 0s 27us/step - loss: 0.0045 - val_loss: 0.0384\n",
      "Epoch 1814/3000\n",
      "2370/2370 [==============================] - 0s 33us/step - loss: 0.0042 - val_loss: 0.0380\n",
      "Epoch 1815/3000\n",
      "2370/2370 [==============================] - 0s 28us/step - loss: 0.0045 - val_loss: 0.0375\n",
      "Epoch 1816/3000\n",
      "2370/2370 [==============================] - 0s 28us/step - loss: 0.0043 - val_loss: 0.0375\n",
      "Epoch 1817/3000\n",
      "2370/2370 [==============================] - 0s 28us/step - loss: 0.0042 - val_loss: 0.0375\n",
      "Epoch 1818/3000\n",
      "2370/2370 [==============================] - 0s 34us/step - loss: 0.0043 - val_loss: 0.0378\n",
      "Epoch 1819/3000\n",
      "2370/2370 [==============================] - 0s 69us/step - loss: 0.0042 - val_loss: 0.0376\n",
      "Epoch 1820/3000\n",
      "2370/2370 [==============================] - 0s 69us/step - loss: 0.0042 - val_loss: 0.0376\n",
      "Epoch 1821/3000\n",
      "2370/2370 [==============================] - 0s 57us/step - loss: 0.0044 - val_loss: 0.0376\n",
      "Epoch 1822/3000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2370/2370 [==============================] - 0s 41us/step - loss: 0.0045 - val_loss: 0.0370\n",
      "Epoch 1823/3000\n",
      "2370/2370 [==============================] - 0s 48us/step - loss: 0.0044 - val_loss: 0.0379\n",
      "Epoch 1824/3000\n",
      "2370/2370 [==============================] - 0s 43us/step - loss: 0.0043 - val_loss: 0.0370\n",
      "Epoch 1825/3000\n",
      "2370/2370 [==============================] - 0s 46us/step - loss: 0.0045 - val_loss: 0.0381\n",
      "Epoch 1826/3000\n",
      "2370/2370 [==============================] - 0s 34us/step - loss: 0.0045 - val_loss: 0.0384\n",
      "Epoch 1827/3000\n",
      "2370/2370 [==============================] - 0s 39us/step - loss: 0.0041 - val_loss: 0.0373\n",
      "Epoch 1828/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0042 - val_loss: 0.0379\n",
      "Epoch 1829/3000\n",
      "2370/2370 [==============================] - 0s 33us/step - loss: 0.0045 - val_loss: 0.0374\n",
      "Epoch 1830/3000\n",
      "2370/2370 [==============================] - 0s 35us/step - loss: 0.0041 - val_loss: 0.0378\n",
      "Epoch 1831/3000\n",
      "2370/2370 [==============================] - 0s 30us/step - loss: 0.0040 - val_loss: 0.0383\n",
      "Epoch 1832/3000\n",
      "2370/2370 [==============================] - 0s 27us/step - loss: 0.0043 - val_loss: 0.0379\n",
      "Epoch 1833/3000\n",
      "2370/2370 [==============================] - 0s 35us/step - loss: 0.0044 - val_loss: 0.0377\n",
      "Epoch 1834/3000\n",
      "2370/2370 [==============================] - 0s 27us/step - loss: 0.0043 - val_loss: 0.0379\n",
      "Epoch 1835/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0041 - val_loss: 0.0376\n",
      "Epoch 1836/3000\n",
      "2370/2370 [==============================] - 0s 36us/step - loss: 0.0043 - val_loss: 0.0373\n",
      "Epoch 1837/3000\n",
      "2370/2370 [==============================] - 0s 30us/step - loss: 0.0043 - val_loss: 0.0378\n",
      "Epoch 1838/3000\n",
      "2370/2370 [==============================] - 0s 28us/step - loss: 0.0042 - val_loss: 0.0382\n",
      "Epoch 1839/3000\n",
      "2370/2370 [==============================] - 0s 27us/step - loss: 0.0042 - val_loss: 0.0383\n",
      "Epoch 1840/3000\n",
      "2370/2370 [==============================] - 0s 28us/step - loss: 0.0043 - val_loss: 0.0379\n",
      "Epoch 1841/3000\n",
      "2370/2370 [==============================] - 0s 28us/step - loss: 0.0042 - val_loss: 0.0375\n",
      "Epoch 1842/3000\n",
      "2370/2370 [==============================] - 0s 28us/step - loss: 0.0043 - val_loss: 0.0380\n",
      "Epoch 1843/3000\n",
      "2370/2370 [==============================] - 0s 32us/step - loss: 0.0043 - val_loss: 0.0387\n",
      "Epoch 1844/3000\n",
      "2370/2370 [==============================] - 0s 35us/step - loss: 0.0042 - val_loss: 0.0383\n",
      "Epoch 1845/3000\n",
      "2370/2370 [==============================] - 0s 33us/step - loss: 0.0043 - val_loss: 0.0381\n",
      "Epoch 1846/3000\n",
      "2370/2370 [==============================] - 0s 28us/step - loss: 0.0042 - val_loss: 0.0375\n",
      "Epoch 1847/3000\n",
      "2370/2370 [==============================] - 0s 28us/step - loss: 0.0044 - val_loss: 0.0383\n",
      "Epoch 1848/3000\n",
      "2370/2370 [==============================] - 0s 27us/step - loss: 0.0042 - val_loss: 0.0378\n",
      "Epoch 1849/3000\n",
      "2370/2370 [==============================] - 0s 36us/step - loss: 0.0041 - val_loss: 0.0370\n",
      "Epoch 1850/3000\n",
      "2370/2370 [==============================] - 0s 78us/step - loss: 0.0042 - val_loss: 0.0367\n",
      "Epoch 1851/3000\n",
      "2370/2370 [==============================] - 0s 65us/step - loss: 0.0044 - val_loss: 0.0380\n",
      "Epoch 1852/3000\n",
      "2370/2370 [==============================] - 0s 32us/step - loss: 0.0043 - val_loss: 0.0377\n",
      "Epoch 1853/3000\n",
      "2370/2370 [==============================] - 0s 36us/step - loss: 0.0044 - val_loss: 0.0375\n",
      "Epoch 1854/3000\n",
      "2370/2370 [==============================] - 0s 51us/step - loss: 0.0042 - val_loss: 0.0381\n",
      "Epoch 1855/3000\n",
      "2370/2370 [==============================] - 0s 57us/step - loss: 0.0042 - val_loss: 0.0373\n",
      "Epoch 1856/3000\n",
      "2370/2370 [==============================] - 0s 61us/step - loss: 0.0042 - val_loss: 0.0376\n",
      "Epoch 1857/3000\n",
      "2370/2370 [==============================] - 0s 50us/step - loss: 0.0044 - val_loss: 0.0386\n",
      "Epoch 1858/3000\n",
      "2370/2370 [==============================] - 0s 37us/step - loss: 0.0042 - val_loss: 0.0382\n",
      "Epoch 1859/3000\n",
      "2370/2370 [==============================] - 0s 38us/step - loss: 0.0040 - val_loss: 0.0378\n",
      "Epoch 1860/3000\n",
      "2370/2370 [==============================] - 0s 33us/step - loss: 0.0042 - val_loss: 0.0374\n",
      "Epoch 1861/3000\n",
      "2370/2370 [==============================] - 0s 33us/step - loss: 0.0045 - val_loss: 0.0387\n",
      "Epoch 1862/3000\n",
      "2370/2370 [==============================] - 0s 32us/step - loss: 0.0043 - val_loss: 0.0377\n",
      "Epoch 1863/3000\n",
      "2370/2370 [==============================] - 0s 34us/step - loss: 0.0040 - val_loss: 0.0376\n",
      "Epoch 1864/3000\n",
      "2370/2370 [==============================] - 0s 58us/step - loss: 0.0041 - val_loss: 0.0386\n",
      "Epoch 1865/3000\n",
      "2370/2370 [==============================] - 0s 82us/step - loss: 0.0044 - val_loss: 0.0378\n",
      "Epoch 1866/3000\n",
      "2370/2370 [==============================] - 0s 51us/step - loss: 0.0045 - val_loss: 0.0368\n",
      "Epoch 1867/3000\n",
      "2370/2370 [==============================] - 0s 33us/step - loss: 0.0042 - val_loss: 0.0372\n",
      "Epoch 1868/3000\n",
      "2370/2370 [==============================] - 0s 36us/step - loss: 0.0041 - val_loss: 0.0381\n",
      "Epoch 1869/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0040 - val_loss: 0.0392\n",
      "Epoch 1870/3000\n",
      "2370/2370 [==============================] - 0s 28us/step - loss: 0.0043 - val_loss: 0.0383\n",
      "Epoch 1871/3000\n",
      "2370/2370 [==============================] - 0s 43us/step - loss: 0.0044 - val_loss: 0.0379\n",
      "Epoch 1872/3000\n",
      "2370/2370 [==============================] - 0s 82us/step - loss: 0.0043 - val_loss: 0.0381\n",
      "Epoch 1873/3000\n",
      "2370/2370 [==============================] - 0s 60us/step - loss: 0.0042 - val_loss: 0.0380\n",
      "Epoch 1874/3000\n",
      "2370/2370 [==============================] - 0s 36us/step - loss: 0.0045 - val_loss: 0.0373\n",
      "Epoch 1875/3000\n",
      "2370/2370 [==============================] - 0s 34us/step - loss: 0.0041 - val_loss: 0.0376\n",
      "Epoch 1876/3000\n",
      "2370/2370 [==============================] - 0s 36us/step - loss: 0.0042 - val_loss: 0.0381\n",
      "Epoch 1877/3000\n",
      "2370/2370 [==============================] - 0s 35us/step - loss: 0.0043 - val_loss: 0.0379\n",
      "Epoch 1878/3000\n",
      "2370/2370 [==============================] - 0s 31us/step - loss: 0.0043 - val_loss: 0.0377\n",
      "Epoch 1879/3000\n",
      "2370/2370 [==============================] - 0s 34us/step - loss: 0.0044 - val_loss: 0.0371\n",
      "Epoch 1880/3000\n",
      "2370/2370 [==============================] - 0s 31us/step - loss: 0.0042 - val_loss: 0.0378\n",
      "Epoch 1881/3000\n",
      "2370/2370 [==============================] - 0s 36us/step - loss: 0.0044 - val_loss: 0.0371\n",
      "Epoch 1882/3000\n",
      "2370/2370 [==============================] - 0s 30us/step - loss: 0.0041 - val_loss: 0.0374\n",
      "Epoch 1883/3000\n",
      "2370/2370 [==============================] - 0s 32us/step - loss: 0.0043 - val_loss: 0.0379\n",
      "Epoch 1884/3000\n",
      "2370/2370 [==============================] - 0s 28us/step - loss: 0.0043 - val_loss: 0.0380\n",
      "Epoch 1885/3000\n",
      "2370/2370 [==============================] - 0s 27us/step - loss: 0.0043 - val_loss: 0.0376\n",
      "Epoch 1886/3000\n",
      "2370/2370 [==============================] - 0s 27us/step - loss: 0.0043 - val_loss: 0.0376\n",
      "Epoch 1887/3000\n",
      "2370/2370 [==============================] - 0s 28us/step - loss: 0.0040 - val_loss: 0.0372\n",
      "Epoch 1888/3000\n",
      "2370/2370 [==============================] - 0s 31us/step - loss: 0.0041 - val_loss: 0.0381\n",
      "Epoch 1889/3000\n",
      "2370/2370 [==============================] - 0s 42us/step - loss: 0.0041 - val_loss: 0.0375\n",
      "Epoch 1890/3000\n",
      "2370/2370 [==============================] - 0s 96us/step - loss: 0.0041 - val_loss: 0.0373\n",
      "Epoch 1891/3000\n",
      "2370/2370 [==============================] - 0s 101us/step - loss: 0.0044 - val_loss: 0.0379\n",
      "Epoch 1892/3000\n",
      "2370/2370 [==============================] - 0s 107us/step - loss: 0.0047 - val_loss: 0.0379\n",
      "Epoch 1893/3000\n",
      "2370/2370 [==============================] - 0s 96us/step - loss: 0.0043 - val_loss: 0.0382\n",
      "Epoch 1894/3000\n",
      "2370/2370 [==============================] - 0s 78us/step - loss: 0.0045 - val_loss: 0.0376\n",
      "Epoch 1895/3000\n",
      "2370/2370 [==============================] - 0s 89us/step - loss: 0.0044 - val_loss: 0.0373\n",
      "Epoch 1896/3000\n",
      "2370/2370 [==============================] - 0s 59us/step - loss: 0.0043 - val_loss: 0.0384\n",
      "Epoch 1897/3000\n",
      "2370/2370 [==============================] - 0s 35us/step - loss: 0.0041 - val_loss: 0.0381\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1898/3000\n",
      "2370/2370 [==============================] - 0s 36us/step - loss: 0.0044 - val_loss: 0.0376\n",
      "Epoch 1899/3000\n",
      "2370/2370 [==============================] - 0s 33us/step - loss: 0.0043 - val_loss: 0.0377\n",
      "Epoch 1900/3000\n",
      "2370/2370 [==============================] - 0s 33us/step - loss: 0.0042 - val_loss: 0.0388\n",
      "Epoch 1901/3000\n",
      "2370/2370 [==============================] - 0s 31us/step - loss: 0.0043 - val_loss: 0.0377\n",
      "Epoch 1902/3000\n",
      "2370/2370 [==============================] - 0s 30us/step - loss: 0.0042 - val_loss: 0.0382\n",
      "Epoch 1903/3000\n",
      "2370/2370 [==============================] - 0s 33us/step - loss: 0.0042 - val_loss: 0.0382\n",
      "Epoch 1904/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0044 - val_loss: 0.0381\n",
      "Epoch 1905/3000\n",
      "2370/2370 [==============================] - 0s 32us/step - loss: 0.0041 - val_loss: 0.0395\n",
      "Epoch 1906/3000\n",
      "2370/2370 [==============================] - 0s 28us/step - loss: 0.0041 - val_loss: 0.0387\n",
      "Epoch 1907/3000\n",
      "2370/2370 [==============================] - 0s 32us/step - loss: 0.0042 - val_loss: 0.0380\n",
      "Epoch 1908/3000\n",
      "2370/2370 [==============================] - 0s 75us/step - loss: 0.0043 - val_loss: 0.0385\n",
      "Epoch 1909/3000\n",
      "2370/2370 [==============================] - 0s 47us/step - loss: 0.0040 - val_loss: 0.0395\n",
      "Epoch 1910/3000\n",
      "2370/2370 [==============================] - 0s 41us/step - loss: 0.0042 - val_loss: 0.0382\n",
      "Epoch 1911/3000\n",
      "2370/2370 [==============================] - 0s 62us/step - loss: 0.0041 - val_loss: 0.0385\n",
      "Epoch 1912/3000\n",
      "2370/2370 [==============================] - 0s 34us/step - loss: 0.0040 - val_loss: 0.0381\n",
      "Epoch 1913/3000\n",
      "2370/2370 [==============================] - 0s 36us/step - loss: 0.0039 - val_loss: 0.0386\n",
      "Epoch 1914/3000\n",
      "2370/2370 [==============================] - 0s 35us/step - loss: 0.0045 - val_loss: 0.0371\n",
      "Epoch 1915/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0042 - val_loss: 0.0380\n",
      "Epoch 1916/3000\n",
      "2370/2370 [==============================] - 0s 32us/step - loss: 0.0042 - val_loss: 0.0371\n",
      "Epoch 1917/3000\n",
      "2370/2370 [==============================] - 0s 88us/step - loss: 0.0042 - val_loss: 0.0379\n",
      "Epoch 1918/3000\n",
      "2370/2370 [==============================] - 0s 55us/step - loss: 0.0043 - val_loss: 0.0381\n",
      "Epoch 1919/3000\n",
      "2370/2370 [==============================] - 0s 46us/step - loss: 0.0042 - val_loss: 0.0380\n",
      "Epoch 1920/3000\n",
      "2370/2370 [==============================] - 0s 36us/step - loss: 0.0041 - val_loss: 0.0376\n",
      "Epoch 1921/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0044 - val_loss: 0.0380\n",
      "Epoch 1922/3000\n",
      "2370/2370 [==============================] - 0s 27us/step - loss: 0.0042 - val_loss: 0.0383\n",
      "Epoch 1923/3000\n",
      "2370/2370 [==============================] - 0s 34us/step - loss: 0.0041 - val_loss: 0.0376\n",
      "Epoch 1924/3000\n",
      "2370/2370 [==============================] - 0s 62us/step - loss: 0.0041 - val_loss: 0.0380\n",
      "Epoch 1925/3000\n",
      "2370/2370 [==============================] - 0s 77us/step - loss: 0.0041 - val_loss: 0.0397\n",
      "Epoch 1926/3000\n",
      "2370/2370 [==============================] - 0s 50us/step - loss: 0.0041 - val_loss: 0.0398\n",
      "Epoch 1927/3000\n",
      "2370/2370 [==============================] - 0s 40us/step - loss: 0.0043 - val_loss: 0.0394\n",
      "Epoch 1928/3000\n",
      "2370/2370 [==============================] - 0s 39us/step - loss: 0.0042 - val_loss: 0.0391\n",
      "Epoch 1929/3000\n",
      "2370/2370 [==============================] - 0s 34us/step - loss: 0.0041 - val_loss: 0.0390\n",
      "Epoch 1930/3000\n",
      "2370/2370 [==============================] - 0s 34us/step - loss: 0.0043 - val_loss: 0.0381\n",
      "Epoch 1931/3000\n",
      "2370/2370 [==============================] - 0s 33us/step - loss: 0.0043 - val_loss: 0.0381\n",
      "Epoch 1932/3000\n",
      "2370/2370 [==============================] - 0s 35us/step - loss: 0.0043 - val_loss: 0.0382\n",
      "Epoch 1933/3000\n",
      "2370/2370 [==============================] - 0s 52us/step - loss: 0.0043 - val_loss: 0.0392\n",
      "Epoch 1934/3000\n",
      "2370/2370 [==============================] - 0s 75us/step - loss: 0.0043 - val_loss: 0.0380\n",
      "Epoch 1935/3000\n",
      "2370/2370 [==============================] - 0s 32us/step - loss: 0.0042 - val_loss: 0.0378\n",
      "Epoch 1936/3000\n",
      "2370/2370 [==============================] - 0s 30us/step - loss: 0.0040 - val_loss: 0.0379\n",
      "Epoch 1937/3000\n",
      "2370/2370 [==============================] - 0s 33us/step - loss: 0.0040 - val_loss: 0.0377\n",
      "Epoch 1938/3000\n",
      "2370/2370 [==============================] - 0s 65us/step - loss: 0.0042 - val_loss: 0.0380\n",
      "Epoch 1939/3000\n",
      "2370/2370 [==============================] - 0s 81us/step - loss: 0.0042 - val_loss: 0.0376\n",
      "Epoch 1940/3000\n",
      "2370/2370 [==============================] - 0s 68us/step - loss: 0.0040 - val_loss: 0.0377\n",
      "Epoch 1941/3000\n",
      "2370/2370 [==============================] - 0s 63us/step - loss: 0.0043 - val_loss: 0.0377\n",
      "Epoch 1942/3000\n",
      "2370/2370 [==============================] - 0s 35us/step - loss: 0.0040 - val_loss: 0.0381\n",
      "Epoch 1943/3000\n",
      "2370/2370 [==============================] - 0s 32us/step - loss: 0.0043 - val_loss: 0.0385\n",
      "Epoch 1944/3000\n",
      "2370/2370 [==============================] - 0s 35us/step - loss: 0.0045 - val_loss: 0.0387\n",
      "Epoch 1945/3000\n",
      "2370/2370 [==============================] - 0s 37us/step - loss: 0.0041 - val_loss: 0.0380\n",
      "Epoch 1946/3000\n",
      "2370/2370 [==============================] - 0s 76us/step - loss: 0.0041 - val_loss: 0.0385\n",
      "Epoch 1947/3000\n",
      "2370/2370 [==============================] - 0s 100us/step - loss: 0.0041 - val_loss: 0.0374\n",
      "Epoch 1948/3000\n",
      "2370/2370 [==============================] - 0s 107us/step - loss: 0.0045 - val_loss: 0.0372\n",
      "Epoch 1949/3000\n",
      "2370/2370 [==============================] - 0s 73us/step - loss: 0.0042 - val_loss: 0.0380\n",
      "Epoch 1950/3000\n",
      "2370/2370 [==============================] - 0s 34us/step - loss: 0.0041 - val_loss: 0.0384\n",
      "Epoch 1951/3000\n",
      "2370/2370 [==============================] - 0s 32us/step - loss: 0.0042 - val_loss: 0.0387\n",
      "Epoch 1952/3000\n",
      "2370/2370 [==============================] - 0s 28us/step - loss: 0.0045 - val_loss: 0.0382\n",
      "Epoch 1953/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0043 - val_loss: 0.0373\n",
      "Epoch 1954/3000\n",
      "2370/2370 [==============================] - 0s 82us/step - loss: 0.0042 - val_loss: 0.0384\n",
      "Epoch 1955/3000\n",
      "2370/2370 [==============================] - 0s 45us/step - loss: 0.0042 - val_loss: 0.0382\n",
      "Epoch 1956/3000\n",
      "2370/2370 [==============================] - 0s 35us/step - loss: 0.0040 - val_loss: 0.0382\n",
      "Epoch 1957/3000\n",
      "2370/2370 [==============================] - 0s 30us/step - loss: 0.0040 - val_loss: 0.0379\n",
      "Epoch 1958/3000\n",
      "2370/2370 [==============================] - 0s 30us/step - loss: 0.0041 - val_loss: 0.0381\n",
      "Epoch 1959/3000\n",
      "2370/2370 [==============================] - 0s 30us/step - loss: 0.0040 - val_loss: 0.0383\n",
      "Epoch 1960/3000\n",
      "2370/2370 [==============================] - 0s 31us/step - loss: 0.0043 - val_loss: 0.0382\n",
      "Epoch 1961/3000\n",
      "2370/2370 [==============================] - 0s 32us/step - loss: 0.0045 - val_loss: 0.0385\n",
      "Epoch 1962/3000\n",
      "2370/2370 [==============================] - 0s 36us/step - loss: 0.0043 - val_loss: 0.0383\n",
      "Epoch 1963/3000\n",
      "2370/2370 [==============================] - 0s 70us/step - loss: 0.0044 - val_loss: 0.0384\n",
      "Epoch 1964/3000\n",
      "2370/2370 [==============================] - 0s 107us/step - loss: 0.0041 - val_loss: 0.0387\n",
      "Epoch 1965/3000\n",
      "2370/2370 [==============================] - 0s 99us/step - loss: 0.0043 - val_loss: 0.0385\n",
      "Epoch 1966/3000\n",
      "2370/2370 [==============================] - 0s 87us/step - loss: 0.0042 - val_loss: 0.0386\n",
      "Epoch 1967/3000\n",
      "2370/2370 [==============================] - 0s 98us/step - loss: 0.0041 - val_loss: 0.0379\n",
      "Epoch 1968/3000\n",
      "2370/2370 [==============================] - 0s 86us/step - loss: 0.0041 - val_loss: 0.0379\n",
      "Epoch 1969/3000\n",
      "2370/2370 [==============================] - 0s 78us/step - loss: 0.0045 - val_loss: 0.0381\n",
      "Epoch 1970/3000\n",
      "2370/2370 [==============================] - 0s 41us/step - loss: 0.0040 - val_loss: 0.0381\n",
      "Epoch 1971/3000\n",
      "2370/2370 [==============================] - 0s 34us/step - loss: 0.0043 - val_loss: 0.0377\n",
      "Epoch 1972/3000\n",
      "2370/2370 [==============================] - 0s 33us/step - loss: 0.0041 - val_loss: 0.0376\n",
      "Epoch 1973/3000\n",
      "2370/2370 [==============================] - 0s 37us/step - loss: 0.0041 - val_loss: 0.0386\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1974/3000\n",
      "2370/2370 [==============================] - 0s 28us/step - loss: 0.0042 - val_loss: 0.0377\n",
      "Epoch 1975/3000\n",
      "2370/2370 [==============================] - 0s 28us/step - loss: 0.0042 - val_loss: 0.0374\n",
      "Epoch 1976/3000\n",
      "2370/2370 [==============================] - 0s 31us/step - loss: 0.0042 - val_loss: 0.0380\n",
      "Epoch 1977/3000\n",
      "2370/2370 [==============================] - 0s 28us/step - loss: 0.0039 - val_loss: 0.0392\n",
      "Epoch 1978/3000\n",
      "2370/2370 [==============================] - 0s 27us/step - loss: 0.0040 - val_loss: 0.0385\n",
      "Epoch 1979/3000\n",
      "2370/2370 [==============================] - 0s 27us/step - loss: 0.0040 - val_loss: 0.0378\n",
      "Epoch 1980/3000\n",
      "2370/2370 [==============================] - 0s 28us/step - loss: 0.0041 - val_loss: 0.0378\n",
      "Epoch 1981/3000\n",
      "2370/2370 [==============================] - 0s 34us/step - loss: 0.0040 - val_loss: 0.0380\n",
      "Epoch 1982/3000\n",
      "2370/2370 [==============================] - 0s 28us/step - loss: 0.0039 - val_loss: 0.0386\n",
      "Epoch 1983/3000\n",
      "2370/2370 [==============================] - 0s 33us/step - loss: 0.0043 - val_loss: 0.0388\n",
      "Epoch 1984/3000\n",
      "2370/2370 [==============================] - 0s 27us/step - loss: 0.0041 - val_loss: 0.0385\n",
      "Epoch 1985/3000\n",
      "2370/2370 [==============================] - 0s 28us/step - loss: 0.0042 - val_loss: 0.0377\n",
      "Epoch 1986/3000\n",
      "2370/2370 [==============================] - 0s 28us/step - loss: 0.0044 - val_loss: 0.0380\n",
      "Epoch 1987/3000\n",
      "2370/2370 [==============================] - 0s 27us/step - loss: 0.0043 - val_loss: 0.0392\n",
      "Epoch 1988/3000\n",
      "2370/2370 [==============================] - 0s 32us/step - loss: 0.0042 - val_loss: 0.0373\n",
      "Epoch 1989/3000\n",
      "2370/2370 [==============================] - 0s 33us/step - loss: 0.0043 - val_loss: 0.0378\n",
      "Epoch 1990/3000\n",
      "2370/2370 [==============================] - 0s 33us/step - loss: 0.0043 - val_loss: 0.0383\n",
      "Epoch 1991/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0041 - val_loss: 0.0375\n",
      "Epoch 1992/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0041 - val_loss: 0.0384\n",
      "Epoch 1993/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0041 - val_loss: 0.0379\n",
      "Epoch 1994/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0043 - val_loss: 0.0379\n",
      "Epoch 1995/3000\n",
      "2370/2370 [==============================] - 0s 35us/step - loss: 0.0041 - val_loss: 0.0376\n",
      "Epoch 1996/3000\n",
      "2370/2370 [==============================] - 0s 37us/step - loss: 0.0042 - val_loss: 0.0378\n",
      "Epoch 1997/3000\n",
      "2370/2370 [==============================] - 0s 35us/step - loss: 0.0041 - val_loss: 0.0374\n",
      "Epoch 1998/3000\n",
      "2370/2370 [==============================] - 0s 30us/step - loss: 0.0039 - val_loss: 0.0376\n",
      "Epoch 1999/3000\n",
      "2370/2370 [==============================] - 0s 30us/step - loss: 0.0042 - val_loss: 0.0368\n",
      "Epoch 2000/3000\n",
      "2370/2370 [==============================] - 0s 30us/step - loss: 0.0041 - val_loss: 0.0377\n",
      "Epoch 2001/3000\n",
      "2370/2370 [==============================] - 0s 30us/step - loss: 0.0042 - val_loss: 0.0380\n",
      "Epoch 2002/3000\n",
      "2370/2370 [==============================] - 0s 35us/step - loss: 0.0042 - val_loss: 0.0384\n",
      "Epoch 2003/3000\n",
      "2370/2370 [==============================] - 0s 34us/step - loss: 0.0042 - val_loss: 0.0376\n",
      "Epoch 2004/3000\n",
      "2370/2370 [==============================] - 0s 28us/step - loss: 0.0042 - val_loss: 0.0383\n",
      "Epoch 2005/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0043 - val_loss: 0.0378\n",
      "Epoch 2006/3000\n",
      "2370/2370 [==============================] - 0s 28us/step - loss: 0.0042 - val_loss: 0.0386\n",
      "Epoch 2007/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0041 - val_loss: 0.0383\n",
      "Epoch 2008/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0044 - val_loss: 0.0376\n",
      "Epoch 2009/3000\n",
      "2370/2370 [==============================] - 0s 85us/step - loss: 0.0043 - val_loss: 0.0381\n",
      "Epoch 2010/3000\n",
      "2370/2370 [==============================] - 0s 160us/step - loss: 0.0041 - val_loss: 0.0388\n",
      "Epoch 2011/3000\n",
      "2370/2370 [==============================] - 0s 132us/step - loss: 0.0042 - val_loss: 0.0386\n",
      "Epoch 2012/3000\n",
      "2370/2370 [==============================] - 0s 85us/step - loss: 0.0039 - val_loss: 0.0385\n",
      "Epoch 2013/3000\n",
      "2370/2370 [==============================] - 0s 108us/step - loss: 0.0043 - val_loss: 0.0388\n",
      "Epoch 2014/3000\n",
      "2370/2370 [==============================] - 0s 96us/step - loss: 0.0042 - val_loss: 0.0381\n",
      "Epoch 2015/3000\n",
      "2370/2370 [==============================] - 0s 93us/step - loss: 0.0041 - val_loss: 0.0379\n",
      "Epoch 2016/3000\n",
      "2370/2370 [==============================] - 0s 57us/step - loss: 0.0042 - val_loss: 0.0379\n",
      "Epoch 2017/3000\n",
      "2370/2370 [==============================] - 0s 31us/step - loss: 0.0040 - val_loss: 0.0378\n",
      "Epoch 2018/3000\n",
      "2370/2370 [==============================] - 0s 33us/step - loss: 0.0041 - val_loss: 0.0374\n",
      "Epoch 2019/3000\n",
      "2370/2370 [==============================] - 0s 35us/step - loss: 0.0042 - val_loss: 0.0383\n",
      "Epoch 2020/3000\n",
      "2370/2370 [==============================] - 0s 33us/step - loss: 0.0042 - val_loss: 0.0381\n",
      "Epoch 2021/3000\n",
      "2370/2370 [==============================] - 0s 30us/step - loss: 0.0041 - val_loss: 0.0392\n",
      "Epoch 2022/3000\n",
      "2370/2370 [==============================] - 0s 37us/step - loss: 0.0041 - val_loss: 0.0391\n",
      "Epoch 2023/3000\n",
      "2370/2370 [==============================] - 0s 40us/step - loss: 0.0040 - val_loss: 0.0385\n",
      "Epoch 2024/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0042 - val_loss: 0.0379\n",
      "Epoch 2025/3000\n",
      "2370/2370 [==============================] - 0s 33us/step - loss: 0.0042 - val_loss: 0.0391\n",
      "Epoch 2026/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0042 - val_loss: 0.0390\n",
      "Epoch 2027/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0042 - val_loss: 0.0390\n",
      "Epoch 2028/3000\n",
      "2370/2370 [==============================] - 0s 30us/step - loss: 0.0043 - val_loss: 0.0384\n",
      "Epoch 2029/3000\n",
      "2370/2370 [==============================] - 0s 30us/step - loss: 0.0040 - val_loss: 0.0389\n",
      "Epoch 2030/3000\n",
      "2370/2370 [==============================] - 0s 34us/step - loss: 0.0039 - val_loss: 0.0381\n",
      "Epoch 2031/3000\n",
      "2370/2370 [==============================] - 0s 40us/step - loss: 0.0042 - val_loss: 0.0381\n",
      "Epoch 2032/3000\n",
      "2370/2370 [==============================] - 0s 35us/step - loss: 0.0041 - val_loss: 0.0389\n",
      "Epoch 2033/3000\n",
      "2370/2370 [==============================] - 0s 32us/step - loss: 0.0044 - val_loss: 0.0390\n",
      "Epoch 2034/3000\n",
      "2370/2370 [==============================] - 0s 35us/step - loss: 0.0043 - val_loss: 0.0389\n",
      "Epoch 2035/3000\n",
      "2370/2370 [==============================] - 0s 28us/step - loss: 0.0042 - val_loss: 0.0392\n",
      "Epoch 2036/3000\n",
      "2370/2370 [==============================] - 0s 33us/step - loss: 0.0043 - val_loss: 0.0383\n",
      "Epoch 2037/3000\n",
      "2370/2370 [==============================] - 0s 32us/step - loss: 0.0044 - val_loss: 0.0390\n",
      "Epoch 2038/3000\n",
      "2370/2370 [==============================] - 0s 43us/step - loss: 0.0042 - val_loss: 0.0388\n",
      "Epoch 2039/3000\n",
      "2370/2370 [==============================] - 0s 89us/step - loss: 0.0041 - val_loss: 0.0385\n",
      "Epoch 2040/3000\n",
      "2370/2370 [==============================] - 0s 58us/step - loss: 0.0042 - val_loss: 0.0384\n",
      "Epoch 2041/3000\n",
      "2370/2370 [==============================] - 0s 32us/step - loss: 0.0041 - val_loss: 0.0387\n",
      "Epoch 2042/3000\n",
      "2370/2370 [==============================] - 0s 30us/step - loss: 0.0043 - val_loss: 0.0385\n",
      "Epoch 2043/3000\n",
      "2370/2370 [==============================] - 0s 52us/step - loss: 0.0044 - val_loss: 0.0394\n",
      "Epoch 2044/3000\n",
      "2370/2370 [==============================] - 0s 73us/step - loss: 0.0043 - val_loss: 0.0385\n",
      "Epoch 2045/3000\n",
      "2370/2370 [==============================] - 0s 44us/step - loss: 0.0045 - val_loss: 0.0384\n",
      "Epoch 2046/3000\n",
      "2370/2370 [==============================] - 0s 51us/step - loss: 0.0043 - val_loss: 0.0387\n",
      "Epoch 2047/3000\n",
      "2370/2370 [==============================] - 0s 35us/step - loss: 0.0042 - val_loss: 0.0389\n",
      "Epoch 2048/3000\n",
      "2370/2370 [==============================] - 0s 39us/step - loss: 0.0040 - val_loss: 0.0386\n",
      "Epoch 2049/3000\n",
      "2370/2370 [==============================] - 0s 34us/step - loss: 0.0041 - val_loss: 0.0391\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2050/3000\n",
      "2370/2370 [==============================] - 0s 34us/step - loss: 0.0041 - val_loss: 0.0385\n",
      "Epoch 2051/3000\n",
      "2370/2370 [==============================] - 0s 36us/step - loss: 0.0042 - val_loss: 0.0376\n",
      "Epoch 2052/3000\n",
      "2370/2370 [==============================] - 0s 38us/step - loss: 0.0042 - val_loss: 0.0378\n",
      "Epoch 2053/3000\n",
      "2370/2370 [==============================] - 0s 34us/step - loss: 0.0043 - val_loss: 0.0382\n",
      "Epoch 2054/3000\n",
      "2370/2370 [==============================] - 0s 30us/step - loss: 0.0043 - val_loss: 0.0389\n",
      "Epoch 2055/3000\n",
      "2370/2370 [==============================] - 0s 32us/step - loss: 0.0045 - val_loss: 0.0380\n",
      "Epoch 2056/3000\n",
      "2370/2370 [==============================] - 0s 31us/step - loss: 0.0042 - val_loss: 0.0383\n",
      "Epoch 2057/3000\n",
      "2370/2370 [==============================] - 0s 28us/step - loss: 0.0043 - val_loss: 0.0383\n",
      "Epoch 2058/3000\n",
      "2370/2370 [==============================] - 0s 36us/step - loss: 0.0042 - val_loss: 0.0382\n",
      "Epoch 2059/3000\n",
      "2370/2370 [==============================] - 0s 32us/step - loss: 0.0043 - val_loss: 0.0378\n",
      "Epoch 2060/3000\n",
      "2370/2370 [==============================] - 0s 28us/step - loss: 0.0040 - val_loss: 0.0379\n",
      "Epoch 2061/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0043 - val_loss: 0.0380\n",
      "Epoch 2062/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0041 - val_loss: 0.0377\n",
      "Epoch 2063/3000\n",
      "2370/2370 [==============================] - 0s 31us/step - loss: 0.0041 - val_loss: 0.0392\n",
      "Epoch 2064/3000\n",
      "2370/2370 [==============================] - 0s 37us/step - loss: 0.0041 - val_loss: 0.0386\n",
      "Epoch 2065/3000\n",
      "2370/2370 [==============================] - 0s 32us/step - loss: 0.0040 - val_loss: 0.0381\n",
      "Epoch 2066/3000\n",
      "2370/2370 [==============================] - 0s 31us/step - loss: 0.0043 - val_loss: 0.0378\n",
      "Epoch 2067/3000\n",
      "2370/2370 [==============================] - 0s 28us/step - loss: 0.0043 - val_loss: 0.0383\n",
      "Epoch 2068/3000\n",
      "2370/2370 [==============================] - 0s 28us/step - loss: 0.0039 - val_loss: 0.0385\n",
      "Epoch 2069/3000\n",
      "2370/2370 [==============================] - 0s 27us/step - loss: 0.0042 - val_loss: 0.0386\n",
      "Epoch 2070/3000\n",
      "2370/2370 [==============================] - 0s 28us/step - loss: 0.0040 - val_loss: 0.0390\n",
      "Epoch 2071/3000\n",
      "2370/2370 [==============================] - 0s 31us/step - loss: 0.0042 - val_loss: 0.0378\n",
      "Epoch 2072/3000\n",
      "2370/2370 [==============================] - 0s 81us/step - loss: 0.0045 - val_loss: 0.0381\n",
      "Epoch 2073/3000\n",
      "2370/2370 [==============================] - 0s 67us/step - loss: 0.0041 - val_loss: 0.0381\n",
      "Epoch 2074/3000\n",
      "2370/2370 [==============================] - 0s 55us/step - loss: 0.0039 - val_loss: 0.0382\n",
      "Epoch 2075/3000\n",
      "2370/2370 [==============================] - 0s 41us/step - loss: 0.0041 - val_loss: 0.0390\n",
      "Epoch 2076/3000\n",
      "2370/2370 [==============================] - 0s 38us/step - loss: 0.0040 - val_loss: 0.0387\n",
      "Epoch 2077/3000\n",
      "2370/2370 [==============================] - 0s 34us/step - loss: 0.0041 - val_loss: 0.0384\n",
      "Epoch 2078/3000\n",
      "2370/2370 [==============================] - 0s 34us/step - loss: 0.0040 - val_loss: 0.0380\n",
      "Epoch 2079/3000\n",
      "2370/2370 [==============================] - 0s 34us/step - loss: 0.0041 - val_loss: 0.0386\n",
      "Epoch 2080/3000\n",
      "2370/2370 [==============================] - 0s 36us/step - loss: 0.0041 - val_loss: 0.0378\n",
      "Epoch 2081/3000\n",
      "2370/2370 [==============================] - 0s 37us/step - loss: 0.0042 - val_loss: 0.0375\n",
      "Epoch 2082/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0041 - val_loss: 0.0378\n",
      "Epoch 2083/3000\n",
      "2370/2370 [==============================] - 0s 34us/step - loss: 0.0040 - val_loss: 0.0372\n",
      "Epoch 2084/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0041 - val_loss: 0.0385\n",
      "Epoch 2085/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0044 - val_loss: 0.0378\n",
      "Epoch 2086/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0041 - val_loss: 0.0381\n",
      "Epoch 2087/3000\n",
      "2370/2370 [==============================] - 0s 33us/step - loss: 0.0042 - val_loss: 0.0376\n",
      "Epoch 2088/3000\n",
      "2370/2370 [==============================] - 0s 34us/step - loss: 0.0039 - val_loss: 0.0380\n",
      "Epoch 2089/3000\n",
      "2370/2370 [==============================] - 0s 28us/step - loss: 0.0042 - val_loss: 0.0381\n",
      "Epoch 2090/3000\n",
      "2370/2370 [==============================] - 0s 27us/step - loss: 0.0043 - val_loss: 0.0376\n",
      "Epoch 2091/3000\n",
      "2370/2370 [==============================] - 0s 30us/step - loss: 0.0041 - val_loss: 0.0379\n",
      "Epoch 2092/3000\n",
      "2370/2370 [==============================] - 0s 34us/step - loss: 0.0039 - val_loss: 0.0372\n",
      "Epoch 2093/3000\n",
      "2370/2370 [==============================] - 0s 35us/step - loss: 0.0041 - val_loss: 0.0377\n",
      "Epoch 2094/3000\n",
      "2370/2370 [==============================] - 0s 39us/step - loss: 0.0042 - val_loss: 0.0379\n",
      "Epoch 2095/3000\n",
      "2370/2370 [==============================] - 0s 34us/step - loss: 0.0043 - val_loss: 0.0378\n",
      "Epoch 2096/3000\n",
      "2370/2370 [==============================] - 0s 35us/step - loss: 0.0041 - val_loss: 0.0378\n",
      "Epoch 2097/3000\n",
      "2370/2370 [==============================] - 0s 37us/step - loss: 0.0042 - val_loss: 0.0371\n",
      "Epoch 2098/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0039 - val_loss: 0.0378\n",
      "Epoch 2099/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0041 - val_loss: 0.0378\n",
      "Epoch 2100/3000\n",
      "2370/2370 [==============================] - 0s 33us/step - loss: 0.0047 - val_loss: 0.0379\n",
      "Epoch 2101/3000\n",
      "2370/2370 [==============================] - 0s 28us/step - loss: 0.0044 - val_loss: 0.0377\n",
      "Epoch 2102/3000\n",
      "2370/2370 [==============================] - 0s 28us/step - loss: 0.0040 - val_loss: 0.0379\n",
      "Epoch 2103/3000\n",
      "2370/2370 [==============================] - 0s 27us/step - loss: 0.0041 - val_loss: 0.0379\n",
      "Epoch 2104/3000\n",
      "2370/2370 [==============================] - 0s 31us/step - loss: 0.0041 - val_loss: 0.0379\n",
      "Epoch 2105/3000\n",
      "2370/2370 [==============================] - 0s 62us/step - loss: 0.0041 - val_loss: 0.0383\n",
      "Epoch 2106/3000\n",
      "2370/2370 [==============================] - 0s 103us/step - loss: 0.0041 - val_loss: 0.0384\n",
      "Epoch 2107/3000\n",
      "2370/2370 [==============================] - 0s 92us/step - loss: 0.0042 - val_loss: 0.0386\n",
      "Epoch 2108/3000\n",
      "2370/2370 [==============================] - 0s 105us/step - loss: 0.0043 - val_loss: 0.0382\n",
      "Epoch 2109/3000\n",
      "2370/2370 [==============================] - 0s 91us/step - loss: 0.0040 - val_loss: 0.0379\n",
      "Epoch 2110/3000\n",
      "2370/2370 [==============================] - 0s 95us/step - loss: 0.0042 - val_loss: 0.0378\n",
      "Epoch 2111/3000\n",
      "2370/2370 [==============================] - 0s 52us/step - loss: 0.0043 - val_loss: 0.0391\n",
      "Epoch 2112/3000\n",
      "2370/2370 [==============================] - 0s 41us/step - loss: 0.0042 - val_loss: 0.0384\n",
      "Epoch 2113/3000\n",
      "2370/2370 [==============================] - 0s 40us/step - loss: 0.0041 - val_loss: 0.0384\n",
      "Epoch 2114/3000\n",
      "2370/2370 [==============================] - 0s 40us/step - loss: 0.0039 - val_loss: 0.0382\n",
      "Epoch 2115/3000\n",
      "2370/2370 [==============================] - 0s 37us/step - loss: 0.0039 - val_loss: 0.0384\n",
      "Epoch 2116/3000\n",
      "2370/2370 [==============================] - 0s 40us/step - loss: 0.0039 - val_loss: 0.0382\n",
      "Epoch 2117/3000\n",
      "2370/2370 [==============================] - 0s 34us/step - loss: 0.0041 - val_loss: 0.0383\n",
      "Epoch 2118/3000\n",
      "2370/2370 [==============================] - 0s 34us/step - loss: 0.0041 - val_loss: 0.0387\n",
      "Epoch 2119/3000\n",
      "2370/2370 [==============================] - 0s 35us/step - loss: 0.0044 - val_loss: 0.0381\n",
      "Epoch 2120/3000\n",
      "2370/2370 [==============================] - 0s 34us/step - loss: 0.0039 - val_loss: 0.0376\n",
      "Epoch 2121/3000\n",
      "2370/2370 [==============================] - 0s 32us/step - loss: 0.0040 - val_loss: 0.0380\n",
      "Epoch 2122/3000\n",
      "2370/2370 [==============================] - 0s 28us/step - loss: 0.0040 - val_loss: 0.0379\n",
      "Epoch 2123/3000\n",
      "2370/2370 [==============================] - 0s 30us/step - loss: 0.0041 - val_loss: 0.0374\n",
      "Epoch 2124/3000\n",
      "2370/2370 [==============================] - 0s 33us/step - loss: 0.0043 - val_loss: 0.0375\n",
      "Epoch 2125/3000\n",
      "2370/2370 [==============================] - 0s 28us/step - loss: 0.0041 - val_loss: 0.0383\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2126/3000\n",
      "2370/2370 [==============================] - 0s 33us/step - loss: 0.0041 - val_loss: 0.0374\n",
      "Epoch 2127/3000\n",
      "2370/2370 [==============================] - 0s 36us/step - loss: 0.0042 - val_loss: 0.0379\n",
      "Epoch 2128/3000\n",
      "2370/2370 [==============================] - 0s 30us/step - loss: 0.0042 - val_loss: 0.0386\n",
      "Epoch 2129/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0041 - val_loss: 0.0376\n",
      "Epoch 2130/3000\n",
      "2370/2370 [==============================] - 0s 30us/step - loss: 0.0042 - val_loss: 0.0379\n",
      "Epoch 2131/3000\n",
      "2370/2370 [==============================] - 0s 30us/step - loss: 0.0041 - val_loss: 0.0385\n",
      "Epoch 2132/3000\n",
      "2370/2370 [==============================] - 0s 34us/step - loss: 0.0041 - val_loss: 0.0383\n",
      "Epoch 2133/3000\n",
      "2370/2370 [==============================] - 0s 35us/step - loss: 0.0042 - val_loss: 0.0382\n",
      "Epoch 2134/3000\n",
      "2370/2370 [==============================] - 0s 41us/step - loss: 0.0042 - val_loss: 0.0379\n",
      "Epoch 2135/3000\n",
      "2370/2370 [==============================] - 0s 41us/step - loss: 0.0040 - val_loss: 0.0382\n",
      "Epoch 2136/3000\n",
      "2370/2370 [==============================] - 0s 34us/step - loss: 0.0044 - val_loss: 0.0380\n",
      "Epoch 2137/3000\n",
      "2370/2370 [==============================] - 0s 34us/step - loss: 0.0041 - val_loss: 0.0380\n",
      "Epoch 2138/3000\n",
      "2370/2370 [==============================] - 0s 36us/step - loss: 0.0041 - val_loss: 0.0386\n",
      "Epoch 2139/3000\n",
      "2370/2370 [==============================] - 0s 78us/step - loss: 0.0039 - val_loss: 0.0386\n",
      "Epoch 2140/3000\n",
      "2370/2370 [==============================] - 0s 45us/step - loss: 0.0042 - val_loss: 0.0381\n",
      "Epoch 2141/3000\n",
      "2370/2370 [==============================] - 0s 36us/step - loss: 0.0043 - val_loss: 0.0381\n",
      "Epoch 2142/3000\n",
      "2370/2370 [==============================] - 0s 33us/step - loss: 0.0041 - val_loss: 0.0380\n",
      "Epoch 2143/3000\n",
      "2370/2370 [==============================] - 0s 33us/step - loss: 0.0042 - val_loss: 0.0380\n",
      "Epoch 2144/3000\n",
      "2370/2370 [==============================] - 0s 34us/step - loss: 0.0040 - val_loss: 0.0380\n",
      "Epoch 2145/3000\n",
      "2370/2370 [==============================] - 0s 28us/step - loss: 0.0042 - val_loss: 0.0388\n",
      "Epoch 2146/3000\n",
      "2370/2370 [==============================] - 0s 36us/step - loss: 0.0042 - val_loss: 0.0372\n",
      "Epoch 2147/3000\n",
      "2370/2370 [==============================] - 0s 38us/step - loss: 0.0043 - val_loss: 0.0366\n",
      "Epoch 2148/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0040 - val_loss: 0.0371\n",
      "Epoch 2149/3000\n",
      "2370/2370 [==============================] - 0s 37us/step - loss: 0.0040 - val_loss: 0.0371\n",
      "Epoch 2150/3000\n",
      "2370/2370 [==============================] - 0s 36us/step - loss: 0.0043 - val_loss: 0.0377\n",
      "Epoch 2151/3000\n",
      "2370/2370 [==============================] - 0s 33us/step - loss: 0.0043 - val_loss: 0.0374\n",
      "Epoch 2152/3000\n",
      "2370/2370 [==============================] - 0s 33us/step - loss: 0.0040 - val_loss: 0.0366\n",
      "Epoch 2153/3000\n",
      "2370/2370 [==============================] - 0s 32us/step - loss: 0.0042 - val_loss: 0.0369\n",
      "Epoch 2154/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0044 - val_loss: 0.0378\n",
      "Epoch 2155/3000\n",
      "2370/2370 [==============================] - 0s 28us/step - loss: 0.0044 - val_loss: 0.0381\n",
      "Epoch 2156/3000\n",
      "2370/2370 [==============================] - 0s 36us/step - loss: 0.0038 - val_loss: 0.0385\n",
      "Epoch 2157/3000\n",
      "2370/2370 [==============================] - 0s 86us/step - loss: 0.0040 - val_loss: 0.0384\n",
      "Epoch 2158/3000\n",
      "2370/2370 [==============================] - 0s 86us/step - loss: 0.0041 - val_loss: 0.0382\n",
      "Epoch 2159/3000\n",
      "2370/2370 [==============================] - 0s 52us/step - loss: 0.0041 - val_loss: 0.0378\n",
      "Epoch 2160/3000\n",
      "2370/2370 [==============================] - 0s 90us/step - loss: 0.0042 - val_loss: 0.0373\n",
      "Epoch 2161/3000\n",
      "2370/2370 [==============================] - 0s 65us/step - loss: 0.0042 - val_loss: 0.0384\n",
      "Epoch 2162/3000\n",
      "2370/2370 [==============================] - 0s 42us/step - loss: 0.0041 - val_loss: 0.0388\n",
      "Epoch 2163/3000\n",
      "2370/2370 [==============================] - 0s 30us/step - loss: 0.0043 - val_loss: 0.0384\n",
      "Epoch 2164/3000\n",
      "2370/2370 [==============================] - 0s 35us/step - loss: 0.0043 - val_loss: 0.0386\n",
      "Epoch 2165/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0040 - val_loss: 0.0381\n",
      "Epoch 2166/3000\n",
      "2370/2370 [==============================] - 0s 30us/step - loss: 0.0040 - val_loss: 0.0389\n",
      "Epoch 2167/3000\n",
      "2370/2370 [==============================] - 0s 40us/step - loss: 0.0041 - val_loss: 0.0382\n",
      "Epoch 2168/3000\n",
      "2370/2370 [==============================] - 0s 39us/step - loss: 0.0041 - val_loss: 0.0384\n",
      "Epoch 2169/3000\n",
      "2370/2370 [==============================] - 0s 42us/step - loss: 0.0040 - val_loss: 0.0393\n",
      "Epoch 2170/3000\n",
      "2370/2370 [==============================] - 0s 39us/step - loss: 0.0042 - val_loss: 0.0390\n",
      "Epoch 2171/3000\n",
      "2370/2370 [==============================] - 0s 33us/step - loss: 0.0044 - val_loss: 0.0383\n",
      "Epoch 2172/3000\n",
      "2370/2370 [==============================] - 0s 31us/step - loss: 0.0042 - val_loss: 0.0374\n",
      "Epoch 2173/3000\n",
      "2370/2370 [==============================] - 0s 36us/step - loss: 0.0041 - val_loss: 0.0377\n",
      "Epoch 2174/3000\n",
      "2370/2370 [==============================] - 0s 37us/step - loss: 0.0043 - val_loss: 0.0376\n",
      "Epoch 2175/3000\n",
      "2370/2370 [==============================] - 0s 36us/step - loss: 0.0044 - val_loss: 0.0376\n",
      "Epoch 2176/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0040 - val_loss: 0.0378\n",
      "Epoch 2177/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0041 - val_loss: 0.0383\n",
      "Epoch 2178/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0041 - val_loss: 0.0378\n",
      "Epoch 2179/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0044 - val_loss: 0.0383\n",
      "Epoch 2180/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0043 - val_loss: 0.0391\n",
      "Epoch 2181/3000\n",
      "2370/2370 [==============================] - 0s 33us/step - loss: 0.0041 - val_loss: 0.0389\n",
      "Epoch 2182/3000\n",
      "2370/2370 [==============================] - 0s 42us/step - loss: 0.0042 - val_loss: 0.0382\n",
      "Epoch 2183/3000\n",
      "2370/2370 [==============================] - 0s 86us/step - loss: 0.0041 - val_loss: 0.0384\n",
      "Epoch 2184/3000\n",
      "2370/2370 [==============================] - 0s 66us/step - loss: 0.0042 - val_loss: 0.0380\n",
      "Epoch 2185/3000\n",
      "2370/2370 [==============================] - 0s 36us/step - loss: 0.0040 - val_loss: 0.0377\n",
      "Epoch 2186/3000\n",
      "2370/2370 [==============================] - 0s 31us/step - loss: 0.0043 - val_loss: 0.0373\n",
      "Epoch 2187/3000\n",
      "2370/2370 [==============================] - 0s 33us/step - loss: 0.0042 - val_loss: 0.0373\n",
      "Epoch 2188/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0042 - val_loss: 0.0377\n",
      "Epoch 2189/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0045 - val_loss: 0.0381\n",
      "Epoch 2190/3000\n",
      "2370/2370 [==============================] - 0s 33us/step - loss: 0.0040 - val_loss: 0.0378\n",
      "Epoch 2191/3000\n",
      "2370/2370 [==============================] - 0s 38us/step - loss: 0.0041 - val_loss: 0.0380\n",
      "Epoch 2192/3000\n",
      "2370/2370 [==============================] - 0s 30us/step - loss: 0.0040 - val_loss: 0.0386\n",
      "Epoch 2193/3000\n",
      "2370/2370 [==============================] - 0s 37us/step - loss: 0.0040 - val_loss: 0.0383\n",
      "Epoch 2194/3000\n",
      "2370/2370 [==============================] - 0s 32us/step - loss: 0.0042 - val_loss: 0.0377\n",
      "Epoch 2195/3000\n",
      "2370/2370 [==============================] - 0s 55us/step - loss: 0.0042 - val_loss: 0.0378\n",
      "Epoch 2196/3000\n",
      "2370/2370 [==============================] - 0s 99us/step - loss: 0.0042 - val_loss: 0.0371\n",
      "Epoch 2197/3000\n",
      "2370/2370 [==============================] - 0s 61us/step - loss: 0.0039 - val_loss: 0.0381\n",
      "Epoch 2198/3000\n",
      "2370/2370 [==============================] - 0s 36us/step - loss: 0.0041 - val_loss: 0.0384\n",
      "Epoch 2199/3000\n",
      "2370/2370 [==============================] - 0s 32us/step - loss: 0.0041 - val_loss: 0.0384\n",
      "Epoch 2200/3000\n",
      "2370/2370 [==============================] - 0s 33us/step - loss: 0.0042 - val_loss: 0.0384\n",
      "Epoch 2201/3000\n",
      "2370/2370 [==============================] - 0s 36us/step - loss: 0.0040 - val_loss: 0.0374\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2202/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0042 - val_loss: 0.0387\n",
      "Epoch 2203/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0039 - val_loss: 0.0394\n",
      "Epoch 2204/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0043 - val_loss: 0.0379\n",
      "Epoch 2205/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0039 - val_loss: 0.0379\n",
      "Epoch 2206/3000\n",
      "2370/2370 [==============================] - 0s 31us/step - loss: 0.0041 - val_loss: 0.0375\n",
      "Epoch 2207/3000\n",
      "2370/2370 [==============================] - 0s 30us/step - loss: 0.0040 - val_loss: 0.0374\n",
      "Epoch 2208/3000\n",
      "2370/2370 [==============================] - 0s 62us/step - loss: 0.0040 - val_loss: 0.0379\n",
      "Epoch 2209/3000\n",
      "2370/2370 [==============================] - 0s 75us/step - loss: 0.0040 - val_loss: 0.0380\n",
      "Epoch 2210/3000\n",
      "2370/2370 [==============================] - 0s 33us/step - loss: 0.0040 - val_loss: 0.0386\n",
      "Epoch 2211/3000\n",
      "2370/2370 [==============================] - 0s 34us/step - loss: 0.0040 - val_loss: 0.0384\n",
      "Epoch 2212/3000\n",
      "2370/2370 [==============================] - 0s 30us/step - loss: 0.0040 - val_loss: 0.0385\n",
      "Epoch 2213/3000\n",
      "2370/2370 [==============================] - 0s 31us/step - loss: 0.0042 - val_loss: 0.0382\n",
      "Epoch 2214/3000\n",
      "2370/2370 [==============================] - 0s 30us/step - loss: 0.0039 - val_loss: 0.0374\n",
      "Epoch 2215/3000\n",
      "2370/2370 [==============================] - 0s 37us/step - loss: 0.0042 - val_loss: 0.0370\n",
      "Epoch 2216/3000\n",
      "2370/2370 [==============================] - 0s 37us/step - loss: 0.0042 - val_loss: 0.0371\n",
      "Epoch 2217/3000\n",
      "2370/2370 [==============================] - 0s 55us/step - loss: 0.0042 - val_loss: 0.0378\n",
      "Epoch 2218/3000\n",
      "2370/2370 [==============================] - 0s 52us/step - loss: 0.0042 - val_loss: 0.0369\n",
      "Epoch 2219/3000\n",
      "2370/2370 [==============================] - 0s 41us/step - loss: 0.0040 - val_loss: 0.0381\n",
      "Epoch 2220/3000\n",
      "2370/2370 [==============================] - 0s 35us/step - loss: 0.0039 - val_loss: 0.0382\n",
      "Epoch 2221/3000\n",
      "2370/2370 [==============================] - 0s 30us/step - loss: 0.0040 - val_loss: 0.0381\n",
      "Epoch 2222/3000\n",
      "2370/2370 [==============================] - 0s 34us/step - loss: 0.0041 - val_loss: 0.0373\n",
      "Epoch 2223/3000\n",
      "2370/2370 [==============================] - 0s 30us/step - loss: 0.0039 - val_loss: 0.0372\n",
      "Epoch 2224/3000\n",
      "2370/2370 [==============================] - 0s 30us/step - loss: 0.0040 - val_loss: 0.0375\n",
      "Epoch 2225/3000\n",
      "2370/2370 [==============================] - 0s 34us/step - loss: 0.0041 - val_loss: 0.0375\n",
      "Epoch 2226/3000\n",
      "2370/2370 [==============================] - 0s 33us/step - loss: 0.0042 - val_loss: 0.0378\n",
      "Epoch 2227/3000\n",
      "2370/2370 [==============================] - 0s 30us/step - loss: 0.0044 - val_loss: 0.0379\n",
      "Epoch 2228/3000\n",
      "2370/2370 [==============================] - 0s 33us/step - loss: 0.0042 - val_loss: 0.0369\n",
      "Epoch 2229/3000\n",
      "2370/2370 [==============================] - 0s 30us/step - loss: 0.0040 - val_loss: 0.0373\n",
      "Epoch 2230/3000\n",
      "2370/2370 [==============================] - 0s 28us/step - loss: 0.0041 - val_loss: 0.0373\n",
      "Epoch 2231/3000\n",
      "2370/2370 [==============================] - 0s 28us/step - loss: 0.0041 - val_loss: 0.0378\n",
      "Epoch 2232/3000\n",
      "2370/2370 [==============================] - 0s 28us/step - loss: 0.0042 - val_loss: 0.0381\n",
      "Epoch 2233/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0042 - val_loss: 0.0371\n",
      "Epoch 2234/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0041 - val_loss: 0.0373\n",
      "Epoch 2235/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0040 - val_loss: 0.0378\n",
      "Epoch 2236/3000\n",
      "2370/2370 [==============================] - 0s 33us/step - loss: 0.0041 - val_loss: 0.0374\n",
      "Epoch 2237/3000\n",
      "2370/2370 [==============================] - 0s 28us/step - loss: 0.0040 - val_loss: 0.0381\n",
      "Epoch 2238/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0042 - val_loss: 0.0376\n",
      "Epoch 2239/3000\n",
      "2370/2370 [==============================] - 0s 28us/step - loss: 0.0039 - val_loss: 0.0374\n",
      "Epoch 2240/3000\n",
      "2370/2370 [==============================] - 0s 28us/step - loss: 0.0042 - val_loss: 0.0373\n",
      "Epoch 2241/3000\n",
      "2370/2370 [==============================] - 0s 28us/step - loss: 0.0042 - val_loss: 0.0379\n",
      "Epoch 2242/3000\n",
      "2370/2370 [==============================] - 0s 28us/step - loss: 0.0039 - val_loss: 0.0373\n",
      "Epoch 2243/3000\n",
      "2370/2370 [==============================] - 0s 31us/step - loss: 0.0040 - val_loss: 0.0376\n",
      "Epoch 2244/3000\n",
      "2370/2370 [==============================] - 0s 28us/step - loss: 0.0040 - val_loss: 0.0379\n",
      "Epoch 2245/3000\n",
      "2370/2370 [==============================] - 0s 27us/step - loss: 0.0041 - val_loss: 0.0387\n",
      "Epoch 2246/3000\n",
      "2370/2370 [==============================] - 0s 28us/step - loss: 0.0040 - val_loss: 0.0378\n",
      "Epoch 2247/3000\n",
      "2370/2370 [==============================] - 0s 27us/step - loss: 0.0042 - val_loss: 0.0388\n",
      "Epoch 2248/3000\n",
      "2370/2370 [==============================] - 0s 27us/step - loss: 0.0044 - val_loss: 0.0389\n",
      "Epoch 2249/3000\n",
      "2370/2370 [==============================] - 0s 27us/step - loss: 0.0043 - val_loss: 0.0387\n",
      "Epoch 2250/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0039 - val_loss: 0.0384\n",
      "Epoch 2251/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0043 - val_loss: 0.0384\n",
      "Epoch 2252/3000\n",
      "2370/2370 [==============================] - 0s 28us/step - loss: 0.0041 - val_loss: 0.0388\n",
      "Epoch 2253/3000\n",
      "2370/2370 [==============================] - 0s 28us/step - loss: 0.0042 - val_loss: 0.0385\n",
      "Epoch 2254/3000\n",
      "2370/2370 [==============================] - 0s 28us/step - loss: 0.0041 - val_loss: 0.0377\n",
      "Epoch 2255/3000\n",
      "2370/2370 [==============================] - 0s 28us/step - loss: 0.0041 - val_loss: 0.0385\n",
      "Epoch 2256/3000\n",
      "2370/2370 [==============================] - 0s 30us/step - loss: 0.0040 - val_loss: 0.0378\n",
      "Epoch 2257/3000\n",
      "2370/2370 [==============================] - 0s 34us/step - loss: 0.0040 - val_loss: 0.0384\n",
      "Epoch 2258/3000\n",
      "2370/2370 [==============================] - 0s 46us/step - loss: 0.0040 - val_loss: 0.0382\n",
      "Epoch 2259/3000\n",
      "2370/2370 [==============================] - 0s 60us/step - loss: 0.0041 - val_loss: 0.0380\n",
      "Epoch 2260/3000\n",
      "2370/2370 [==============================] - 0s 41us/step - loss: 0.0045 - val_loss: 0.0375\n",
      "Epoch 2261/3000\n",
      "2370/2370 [==============================] - 0s 36us/step - loss: 0.0041 - val_loss: 0.0375\n",
      "Epoch 2262/3000\n",
      "2370/2370 [==============================] - 0s 56us/step - loss: 0.0043 - val_loss: 0.0383\n",
      "Epoch 2263/3000\n",
      "2370/2370 [==============================] - 0s 82us/step - loss: 0.0042 - val_loss: 0.0383\n",
      "Epoch 2264/3000\n",
      "2370/2370 [==============================] - 0s 32us/step - loss: 0.0042 - val_loss: 0.0381\n",
      "Epoch 2265/3000\n",
      "2370/2370 [==============================] - 0s 32us/step - loss: 0.0041 - val_loss: 0.0376\n",
      "Epoch 2266/3000\n",
      "2370/2370 [==============================] - 0s 33us/step - loss: 0.0041 - val_loss: 0.0381\n",
      "Epoch 2267/3000\n",
      "2370/2370 [==============================] - 0s 31us/step - loss: 0.0040 - val_loss: 0.0383\n",
      "Epoch 2268/3000\n",
      "2370/2370 [==============================] - 0s 28us/step - loss: 0.0043 - val_loss: 0.0373\n",
      "Epoch 2269/3000\n",
      "2370/2370 [==============================] - 0s 28us/step - loss: 0.0041 - val_loss: 0.0372\n",
      "Epoch 2270/3000\n",
      "2370/2370 [==============================] - 0s 28us/step - loss: 0.0039 - val_loss: 0.0373\n",
      "Epoch 2271/3000\n",
      "2370/2370 [==============================] - 0s 28us/step - loss: 0.0040 - val_loss: 0.0374\n",
      "Epoch 2272/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0041 - val_loss: 0.0380\n",
      "Epoch 2273/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0039 - val_loss: 0.0375\n",
      "Epoch 2274/3000\n",
      "2370/2370 [==============================] - 0s 33us/step - loss: 0.0043 - val_loss: 0.0378\n",
      "Epoch 2275/3000\n",
      "2370/2370 [==============================] - 0s 28us/step - loss: 0.0040 - val_loss: 0.0381\n",
      "Epoch 2276/3000\n",
      "2370/2370 [==============================] - 0s 28us/step - loss: 0.0039 - val_loss: 0.0377\n",
      "Epoch 2277/3000\n",
      "2370/2370 [==============================] - 0s 28us/step - loss: 0.0041 - val_loss: 0.0376\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2278/3000\n",
      "2370/2370 [==============================] - 0s 27us/step - loss: 0.0039 - val_loss: 0.0376\n",
      "Epoch 2279/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0041 - val_loss: 0.0369\n",
      "Epoch 2280/3000\n",
      "2370/2370 [==============================] - 0s 38us/step - loss: 0.0040 - val_loss: 0.0380\n",
      "Epoch 2281/3000\n",
      "2370/2370 [==============================] - 0s 73us/step - loss: 0.0043 - val_loss: 0.0385\n",
      "Epoch 2282/3000\n",
      "2370/2370 [==============================] - 0s 64us/step - loss: 0.0042 - val_loss: 0.0391\n",
      "Epoch 2283/3000\n",
      "2370/2370 [==============================] - 0s 34us/step - loss: 0.0041 - val_loss: 0.0384\n",
      "Epoch 2284/3000\n",
      "2370/2370 [==============================] - 0s 33us/step - loss: 0.0040 - val_loss: 0.0382\n",
      "Epoch 2285/3000\n",
      "2370/2370 [==============================] - 0s 33us/step - loss: 0.0041 - val_loss: 0.0379\n",
      "Epoch 2286/3000\n",
      "2370/2370 [==============================] - 0s 28us/step - loss: 0.0040 - val_loss: 0.0380\n",
      "Epoch 2287/3000\n",
      "2370/2370 [==============================] - 0s 28us/step - loss: 0.0040 - val_loss: 0.0378\n",
      "Epoch 2288/3000\n",
      "2370/2370 [==============================] - 0s 28us/step - loss: 0.0040 - val_loss: 0.0382\n",
      "Epoch 2289/3000\n",
      "2370/2370 [==============================] - 0s 34us/step - loss: 0.0043 - val_loss: 0.0371\n",
      "Epoch 2290/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0039 - val_loss: 0.0377\n",
      "Epoch 2291/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0040 - val_loss: 0.0376\n",
      "Epoch 2292/3000\n",
      "2370/2370 [==============================] - 0s 34us/step - loss: 0.0043 - val_loss: 0.0379\n",
      "Epoch 2293/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0040 - val_loss: 0.0379\n",
      "Epoch 2294/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0041 - val_loss: 0.0379\n",
      "Epoch 2295/3000\n",
      "2370/2370 [==============================] - 0s 37us/step - loss: 0.0041 - val_loss: 0.0382\n",
      "Epoch 2296/3000\n",
      "2370/2370 [==============================] - 0s 40us/step - loss: 0.0042 - val_loss: 0.0380\n",
      "Epoch 2297/3000\n",
      "2370/2370 [==============================] - 0s 83us/step - loss: 0.0041 - val_loss: 0.0383\n",
      "Epoch 2298/3000\n",
      "2370/2370 [==============================] - 0s 125us/step - loss: 0.0043 - val_loss: 0.0384\n",
      "Epoch 2299/3000\n",
      "2370/2370 [==============================] - 0s 91us/step - loss: 0.0040 - val_loss: 0.0385\n",
      "Epoch 2300/3000\n",
      "2370/2370 [==============================] - 0s 79us/step - loss: 0.0042 - val_loss: 0.0383\n",
      "Epoch 2301/3000\n",
      "2370/2370 [==============================] - 0s 34us/step - loss: 0.0040 - val_loss: 0.0379\n",
      "Epoch 2302/3000\n",
      "2370/2370 [==============================] - 0s 34us/step - loss: 0.0040 - val_loss: 0.0381\n",
      "Epoch 2303/3000\n",
      "2370/2370 [==============================] - 0s 32us/step - loss: 0.0040 - val_loss: 0.0384\n",
      "Epoch 2304/3000\n",
      "2370/2370 [==============================] - 0s 32us/step - loss: 0.0040 - val_loss: 0.0379\n",
      "Epoch 2305/3000\n",
      "2370/2370 [==============================] - 0s 36us/step - loss: 0.0040 - val_loss: 0.0381\n",
      "Epoch 2306/3000\n",
      "2370/2370 [==============================] - 0s 31us/step - loss: 0.0041 - val_loss: 0.0373\n",
      "Epoch 2307/3000\n",
      "2370/2370 [==============================] - 0s 30us/step - loss: 0.0043 - val_loss: 0.0372\n",
      "Epoch 2308/3000\n",
      "2370/2370 [==============================] - 0s 35us/step - loss: 0.0042 - val_loss: 0.0379\n",
      "Epoch 2309/3000\n",
      "2370/2370 [==============================] - 0s 30us/step - loss: 0.0041 - val_loss: 0.0386\n",
      "Epoch 2310/3000\n",
      "2370/2370 [==============================] - 0s 34us/step - loss: 0.0038 - val_loss: 0.0374\n",
      "Epoch 2311/3000\n",
      "2370/2370 [==============================] - 0s 31us/step - loss: 0.0040 - val_loss: 0.0378\n",
      "Epoch 2312/3000\n",
      "2370/2370 [==============================] - 0s 30us/step - loss: 0.0038 - val_loss: 0.0372\n",
      "Epoch 2313/3000\n",
      "2370/2370 [==============================] - 0s 30us/step - loss: 0.0040 - val_loss: 0.0378\n",
      "Epoch 2314/3000\n",
      "2370/2370 [==============================] - 0s 30us/step - loss: 0.0042 - val_loss: 0.0382\n",
      "Epoch 2315/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0039 - val_loss: 0.0378\n",
      "Epoch 2316/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0038 - val_loss: 0.0380\n",
      "Epoch 2317/3000\n",
      "2370/2370 [==============================] - 0s 28us/step - loss: 0.0041 - val_loss: 0.0377\n",
      "Epoch 2318/3000\n",
      "2370/2370 [==============================] - 0s 28us/step - loss: 0.0040 - val_loss: 0.0372\n",
      "Epoch 2319/3000\n",
      "2370/2370 [==============================] - 0s 28us/step - loss: 0.0039 - val_loss: 0.0374\n",
      "Epoch 2320/3000\n",
      "2370/2370 [==============================] - 0s 30us/step - loss: 0.0040 - val_loss: 0.0374\n",
      "Epoch 2321/3000\n",
      "2370/2370 [==============================] - 0s 28us/step - loss: 0.0040 - val_loss: 0.0380\n",
      "Epoch 2322/3000\n",
      "2370/2370 [==============================] - 0s 31us/step - loss: 0.0042 - val_loss: 0.0387\n",
      "Epoch 2323/3000\n",
      "2370/2370 [==============================] - 0s 28us/step - loss: 0.0039 - val_loss: 0.0382\n",
      "Epoch 2324/3000\n",
      "2370/2370 [==============================] - 0s 31us/step - loss: 0.0042 - val_loss: 0.0379\n",
      "Epoch 2325/3000\n",
      "2370/2370 [==============================] - 0s 31us/step - loss: 0.0041 - val_loss: 0.0380\n",
      "Epoch 2326/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0040 - val_loss: 0.0384\n",
      "Epoch 2327/3000\n",
      "2370/2370 [==============================] - 0s 28us/step - loss: 0.0039 - val_loss: 0.0381\n",
      "Epoch 2328/3000\n",
      "2370/2370 [==============================] - 0s 28us/step - loss: 0.0040 - val_loss: 0.0386\n",
      "Epoch 2329/3000\n",
      "2370/2370 [==============================] - 0s 32us/step - loss: 0.0040 - val_loss: 0.0384\n",
      "Epoch 2330/3000\n",
      "2370/2370 [==============================] - 0s 31us/step - loss: 0.0039 - val_loss: 0.0392\n",
      "Epoch 2331/3000\n",
      "2370/2370 [==============================] - 0s 34us/step - loss: 0.0044 - val_loss: 0.0385\n",
      "Epoch 2332/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0043 - val_loss: 0.0383\n",
      "Epoch 2333/3000\n",
      "2370/2370 [==============================] - 0s 30us/step - loss: 0.0040 - val_loss: 0.0384\n",
      "Epoch 2334/3000\n",
      "2370/2370 [==============================] - 0s 31us/step - loss: 0.0040 - val_loss: 0.0387\n",
      "Epoch 2335/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0040 - val_loss: 0.0378\n",
      "Epoch 2336/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0042 - val_loss: 0.0377\n",
      "Epoch 2337/3000\n",
      "2370/2370 [==============================] - 0s 28us/step - loss: 0.0041 - val_loss: 0.0383\n",
      "Epoch 2338/3000\n",
      "2370/2370 [==============================] - 0s 33us/step - loss: 0.0040 - val_loss: 0.0392\n",
      "Epoch 2339/3000\n",
      "2370/2370 [==============================] - 0s 30us/step - loss: 0.0041 - val_loss: 0.0382\n",
      "Epoch 2340/3000\n",
      "2370/2370 [==============================] - 0s 34us/step - loss: 0.0044 - val_loss: 0.0386\n",
      "Epoch 2341/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0041 - val_loss: 0.0384\n",
      "Epoch 2342/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0039 - val_loss: 0.0386\n",
      "Epoch 2343/3000\n",
      "2370/2370 [==============================] - 0s 28us/step - loss: 0.0041 - val_loss: 0.0376\n",
      "Epoch 2344/3000\n",
      "2370/2370 [==============================] - 0s 28us/step - loss: 0.0042 - val_loss: 0.0383\n",
      "Epoch 2345/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0041 - val_loss: 0.0374\n",
      "Epoch 2346/3000\n",
      "2370/2370 [==============================] - 0s 28us/step - loss: 0.0041 - val_loss: 0.0382\n",
      "Epoch 2347/3000\n",
      "2370/2370 [==============================] - 0s 27us/step - loss: 0.0040 - val_loss: 0.0383\n",
      "Epoch 2348/3000\n",
      "2370/2370 [==============================] - 0s 31us/step - loss: 0.0038 - val_loss: 0.0389\n",
      "Epoch 2349/3000\n",
      "2370/2370 [==============================] - 0s 31us/step - loss: 0.0039 - val_loss: 0.0382\n",
      "Epoch 2350/3000\n",
      "2370/2370 [==============================] - 0s 31us/step - loss: 0.0039 - val_loss: 0.0380\n",
      "Epoch 2351/3000\n",
      "2370/2370 [==============================] - 0s 30us/step - loss: 0.0043 - val_loss: 0.0387\n",
      "Epoch 2352/3000\n",
      "2370/2370 [==============================] - 0s 33us/step - loss: 0.0039 - val_loss: 0.0384\n",
      "Epoch 2353/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0042 - val_loss: 0.0380\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2354/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0041 - val_loss: 0.0381\n",
      "Epoch 2355/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0040 - val_loss: 0.0381\n",
      "Epoch 2356/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0039 - val_loss: 0.0377\n",
      "Epoch 2357/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0041 - val_loss: 0.0378\n",
      "Epoch 2358/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0041 - val_loss: 0.0374\n",
      "Epoch 2359/3000\n",
      "2370/2370 [==============================] - 0s 34us/step - loss: 0.0042 - val_loss: 0.0374\n",
      "Epoch 2360/3000\n",
      "2370/2370 [==============================] - 0s 30us/step - loss: 0.0042 - val_loss: 0.0372\n",
      "Epoch 2361/3000\n",
      "2370/2370 [==============================] - 0s 30us/step - loss: 0.0040 - val_loss: 0.0387\n",
      "Epoch 2362/3000\n",
      "2370/2370 [==============================] - 0s 30us/step - loss: 0.0039 - val_loss: 0.0380\n",
      "Epoch 2363/3000\n",
      "2370/2370 [==============================] - 0s 32us/step - loss: 0.0042 - val_loss: 0.0380\n",
      "Epoch 2364/3000\n",
      "2370/2370 [==============================] - 0s 31us/step - loss: 0.0039 - val_loss: 0.0384\n",
      "Epoch 2365/3000\n",
      "2370/2370 [==============================] - 0s 32us/step - loss: 0.0040 - val_loss: 0.0376\n",
      "Epoch 2366/3000\n",
      "2370/2370 [==============================] - 0s 32us/step - loss: 0.0039 - val_loss: 0.0376\n",
      "Epoch 2367/3000\n",
      "2370/2370 [==============================] - 0s 28us/step - loss: 0.0038 - val_loss: 0.0378\n",
      "Epoch 2368/3000\n",
      "2370/2370 [==============================] - 0s 37us/step - loss: 0.0041 - val_loss: 0.0381\n",
      "Epoch 2369/3000\n",
      "2370/2370 [==============================] - 0s 96us/step - loss: 0.0041 - val_loss: 0.0375\n",
      "Epoch 2370/3000\n",
      "2370/2370 [==============================] - 0s 60us/step - loss: 0.0039 - val_loss: 0.0375\n",
      "Epoch 2371/3000\n",
      "2370/2370 [==============================] - 0s 51us/step - loss: 0.0042 - val_loss: 0.0381\n",
      "Epoch 2372/3000\n",
      "2370/2370 [==============================] - 0s 58us/step - loss: 0.0041 - val_loss: 0.0376\n",
      "Epoch 2373/3000\n",
      "2370/2370 [==============================] - 0s 103us/step - loss: 0.0040 - val_loss: 0.0378\n",
      "Epoch 2374/3000\n",
      "2370/2370 [==============================] - 0s 136us/step - loss: 0.0040 - val_loss: 0.0378\n",
      "Epoch 2375/3000\n",
      "2370/2370 [==============================] - 0s 106us/step - loss: 0.0043 - val_loss: 0.0383\n",
      "Epoch 2376/3000\n",
      "2370/2370 [==============================] - 0s 98us/step - loss: 0.0040 - val_loss: 0.0384\n",
      "Epoch 2377/3000\n",
      "2370/2370 [==============================] - 0s 119us/step - loss: 0.0041 - val_loss: 0.0376\n",
      "Epoch 2378/3000\n",
      "2370/2370 [==============================] - 0s 117us/step - loss: 0.0039 - val_loss: 0.0384\n",
      "Epoch 2379/3000\n",
      "2370/2370 [==============================] - 0s 90us/step - loss: 0.0040 - val_loss: 0.0383\n",
      "Epoch 2380/3000\n",
      "2370/2370 [==============================] - 0s 92us/step - loss: 0.0041 - val_loss: 0.0372\n",
      "Epoch 2381/3000\n",
      "2370/2370 [==============================] - 0s 98us/step - loss: 0.0040 - val_loss: 0.0380\n",
      "Epoch 2382/3000\n",
      "2370/2370 [==============================] - 0s 89us/step - loss: 0.0042 - val_loss: 0.0380\n",
      "Epoch 2383/3000\n",
      "2370/2370 [==============================] - 0s 35us/step - loss: 0.0037 - val_loss: 0.0386\n",
      "Epoch 2384/3000\n",
      "2370/2370 [==============================] - 0s 32us/step - loss: 0.0040 - val_loss: 0.0384\n",
      "Epoch 2385/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0040 - val_loss: 0.0388\n",
      "Epoch 2386/3000\n",
      "2370/2370 [==============================] - 0s 37us/step - loss: 0.0042 - val_loss: 0.0379\n",
      "Epoch 2387/3000\n",
      "2370/2370 [==============================] - 0s 38us/step - loss: 0.0041 - val_loss: 0.0381\n",
      "Epoch 2388/3000\n",
      "2370/2370 [==============================] - 0s 91us/step - loss: 0.0040 - val_loss: 0.0383\n",
      "Epoch 2389/3000\n",
      "2370/2370 [==============================] - 0s 125us/step - loss: 0.0040 - val_loss: 0.0381\n",
      "Epoch 2390/3000\n",
      "2370/2370 [==============================] - 0s 104us/step - loss: 0.0040 - val_loss: 0.0388\n",
      "Epoch 2391/3000\n",
      "2370/2370 [==============================] - 0s 92us/step - loss: 0.0037 - val_loss: 0.0384\n",
      "Epoch 2392/3000\n",
      "2370/2370 [==============================] - 0s 103us/step - loss: 0.0041 - val_loss: 0.0386\n",
      "Epoch 2393/3000\n",
      "2370/2370 [==============================] - 0s 50us/step - loss: 0.0040 - val_loss: 0.0382\n",
      "Epoch 2394/3000\n",
      "2370/2370 [==============================] - 0s 45us/step - loss: 0.0041 - val_loss: 0.0375\n",
      "Epoch 2395/3000\n",
      "2370/2370 [==============================] - 0s 30us/step - loss: 0.0039 - val_loss: 0.0379\n",
      "Epoch 2396/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0039 - val_loss: 0.0375\n",
      "Epoch 2397/3000\n",
      "2370/2370 [==============================] - 0s 31us/step - loss: 0.0041 - val_loss: 0.0379\n",
      "Epoch 2398/3000\n",
      "2370/2370 [==============================] - 0s 31us/step - loss: 0.0040 - val_loss: 0.0379\n",
      "Epoch 2399/3000\n",
      "2370/2370 [==============================] - 0s 27us/step - loss: 0.0042 - val_loss: 0.0375\n",
      "Epoch 2400/3000\n",
      "2370/2370 [==============================] - 0s 28us/step - loss: 0.0041 - val_loss: 0.0371\n",
      "Epoch 2401/3000\n",
      "2370/2370 [==============================] - 0s 31us/step - loss: 0.0040 - val_loss: 0.0379\n",
      "Epoch 2402/3000\n",
      "2370/2370 [==============================] - 0s 30us/step - loss: 0.0042 - val_loss: 0.0376\n",
      "Epoch 2403/3000\n",
      "2370/2370 [==============================] - 0s 34us/step - loss: 0.0041 - val_loss: 0.0383\n",
      "Epoch 2404/3000\n",
      "2370/2370 [==============================] - 0s 32us/step - loss: 0.0042 - val_loss: 0.0388\n",
      "Epoch 2405/3000\n",
      "2370/2370 [==============================] - 0s 33us/step - loss: 0.0040 - val_loss: 0.0377\n",
      "Epoch 2406/3000\n",
      "2370/2370 [==============================] - 0s 30us/step - loss: 0.0041 - val_loss: 0.0375\n",
      "Epoch 2407/3000\n",
      "2370/2370 [==============================] - 0s 28us/step - loss: 0.0041 - val_loss: 0.0378\n",
      "Epoch 2408/3000\n",
      "2370/2370 [==============================] - 0s 30us/step - loss: 0.0040 - val_loss: 0.0377\n",
      "Epoch 2409/3000\n",
      "2370/2370 [==============================] - 0s 31us/step - loss: 0.0040 - val_loss: 0.0374\n",
      "Epoch 2410/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0042 - val_loss: 0.0382\n",
      "Epoch 2411/3000\n",
      "2370/2370 [==============================] - 0s 31us/step - loss: 0.0039 - val_loss: 0.0384\n",
      "Epoch 2412/3000\n",
      "2370/2370 [==============================] - 0s 32us/step - loss: 0.0040 - val_loss: 0.0384\n",
      "Epoch 2413/3000\n",
      "2370/2370 [==============================] - 0s 28us/step - loss: 0.0039 - val_loss: 0.0379\n",
      "Epoch 2414/3000\n",
      "2370/2370 [==============================] - 0s 30us/step - loss: 0.0041 - val_loss: 0.0381\n",
      "Epoch 2415/3000\n",
      "2370/2370 [==============================] - 0s 35us/step - loss: 0.0043 - val_loss: 0.0383\n",
      "Epoch 2416/3000\n",
      "2370/2370 [==============================] - 0s 28us/step - loss: 0.0041 - val_loss: 0.0383\n",
      "Epoch 2417/3000\n",
      "2370/2370 [==============================] - 0s 28us/step - loss: 0.0039 - val_loss: 0.0384\n",
      "Epoch 2418/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0040 - val_loss: 0.0382\n",
      "Epoch 2419/3000\n",
      "2370/2370 [==============================] - 0s 34us/step - loss: 0.0043 - val_loss: 0.0384\n",
      "Epoch 2420/3000\n",
      "2370/2370 [==============================] - 0s 31us/step - loss: 0.0041 - val_loss: 0.0381\n",
      "Epoch 2421/3000\n",
      "2370/2370 [==============================] - 0s 30us/step - loss: 0.0040 - val_loss: 0.0380\n",
      "Epoch 2422/3000\n",
      "2370/2370 [==============================] - 0s 30us/step - loss: 0.0041 - val_loss: 0.0383\n",
      "Epoch 2423/3000\n",
      "2370/2370 [==============================] - 0s 30us/step - loss: 0.0040 - val_loss: 0.0382\n",
      "Epoch 2424/3000\n",
      "2370/2370 [==============================] - 0s 30us/step - loss: 0.0039 - val_loss: 0.0373\n",
      "Epoch 2425/3000\n",
      "2370/2370 [==============================] - 0s 37us/step - loss: 0.0040 - val_loss: 0.0376\n",
      "Epoch 2426/3000\n",
      "2370/2370 [==============================] - 0s 32us/step - loss: 0.0041 - val_loss: 0.0378\n",
      "Epoch 2427/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0041 - val_loss: 0.0381\n",
      "Epoch 2428/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0041 - val_loss: 0.0376\n",
      "Epoch 2429/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0041 - val_loss: 0.0374\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2430/3000\n",
      "2370/2370 [==============================] - 0s 30us/step - loss: 0.0041 - val_loss: 0.0374\n",
      "Epoch 2431/3000\n",
      "2370/2370 [==============================] - 0s 28us/step - loss: 0.0041 - val_loss: 0.0387\n",
      "Epoch 2432/3000\n",
      "2370/2370 [==============================] - 0s 32us/step - loss: 0.0041 - val_loss: 0.0380\n",
      "Epoch 2433/3000\n",
      "2370/2370 [==============================] - 0s 27us/step - loss: 0.0040 - val_loss: 0.0375\n",
      "Epoch 2434/3000\n",
      "2370/2370 [==============================] - 0s 36us/step - loss: 0.0040 - val_loss: 0.0377\n",
      "Epoch 2435/3000\n",
      "2370/2370 [==============================] - 0s 36us/step - loss: 0.0040 - val_loss: 0.0370\n",
      "Epoch 2436/3000\n",
      "2370/2370 [==============================] - 0s 44us/step - loss: 0.0038 - val_loss: 0.0378\n",
      "Epoch 2437/3000\n",
      "2370/2370 [==============================] - 0s 54us/step - loss: 0.0039 - val_loss: 0.0378\n",
      "Epoch 2438/3000\n",
      "2370/2370 [==============================] - 0s 54us/step - loss: 0.0042 - val_loss: 0.0381\n",
      "Epoch 2439/3000\n",
      "2370/2370 [==============================] - 0s 87us/step - loss: 0.0040 - val_loss: 0.0383\n",
      "Epoch 2440/3000\n",
      "2370/2370 [==============================] - 0s 114us/step - loss: 0.0039 - val_loss: 0.0384\n",
      "Epoch 2441/3000\n",
      "2370/2370 [==============================] - 0s 137us/step - loss: 0.0039 - val_loss: 0.0381\n",
      "Epoch 2442/3000\n",
      "2370/2370 [==============================] - 0s 108us/step - loss: 0.0044 - val_loss: 0.0378\n",
      "Epoch 2443/3000\n",
      "2370/2370 [==============================] - 0s 80us/step - loss: 0.0043 - val_loss: 0.0374\n",
      "Epoch 2444/3000\n",
      "2370/2370 [==============================] - 0s 58us/step - loss: 0.0043 - val_loss: 0.0376\n",
      "Epoch 2445/3000\n",
      "2370/2370 [==============================] - 0s 33us/step - loss: 0.0039 - val_loss: 0.0387\n",
      "Epoch 2446/3000\n",
      "2370/2370 [==============================] - 0s 28us/step - loss: 0.0040 - val_loss: 0.0386\n",
      "Epoch 2447/3000\n",
      "2370/2370 [==============================] - 0s 27us/step - loss: 0.0041 - val_loss: 0.0381\n",
      "Epoch 2448/3000\n",
      "2370/2370 [==============================] - 0s 27us/step - loss: 0.0040 - val_loss: 0.0378\n",
      "Epoch 2449/3000\n",
      "2370/2370 [==============================] - 0s 28us/step - loss: 0.0041 - val_loss: 0.0368\n",
      "Epoch 2450/3000\n",
      "2370/2370 [==============================] - 0s 28us/step - loss: 0.0039 - val_loss: 0.0370\n",
      "Epoch 2451/3000\n",
      "2370/2370 [==============================] - 0s 28us/step - loss: 0.0038 - val_loss: 0.0372\n",
      "Epoch 2452/3000\n",
      "2370/2370 [==============================] - 0s 32us/step - loss: 0.0040 - val_loss: 0.0371\n",
      "Epoch 2453/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0039 - val_loss: 0.0376\n",
      "Epoch 2454/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0043 - val_loss: 0.0380\n",
      "Epoch 2455/3000\n",
      "2370/2370 [==============================] - 0s 28us/step - loss: 0.0040 - val_loss: 0.0381\n",
      "Epoch 2456/3000\n",
      "2370/2370 [==============================] - 0s 30us/step - loss: 0.0040 - val_loss: 0.0369\n",
      "Epoch 2457/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0041 - val_loss: 0.0377\n",
      "Epoch 2458/3000\n",
      "2370/2370 [==============================] - 0s 28us/step - loss: 0.0040 - val_loss: 0.0378\n",
      "Epoch 2459/3000\n",
      "2370/2370 [==============================] - 0s 33us/step - loss: 0.0041 - val_loss: 0.0379\n",
      "Epoch 2460/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0043 - val_loss: 0.0378\n",
      "Epoch 2461/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0039 - val_loss: 0.0379\n",
      "Epoch 2462/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0040 - val_loss: 0.0377\n",
      "Epoch 2463/3000\n",
      "2370/2370 [==============================] - 0s 32us/step - loss: 0.0039 - val_loss: 0.0378\n",
      "Epoch 2464/3000\n",
      "2370/2370 [==============================] - 0s 40us/step - loss: 0.0040 - val_loss: 0.0368\n",
      "Epoch 2465/3000\n",
      "2370/2370 [==============================] - 0s 110us/step - loss: 0.0042 - val_loss: 0.0378\n",
      "Epoch 2466/3000\n",
      "2370/2370 [==============================] - 0s 87us/step - loss: 0.0040 - val_loss: 0.0379\n",
      "Epoch 2467/3000\n",
      "2370/2370 [==============================] - 0s 63us/step - loss: 0.0042 - val_loss: 0.0379\n",
      "Epoch 2468/3000\n",
      "2370/2370 [==============================] - 0s 32us/step - loss: 0.0041 - val_loss: 0.0368\n",
      "Epoch 2469/3000\n",
      "2370/2370 [==============================] - 0s 32us/step - loss: 0.0038 - val_loss: 0.0373\n",
      "Epoch 2470/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0038 - val_loss: 0.0373\n",
      "Epoch 2471/3000\n",
      "2370/2370 [==============================] - 0s 28us/step - loss: 0.0040 - val_loss: 0.0373\n",
      "Epoch 2472/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0040 - val_loss: 0.0374\n",
      "Epoch 2473/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0041 - val_loss: 0.0376\n",
      "Epoch 2474/3000\n",
      "2370/2370 [==============================] - 0s 34us/step - loss: 0.0043 - val_loss: 0.0378\n",
      "Epoch 2475/3000\n",
      "2370/2370 [==============================] - 0s 32us/step - loss: 0.0038 - val_loss: 0.0379\n",
      "Epoch 2476/3000\n",
      "2370/2370 [==============================] - 0s 34us/step - loss: 0.0041 - val_loss: 0.0377\n",
      "Epoch 2477/3000\n",
      "2370/2370 [==============================] - 0s 34us/step - loss: 0.0041 - val_loss: 0.0381\n",
      "Epoch 2478/3000\n",
      "2370/2370 [==============================] - 0s 31us/step - loss: 0.0040 - val_loss: 0.0384\n",
      "Epoch 2479/3000\n",
      "2370/2370 [==============================] - 0s 63us/step - loss: 0.0043 - val_loss: 0.0387\n",
      "Epoch 2480/3000\n",
      "2370/2370 [==============================] - 0s 106us/step - loss: 0.0041 - val_loss: 0.0382\n",
      "Epoch 2481/3000\n",
      "2370/2370 [==============================] - 0s 57us/step - loss: 0.0044 - val_loss: 0.0377\n",
      "Epoch 2482/3000\n",
      "2370/2370 [==============================] - 0s 41us/step - loss: 0.0042 - val_loss: 0.0380\n",
      "Epoch 2483/3000\n",
      "2370/2370 [==============================] - 0s 96us/step - loss: 0.0043 - val_loss: 0.0387\n",
      "Epoch 2484/3000\n",
      "2370/2370 [==============================] - 0s 87us/step - loss: 0.0042 - val_loss: 0.0378\n",
      "Epoch 2485/3000\n",
      "2370/2370 [==============================] - 0s 51us/step - loss: 0.0040 - val_loss: 0.0379\n",
      "Epoch 2486/3000\n",
      "2370/2370 [==============================] - 0s 40us/step - loss: 0.0039 - val_loss: 0.0384\n",
      "Epoch 2487/3000\n",
      "2370/2370 [==============================] - 0s 41us/step - loss: 0.0038 - val_loss: 0.0385\n",
      "Epoch 2488/3000\n",
      "2370/2370 [==============================] - 0s 35us/step - loss: 0.0040 - val_loss: 0.0373\n",
      "Epoch 2489/3000\n",
      "2370/2370 [==============================] - 0s 28us/step - loss: 0.0041 - val_loss: 0.0384\n",
      "Epoch 2490/3000\n",
      "2370/2370 [==============================] - 0s 28us/step - loss: 0.0040 - val_loss: 0.0379\n",
      "Epoch 2491/3000\n",
      "2370/2370 [==============================] - 0s 34us/step - loss: 0.0040 - val_loss: 0.0386\n",
      "Epoch 2492/3000\n",
      "2370/2370 [==============================] - 0s 27us/step - loss: 0.0039 - val_loss: 0.0387\n",
      "Epoch 2493/3000\n",
      "2370/2370 [==============================] - 0s 28us/step - loss: 0.0044 - val_loss: 0.0388\n",
      "Epoch 2494/3000\n",
      "2370/2370 [==============================] - 0s 27us/step - loss: 0.0039 - val_loss: 0.0387\n",
      "Epoch 2495/3000\n",
      "2370/2370 [==============================] - 0s 27us/step - loss: 0.0039 - val_loss: 0.0381\n",
      "Epoch 2496/3000\n",
      "2370/2370 [==============================] - 0s 27us/step - loss: 0.0037 - val_loss: 0.0376\n",
      "Epoch 2497/3000\n",
      "2370/2370 [==============================] - 0s 27us/step - loss: 0.0039 - val_loss: 0.0379\n",
      "Epoch 2498/3000\n",
      "2370/2370 [==============================] - 0s 31us/step - loss: 0.0041 - val_loss: 0.0376\n",
      "Epoch 2499/3000\n",
      "2370/2370 [==============================] - 0s 28us/step - loss: 0.0039 - val_loss: 0.0382\n",
      "Epoch 2500/3000\n",
      "2370/2370 [==============================] - 0s 28us/step - loss: 0.0041 - val_loss: 0.0382\n",
      "Epoch 2501/3000\n",
      "2370/2370 [==============================] - 0s 28us/step - loss: 0.0041 - val_loss: 0.0375\n",
      "Epoch 2502/3000\n",
      "2370/2370 [==============================] - 0s 27us/step - loss: 0.0039 - val_loss: 0.0377\n",
      "Epoch 2503/3000\n",
      "2370/2370 [==============================] - 0s 28us/step - loss: 0.0041 - val_loss: 0.0377\n",
      "Epoch 2504/3000\n",
      "2370/2370 [==============================] - 0s 28us/step - loss: 0.0041 - val_loss: 0.0376\n",
      "Epoch 2505/3000\n",
      "2370/2370 [==============================] - 0s 28us/step - loss: 0.0041 - val_loss: 0.0373\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2506/3000\n",
      "2370/2370 [==============================] - 0s 30us/step - loss: 0.0040 - val_loss: 0.0376\n",
      "Epoch 2507/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0042 - val_loss: 0.0373\n",
      "Epoch 2508/3000\n",
      "2370/2370 [==============================] - 0s 28us/step - loss: 0.0040 - val_loss: 0.0377\n",
      "Epoch 2509/3000\n",
      "2370/2370 [==============================] - 0s 27us/step - loss: 0.0040 - val_loss: 0.0385\n",
      "Epoch 2510/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0039 - val_loss: 0.0373\n",
      "Epoch 2511/3000\n",
      "2370/2370 [==============================] - 0s 68us/step - loss: 0.0039 - val_loss: 0.0378\n",
      "Epoch 2512/3000\n",
      "2370/2370 [==============================] - 0s 108us/step - loss: 0.0042 - val_loss: 0.0373\n",
      "Epoch 2513/3000\n",
      "2370/2370 [==============================] - 0s 83us/step - loss: 0.0041 - val_loss: 0.0370\n",
      "Epoch 2514/3000\n",
      "2370/2370 [==============================] - 0s 51us/step - loss: 0.0039 - val_loss: 0.0370\n",
      "Epoch 2515/3000\n",
      "2370/2370 [==============================] - 0s 32us/step - loss: 0.0044 - val_loss: 0.0372\n",
      "Epoch 2516/3000\n",
      "2370/2370 [==============================] - 0s 36us/step - loss: 0.0038 - val_loss: 0.0369\n",
      "Epoch 2517/3000\n",
      "2370/2370 [==============================] - 0s 27us/step - loss: 0.0040 - val_loss: 0.0370\n",
      "Epoch 2518/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0040 - val_loss: 0.0374\n",
      "Epoch 2519/3000\n",
      "2370/2370 [==============================] - 0s 32us/step - loss: 0.0038 - val_loss: 0.0380\n",
      "Epoch 2520/3000\n",
      "2370/2370 [==============================] - 0s 38us/step - loss: 0.0038 - val_loss: 0.0376\n",
      "Epoch 2521/3000\n",
      "2370/2370 [==============================] - 0s 35us/step - loss: 0.0042 - val_loss: 0.0382\n",
      "Epoch 2522/3000\n",
      "2370/2370 [==============================] - 0s 43us/step - loss: 0.0042 - val_loss: 0.0386\n",
      "Epoch 2523/3000\n",
      "2370/2370 [==============================] - 0s 117us/step - loss: 0.0040 - val_loss: 0.0373\n",
      "Epoch 2524/3000\n",
      "2370/2370 [==============================] - 0s 96us/step - loss: 0.0040 - val_loss: 0.0376\n",
      "Epoch 2525/3000\n",
      "2370/2370 [==============================] - 0s 115us/step - loss: 0.0041 - val_loss: 0.0375\n",
      "Epoch 2526/3000\n",
      "2370/2370 [==============================] - 0s 114us/step - loss: 0.0039 - val_loss: 0.0376\n",
      "Epoch 2527/3000\n",
      "2370/2370 [==============================] - 0s 117us/step - loss: 0.0038 - val_loss: 0.0374\n",
      "Epoch 2528/3000\n",
      "2370/2370 [==============================] - 0s 106us/step - loss: 0.0040 - val_loss: 0.0382\n",
      "Epoch 2529/3000\n",
      "2370/2370 [==============================] - 0s 107us/step - loss: 0.0038 - val_loss: 0.0378\n",
      "Epoch 2530/3000\n",
      "2370/2370 [==============================] - 0s 113us/step - loss: 0.0038 - val_loss: 0.0383\n",
      "Epoch 2531/3000\n",
      "2370/2370 [==============================] - 0s 132us/step - loss: 0.0040 - val_loss: 0.0373\n",
      "Epoch 2532/3000\n",
      "2370/2370 [==============================] - 0s 102us/step - loss: 0.0041 - val_loss: 0.0377\n",
      "Epoch 2533/3000\n",
      "2370/2370 [==============================] - 0s 63us/step - loss: 0.0039 - val_loss: 0.0377\n",
      "Epoch 2534/3000\n",
      "2370/2370 [==============================] - 0s 36us/step - loss: 0.0039 - val_loss: 0.0378\n",
      "Epoch 2535/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0041 - val_loss: 0.0380\n",
      "Epoch 2536/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0038 - val_loss: 0.0387\n",
      "Epoch 2537/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0041 - val_loss: 0.0374\n",
      "Epoch 2538/3000\n",
      "2370/2370 [==============================] - 0s 28us/step - loss: 0.0040 - val_loss: 0.0375\n",
      "Epoch 2539/3000\n",
      "2370/2370 [==============================] - 0s 30us/step - loss: 0.0039 - val_loss: 0.0368\n",
      "Epoch 2540/3000\n",
      "2370/2370 [==============================] - 0s 35us/step - loss: 0.0041 - val_loss: 0.0370\n",
      "Epoch 2541/3000\n",
      "2370/2370 [==============================] - 0s 34us/step - loss: 0.0039 - val_loss: 0.0373\n",
      "Epoch 2542/3000\n",
      "2370/2370 [==============================] - 0s 30us/step - loss: 0.0039 - val_loss: 0.0375\n",
      "Epoch 2543/3000\n",
      "2370/2370 [==============================] - 0s 31us/step - loss: 0.0039 - val_loss: 0.0373\n",
      "Epoch 2544/3000\n",
      "2370/2370 [==============================] - 0s 91us/step - loss: 0.0038 - val_loss: 0.0371\n",
      "Epoch 2545/3000\n",
      "2370/2370 [==============================] - 0s 61us/step - loss: 0.0045 - val_loss: 0.0374\n",
      "Epoch 2546/3000\n",
      "2370/2370 [==============================] - 0s 48us/step - loss: 0.0042 - val_loss: 0.0370\n",
      "Epoch 2547/3000\n",
      "2370/2370 [==============================] - 0s 40us/step - loss: 0.0040 - val_loss: 0.0373\n",
      "Epoch 2548/3000\n",
      "2370/2370 [==============================] - 0s 46us/step - loss: 0.0041 - val_loss: 0.0380\n",
      "Epoch 2549/3000\n",
      "2370/2370 [==============================] - 0s 89us/step - loss: 0.0040 - val_loss: 0.0376\n",
      "Epoch 2550/3000\n",
      "2370/2370 [==============================] - 0s 46us/step - loss: 0.0042 - val_loss: 0.0370\n",
      "Epoch 2551/3000\n",
      "2370/2370 [==============================] - 0s 32us/step - loss: 0.0039 - val_loss: 0.0377\n",
      "Epoch 2552/3000\n",
      "2370/2370 [==============================] - 0s 32us/step - loss: 0.0041 - val_loss: 0.0379\n",
      "Epoch 2553/3000\n",
      "2370/2370 [==============================] - 0s 37us/step - loss: 0.0040 - val_loss: 0.0377\n",
      "Epoch 2554/3000\n",
      "2370/2370 [==============================] - 0s 85us/step - loss: 0.0038 - val_loss: 0.0379\n",
      "Epoch 2555/3000\n",
      "2370/2370 [==============================] - 0s 58us/step - loss: 0.0043 - val_loss: 0.0381\n",
      "Epoch 2556/3000\n",
      "2370/2370 [==============================] - 0s 38us/step - loss: 0.0039 - val_loss: 0.0381\n",
      "Epoch 2557/3000\n",
      "2370/2370 [==============================] - 0s 33us/step - loss: 0.0040 - val_loss: 0.0376\n",
      "Epoch 2558/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0040 - val_loss: 0.0376\n",
      "Epoch 2559/3000\n",
      "2370/2370 [==============================] - 0s 38us/step - loss: 0.0040 - val_loss: 0.0376\n",
      "Epoch 2560/3000\n",
      "2370/2370 [==============================] - 0s 83us/step - loss: 0.0043 - val_loss: 0.0376\n",
      "Epoch 2561/3000\n",
      "2370/2370 [==============================] - 0s 106us/step - loss: 0.0040 - val_loss: 0.0381\n",
      "Epoch 2562/3000\n",
      "2370/2370 [==============================] - 0s 86us/step - loss: 0.0039 - val_loss: 0.0375\n",
      "Epoch 2563/3000\n",
      "2370/2370 [==============================] - 0s 58us/step - loss: 0.0038 - val_loss: 0.0376\n",
      "Epoch 2564/3000\n",
      "2370/2370 [==============================] - 0s 62us/step - loss: 0.0038 - val_loss: 0.0380\n",
      "Epoch 2565/3000\n",
      "2370/2370 [==============================] - 0s 48us/step - loss: 0.0041 - val_loss: 0.0378\n",
      "Epoch 2566/3000\n",
      "2370/2370 [==============================] - 0s 30us/step - loss: 0.0039 - val_loss: 0.0380\n",
      "Epoch 2567/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0040 - val_loss: 0.0381\n",
      "Epoch 2568/3000\n",
      "2370/2370 [==============================] - 0s 30us/step - loss: 0.0039 - val_loss: 0.0376\n",
      "Epoch 2569/3000\n",
      "2370/2370 [==============================] - 0s 32us/step - loss: 0.0037 - val_loss: 0.0389\n",
      "Epoch 2570/3000\n",
      "2370/2370 [==============================] - 0s 28us/step - loss: 0.0038 - val_loss: 0.0381\n",
      "Epoch 2571/3000\n",
      "2370/2370 [==============================] - 0s 28us/step - loss: 0.0041 - val_loss: 0.0378\n",
      "Epoch 2572/3000\n",
      "2370/2370 [==============================] - 0s 28us/step - loss: 0.0040 - val_loss: 0.0384\n",
      "Epoch 2573/3000\n",
      "2370/2370 [==============================] - 0s 27us/step - loss: 0.0038 - val_loss: 0.0375\n",
      "Epoch 2574/3000\n",
      "2370/2370 [==============================] - 0s 28us/step - loss: 0.0040 - val_loss: 0.0379\n",
      "Epoch 2575/3000\n",
      "2370/2370 [==============================] - 0s 33us/step - loss: 0.0040 - val_loss: 0.0374\n",
      "Epoch 2576/3000\n",
      "2370/2370 [==============================] - 0s 32us/step - loss: 0.0039 - val_loss: 0.0385\n",
      "Epoch 2577/3000\n",
      "2370/2370 [==============================] - 0s 40us/step - loss: 0.0039 - val_loss: 0.0387\n",
      "Epoch 2578/3000\n",
      "2370/2370 [==============================] - 0s 99us/step - loss: 0.0038 - val_loss: 0.0382\n",
      "Epoch 2579/3000\n",
      "2370/2370 [==============================] - 0s 63us/step - loss: 0.0039 - val_loss: 0.0376\n",
      "Epoch 2580/3000\n",
      "2370/2370 [==============================] - 0s 28us/step - loss: 0.0039 - val_loss: 0.0375\n",
      "Epoch 2581/3000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2370/2370 [==============================] - 0s 27us/step - loss: 0.0039 - val_loss: 0.0385\n",
      "Epoch 2582/3000\n",
      "2370/2370 [==============================] - 0s 28us/step - loss: 0.0039 - val_loss: 0.0383\n",
      "Epoch 2583/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0039 - val_loss: 0.0383\n",
      "Epoch 2584/3000\n",
      "2370/2370 [==============================] - 0s 66us/step - loss: 0.0038 - val_loss: 0.0386\n",
      "Epoch 2585/3000\n",
      "2370/2370 [==============================] - 0s 49us/step - loss: 0.0038 - val_loss: 0.0382\n",
      "Epoch 2586/3000\n",
      "2370/2370 [==============================] - 0s 33us/step - loss: 0.0038 - val_loss: 0.0385\n",
      "Epoch 2587/3000\n",
      "2370/2370 [==============================] - 0s 39us/step - loss: 0.0039 - val_loss: 0.0384\n",
      "Epoch 2588/3000\n",
      "2370/2370 [==============================] - 0s 43us/step - loss: 0.0038 - val_loss: 0.0381\n",
      "Epoch 2589/3000\n",
      "2370/2370 [==============================] - 0s 30us/step - loss: 0.0043 - val_loss: 0.0380\n",
      "Epoch 2590/3000\n",
      "2370/2370 [==============================] - 0s 31us/step - loss: 0.0040 - val_loss: 0.0378\n",
      "Epoch 2591/3000\n",
      "2370/2370 [==============================] - 0s 34us/step - loss: 0.0040 - val_loss: 0.0377\n",
      "Epoch 2592/3000\n",
      "2370/2370 [==============================] - 0s 33us/step - loss: 0.0040 - val_loss: 0.0387\n",
      "Epoch 2593/3000\n",
      "2370/2370 [==============================] - 0s 31us/step - loss: 0.0039 - val_loss: 0.0388\n",
      "Epoch 2594/3000\n",
      "2370/2370 [==============================] - 0s 32us/step - loss: 0.0037 - val_loss: 0.0390\n",
      "Epoch 2595/3000\n",
      "2370/2370 [==============================] - 0s 32us/step - loss: 0.0041 - val_loss: 0.0385\n",
      "Epoch 2596/3000\n",
      "2370/2370 [==============================] - 0s 32us/step - loss: 0.0038 - val_loss: 0.0380\n",
      "Epoch 2597/3000\n",
      "2370/2370 [==============================] - 0s 34us/step - loss: 0.0041 - val_loss: 0.0374\n",
      "Epoch 2598/3000\n",
      "2370/2370 [==============================] - 0s 28us/step - loss: 0.0040 - val_loss: 0.0381\n",
      "Epoch 2599/3000\n",
      "2370/2370 [==============================] - 0s 33us/step - loss: 0.0038 - val_loss: 0.0381\n",
      "Epoch 2600/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0039 - val_loss: 0.0382\n",
      "Epoch 2601/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0038 - val_loss: 0.0388\n",
      "Epoch 2602/3000\n",
      "2370/2370 [==============================] - 0s 30us/step - loss: 0.0039 - val_loss: 0.0394\n",
      "Epoch 2603/3000\n",
      "2370/2370 [==============================] - 0s 33us/step - loss: 0.0038 - val_loss: 0.0386\n",
      "Epoch 2604/3000\n",
      "2370/2370 [==============================] - 0s 36us/step - loss: 0.0039 - val_loss: 0.0381\n",
      "Epoch 2605/3000\n",
      "2370/2370 [==============================] - 0s 34us/step - loss: 0.0041 - val_loss: 0.0381\n",
      "Epoch 2606/3000\n",
      "2370/2370 [==============================] - 0s 34us/step - loss: 0.0038 - val_loss: 0.0382\n",
      "Epoch 2607/3000\n",
      "2370/2370 [==============================] - 0s 34us/step - loss: 0.0041 - val_loss: 0.0377\n",
      "Epoch 2608/3000\n",
      "2370/2370 [==============================] - 0s 34us/step - loss: 0.0039 - val_loss: 0.0381\n",
      "Epoch 2609/3000\n",
      "2370/2370 [==============================] - 0s 33us/step - loss: 0.0039 - val_loss: 0.0381\n",
      "Epoch 2610/3000\n",
      "2370/2370 [==============================] - 0s 33us/step - loss: 0.0040 - val_loss: 0.0370\n",
      "Epoch 2611/3000\n",
      "2370/2370 [==============================] - 0s 28us/step - loss: 0.0040 - val_loss: 0.0374\n",
      "Epoch 2612/3000\n",
      "2370/2370 [==============================] - 0s 28us/step - loss: 0.0039 - val_loss: 0.0382\n",
      "Epoch 2613/3000\n",
      "2370/2370 [==============================] - 0s 27us/step - loss: 0.0039 - val_loss: 0.0375\n",
      "Epoch 2614/3000\n",
      "2370/2370 [==============================] - 0s 27us/step - loss: 0.0041 - val_loss: 0.0369\n",
      "Epoch 2615/3000\n",
      "2370/2370 [==============================] - 0s 27us/step - loss: 0.0040 - val_loss: 0.0371\n",
      "Epoch 2616/3000\n",
      "2370/2370 [==============================] - 0s 28us/step - loss: 0.0040 - val_loss: 0.0371\n",
      "Epoch 2617/3000\n",
      "2370/2370 [==============================] - 0s 33us/step - loss: 0.0038 - val_loss: 0.0374\n",
      "Epoch 2618/3000\n",
      "2370/2370 [==============================] - 0s 28us/step - loss: 0.0040 - val_loss: 0.0371\n",
      "Epoch 2619/3000\n",
      "2370/2370 [==============================] - 0s 28us/step - loss: 0.0040 - val_loss: 0.0376\n",
      "Epoch 2620/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0038 - val_loss: 0.0372\n",
      "Epoch 2621/3000\n",
      "2370/2370 [==============================] - 0s 28us/step - loss: 0.0038 - val_loss: 0.0372\n",
      "Epoch 2622/3000\n",
      "2370/2370 [==============================] - 0s 27us/step - loss: 0.0042 - val_loss: 0.0374\n",
      "Epoch 2623/3000\n",
      "2370/2370 [==============================] - 0s 31us/step - loss: 0.0038 - val_loss: 0.0373\n",
      "Epoch 2624/3000\n",
      "2370/2370 [==============================] - 0s 41us/step - loss: 0.0041 - val_loss: 0.0372\n",
      "Epoch 2625/3000\n",
      "2370/2370 [==============================] - 0s 69us/step - loss: 0.0041 - val_loss: 0.0374\n",
      "Epoch 2626/3000\n",
      "2370/2370 [==============================] - 0s 89us/step - loss: 0.0040 - val_loss: 0.0376\n",
      "Epoch 2627/3000\n",
      "2370/2370 [==============================] - 0s 40us/step - loss: 0.0041 - val_loss: 0.0375\n",
      "Epoch 2628/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0038 - val_loss: 0.0372\n",
      "Epoch 2629/3000\n",
      "2370/2370 [==============================] - 0s 28us/step - loss: 0.0039 - val_loss: 0.0374\n",
      "Epoch 2630/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0038 - val_loss: 0.0373\n",
      "Epoch 2631/3000\n",
      "2370/2370 [==============================] - 0s 28us/step - loss: 0.0037 - val_loss: 0.0375\n",
      "Epoch 2632/3000\n",
      "2370/2370 [==============================] - 0s 28us/step - loss: 0.0041 - val_loss: 0.0373\n",
      "Epoch 2633/3000\n",
      "2370/2370 [==============================] - 0s 28us/step - loss: 0.0039 - val_loss: 0.0371\n",
      "Epoch 2634/3000\n",
      "2370/2370 [==============================] - 0s 32us/step - loss: 0.0039 - val_loss: 0.0379\n",
      "Epoch 2635/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0040 - val_loss: 0.0373\n",
      "Epoch 2636/3000\n",
      "2370/2370 [==============================] - 0s 30us/step - loss: 0.0040 - val_loss: 0.0365\n",
      "Epoch 2637/3000\n",
      "2370/2370 [==============================] - 0s 35us/step - loss: 0.0039 - val_loss: 0.0374\n",
      "Epoch 2638/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0044 - val_loss: 0.0373\n",
      "Epoch 2639/3000\n",
      "2370/2370 [==============================] - 0s 67us/step - loss: 0.0040 - val_loss: 0.0369\n",
      "Epoch 2640/3000\n",
      "2370/2370 [==============================] - 0s 109us/step - loss: 0.0038 - val_loss: 0.0367\n",
      "Epoch 2641/3000\n",
      "2370/2370 [==============================] - 0s 49us/step - loss: 0.0039 - val_loss: 0.0371\n",
      "Epoch 2642/3000\n",
      "2370/2370 [==============================] - 0s 59us/step - loss: 0.0039 - val_loss: 0.0376\n",
      "Epoch 2643/3000\n",
      "2370/2370 [==============================] - 0s 103us/step - loss: 0.0039 - val_loss: 0.0379\n",
      "Epoch 2644/3000\n",
      "2370/2370 [==============================] - 0s 90us/step - loss: 0.0042 - val_loss: 0.0378\n",
      "Epoch 2645/3000\n",
      "2370/2370 [==============================] - 0s 74us/step - loss: 0.0041 - val_loss: 0.0374\n",
      "Epoch 2646/3000\n",
      "2370/2370 [==============================] - 0s 56us/step - loss: 0.0036 - val_loss: 0.0378\n",
      "Epoch 2647/3000\n",
      "2370/2370 [==============================] - 0s 57us/step - loss: 0.0039 - val_loss: 0.0372\n",
      "Epoch 2648/3000\n",
      "2370/2370 [==============================] - 0s 52us/step - loss: 0.0041 - val_loss: 0.0367\n",
      "Epoch 2649/3000\n",
      "2370/2370 [==============================] - 0s 32us/step - loss: 0.0040 - val_loss: 0.0372\n",
      "Epoch 2650/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0039 - val_loss: 0.0370\n",
      "Epoch 2651/3000\n",
      "2370/2370 [==============================] - 0s 30us/step - loss: 0.0040 - val_loss: 0.0377\n",
      "Epoch 2652/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0041 - val_loss: 0.0385\n",
      "Epoch 2653/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0040 - val_loss: 0.0379\n",
      "Epoch 2654/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0040 - val_loss: 0.0375\n",
      "Epoch 2655/3000\n",
      "2370/2370 [==============================] - 0s 31us/step - loss: 0.0040 - val_loss: 0.0380\n",
      "Epoch 2656/3000\n",
      "2370/2370 [==============================] - 0s 30us/step - loss: 0.0042 - val_loss: 0.0376\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2657/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0040 - val_loss: 0.0376\n",
      "Epoch 2658/3000\n",
      "2370/2370 [==============================] - 0s 30us/step - loss: 0.0039 - val_loss: 0.0374\n",
      "Epoch 2659/3000\n",
      "2370/2370 [==============================] - 0s 30us/step - loss: 0.0039 - val_loss: 0.0379\n",
      "Epoch 2660/3000\n",
      "2370/2370 [==============================] - 0s 30us/step - loss: 0.0038 - val_loss: 0.0380\n",
      "Epoch 2661/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0041 - val_loss: 0.0372\n",
      "Epoch 2662/3000\n",
      "2370/2370 [==============================] - 0s 41us/step - loss: 0.0040 - val_loss: 0.0369\n",
      "Epoch 2663/3000\n",
      "2370/2370 [==============================] - 0s 71us/step - loss: 0.0038 - val_loss: 0.0372\n",
      "Epoch 2664/3000\n",
      "2370/2370 [==============================] - 0s 37us/step - loss: 0.0040 - val_loss: 0.0371\n",
      "Epoch 2665/3000\n",
      "2370/2370 [==============================] - 0s 39us/step - loss: 0.0039 - val_loss: 0.0369\n",
      "Epoch 2666/3000\n",
      "2370/2370 [==============================] - 0s 39us/step - loss: 0.0039 - val_loss: 0.0373\n",
      "Epoch 2667/3000\n",
      "2370/2370 [==============================] - 0s 33us/step - loss: 0.0037 - val_loss: 0.0372\n",
      "Epoch 2668/3000\n",
      "2370/2370 [==============================] - 0s 28us/step - loss: 0.0039 - val_loss: 0.0378\n",
      "Epoch 2669/3000\n",
      "2370/2370 [==============================] - 0s 28us/step - loss: 0.0041 - val_loss: 0.0374\n",
      "Epoch 2670/3000\n",
      "2370/2370 [==============================] - 0s 28us/step - loss: 0.0039 - val_loss: 0.0385\n",
      "Epoch 2671/3000\n",
      "2370/2370 [==============================] - 0s 28us/step - loss: 0.0040 - val_loss: 0.0378\n",
      "Epoch 2672/3000\n",
      "2370/2370 [==============================] - 0s 31us/step - loss: 0.0040 - val_loss: 0.0377\n",
      "Epoch 2673/3000\n",
      "2370/2370 [==============================] - 0s 34us/step - loss: 0.0040 - val_loss: 0.0378\n",
      "Epoch 2674/3000\n",
      "2370/2370 [==============================] - 0s 88us/step - loss: 0.0041 - val_loss: 0.0381\n",
      "Epoch 2675/3000\n",
      "2370/2370 [==============================] - 0s 62us/step - loss: 0.0038 - val_loss: 0.0378\n",
      "Epoch 2676/3000\n",
      "2370/2370 [==============================] - 0s 34us/step - loss: 0.0040 - val_loss: 0.0378\n",
      "Epoch 2677/3000\n",
      "2370/2370 [==============================] - 0s 31us/step - loss: 0.0038 - val_loss: 0.0381\n",
      "Epoch 2678/3000\n",
      "2370/2370 [==============================] - 0s 31us/step - loss: 0.0037 - val_loss: 0.0387\n",
      "Epoch 2679/3000\n",
      "2370/2370 [==============================] - 0s 28us/step - loss: 0.0040 - val_loss: 0.0381\n",
      "Epoch 2680/3000\n",
      "2370/2370 [==============================] - 0s 28us/step - loss: 0.0039 - val_loss: 0.0377\n",
      "Epoch 2681/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0039 - val_loss: 0.0383\n",
      "Epoch 2682/3000\n",
      "2370/2370 [==============================] - 0s 28us/step - loss: 0.0038 - val_loss: 0.0382\n",
      "Epoch 2683/3000\n",
      "2370/2370 [==============================] - 0s 28us/step - loss: 0.0039 - val_loss: 0.0374\n",
      "Epoch 2684/3000\n",
      "2370/2370 [==============================] - 0s 28us/step - loss: 0.0041 - val_loss: 0.0383\n",
      "Epoch 2685/3000\n",
      "2370/2370 [==============================] - 0s 45us/step - loss: 0.0041 - val_loss: 0.0377\n",
      "Epoch 2686/3000\n",
      "2370/2370 [==============================] - 0s 40us/step - loss: 0.0039 - val_loss: 0.0378\n",
      "Epoch 2687/3000\n",
      "2370/2370 [==============================] - 0s 34us/step - loss: 0.0039 - val_loss: 0.0379\n",
      "Epoch 2688/3000\n",
      "2370/2370 [==============================] - 0s 36us/step - loss: 0.0039 - val_loss: 0.0384\n",
      "Epoch 2689/3000\n",
      "2370/2370 [==============================] - 0s 102us/step - loss: 0.0039 - val_loss: 0.0385\n",
      "Epoch 2690/3000\n",
      "2370/2370 [==============================] - 0s 120us/step - loss: 0.0041 - val_loss: 0.0384\n",
      "Epoch 2691/3000\n",
      "2370/2370 [==============================] - 0s 106us/step - loss: 0.0039 - val_loss: 0.0384\n",
      "Epoch 2692/3000\n",
      "2370/2370 [==============================] - 0s 55us/step - loss: 0.0041 - val_loss: 0.0384\n",
      "Epoch 2693/3000\n",
      "2370/2370 [==============================] - 0s 64us/step - loss: 0.0039 - val_loss: 0.0384\n",
      "Epoch 2694/3000\n",
      "2370/2370 [==============================] - 0s 51us/step - loss: 0.0038 - val_loss: 0.0382\n",
      "Epoch 2695/3000\n",
      "2370/2370 [==============================] - 0s 32us/step - loss: 0.0039 - val_loss: 0.0372\n",
      "Epoch 2696/3000\n",
      "2370/2370 [==============================] - 0s 32us/step - loss: 0.0040 - val_loss: 0.0374\n",
      "Epoch 2697/3000\n",
      "2370/2370 [==============================] - 0s 31us/step - loss: 0.0039 - val_loss: 0.0375\n",
      "Epoch 2698/3000\n",
      "2370/2370 [==============================] - 0s 33us/step - loss: 0.0039 - val_loss: 0.0385\n",
      "Epoch 2699/3000\n",
      "2370/2370 [==============================] - 0s 55us/step - loss: 0.0037 - val_loss: 0.0388\n",
      "Epoch 2700/3000\n",
      "2370/2370 [==============================] - 0s 80us/step - loss: 0.0039 - val_loss: 0.0381\n",
      "Epoch 2701/3000\n",
      "2370/2370 [==============================] - 0s 56us/step - loss: 0.0037 - val_loss: 0.0381\n",
      "Epoch 2702/3000\n",
      "2370/2370 [==============================] - 0s 41us/step - loss: 0.0040 - val_loss: 0.0383\n",
      "Epoch 2703/3000\n",
      "2370/2370 [==============================] - 0s 70us/step - loss: 0.0040 - val_loss: 0.0381\n",
      "Epoch 2704/3000\n",
      "2370/2370 [==============================] - 0s 74us/step - loss: 0.0042 - val_loss: 0.0383\n",
      "Epoch 2705/3000\n",
      "2370/2370 [==============================] - 0s 28us/step - loss: 0.0038 - val_loss: 0.0384\n",
      "Epoch 2706/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0039 - val_loss: 0.0376\n",
      "Epoch 2707/3000\n",
      "2370/2370 [==============================] - 0s 40us/step - loss: 0.0042 - val_loss: 0.0377\n",
      "Epoch 2708/3000\n",
      "2370/2370 [==============================] - 0s 37us/step - loss: 0.0040 - val_loss: 0.0377\n",
      "Epoch 2709/3000\n",
      "2370/2370 [==============================] - 0s 34us/step - loss: 0.0039 - val_loss: 0.0377\n",
      "Epoch 2710/3000\n",
      "2370/2370 [==============================] - 0s 34us/step - loss: 0.0041 - val_loss: 0.0375\n",
      "Epoch 2711/3000\n",
      "2370/2370 [==============================] - 0s 35us/step - loss: 0.0040 - val_loss: 0.0372\n",
      "Epoch 2712/3000\n",
      "2370/2370 [==============================] - 0s 41us/step - loss: 0.0038 - val_loss: 0.0374\n",
      "Epoch 2713/3000\n",
      "2370/2370 [==============================] - 0s 33us/step - loss: 0.0040 - val_loss: 0.0375\n",
      "Epoch 2714/3000\n",
      "2370/2370 [==============================] - 0s 32us/step - loss: 0.0038 - val_loss: 0.0377\n",
      "Epoch 2715/3000\n",
      "2370/2370 [==============================] - 0s 31us/step - loss: 0.0039 - val_loss: 0.0385\n",
      "Epoch 2716/3000\n",
      "2370/2370 [==============================] - 0s 28us/step - loss: 0.0039 - val_loss: 0.0381\n",
      "Epoch 2717/3000\n",
      "2370/2370 [==============================] - 0s 31us/step - loss: 0.0042 - val_loss: 0.0376\n",
      "Epoch 2718/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0039 - val_loss: 0.0388\n",
      "Epoch 2719/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0040 - val_loss: 0.0382\n",
      "Epoch 2720/3000\n",
      "2370/2370 [==============================] - 0s 35us/step - loss: 0.0038 - val_loss: 0.0378\n",
      "Epoch 2721/3000\n",
      "2370/2370 [==============================] - 0s 40us/step - loss: 0.0042 - val_loss: 0.0386\n",
      "Epoch 2722/3000\n",
      "2370/2370 [==============================] - 0s 44us/step - loss: 0.0040 - val_loss: 0.0380\n",
      "Epoch 2723/3000\n",
      "2370/2370 [==============================] - 0s 35us/step - loss: 0.0038 - val_loss: 0.0379\n",
      "Epoch 2724/3000\n",
      "2370/2370 [==============================] - 0s 39us/step - loss: 0.0041 - val_loss: 0.0377\n",
      "Epoch 2725/3000\n",
      "2370/2370 [==============================] - 0s 35us/step - loss: 0.0038 - val_loss: 0.0372\n",
      "Epoch 2726/3000\n",
      "2370/2370 [==============================] - 0s 46us/step - loss: 0.0039 - val_loss: 0.0378\n",
      "Epoch 2727/3000\n",
      "2370/2370 [==============================] - 0s 98us/step - loss: 0.0039 - val_loss: 0.0379\n",
      "Epoch 2728/3000\n",
      "2370/2370 [==============================] - 0s 50us/step - loss: 0.0042 - val_loss: 0.0381\n",
      "Epoch 2729/3000\n",
      "2370/2370 [==============================] - 0s 60us/step - loss: 0.0040 - val_loss: 0.0378\n",
      "Epoch 2730/3000\n",
      "2370/2370 [==============================] - 0s 77us/step - loss: 0.0042 - val_loss: 0.0388\n",
      "Epoch 2731/3000\n",
      "2370/2370 [==============================] - 0s 31us/step - loss: 0.0040 - val_loss: 0.0379\n",
      "Epoch 2732/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0040 - val_loss: 0.0383\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2733/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0042 - val_loss: 0.0373\n",
      "Epoch 2734/3000\n",
      "2370/2370 [==============================] - 0s 46us/step - loss: 0.0040 - val_loss: 0.0374\n",
      "Epoch 2735/3000\n",
      "2370/2370 [==============================] - 0s 75us/step - loss: 0.0041 - val_loss: 0.0385\n",
      "Epoch 2736/3000\n",
      "2370/2370 [==============================] - 0s 34us/step - loss: 0.0038 - val_loss: 0.0381\n",
      "Epoch 2737/3000\n",
      "2370/2370 [==============================] - 0s 33us/step - loss: 0.0039 - val_loss: 0.0382\n",
      "Epoch 2738/3000\n",
      "2370/2370 [==============================] - 0s 32us/step - loss: 0.0041 - val_loss: 0.0381\n",
      "Epoch 2739/3000\n",
      "2370/2370 [==============================] - 0s 38us/step - loss: 0.0039 - val_loss: 0.0377\n",
      "Epoch 2740/3000\n",
      "2370/2370 [==============================] - 0s 33us/step - loss: 0.0040 - val_loss: 0.0384\n",
      "Epoch 2741/3000\n",
      "2370/2370 [==============================] - 0s 32us/step - loss: 0.0041 - val_loss: 0.0377\n",
      "Epoch 2742/3000\n",
      "2370/2370 [==============================] - 0s 31us/step - loss: 0.0039 - val_loss: 0.0377\n",
      "Epoch 2743/3000\n",
      "2370/2370 [==============================] - 0s 30us/step - loss: 0.0038 - val_loss: 0.0384\n",
      "Epoch 2744/3000\n",
      "2370/2370 [==============================] - 0s 36us/step - loss: 0.0040 - val_loss: 0.0381\n",
      "Epoch 2745/3000\n",
      "2370/2370 [==============================] - 0s 33us/step - loss: 0.0037 - val_loss: 0.0375\n",
      "Epoch 2746/3000\n",
      "2370/2370 [==============================] - 0s 86us/step - loss: 0.0037 - val_loss: 0.0383\n",
      "Epoch 2747/3000\n",
      "2370/2370 [==============================] - 0s 67us/step - loss: 0.0039 - val_loss: 0.0385\n",
      "Epoch 2748/3000\n",
      "2370/2370 [==============================] - 0s 44us/step - loss: 0.0039 - val_loss: 0.0377\n",
      "Epoch 2749/3000\n",
      "2370/2370 [==============================] - ETA: 0s - loss: 0.004 - 0s 85us/step - loss: 0.0041 - val_loss: 0.0380\n",
      "Epoch 2750/3000\n",
      "2370/2370 [==============================] - 0s 86us/step - loss: 0.0039 - val_loss: 0.0378\n",
      "Epoch 2751/3000\n",
      "2370/2370 [==============================] - 0s 54us/step - loss: 0.0039 - val_loss: 0.0376\n",
      "Epoch 2752/3000\n",
      "2370/2370 [==============================] - 0s 76us/step - loss: 0.0038 - val_loss: 0.0379\n",
      "Epoch 2753/3000\n",
      "2370/2370 [==============================] - 0s 72us/step - loss: 0.0038 - val_loss: 0.0376\n",
      "Epoch 2754/3000\n",
      "2370/2370 [==============================] - 0s 53us/step - loss: 0.0038 - val_loss: 0.0377\n",
      "Epoch 2755/3000\n",
      "2370/2370 [==============================] - 0s 82us/step - loss: 0.0039 - val_loss: 0.0376\n",
      "Epoch 2756/3000\n",
      "2370/2370 [==============================] - 0s 43us/step - loss: 0.0037 - val_loss: 0.0381\n",
      "Epoch 2757/3000\n",
      "2370/2370 [==============================] - 0s 37us/step - loss: 0.0037 - val_loss: 0.0378\n",
      "Epoch 2758/3000\n",
      "2370/2370 [==============================] - 0s 39us/step - loss: 0.0041 - val_loss: 0.0389\n",
      "Epoch 2759/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0041 - val_loss: 0.0384\n",
      "Epoch 2760/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0037 - val_loss: 0.0382\n",
      "Epoch 2761/3000\n",
      "2370/2370 [==============================] - 0s 30us/step - loss: 0.0040 - val_loss: 0.0386\n",
      "Epoch 2762/3000\n",
      "2370/2370 [==============================] - 0s 78us/step - loss: 0.0039 - val_loss: 0.0382\n",
      "Epoch 2763/3000\n",
      "2370/2370 [==============================] - 0s 78us/step - loss: 0.0040 - val_loss: 0.0373\n",
      "Epoch 2764/3000\n",
      "2370/2370 [==============================] - 0s 58us/step - loss: 0.0043 - val_loss: 0.0377\n",
      "Epoch 2765/3000\n",
      "2370/2370 [==============================] - 0s 35us/step - loss: 0.0037 - val_loss: 0.0375\n",
      "Epoch 2766/3000\n",
      "2370/2370 [==============================] - 0s 31us/step - loss: 0.0040 - val_loss: 0.0381\n",
      "Epoch 2767/3000\n",
      "2370/2370 [==============================] - 0s 34us/step - loss: 0.0040 - val_loss: 0.0375\n",
      "Epoch 2768/3000\n",
      "2370/2370 [==============================] - 0s 32us/step - loss: 0.0037 - val_loss: 0.0379\n",
      "Epoch 2769/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0039 - val_loss: 0.0385\n",
      "Epoch 2770/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0040 - val_loss: 0.0383\n",
      "Epoch 2771/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0039 - val_loss: 0.0386\n",
      "Epoch 2772/3000\n",
      "2370/2370 [==============================] - 0s 34us/step - loss: 0.0041 - val_loss: 0.0382\n",
      "Epoch 2773/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0039 - val_loss: 0.0378\n",
      "Epoch 2774/3000\n",
      "2370/2370 [==============================] - 0s 31us/step - loss: 0.0038 - val_loss: 0.0374\n",
      "Epoch 2775/3000\n",
      "2370/2370 [==============================] - 0s 68us/step - loss: 0.0036 - val_loss: 0.0375\n",
      "Epoch 2776/3000\n",
      "2370/2370 [==============================] - 0s 46us/step - loss: 0.0043 - val_loss: 0.0372\n",
      "Epoch 2777/3000\n",
      "2370/2370 [==============================] - 0s 35us/step - loss: 0.0039 - val_loss: 0.0377\n",
      "Epoch 2778/3000\n",
      "2370/2370 [==============================] - 0s 36us/step - loss: 0.0039 - val_loss: 0.0375\n",
      "Epoch 2779/3000\n",
      "2370/2370 [==============================] - 0s 35us/step - loss: 0.0037 - val_loss: 0.0376\n",
      "Epoch 2780/3000\n",
      "2370/2370 [==============================] - 0s 58us/step - loss: 0.0039 - val_loss: 0.0380\n",
      "Epoch 2781/3000\n",
      "2370/2370 [==============================] - 0s 103us/step - loss: 0.0041 - val_loss: 0.0382\n",
      "Epoch 2782/3000\n",
      "2370/2370 [==============================] - 0s 105us/step - loss: 0.0039 - val_loss: 0.0376\n",
      "Epoch 2783/3000\n",
      "2370/2370 [==============================] - 0s 80us/step - loss: 0.0041 - val_loss: 0.0376\n",
      "Epoch 2784/3000\n",
      "2370/2370 [==============================] - 0s 42us/step - loss: 0.0040 - val_loss: 0.0382\n",
      "Epoch 2785/3000\n",
      "2370/2370 [==============================] - 0s 32us/step - loss: 0.0037 - val_loss: 0.0373\n",
      "Epoch 2786/3000\n",
      "2370/2370 [==============================] - 0s 70us/step - loss: 0.0039 - val_loss: 0.0372\n",
      "Epoch 2787/3000\n",
      "2370/2370 [==============================] - 0s 79us/step - loss: 0.0039 - val_loss: 0.0378\n",
      "Epoch 2788/3000\n",
      "2370/2370 [==============================] - 0s 31us/step - loss: 0.0040 - val_loss: 0.0368\n",
      "Epoch 2789/3000\n",
      "2370/2370 [==============================] - 0s 32us/step - loss: 0.0036 - val_loss: 0.0378\n",
      "Epoch 2790/3000\n",
      "2370/2370 [==============================] - 0s 55us/step - loss: 0.0037 - val_loss: 0.0376\n",
      "Epoch 2791/3000\n",
      "2370/2370 [==============================] - 0s 74us/step - loss: 0.0037 - val_loss: 0.0372\n",
      "Epoch 2792/3000\n",
      "2370/2370 [==============================] - 0s 31us/step - loss: 0.0040 - val_loss: 0.0378\n",
      "Epoch 2793/3000\n",
      "2370/2370 [==============================] - 0s 32us/step - loss: 0.0040 - val_loss: 0.0372\n",
      "Epoch 2794/3000\n",
      "2370/2370 [==============================] - 0s 30us/step - loss: 0.0040 - val_loss: 0.0377\n",
      "Epoch 2795/3000\n",
      "2370/2370 [==============================] - 0s 64us/step - loss: 0.0039 - val_loss: 0.0376\n",
      "Epoch 2796/3000\n",
      "2370/2370 [==============================] - 0s 75us/step - loss: 0.0041 - val_loss: 0.0380\n",
      "Epoch 2797/3000\n",
      "2370/2370 [==============================] - 0s 34us/step - loss: 0.0037 - val_loss: 0.0380\n",
      "Epoch 2798/3000\n",
      "2370/2370 [==============================] - 0s 33us/step - loss: 0.0040 - val_loss: 0.0378\n",
      "Epoch 2799/3000\n",
      "2370/2370 [==============================] - 0s 79us/step - loss: 0.0037 - val_loss: 0.0374\n",
      "Epoch 2800/3000\n",
      "2370/2370 [==============================] - 0s 74us/step - loss: 0.0037 - val_loss: 0.0368\n",
      "Epoch 2801/3000\n",
      "2370/2370 [==============================] - 0s 73us/step - loss: 0.0038 - val_loss: 0.0378\n",
      "Epoch 2802/3000\n",
      "2370/2370 [==============================] - 0s 70us/step - loss: 0.0038 - val_loss: 0.0375\n",
      "Epoch 2803/3000\n",
      "2370/2370 [==============================] - 0s 32us/step - loss: 0.0039 - val_loss: 0.0377\n",
      "Epoch 2804/3000\n",
      "2370/2370 [==============================] - 0s 32us/step - loss: 0.0039 - val_loss: 0.0373\n",
      "Epoch 2805/3000\n",
      "2370/2370 [==============================] - 0s 44us/step - loss: 0.0041 - val_loss: 0.0377\n",
      "Epoch 2806/3000\n",
      "2370/2370 [==============================] - 0s 94us/step - loss: 0.0037 - val_loss: 0.0381\n",
      "Epoch 2807/3000\n",
      "2370/2370 [==============================] - 0s 51us/step - loss: 0.0039 - val_loss: 0.0377\n",
      "Epoch 2808/3000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2370/2370 [==============================] - 0s 33us/step - loss: 0.0042 - val_loss: 0.0377\n",
      "Epoch 2809/3000\n",
      "2370/2370 [==============================] - 0s 39us/step - loss: 0.0039 - val_loss: 0.0377\n",
      "Epoch 2810/3000\n",
      "2370/2370 [==============================] - 0s 37us/step - loss: 0.0037 - val_loss: 0.0381\n",
      "Epoch 2811/3000\n",
      "2370/2370 [==============================] - 0s 44us/step - loss: 0.0039 - val_loss: 0.0379\n",
      "Epoch 2812/3000\n",
      "2370/2370 [==============================] - 0s 43us/step - loss: 0.0038 - val_loss: 0.0380\n",
      "Epoch 2813/3000\n",
      "2370/2370 [==============================] - 0s 44us/step - loss: 0.0037 - val_loss: 0.0387\n",
      "Epoch 2814/3000\n",
      "2370/2370 [==============================] - 0s 98us/step - loss: 0.0043 - val_loss: 0.0371\n",
      "Epoch 2815/3000\n",
      "2370/2370 [==============================] - 0s 103us/step - loss: 0.0040 - val_loss: 0.0385\n",
      "Epoch 2816/3000\n",
      "2370/2370 [==============================] - 0s 43us/step - loss: 0.0041 - val_loss: 0.0380\n",
      "Epoch 2817/3000\n",
      "2370/2370 [==============================] - 0s 30us/step - loss: 0.0038 - val_loss: 0.0382\n",
      "Epoch 2818/3000\n",
      "2370/2370 [==============================] - 0s 35us/step - loss: 0.0038 - val_loss: 0.0383\n",
      "Epoch 2819/3000\n",
      "2370/2370 [==============================] - 0s 32us/step - loss: 0.0037 - val_loss: 0.0379\n",
      "Epoch 2820/3000\n",
      "2370/2370 [==============================] - 0s 30us/step - loss: 0.0039 - val_loss: 0.0376\n",
      "Epoch 2821/3000\n",
      "2370/2370 [==============================] - 0s 35us/step - loss: 0.0037 - val_loss: 0.0377\n",
      "Epoch 2822/3000\n",
      "2370/2370 [==============================] - 0s 34us/step - loss: 0.0038 - val_loss: 0.0380\n",
      "Epoch 2823/3000\n",
      "2370/2370 [==============================] - 0s 30us/step - loss: 0.0037 - val_loss: 0.0378\n",
      "Epoch 2824/3000\n",
      "2370/2370 [==============================] - 0s 30us/step - loss: 0.0039 - val_loss: 0.0384\n",
      "Epoch 2825/3000\n",
      "2370/2370 [==============================] - 0s 35us/step - loss: 0.0038 - val_loss: 0.0372\n",
      "Epoch 2826/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0041 - val_loss: 0.0374\n",
      "Epoch 2827/3000\n",
      "2370/2370 [==============================] - 0s 27us/step - loss: 0.0041 - val_loss: 0.0377\n",
      "Epoch 2828/3000\n",
      "2370/2370 [==============================] - 0s 28us/step - loss: 0.0040 - val_loss: 0.0375\n",
      "Epoch 2829/3000\n",
      "2370/2370 [==============================] - 0s 37us/step - loss: 0.0039 - val_loss: 0.0381\n",
      "Epoch 2830/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0039 - val_loss: 0.0381\n",
      "Epoch 2831/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0040 - val_loss: 0.0381\n",
      "Epoch 2832/3000\n",
      "2370/2370 [==============================] - 0s 32us/step - loss: 0.0036 - val_loss: 0.0376\n",
      "Epoch 2833/3000\n",
      "2370/2370 [==============================] - 0s 31us/step - loss: 0.0039 - val_loss: 0.0378\n",
      "Epoch 2834/3000\n",
      "2370/2370 [==============================] - 0s 28us/step - loss: 0.0036 - val_loss: 0.0373\n",
      "Epoch 2835/3000\n",
      "2370/2370 [==============================] - 0s 34us/step - loss: 0.0039 - val_loss: 0.0371\n",
      "Epoch 2836/3000\n",
      "2370/2370 [==============================] - 0s 104us/step - loss: 0.0038 - val_loss: 0.0369\n",
      "Epoch 2837/3000\n",
      "2370/2370 [==============================] - 0s 64us/step - loss: 0.0040 - val_loss: 0.0373\n",
      "Epoch 2838/3000\n",
      "2370/2370 [==============================] - 0s 51us/step - loss: 0.0039 - val_loss: 0.0381\n",
      "Epoch 2839/3000\n",
      "2370/2370 [==============================] - 0s 31us/step - loss: 0.0038 - val_loss: 0.0376\n",
      "Epoch 2840/3000\n",
      "2370/2370 [==============================] - 0s 52us/step - loss: 0.0039 - val_loss: 0.0379\n",
      "Epoch 2841/3000\n",
      "2370/2370 [==============================] - 0s 32us/step - loss: 0.0038 - val_loss: 0.0372\n",
      "Epoch 2842/3000\n",
      "2370/2370 [==============================] - 0s 57us/step - loss: 0.0039 - val_loss: 0.0374\n",
      "Epoch 2843/3000\n",
      "2370/2370 [==============================] - 0s 109us/step - loss: 0.0039 - val_loss: 0.0379\n",
      "Epoch 2844/3000\n",
      "2370/2370 [==============================] - 0s 90us/step - loss: 0.0040 - val_loss: 0.0369\n",
      "Epoch 2845/3000\n",
      "2370/2370 [==============================] - 0s 86us/step - loss: 0.0039 - val_loss: 0.0376\n",
      "Epoch 2846/3000\n",
      "2370/2370 [==============================] - 0s 67us/step - loss: 0.0038 - val_loss: 0.0370\n",
      "Epoch 2847/3000\n",
      "2370/2370 [==============================] - 0s 32us/step - loss: 0.0039 - val_loss: 0.0379\n",
      "Epoch 2848/3000\n",
      "2370/2370 [==============================] - 0s 33us/step - loss: 0.0039 - val_loss: 0.0375\n",
      "Epoch 2849/3000\n",
      "2370/2370 [==============================] - 0s 28us/step - loss: 0.0040 - val_loss: 0.0372\n",
      "Epoch 2850/3000\n",
      "2370/2370 [==============================] - 0s 33us/step - loss: 0.0038 - val_loss: 0.0378\n",
      "Epoch 2851/3000\n",
      "2370/2370 [==============================] - 0s 31us/step - loss: 0.0040 - val_loss: 0.0376\n",
      "Epoch 2852/3000\n",
      "2370/2370 [==============================] - 0s 35us/step - loss: 0.0037 - val_loss: 0.0378\n",
      "Epoch 2853/3000\n",
      "2370/2370 [==============================] - 0s 33us/step - loss: 0.0039 - val_loss: 0.0377\n",
      "Epoch 2854/3000\n",
      "2370/2370 [==============================] - 0s 31us/step - loss: 0.0036 - val_loss: 0.0380\n",
      "Epoch 2855/3000\n",
      "2370/2370 [==============================] - 0s 28us/step - loss: 0.0038 - val_loss: 0.0370\n",
      "Epoch 2856/3000\n",
      "2370/2370 [==============================] - 0s 36us/step - loss: 0.0041 - val_loss: 0.0373\n",
      "Epoch 2857/3000\n",
      "2370/2370 [==============================] - 0s 31us/step - loss: 0.0038 - val_loss: 0.0388\n",
      "Epoch 2858/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0038 - val_loss: 0.0376\n",
      "Epoch 2859/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0037 - val_loss: 0.0377\n",
      "Epoch 2860/3000\n",
      "2370/2370 [==============================] - 0s 30us/step - loss: 0.0039 - val_loss: 0.0375\n",
      "Epoch 2861/3000\n",
      "2370/2370 [==============================] - 0s 33us/step - loss: 0.0039 - val_loss: 0.0372\n",
      "Epoch 2862/3000\n",
      "2370/2370 [==============================] - 0s 28us/step - loss: 0.0038 - val_loss: 0.0385\n",
      "Epoch 2863/3000\n",
      "2370/2370 [==============================] - 0s 32us/step - loss: 0.0038 - val_loss: 0.0383\n",
      "Epoch 2864/3000\n",
      "2370/2370 [==============================] - 0s 33us/step - loss: 0.0040 - val_loss: 0.0386\n",
      "Epoch 2865/3000\n",
      "2370/2370 [==============================] - 0s 28us/step - loss: 0.0038 - val_loss: 0.0378\n",
      "Epoch 2866/3000\n",
      "2370/2370 [==============================] - 0s 27us/step - loss: 0.0039 - val_loss: 0.0376\n",
      "Epoch 2867/3000\n",
      "2370/2370 [==============================] - 0s 27us/step - loss: 0.0036 - val_loss: 0.0378\n",
      "Epoch 2868/3000\n",
      "2370/2370 [==============================] - 0s 33us/step - loss: 0.0037 - val_loss: 0.0374\n",
      "Epoch 2869/3000\n",
      "2370/2370 [==============================] - 0s 27us/step - loss: 0.0034 - val_loss: 0.0383\n",
      "Epoch 2870/3000\n",
      "2370/2370 [==============================] - 0s 32us/step - loss: 0.0039 - val_loss: 0.0379\n",
      "Epoch 2871/3000\n",
      "2370/2370 [==============================] - 0s 31us/step - loss: 0.0039 - val_loss: 0.0379\n",
      "Epoch 2872/3000\n",
      "2370/2370 [==============================] - 0s 33us/step - loss: 0.0037 - val_loss: 0.0383\n",
      "Epoch 2873/3000\n",
      "2370/2370 [==============================] - 0s 31us/step - loss: 0.0040 - val_loss: 0.0377\n",
      "Epoch 2874/3000\n",
      "2370/2370 [==============================] - 0s 37us/step - loss: 0.0039 - val_loss: 0.0384\n",
      "Epoch 2875/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0038 - val_loss: 0.0378\n",
      "Epoch 2876/3000\n",
      "2370/2370 [==============================] - 0s 31us/step - loss: 0.0042 - val_loss: 0.0382\n",
      "Epoch 2877/3000\n",
      "2370/2370 [==============================] - 0s 30us/step - loss: 0.0039 - val_loss: 0.0382\n",
      "Epoch 2878/3000\n",
      "2370/2370 [==============================] - 0s 28us/step - loss: 0.0038 - val_loss: 0.0376\n",
      "Epoch 2879/3000\n",
      "2370/2370 [==============================] - 0s 28us/step - loss: 0.0039 - val_loss: 0.0377\n",
      "Epoch 2880/3000\n",
      "2370/2370 [==============================] - 0s 32us/step - loss: 0.0039 - val_loss: 0.0382\n",
      "Epoch 2881/3000\n",
      "2370/2370 [==============================] - 0s 35us/step - loss: 0.0042 - val_loss: 0.0379\n",
      "Epoch 2882/3000\n",
      "2370/2370 [==============================] - 0s 34us/step - loss: 0.0040 - val_loss: 0.0372\n",
      "Epoch 2883/3000\n",
      "2370/2370 [==============================] - 0s 32us/step - loss: 0.0040 - val_loss: 0.0376\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2884/3000\n",
      "2370/2370 [==============================] - 0s 28us/step - loss: 0.0039 - val_loss: 0.0379\n",
      "Epoch 2885/3000\n",
      "2370/2370 [==============================] - 0s 28us/step - loss: 0.0039 - val_loss: 0.0382\n",
      "Epoch 2886/3000\n",
      "2370/2370 [==============================] - 0s 27us/step - loss: 0.0038 - val_loss: 0.0387\n",
      "Epoch 2887/3000\n",
      "2370/2370 [==============================] - 0s 28us/step - loss: 0.0041 - val_loss: 0.0381\n",
      "Epoch 2888/3000\n",
      "2370/2370 [==============================] - 0s 30us/step - loss: 0.0040 - val_loss: 0.0387\n",
      "Epoch 2889/3000\n",
      "2370/2370 [==============================] - 0s 35us/step - loss: 0.0040 - val_loss: 0.0381\n",
      "Epoch 2890/3000\n",
      "2370/2370 [==============================] - 0s 34us/step - loss: 0.0038 - val_loss: 0.0384\n",
      "Epoch 2891/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0039 - val_loss: 0.0380\n",
      "Epoch 2892/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0038 - val_loss: 0.0384\n",
      "Epoch 2893/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0039 - val_loss: 0.0385\n",
      "Epoch 2894/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0036 - val_loss: 0.0378\n",
      "Epoch 2895/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0037 - val_loss: 0.0370\n",
      "Epoch 2896/3000\n",
      "2370/2370 [==============================] - 0s 35us/step - loss: 0.0038 - val_loss: 0.0380\n",
      "Epoch 2897/3000\n",
      "2370/2370 [==============================] - 0s 67us/step - loss: 0.0038 - val_loss: 0.0375\n",
      "Epoch 2898/3000\n",
      "2370/2370 [==============================] - 0s 66us/step - loss: 0.0040 - val_loss: 0.0376\n",
      "Epoch 2899/3000\n",
      "2370/2370 [==============================] - 0s 63us/step - loss: 0.0038 - val_loss: 0.0375\n",
      "Epoch 2900/3000\n",
      "2370/2370 [==============================] - 0s 37us/step - loss: 0.0040 - val_loss: 0.0370\n",
      "Epoch 2901/3000\n",
      "2370/2370 [==============================] - 0s 33us/step - loss: 0.0040 - val_loss: 0.0375\n",
      "Epoch 2902/3000\n",
      "2370/2370 [==============================] - 0s 31us/step - loss: 0.0039 - val_loss: 0.0375\n",
      "Epoch 2903/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0040 - val_loss: 0.0376\n",
      "Epoch 2904/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0037 - val_loss: 0.0374\n",
      "Epoch 2905/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0040 - val_loss: 0.0369\n",
      "Epoch 2906/3000\n",
      "2370/2370 [==============================] - 0s 30us/step - loss: 0.0038 - val_loss: 0.0374\n",
      "Epoch 2907/3000\n",
      "2370/2370 [==============================] - 0s 39us/step - loss: 0.0045 - val_loss: 0.0377\n",
      "Epoch 2908/3000\n",
      "2370/2370 [==============================] - 0s 89us/step - loss: 0.0039 - val_loss: 0.0383\n",
      "Epoch 2909/3000\n",
      "2370/2370 [==============================] - 0s 84us/step - loss: 0.0040 - val_loss: 0.0386\n",
      "Epoch 2910/3000\n",
      "2370/2370 [==============================] - 0s 96us/step - loss: 0.0037 - val_loss: 0.0381\n",
      "Epoch 2911/3000\n",
      "2370/2370 [==============================] - 0s 88us/step - loss: 0.0040 - val_loss: 0.0378\n",
      "Epoch 2912/3000\n",
      "2370/2370 [==============================] - 0s 98us/step - loss: 0.0036 - val_loss: 0.0376\n",
      "Epoch 2913/3000\n",
      "2370/2370 [==============================] - 0s 110us/step - loss: 0.0037 - val_loss: 0.0381\n",
      "Epoch 2914/3000\n",
      "2370/2370 [==============================] - 0s 103us/step - loss: 0.0040 - val_loss: 0.0381\n",
      "Epoch 2915/3000\n",
      "2370/2370 [==============================] - 0s 83us/step - loss: 0.0037 - val_loss: 0.0381\n",
      "Epoch 2916/3000\n",
      "2370/2370 [==============================] - 0s 66us/step - loss: 0.0039 - val_loss: 0.0382\n",
      "Epoch 2917/3000\n",
      "2370/2370 [==============================] - 0s 88us/step - loss: 0.0036 - val_loss: 0.0384\n",
      "Epoch 2918/3000\n",
      "2370/2370 [==============================] - 0s 73us/step - loss: 0.0037 - val_loss: 0.0375\n",
      "Epoch 2919/3000\n",
      "2370/2370 [==============================] - 0s 35us/step - loss: 0.0040 - val_loss: 0.0379\n",
      "Epoch 2920/3000\n",
      "2370/2370 [==============================] - 0s 34us/step - loss: 0.0039 - val_loss: 0.0373\n",
      "Epoch 2921/3000\n",
      "2370/2370 [==============================] - 0s 34us/step - loss: 0.0042 - val_loss: 0.0371\n",
      "Epoch 2922/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0037 - val_loss: 0.0375\n",
      "Epoch 2923/3000\n",
      "2370/2370 [==============================] - 0s 31us/step - loss: 0.0040 - val_loss: 0.0381\n",
      "Epoch 2924/3000\n",
      "2370/2370 [==============================] - 0s 30us/step - loss: 0.0039 - val_loss: 0.0377\n",
      "Epoch 2925/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0039 - val_loss: 0.0382\n",
      "Epoch 2926/3000\n",
      "2370/2370 [==============================] - 0s 30us/step - loss: 0.0036 - val_loss: 0.0374\n",
      "Epoch 2927/3000\n",
      "2370/2370 [==============================] - 0s 32us/step - loss: 0.0039 - val_loss: 0.0371\n",
      "Epoch 2928/3000\n",
      "2370/2370 [==============================] - 0s 34us/step - loss: 0.0039 - val_loss: 0.0373\n",
      "Epoch 2929/3000\n",
      "2370/2370 [==============================] - 0s 27us/step - loss: 0.0036 - val_loss: 0.0370\n",
      "Epoch 2930/3000\n",
      "2370/2370 [==============================] - 0s 28us/step - loss: 0.0040 - val_loss: 0.0371\n",
      "Epoch 2931/3000\n",
      "2370/2370 [==============================] - 0s 27us/step - loss: 0.0039 - val_loss: 0.0378\n",
      "Epoch 2932/3000\n",
      "2370/2370 [==============================] - 0s 28us/step - loss: 0.0039 - val_loss: 0.0375\n",
      "Epoch 2933/3000\n",
      "2370/2370 [==============================] - 0s 28us/step - loss: 0.0039 - val_loss: 0.0378\n",
      "Epoch 2934/3000\n",
      "2370/2370 [==============================] - 0s 31us/step - loss: 0.0038 - val_loss: 0.0371\n",
      "Epoch 2935/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0038 - val_loss: 0.0364\n",
      "Epoch 2936/3000\n",
      "2370/2370 [==============================] - 0s 35us/step - loss: 0.0038 - val_loss: 0.0367\n",
      "Epoch 2937/3000\n",
      "2370/2370 [==============================] - 0s 34us/step - loss: 0.0039 - val_loss: 0.0378\n",
      "Epoch 2938/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0036 - val_loss: 0.0373\n",
      "Epoch 2939/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0037 - val_loss: 0.0383\n",
      "Epoch 2940/3000\n",
      "2370/2370 [==============================] - 0s 30us/step - loss: 0.0038 - val_loss: 0.0382\n",
      "Epoch 2941/3000\n",
      "2370/2370 [==============================] - 0s 31us/step - loss: 0.0039 - val_loss: 0.0374\n",
      "Epoch 2942/3000\n",
      "2370/2370 [==============================] - 0s 28us/step - loss: 0.0042 - val_loss: 0.0375\n",
      "Epoch 2943/3000\n",
      "2370/2370 [==============================] - 0s 28us/step - loss: 0.0041 - val_loss: 0.0370\n",
      "Epoch 2944/3000\n",
      "2370/2370 [==============================] - 0s 33us/step - loss: 0.0037 - val_loss: 0.0378\n",
      "Epoch 2945/3000\n",
      "2370/2370 [==============================] - 0s 33us/step - loss: 0.0038 - val_loss: 0.0382\n",
      "Epoch 2946/3000\n",
      "2370/2370 [==============================] - 0s 28us/step - loss: 0.0040 - val_loss: 0.0381\n",
      "Epoch 2947/3000\n",
      "2370/2370 [==============================] - 0s 42us/step - loss: 0.0039 - val_loss: 0.0381\n",
      "Epoch 2948/3000\n",
      "2370/2370 [==============================] - 0s 84us/step - loss: 0.0039 - val_loss: 0.0376\n",
      "Epoch 2949/3000\n",
      "2370/2370 [==============================] - 0s 77us/step - loss: 0.0037 - val_loss: 0.0372\n",
      "Epoch 2950/3000\n",
      "2370/2370 [==============================] - 0s 34us/step - loss: 0.0036 - val_loss: 0.0381\n",
      "Epoch 2951/3000\n",
      "2370/2370 [==============================] - 0s 36us/step - loss: 0.0038 - val_loss: 0.0373\n",
      "Epoch 2952/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0041 - val_loss: 0.0373\n",
      "Epoch 2953/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0039 - val_loss: 0.0374\n",
      "Epoch 2954/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0035 - val_loss: 0.0377\n",
      "Epoch 2955/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0038 - val_loss: 0.0390\n",
      "Epoch 2956/3000\n",
      "2370/2370 [==============================] - 0s 32us/step - loss: 0.0038 - val_loss: 0.0377\n",
      "Epoch 2957/3000\n",
      "2370/2370 [==============================] - 0s 43us/step - loss: 0.0038 - val_loss: 0.0379\n",
      "Epoch 2958/3000\n",
      "2370/2370 [==============================] - 0s 77us/step - loss: 0.0041 - val_loss: 0.0374\n",
      "Epoch 2959/3000\n",
      "2370/2370 [==============================] - 0s 39us/step - loss: 0.0042 - val_loss: 0.0376\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2960/3000\n",
      "2370/2370 [==============================] - 0s 31us/step - loss: 0.0039 - val_loss: 0.0375\n",
      "Epoch 2961/3000\n",
      "2370/2370 [==============================] - 0s 31us/step - loss: 0.0037 - val_loss: 0.0380\n",
      "Epoch 2962/3000\n",
      "2370/2370 [==============================] - 0s 35us/step - loss: 0.0036 - val_loss: 0.0368\n",
      "Epoch 2963/3000\n",
      "2370/2370 [==============================] - 0s 36us/step - loss: 0.0039 - val_loss: 0.0370\n",
      "Epoch 2964/3000\n",
      "2370/2370 [==============================] - 0s 37us/step - loss: 0.0038 - val_loss: 0.0379\n",
      "Epoch 2965/3000\n",
      "2370/2370 [==============================] - 0s 30us/step - loss: 0.0039 - val_loss: 0.0373\n",
      "Epoch 2966/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0038 - val_loss: 0.0383\n",
      "Epoch 2967/3000\n",
      "2370/2370 [==============================] - 0s 29us/step - loss: 0.0039 - val_loss: 0.0382\n",
      "Epoch 2968/3000\n",
      "2370/2370 [==============================] - 0s 31us/step - loss: 0.0037 - val_loss: 0.0383\n",
      "Epoch 2969/3000\n",
      "2370/2370 [==============================] - 0s 30us/step - loss: 0.0041 - val_loss: 0.0371\n",
      "Epoch 2970/3000\n",
      "2370/2370 [==============================] - 0s 35us/step - loss: 0.0039 - val_loss: 0.0384\n",
      "Epoch 2971/3000\n",
      "2370/2370 [==============================] - 0s 41us/step - loss: 0.0038 - val_loss: 0.0382\n",
      "Epoch 2972/3000\n",
      "2370/2370 [==============================] - 0s 61us/step - loss: 0.0037 - val_loss: 0.0373\n",
      "Epoch 2973/3000\n",
      "2370/2370 [==============================] - 0s 55us/step - loss: 0.0040 - val_loss: 0.0376\n",
      "Epoch 2974/3000\n",
      "2370/2370 [==============================] - 0s 34us/step - loss: 0.0037 - val_loss: 0.0375\n",
      "Epoch 2975/3000\n",
      "2370/2370 [==============================] - 0s 34us/step - loss: 0.0037 - val_loss: 0.0379\n",
      "Epoch 2976/3000\n",
      "2370/2370 [==============================] - 0s 34us/step - loss: 0.0038 - val_loss: 0.0375\n",
      "Epoch 2977/3000\n",
      "2370/2370 [==============================] - 0s 34us/step - loss: 0.0041 - val_loss: 0.0374\n",
      "Epoch 2978/3000\n",
      "2370/2370 [==============================] - 0s 35us/step - loss: 0.0039 - val_loss: 0.0383\n",
      "Epoch 2979/3000\n",
      "2370/2370 [==============================] - 0s 45us/step - loss: 0.0042 - val_loss: 0.0369\n",
      "Epoch 2980/3000\n",
      "2370/2370 [==============================] - 0s 100us/step - loss: 0.0040 - val_loss: 0.0366\n",
      "Epoch 2981/3000\n",
      "2370/2370 [==============================] - 0s 96us/step - loss: 0.0039 - val_loss: 0.0370\n",
      "Epoch 2982/3000\n",
      "2370/2370 [==============================] - 0s 62us/step - loss: 0.0039 - val_loss: 0.0373\n",
      "Epoch 2983/3000\n",
      "2370/2370 [==============================] - 0s 54us/step - loss: 0.0041 - val_loss: 0.0367\n",
      "Epoch 2984/3000\n",
      "2370/2370 [==============================] - 0s 49us/step - loss: 0.0040 - val_loss: 0.0376\n",
      "Epoch 2985/3000\n",
      "2370/2370 [==============================] - 0s 37us/step - loss: 0.0038 - val_loss: 0.0382\n",
      "Epoch 2986/3000\n",
      "2370/2370 [==============================] - 0s 37us/step - loss: 0.0038 - val_loss: 0.0371\n",
      "Epoch 2987/3000\n",
      "2370/2370 [==============================] - 0s 36us/step - loss: 0.0041 - val_loss: 0.0380\n",
      "Epoch 2988/3000\n",
      "2370/2370 [==============================] - 0s 34us/step - loss: 0.0037 - val_loss: 0.0384\n",
      "Epoch 2989/3000\n",
      "2370/2370 [==============================] - 0s 31us/step - loss: 0.0040 - val_loss: 0.0380\n",
      "Epoch 2990/3000\n",
      "2370/2370 [==============================] - 0s 30us/step - loss: 0.0038 - val_loss: 0.0383\n",
      "Epoch 2991/3000\n",
      "2370/2370 [==============================] - 0s 30us/step - loss: 0.0037 - val_loss: 0.0382\n",
      "Epoch 2992/3000\n",
      "2370/2370 [==============================] - 0s 34us/step - loss: 0.0036 - val_loss: 0.0381\n",
      "Epoch 2993/3000\n",
      "2370/2370 [==============================] - 0s 36us/step - loss: 0.0038 - val_loss: 0.0371\n",
      "Epoch 2994/3000\n",
      "2370/2370 [==============================] - 0s 30us/step - loss: 0.0039 - val_loss: 0.0380\n",
      "Epoch 2995/3000\n",
      "2370/2370 [==============================] - 0s 34us/step - loss: 0.0038 - val_loss: 0.0378\n",
      "Epoch 2996/3000\n",
      "2370/2370 [==============================] - 0s 28us/step - loss: 0.0038 - val_loss: 0.0382\n",
      "Epoch 2997/3000\n",
      "2370/2370 [==============================] - 0s 28us/step - loss: 0.0040 - val_loss: 0.0375\n",
      "Epoch 2998/3000\n",
      "2370/2370 [==============================] - 0s 30us/step - loss: 0.0038 - val_loss: 0.0388\n",
      "Epoch 2999/3000\n",
      "2370/2370 [==============================] - 0s 30us/step - loss: 0.0037 - val_loss: 0.0374\n",
      "Epoch 3000/3000\n",
      "2370/2370 [==============================] - 0s 27us/step - loss: 0.0041 - val_loss: 0.0384\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "history=seq_model.fit(x_train, y_train, epochs=3000, batch_size=50, verbose=1, validation_data=(x_test, y_test));\n",
    "end = time.time()\n",
    "time_seq = end-start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2370/2370 [==============================] - 0s 15us/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.00302896274456102"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seq_model.evaluate(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from matplotlib import pyplot\n",
    "pyplot.plot(history.history['loss'], label='Training Loss')\n",
    "pyplot.plot(history.history['val_loss'], label='Testing Loss')\n",
    "plt.yscale(\"log\")\n",
    "pyplot.legend()\n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rolling_seq_prediction(model,x_test,y_test):\n",
    "    predictions = list()\n",
    "    for i in x_test.index:\n",
    "        # make one-step forecast\n",
    "        X = x_test.loc[i].to_numpy().reshape(1,x_test.shape[1]) # reshape single line to have just 1 row\n",
    "        y = y_test.loc[i]\n",
    "        yhat = seq_model.predict(X).reshape(1) # reshape prediction to be just a 1D array\n",
    "        # store forecast\n",
    "        predictions.append(yhat[0])\n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "yhat = rolling_seq_prediction(seq_model,x_test,y_test)\n",
    "raw_seq_predictions = pd.DataFrame(yhat,columns = ['seq_prediction'], index = test_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Invert scaling for prediction data\n",
    "x_test = pd.DataFrame(x_test, columns = x_columns, index = test_index)\n",
    "unscaled_seq_predictions = pd.concat([x_test, raw_seq_predictions], axis=1)\n",
    "unscaled_seq_predictions = pd.concat([unscaled_seq_predictions, test_data.google_45d], axis=1)\n",
    "unscaled_seq_predictions = pd.DataFrame(scaler.inverse_transform(unscaled_seq_predictions), columns=unscaled_seq_predictions.columns, index=unscaled_seq_predictions.index)\n",
    "\n",
    "# Invert scaling for actual data\n",
    "unscaled_data = pd.concat([x_test, y_test], axis=1)\n",
    "unscaled_data = pd.concat([unscaled_data, test_data.google_45d], axis=1)\n",
    "unscaled_data = pd.DataFrame(scaler.inverse_transform(unscaled_data), columns=unscaled_data.columns, index=unscaled_data.index)\n",
    "#unscaled_data = unscaled_data['google_45d_sta']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "backup['unscaled_seq_predictions'] = unscaled_seq_predictions\n",
    "backup['unscaled_data'] = unscaled_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>google_open_sta</th>\n",
       "      <th>google_high_sta</th>\n",
       "      <th>google_low_sta</th>\n",
       "      <th>google_close_sta</th>\n",
       "      <th>google_sta</th>\n",
       "      <th>google_volume</th>\n",
       "      <th>google_ra_04_sta</th>\n",
       "      <th>google_std_04</th>\n",
       "      <th>google_ra_09_sta</th>\n",
       "      <th>google_std_09</th>\n",
       "      <th>...</th>\n",
       "      <th>nasdaq_std_04</th>\n",
       "      <th>nasdaq_ra_09_sta</th>\n",
       "      <th>nasdaq_std_09</th>\n",
       "      <th>nasdaq_ra_18_sta</th>\n",
       "      <th>nasdaq_std_18</th>\n",
       "      <th>google_daily_vol</th>\n",
       "      <th>s&amp;p_daily_vol</th>\n",
       "      <th>nasdaq_daily_vol</th>\n",
       "      <th>seq_prediction</th>\n",
       "      <th>google_45d</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Date</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2018-08-01</th>\n",
       "      <td>-7.989990</td>\n",
       "      <td>-5.881958</td>\n",
       "      <td>-4.609985</td>\n",
       "      <td>-2.750000</td>\n",
       "      <td>-2.750000</td>\n",
       "      <td>1567200.0</td>\n",
       "      <td>-0.503337</td>\n",
       "      <td>1.515790</td>\n",
       "      <td>-2.072859</td>\n",
       "      <td>21.433011</td>\n",
       "      <td>...</td>\n",
       "      <td>38.687654</td>\n",
       "      <td>19.225725</td>\n",
       "      <td>109.753646</td>\n",
       "      <td>7.561348</td>\n",
       "      <td>85.881161</td>\n",
       "      <td>0.285090</td>\n",
       "      <td>0.083101</td>\n",
       "      <td>0.146205</td>\n",
       "      <td>3.985910</td>\n",
       "      <td>1161.219971</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-08-02</th>\n",
       "      <td>22.099976</td>\n",
       "      <td>3.589966</td>\n",
       "      <td>5.419922</td>\n",
       "      <td>-6.140014</td>\n",
       "      <td>-6.140014</td>\n",
       "      <td>1531300.0</td>\n",
       "      <td>-1.786672</td>\n",
       "      <td>3.781604</td>\n",
       "      <td>3.132847</td>\n",
       "      <td>21.550840</td>\n",
       "      <td>...</td>\n",
       "      <td>73.619602</td>\n",
       "      <td>5.440011</td>\n",
       "      <td>106.402575</td>\n",
       "      <td>-0.333123</td>\n",
       "      <td>82.521365</td>\n",
       "      <td>0.271257</td>\n",
       "      <td>0.086055</td>\n",
       "      <td>0.164864</td>\n",
       "      <td>2.815065</td>\n",
       "      <td>1171.089966</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-08-03</th>\n",
       "      <td>-23.719971</td>\n",
       "      <td>-0.119995</td>\n",
       "      <td>-10.270020</td>\n",
       "      <td>2.440063</td>\n",
       "      <td>2.440063</td>\n",
       "      <td>1089600.0</td>\n",
       "      <td>-0.992493</td>\n",
       "      <td>3.932133</td>\n",
       "      <td>5.712856</td>\n",
       "      <td>18.079647</td>\n",
       "      <td>...</td>\n",
       "      <td>69.648419</td>\n",
       "      <td>17.175781</td>\n",
       "      <td>81.003961</td>\n",
       "      <td>-0.449254</td>\n",
       "      <td>82.581619</td>\n",
       "      <td>0.214106</td>\n",
       "      <td>0.085981</td>\n",
       "      <td>0.165144</td>\n",
       "      <td>-3.209007</td>\n",
       "      <td>1186.869995</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-08-06</th>\n",
       "      <td>4.619995</td>\n",
       "      <td>3.911987</td>\n",
       "      <td>-0.736938</td>\n",
       "      <td>-1.060059</td>\n",
       "      <td>-1.060059</td>\n",
       "      <td>1081700.0</td>\n",
       "      <td>-2.457489</td>\n",
       "      <td>0.749575</td>\n",
       "      <td>8.588562</td>\n",
       "      <td>3.444802</td>\n",
       "      <td>...</td>\n",
       "      <td>33.708070</td>\n",
       "      <td>-2.474772</td>\n",
       "      <td>90.494779</td>\n",
       "      <td>6.463542</td>\n",
       "      <td>87.849464</td>\n",
       "      <td>0.151643</td>\n",
       "      <td>0.065983</td>\n",
       "      <td>0.139857</td>\n",
       "      <td>12.214299</td>\n",
       "      <td>1166.089966</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-08-07</th>\n",
       "      <td>-12.000000</td>\n",
       "      <td>-25.082031</td>\n",
       "      <td>-20.373047</td>\n",
       "      <td>-17.449951</td>\n",
       "      <td>-17.449951</td>\n",
       "      <td>1494000.0</td>\n",
       "      <td>-9.255005</td>\n",
       "      <td>12.338979</td>\n",
       "      <td>-2.897138</td>\n",
       "      <td>8.285092</td>\n",
       "      <td>...</td>\n",
       "      <td>16.956406</td>\n",
       "      <td>-19.488118</td>\n",
       "      <td>97.380328</td>\n",
       "      <td>-5.288330</td>\n",
       "      <td>91.512420</td>\n",
       "      <td>0.164522</td>\n",
       "      <td>0.060471</td>\n",
       "      <td>0.127694</td>\n",
       "      <td>0.055840</td>\n",
       "      <td>1173.369995</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 41 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            google_open_sta  google_high_sta  google_low_sta  \\\n",
       "Date                                                           \n",
       "2018-08-01        -7.989990        -5.881958       -4.609985   \n",
       "2018-08-02        22.099976         3.589966        5.419922   \n",
       "2018-08-03       -23.719971        -0.119995      -10.270020   \n",
       "2018-08-06         4.619995         3.911987       -0.736938   \n",
       "2018-08-07       -12.000000       -25.082031      -20.373047   \n",
       "\n",
       "            google_close_sta  google_sta  google_volume  google_ra_04_sta  \\\n",
       "Date                                                                        \n",
       "2018-08-01         -2.750000   -2.750000      1567200.0         -0.503337   \n",
       "2018-08-02         -6.140014   -6.140014      1531300.0         -1.786672   \n",
       "2018-08-03          2.440063    2.440063      1089600.0         -0.992493   \n",
       "2018-08-06         -1.060059   -1.060059      1081700.0         -2.457489   \n",
       "2018-08-07        -17.449951  -17.449951      1494000.0         -9.255005   \n",
       "\n",
       "            google_std_04  google_ra_09_sta  google_std_09  ...  \\\n",
       "Date                                                        ...   \n",
       "2018-08-01       1.515790         -2.072859      21.433011  ...   \n",
       "2018-08-02       3.781604          3.132847      21.550840  ...   \n",
       "2018-08-03       3.932133          5.712856      18.079647  ...   \n",
       "2018-08-06       0.749575          8.588562       3.444802  ...   \n",
       "2018-08-07      12.338979         -2.897138       8.285092  ...   \n",
       "\n",
       "            nasdaq_std_04  nasdaq_ra_09_sta  nasdaq_std_09  nasdaq_ra_18_sta  \\\n",
       "Date                                                                           \n",
       "2018-08-01      38.687654         19.225725     109.753646          7.561348   \n",
       "2018-08-02      73.619602          5.440011     106.402575         -0.333123   \n",
       "2018-08-03      69.648419         17.175781      81.003961         -0.449254   \n",
       "2018-08-06      33.708070         -2.474772      90.494779          6.463542   \n",
       "2018-08-07      16.956406        -19.488118      97.380328         -5.288330   \n",
       "\n",
       "            nasdaq_std_18  google_daily_vol  s&p_daily_vol  nasdaq_daily_vol  \\\n",
       "Date                                                                           \n",
       "2018-08-01      85.881161          0.285090       0.083101          0.146205   \n",
       "2018-08-02      82.521365          0.271257       0.086055          0.164864   \n",
       "2018-08-03      82.581619          0.214106       0.085981          0.165144   \n",
       "2018-08-06      87.849464          0.151643       0.065983          0.139857   \n",
       "2018-08-07      91.512420          0.164522       0.060471          0.127694   \n",
       "\n",
       "            seq_prediction   google_45d  \n",
       "Date                                     \n",
       "2018-08-01        3.985910  1161.219971  \n",
       "2018-08-02        2.815065  1171.089966  \n",
       "2018-08-03       -3.209007  1186.869995  \n",
       "2018-08-06       12.214299  1166.089966  \n",
       "2018-08-07        0.055840  1173.369995  \n",
       "\n",
       "[5 rows x 41 columns]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unscaled_seq_predictions.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>google_open_sta</th>\n",
       "      <th>google_high_sta</th>\n",
       "      <th>google_low_sta</th>\n",
       "      <th>google_close_sta</th>\n",
       "      <th>google_sta</th>\n",
       "      <th>google_volume</th>\n",
       "      <th>google_ra_04_sta</th>\n",
       "      <th>google_std_04</th>\n",
       "      <th>google_ra_09_sta</th>\n",
       "      <th>google_std_09</th>\n",
       "      <th>...</th>\n",
       "      <th>nasdaq_std_04</th>\n",
       "      <th>nasdaq_ra_09_sta</th>\n",
       "      <th>nasdaq_std_09</th>\n",
       "      <th>nasdaq_ra_18_sta</th>\n",
       "      <th>nasdaq_std_18</th>\n",
       "      <th>google_daily_vol</th>\n",
       "      <th>s&amp;p_daily_vol</th>\n",
       "      <th>nasdaq_daily_vol</th>\n",
       "      <th>google_45d_sta</th>\n",
       "      <th>google_45d</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Date</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2018-08-01</th>\n",
       "      <td>-7.989990</td>\n",
       "      <td>-5.881958</td>\n",
       "      <td>-4.609985</td>\n",
       "      <td>-2.750000</td>\n",
       "      <td>-2.750000</td>\n",
       "      <td>1567200.0</td>\n",
       "      <td>-0.503337</td>\n",
       "      <td>1.515790</td>\n",
       "      <td>-2.072859</td>\n",
       "      <td>21.433011</td>\n",
       "      <td>...</td>\n",
       "      <td>38.687654</td>\n",
       "      <td>19.225725</td>\n",
       "      <td>109.753646</td>\n",
       "      <td>7.561348</td>\n",
       "      <td>85.881161</td>\n",
       "      <td>0.285090</td>\n",
       "      <td>0.083101</td>\n",
       "      <td>0.146205</td>\n",
       "      <td>-5.169922</td>\n",
       "      <td>1161.219971</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-08-02</th>\n",
       "      <td>22.099976</td>\n",
       "      <td>3.589966</td>\n",
       "      <td>5.419922</td>\n",
       "      <td>-6.140014</td>\n",
       "      <td>-6.140014</td>\n",
       "      <td>1531300.0</td>\n",
       "      <td>-1.786672</td>\n",
       "      <td>3.781604</td>\n",
       "      <td>3.132847</td>\n",
       "      <td>21.550840</td>\n",
       "      <td>...</td>\n",
       "      <td>73.619602</td>\n",
       "      <td>5.440011</td>\n",
       "      <td>106.402575</td>\n",
       "      <td>-0.333123</td>\n",
       "      <td>82.521365</td>\n",
       "      <td>0.271257</td>\n",
       "      <td>0.086055</td>\n",
       "      <td>0.164864</td>\n",
       "      <td>-9.869995</td>\n",
       "      <td>1171.089966</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-08-03</th>\n",
       "      <td>-23.719971</td>\n",
       "      <td>-0.119995</td>\n",
       "      <td>-10.270020</td>\n",
       "      <td>2.440063</td>\n",
       "      <td>2.440063</td>\n",
       "      <td>1089600.0</td>\n",
       "      <td>-0.992493</td>\n",
       "      <td>3.932133</td>\n",
       "      <td>5.712856</td>\n",
       "      <td>18.079647</td>\n",
       "      <td>...</td>\n",
       "      <td>69.648419</td>\n",
       "      <td>17.175781</td>\n",
       "      <td>81.003961</td>\n",
       "      <td>-0.449254</td>\n",
       "      <td>82.581619</td>\n",
       "      <td>0.214106</td>\n",
       "      <td>0.085981</td>\n",
       "      <td>0.165144</td>\n",
       "      <td>-15.780029</td>\n",
       "      <td>1186.869995</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-08-06</th>\n",
       "      <td>4.619995</td>\n",
       "      <td>3.911987</td>\n",
       "      <td>-0.736938</td>\n",
       "      <td>-1.060059</td>\n",
       "      <td>-1.060059</td>\n",
       "      <td>1081700.0</td>\n",
       "      <td>-2.457489</td>\n",
       "      <td>0.749575</td>\n",
       "      <td>8.588562</td>\n",
       "      <td>3.444802</td>\n",
       "      <td>...</td>\n",
       "      <td>33.708070</td>\n",
       "      <td>-2.474772</td>\n",
       "      <td>90.494779</td>\n",
       "      <td>6.463542</td>\n",
       "      <td>87.849464</td>\n",
       "      <td>0.151643</td>\n",
       "      <td>0.065983</td>\n",
       "      <td>0.139857</td>\n",
       "      <td>20.780029</td>\n",
       "      <td>1166.089966</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-08-07</th>\n",
       "      <td>-12.000000</td>\n",
       "      <td>-25.082031</td>\n",
       "      <td>-20.373047</td>\n",
       "      <td>-17.449951</td>\n",
       "      <td>-17.449951</td>\n",
       "      <td>1494000.0</td>\n",
       "      <td>-9.255005</td>\n",
       "      <td>12.338979</td>\n",
       "      <td>-2.897138</td>\n",
       "      <td>8.285092</td>\n",
       "      <td>...</td>\n",
       "      <td>16.956406</td>\n",
       "      <td>-19.488118</td>\n",
       "      <td>97.380328</td>\n",
       "      <td>-5.288330</td>\n",
       "      <td>91.512420</td>\n",
       "      <td>0.164522</td>\n",
       "      <td>0.060471</td>\n",
       "      <td>0.127694</td>\n",
       "      <td>-7.280029</td>\n",
       "      <td>1173.369995</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 41 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            google_open_sta  google_high_sta  google_low_sta  \\\n",
       "Date                                                           \n",
       "2018-08-01        -7.989990        -5.881958       -4.609985   \n",
       "2018-08-02        22.099976         3.589966        5.419922   \n",
       "2018-08-03       -23.719971        -0.119995      -10.270020   \n",
       "2018-08-06         4.619995         3.911987       -0.736938   \n",
       "2018-08-07       -12.000000       -25.082031      -20.373047   \n",
       "\n",
       "            google_close_sta  google_sta  google_volume  google_ra_04_sta  \\\n",
       "Date                                                                        \n",
       "2018-08-01         -2.750000   -2.750000      1567200.0         -0.503337   \n",
       "2018-08-02         -6.140014   -6.140014      1531300.0         -1.786672   \n",
       "2018-08-03          2.440063    2.440063      1089600.0         -0.992493   \n",
       "2018-08-06         -1.060059   -1.060059      1081700.0         -2.457489   \n",
       "2018-08-07        -17.449951  -17.449951      1494000.0         -9.255005   \n",
       "\n",
       "            google_std_04  google_ra_09_sta  google_std_09  ...  \\\n",
       "Date                                                        ...   \n",
       "2018-08-01       1.515790         -2.072859      21.433011  ...   \n",
       "2018-08-02       3.781604          3.132847      21.550840  ...   \n",
       "2018-08-03       3.932133          5.712856      18.079647  ...   \n",
       "2018-08-06       0.749575          8.588562       3.444802  ...   \n",
       "2018-08-07      12.338979         -2.897138       8.285092  ...   \n",
       "\n",
       "            nasdaq_std_04  nasdaq_ra_09_sta  nasdaq_std_09  nasdaq_ra_18_sta  \\\n",
       "Date                                                                           \n",
       "2018-08-01      38.687654         19.225725     109.753646          7.561348   \n",
       "2018-08-02      73.619602          5.440011     106.402575         -0.333123   \n",
       "2018-08-03      69.648419         17.175781      81.003961         -0.449254   \n",
       "2018-08-06      33.708070         -2.474772      90.494779          6.463542   \n",
       "2018-08-07      16.956406        -19.488118      97.380328         -5.288330   \n",
       "\n",
       "            nasdaq_std_18  google_daily_vol  s&p_daily_vol  nasdaq_daily_vol  \\\n",
       "Date                                                                           \n",
       "2018-08-01      85.881161          0.285090       0.083101          0.146205   \n",
       "2018-08-02      82.521365          0.271257       0.086055          0.164864   \n",
       "2018-08-03      82.581619          0.214106       0.085981          0.165144   \n",
       "2018-08-06      87.849464          0.151643       0.065983          0.139857   \n",
       "2018-08-07      91.512420          0.164522       0.060471          0.127694   \n",
       "\n",
       "            google_45d_sta   google_45d  \n",
       "Date                                     \n",
       "2018-08-01       -5.169922  1161.219971  \n",
       "2018-08-02       -9.869995  1171.089966  \n",
       "2018-08-03      -15.780029  1186.869995  \n",
       "2018-08-06       20.780029  1166.089966  \n",
       "2018-08-07       -7.280029  1173.369995  \n",
       "\n",
       "[5 rows x 41 columns]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unscaled_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot prediction vs actual\n",
    "plt.plot(unscaled_data['google_45d_sta'], label='Actual Adjusted Close')\n",
    "plt.plot(unscaled_seq_predictions['seq_prediction'], label='Predicted Adjusted Close')\n",
    "plt.grid()\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Reverse stationarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reverse_sta(df, sta_column, series):\n",
    "    sum_prediction = []\n",
    "    rolling_sum = 0\n",
    "    for i in df.index:\n",
    "        if rolling_sum == 0:\n",
    "            rolling_sum = series.loc[i]\n",
    "        rolling_sum += df[sta_column].loc[i]\n",
    "        sum_prediction.append(rolling_sum)\n",
    "    return sum_prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum_prediction = reverse_sta(unscaled_seq_predictions, 'seq_prediction', df.google_45d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>seq_prediction</th>\n",
       "      <th>google_45d</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Date</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2018-08-01</th>\n",
       "      <td>1165.205881</td>\n",
       "      <td>1161.219971</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-08-02</th>\n",
       "      <td>1168.020946</td>\n",
       "      <td>1171.089966</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-08-03</th>\n",
       "      <td>1164.811938</td>\n",
       "      <td>1186.869995</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-08-06</th>\n",
       "      <td>1177.026237</td>\n",
       "      <td>1166.089966</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-08-07</th>\n",
       "      <td>1177.082077</td>\n",
       "      <td>1173.369995</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            seq_prediction   google_45d\n",
       "Date                                   \n",
       "2018-08-01     1165.205881  1161.219971\n",
       "2018-08-02     1168.020946  1171.089966\n",
       "2018-08-03     1164.811938  1186.869995\n",
       "2018-08-06     1177.026237  1166.089966\n",
       "2018-08-07     1177.082077  1173.369995"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_predicted = pd.DataFrame(index=unscaled_seq_predictions.index)\n",
    "df_predicted['seq_prediction'] = sum_prediction \n",
    "df_predicted = pd.concat([df_predicted,df.google_45d],axis=1)\n",
    "df_predicted.dropna(inplace=True)\n",
    "df_predicted.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot prediction vs actual\n",
    "plt.plot(df_predicted.google_45d, label='Actual Adjusted Close')\n",
    "plt.plot(df_predicted.seq_prediction, label='FF prediction')\n",
    "plt.grid()\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "92.60238579562633"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seq_accuracy=np.sqrt(metrics.mean_squared_error(df_predicted.google_45d, df_predicted.seq_prediction))\n",
    "seq_accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LSTM model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Split into train and test set\n",
    "And here you have to be careful according to https://machinelearningmastery.com/time-series-forecasting-long-short-term-memory-network-python/ :\n",
    "\n",
    "\"A benefit of this type of network is that it can learn and remember over long sequences and does not rely on a pre-specified window lagged observation as input. In Keras, this is referred to as stateful, and involves setting the “stateful” argument to “True” when defining an LSTM layer. By default, an LSTM layer in Keras maintains state between data within one batch. A batch of data is a fixed-sized number of rows from the training dataset that defines how many patterns to process before updating the weights of the network. State in the LSTM layer between batches is cleared by default, therefore we must make the LSTM stateful. This gives us fine-grained control over when state of the LSTM layer is cleared, by calling the reset_states() function.\"\n",
    "\n",
    "But in order to can use \"stateful = True\" the data has to be a multiple of the batch size. Therefore I have to use modulo to get the test size correctly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 10\n",
    "train_test_split = 0.9\n",
    "x_train, x_test, y_train, y_test, test_data, test_index, x_columns = get_train_test_data(df_sta_scaled, train_test_split, batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train=x_train.to_numpy()\n",
    "y_train=y_train.to_numpy()\n",
    "x_test = x_test.to_numpy()\n",
    "y_test = y_test.to_numpy()\n",
    "# Reshape data for (Sample, Timesteps, Features)\n",
    "x_train = x_train.reshape(x_train.shape[0], 1, x_train.shape[1])\n",
    "x_test_reshaped = x_test.reshape(x_test.shape[0], 1, x_test.shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_1 (LSTM)                (None, 1, 100)            56000     \n",
      "_________________________________________________________________\n",
      "lstm_2 (LSTM)                (None, 100)               80400     \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 1)                 101       \n",
      "=================================================================\n",
      "Total params: 136,501\n",
      "Trainable params: 136,501\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "lstm_model = Sequential()\n",
    "lstm_model.add(LSTM(units=100, input_shape = (x_train.shape[1], x_train.shape[2]), dropout = 0.2, return_sequences=True))#, stateful=True, batch_size=batch_size))\n",
    "lstm_model.add(LSTM(units = 100, dropout = 0.2))#, return_sequences=False, stateful=True, batch_size=batch_size))\n",
    "lstm_model.add(Dense(1))\n",
    "adam = optimizers.Adam(lr=0.005)\n",
    "lstm_model.compile(optimizer = adam, loss='mse')\n",
    "lstm_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/matthias/anaconda3/lib/python3.7/site-packages/tensorflow/python/ops/math_grad.py:1250: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "Train on 2370 samples, validate on 260 samples\n",
      "Epoch 1/100\n",
      "2370/2370 [==============================] - 5s 2ms/step - loss: 0.0065 - val_loss: 0.0360\n",
      "Epoch 2/100\n",
      "2370/2370 [==============================] - 4s 2ms/step - loss: 0.0062 - val_loss: 0.0360\n",
      "Epoch 3/100\n",
      "2370/2370 [==============================] - 4s 2ms/step - loss: 0.0062 - val_loss: 0.0379\n",
      "Epoch 4/100\n",
      "2370/2370 [==============================] - 4s 2ms/step - loss: 0.0062 - val_loss: 0.0380\n",
      "Epoch 5/100\n",
      "2370/2370 [==============================] - 4s 2ms/step - loss: 0.0062 - val_loss: 0.0381\n",
      "Epoch 6/100\n",
      "2370/2370 [==============================] - 4s 2ms/step - loss: 0.0061 - val_loss: 0.0382\n",
      "Epoch 7/100\n",
      "2370/2370 [==============================] - 4s 2ms/step - loss: 0.0061 - val_loss: 0.0403\n",
      "Epoch 8/100\n",
      "2370/2370 [==============================] - 4s 2ms/step - loss: 0.0063 - val_loss: 0.0366\n",
      "Epoch 9/100\n",
      "2370/2370 [==============================] - 4s 2ms/step - loss: 0.0063 - val_loss: 0.0368\n",
      "Epoch 10/100\n",
      "2370/2370 [==============================] - 4s 2ms/step - loss: 0.0062 - val_loss: 0.0371\n",
      "Epoch 11/100\n",
      "2370/2370 [==============================] - 4s 2ms/step - loss: 0.0062 - val_loss: 0.0357\n",
      "Epoch 12/100\n",
      "2370/2370 [==============================] - 4s 2ms/step - loss: 0.0062 - val_loss: 0.0374\n",
      "Epoch 13/100\n",
      "2370/2370 [==============================] - 4s 2ms/step - loss: 0.0062 - val_loss: 0.0363\n",
      "Epoch 14/100\n",
      "2370/2370 [==============================] - 4s 2ms/step - loss: 0.0061 - val_loss: 0.0368\n",
      "Epoch 15/100\n",
      "2370/2370 [==============================] - 4s 2ms/step - loss: 0.0061 - val_loss: 0.0369\n",
      "Epoch 16/100\n",
      "2370/2370 [==============================] - 4s 2ms/step - loss: 0.0061 - val_loss: 0.0366\n",
      "Epoch 17/100\n",
      "2370/2370 [==============================] - 4s 2ms/step - loss: 0.0061 - val_loss: 0.0364\n",
      "Epoch 18/100\n",
      "2370/2370 [==============================] - 4s 2ms/step - loss: 0.0062 - val_loss: 0.0373\n",
      "Epoch 19/100\n",
      "2370/2370 [==============================] - 4s 2ms/step - loss: 0.0061 - val_loss: 0.0398\n",
      "Epoch 20/100\n",
      "2370/2370 [==============================] - 4s 2ms/step - loss: 0.0061 - val_loss: 0.0374\n",
      "Epoch 21/100\n",
      "2370/2370 [==============================] - 3s 1ms/step - loss: 0.0061 - val_loss: 0.0369\n",
      "Epoch 22/100\n",
      "2370/2370 [==============================] - 3s 1ms/step - loss: 0.0060 - val_loss: 0.0365\n",
      "Epoch 23/100\n",
      "2370/2370 [==============================] - 4s 2ms/step - loss: 0.0060 - val_loss: 0.0384\n",
      "Epoch 24/100\n",
      "2370/2370 [==============================] - 3s 1ms/step - loss: 0.0060 - val_loss: 0.0361\n",
      "Epoch 25/100\n",
      "2370/2370 [==============================] - 4s 2ms/step - loss: 0.0060 - val_loss: 0.0376\n",
      "Epoch 26/100\n",
      "2370/2370 [==============================] - 3s 1ms/step - loss: 0.0061 - val_loss: 0.0368\n",
      "Epoch 27/100\n",
      "2370/2370 [==============================] - 4s 2ms/step - loss: 0.0060 - val_loss: 0.0364\n",
      "Epoch 28/100\n",
      "2370/2370 [==============================] - 3s 1ms/step - loss: 0.0060 - val_loss: 0.0366\n",
      "Epoch 29/100\n",
      "2370/2370 [==============================] - 3s 1ms/step - loss: 0.0060 - val_loss: 0.0362\n",
      "Epoch 30/100\n",
      "2370/2370 [==============================] - 4s 2ms/step - loss: 0.0061 - val_loss: 0.0379\n",
      "Epoch 31/100\n",
      "2370/2370 [==============================] - 3s 1ms/step - loss: 0.0060 - val_loss: 0.0384\n",
      "Epoch 32/100\n",
      "2370/2370 [==============================] - 3s 1ms/step - loss: 0.0060 - val_loss: 0.0366\n",
      "Epoch 33/100\n",
      "2370/2370 [==============================] - 4s 2ms/step - loss: 0.0060 - val_loss: 0.0389\n",
      "Epoch 34/100\n",
      "2370/2370 [==============================] - 3s 1ms/step - loss: 0.0060 - val_loss: 0.0363\n",
      "Epoch 35/100\n",
      "2370/2370 [==============================] - 4s 2ms/step - loss: 0.0059 - val_loss: 0.0380\n",
      "Epoch 36/100\n",
      "2370/2370 [==============================] - 4s 2ms/step - loss: 0.0059 - val_loss: 0.0368\n",
      "Epoch 37/100\n",
      "2370/2370 [==============================] - 4s 2ms/step - loss: 0.0059 - val_loss: 0.0382\n",
      "Epoch 38/100\n",
      "2370/2370 [==============================] - 3s 1ms/step - loss: 0.0059 - val_loss: 0.0382\n",
      "Epoch 39/100\n",
      "2370/2370 [==============================] - 4s 2ms/step - loss: 0.0060 - val_loss: 0.0366\n",
      "Epoch 40/100\n",
      "2370/2370 [==============================] - 3s 1ms/step - loss: 0.0060 - val_loss: 0.0371\n",
      "Epoch 41/100\n",
      "2370/2370 [==============================] - 3s 1ms/step - loss: 0.0059 - val_loss: 0.0376\n",
      "Epoch 42/100\n",
      "2370/2370 [==============================] - 3s 1ms/step - loss: 0.0059 - val_loss: 0.0387\n",
      "Epoch 43/100\n",
      "2370/2370 [==============================] - 3s 1ms/step - loss: 0.0058 - val_loss: 0.0389\n",
      "Epoch 44/100\n",
      "2370/2370 [==============================] - 3s 1ms/step - loss: 0.0058 - val_loss: 0.0376\n",
      "Epoch 45/100\n",
      "2370/2370 [==============================] - 3s 1ms/step - loss: 0.0057 - val_loss: 0.0400\n",
      "Epoch 46/100\n",
      "2370/2370 [==============================] - 3s 1ms/step - loss: 0.0059 - val_loss: 0.0379\n",
      "Epoch 47/100\n",
      "2370/2370 [==============================] - 3s 1ms/step - loss: 0.0058 - val_loss: 0.0393\n",
      "Epoch 48/100\n",
      "2370/2370 [==============================] - 3s 1ms/step - loss: 0.0058 - val_loss: 0.0391\n",
      "Epoch 49/100\n",
      "2370/2370 [==============================] - 4s 2ms/step - loss: 0.0058 - val_loss: 0.0393\n",
      "Epoch 50/100\n",
      "2370/2370 [==============================] - 3s 1ms/step - loss: 0.0059 - val_loss: 0.0370\n",
      "Epoch 51/100\n",
      "2370/2370 [==============================] - 3s 1ms/step - loss: 0.0058 - val_loss: 0.0383\n",
      "Epoch 52/100\n",
      "2370/2370 [==============================] - 3s 1ms/step - loss: 0.0058 - val_loss: 0.0391\n",
      "Epoch 53/100\n",
      "2370/2370 [==============================] - 3s 1ms/step - loss: 0.0057 - val_loss: 0.0388\n",
      "Epoch 54/100\n",
      "2370/2370 [==============================] - 3s 1ms/step - loss: 0.0057 - val_loss: 0.0392\n",
      "Epoch 55/100\n",
      "2370/2370 [==============================] - 3s 1ms/step - loss: 0.0058 - val_loss: 0.0403\n",
      "Epoch 56/100\n",
      "2370/2370 [==============================] - 3s 1ms/step - loss: 0.0058 - val_loss: 0.0404\n",
      "Epoch 57/100\n",
      "2370/2370 [==============================] - 3s 1ms/step - loss: 0.0057 - val_loss: 0.0377\n",
      "Epoch 58/100\n",
      "2370/2370 [==============================] - 3s 1ms/step - loss: 0.0058 - val_loss: 0.0391\n",
      "Epoch 59/100\n",
      "2370/2370 [==============================] - 3s 1ms/step - loss: 0.0058 - val_loss: 0.0389\n",
      "Epoch 60/100\n",
      "2370/2370 [==============================] - 3s 1ms/step - loss: 0.0057 - val_loss: 0.0404\n",
      "Epoch 61/100\n",
      "2370/2370 [==============================] - 3s 1ms/step - loss: 0.0057 - val_loss: 0.0416\n",
      "Epoch 62/100\n",
      "2370/2370 [==============================] - 3s 1ms/step - loss: 0.0057 - val_loss: 0.0408\n",
      "Epoch 63/100\n",
      "2370/2370 [==============================] - 4s 2ms/step - loss: 0.0057 - val_loss: 0.0392\n",
      "Epoch 64/100\n",
      "2370/2370 [==============================] - 3s 1ms/step - loss: 0.0057 - val_loss: 0.0407\n",
      "Epoch 65/100\n",
      "2370/2370 [==============================] - 3s 1ms/step - loss: 0.0057 - val_loss: 0.0418\n",
      "Epoch 66/100\n",
      "2370/2370 [==============================] - 3s 1ms/step - loss: 0.0057 - val_loss: 0.0397\n",
      "Epoch 67/100\n",
      "2370/2370 [==============================] - 3s 1ms/step - loss: 0.0058 - val_loss: 0.0398\n",
      "Epoch 68/100\n",
      "2370/2370 [==============================] - 3s 1ms/step - loss: 0.0057 - val_loss: 0.0407\n",
      "Epoch 69/100\n",
      "2370/2370 [==============================] - 3s 1ms/step - loss: 0.0057 - val_loss: 0.0419\n",
      "Epoch 70/100\n",
      "2370/2370 [==============================] - 4s 1ms/step - loss: 0.0057 - val_loss: 0.0378\n",
      "Epoch 71/100\n",
      "2370/2370 [==============================] - 3s 1ms/step - loss: 0.0057 - val_loss: 0.0399\n",
      "Epoch 72/100\n",
      "2370/2370 [==============================] - 3s 1ms/step - loss: 0.0057 - val_loss: 0.0371\n",
      "Epoch 73/100\n",
      "2370/2370 [==============================] - 3s 1ms/step - loss: 0.0057 - val_loss: 0.0403\n",
      "Epoch 74/100\n",
      "2370/2370 [==============================] - 3s 1ms/step - loss: 0.0057 - val_loss: 0.0414\n",
      "Epoch 75/100\n",
      "2370/2370 [==============================] - 3s 1ms/step - loss: 0.0057 - val_loss: 0.0395\n",
      "Epoch 76/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2370/2370 [==============================] - 2s 1ms/step - loss: 0.0057 - val_loss: 0.0392\n",
      "Epoch 77/100\n",
      "2370/2370 [==============================] - 3s 1ms/step - loss: 0.0056 - val_loss: 0.0437\n",
      "Epoch 78/100\n",
      "2370/2370 [==============================] - 3s 1ms/step - loss: 0.0058 - val_loss: 0.0398\n",
      "Epoch 79/100\n",
      "2370/2370 [==============================] - 2s 1ms/step - loss: 0.0057 - val_loss: 0.0392\n",
      "Epoch 80/100\n",
      "2370/2370 [==============================] - 2s 1ms/step - loss: 0.0057 - val_loss: 0.0395\n",
      "Epoch 81/100\n",
      "2370/2370 [==============================] - 3s 1ms/step - loss: 0.0057 - val_loss: 0.0423\n",
      "Epoch 82/100\n",
      "2370/2370 [==============================] - 3s 1ms/step - loss: 0.0056 - val_loss: 0.0392\n",
      "Epoch 83/100\n",
      "2370/2370 [==============================] - 2s 1ms/step - loss: 0.0057 - val_loss: 0.0406\n",
      "Epoch 84/100\n",
      "2370/2370 [==============================] - 2s 1ms/step - loss: 0.0058 - val_loss: 0.0405\n",
      "Epoch 85/100\n",
      "2370/2370 [==============================] - 2s 1ms/step - loss: 0.0055 - val_loss: 0.0422\n",
      "Epoch 86/100\n",
      "2370/2370 [==============================] - 2s 1ms/step - loss: 0.0057 - val_loss: 0.0393\n",
      "Epoch 87/100\n",
      "2370/2370 [==============================] - 2s 1ms/step - loss: 0.0056 - val_loss: 0.0427\n",
      "Epoch 88/100\n",
      "2370/2370 [==============================] - 2s 1ms/step - loss: 0.0057 - val_loss: 0.0421\n",
      "Epoch 89/100\n",
      "2370/2370 [==============================] - 2s 1ms/step - loss: 0.0057 - val_loss: 0.0422\n",
      "Epoch 90/100\n",
      "2370/2370 [==============================] - 2s 1ms/step - loss: 0.0057 - val_loss: 0.0382\n",
      "Epoch 91/100\n",
      "2370/2370 [==============================] - 3s 1ms/step - loss: 0.0056 - val_loss: 0.0411\n",
      "Epoch 92/100\n",
      "2370/2370 [==============================] - 2s 1ms/step - loss: 0.0056 - val_loss: 0.0412\n",
      "Epoch 93/100\n",
      "2370/2370 [==============================] - 2s 1ms/step - loss: 0.0057 - val_loss: 0.0424\n",
      "Epoch 94/100\n",
      "2370/2370 [==============================] - 2s 1ms/step - loss: 0.0057 - val_loss: 0.0412\n",
      "Epoch 95/100\n",
      "2370/2370 [==============================] - 3s 1ms/step - loss: 0.0056 - val_loss: 0.0412\n",
      "Epoch 96/100\n",
      "2370/2370 [==============================] - 2s 1ms/step - loss: 0.0056 - val_loss: 0.0411\n",
      "Epoch 97/100\n",
      "2370/2370 [==============================] - 3s 1ms/step - loss: 0.0056 - val_loss: 0.0412\n",
      "Epoch 98/100\n",
      "2370/2370 [==============================] - 2s 1ms/step - loss: 0.0057 - val_loss: 0.0392\n",
      "Epoch 99/100\n",
      "2370/2370 [==============================] - 2s 1ms/step - loss: 0.0055 - val_loss: 0.0411\n",
      "Epoch 100/100\n",
      "2370/2370 [==============================] - 2s 1ms/step - loss: 0.0056 - val_loss: 0.0421\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "epochs = 100\n",
    "lstm_model.fit(x_train, y_train, epochs=epochs, batch_size=batch_size, verbose=1, validation_data=(x_test_reshaped, y_test),shuffle=False);\n",
    "end = time.time()\n",
    "time_lstm = end-start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2370/2370 [==============================] - 1s 240us/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.0053520484293643035"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lstm_model.evaluate(x_train, y_train, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x7fea21c9af98>"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from matplotlib import pyplot\n",
    "pyplot.plot(history.history['loss'], label='Training Loss')\n",
    "pyplot.plot(history.history['val_loss'], label='Testing Loss')\n",
    "plt.yscale(\"log\")\n",
    "pyplot.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Get predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rolling_lstm_prediction(model,x_test,y_test):\n",
    "    predictions = list()\n",
    "    for i in range(len(x_test)):\n",
    "        # make one-step forecast\n",
    "        X = x_test[i,:] # select single line\n",
    "        X = X.reshape(1, 1, len(X))  # reshape for LSTM\n",
    "        y = y_test[i]\n",
    "        yhat = lstm_model.predict(X, batch_size=1)[0] # reshape prediction to be just a 1D array\n",
    "        # store forecast\n",
    "        predictions.append(yhat[0])\n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate predictions\n",
    "yhat = rolling_lstm_prediction(lstm_model,x_test,y_test);\n",
    "raw_lstm_predictions = pd.DataFrame(yhat,columns = ['lstm_prediction'], index = test_index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Convert data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test = pd.DataFrame(x_test, columns = x_columns, index = test_index)\n",
    "y_test = pd.DataFrame(y_test, columns = ['google_45d_sta'], index = test_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Invert scaling for prediction data\n",
    "unscaled_lstm_predictions = pd.concat([x_test, raw_lstm_predictions], axis=1)\n",
    "unscaled_lstm_predictions = pd.concat([unscaled_lstm_predictions, test_data.google_45d], axis=1)\n",
    "unscaled_lstm_predictions = pd.DataFrame(scaler.inverse_transform(unscaled_lstm_predictions), columns=unscaled_lstm_predictions.columns, index=unscaled_lstm_predictions.index)\n",
    "\n",
    "# Invert scaling for actual data\n",
    "test_data_unscaled = pd.concat([x_test, y_test], axis=1)\n",
    "test_data_unscaled = pd.concat([test_data_unscaled, test_data.google_45d], axis=1)\n",
    "test_data_unscaled = pd.DataFrame(scaler.inverse_transform(test_data_unscaled), columns=test_data_unscaled.columns, index=test_data_unscaled.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "backup['unscaled_lstm_predictions'] = unscaled_lstm_predictions\n",
    "backup['test_data_unscaled'] = test_data_unscaled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Plot prediction vs actual\n",
    "pyplot.plot(test_data_unscaled['google_45d_sta'], label='Stationary adjusted close')\n",
    "pyplot.plot(unscaled_seq_predictions['seq_prediction'], label='FF Predicted')\n",
    "pyplot.plot(unscaled_lstm_predictions['lstm_prediction'], label='LSTM Predicted')\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('USD')\n",
    "pyplot.grid()\n",
    "pyplot.legend()\n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unscaled_lstm_predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Reverse stationary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_predicted['lstm_prediction'] = reverse_sta(unscaled_lstm_predictions, 'lstm_prediction', df.google_45d)\n",
    "df_predicted.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot google_45d and predicted\n",
    "plt.plot(df_predicted.google_45d, label = 'Adjusted Close')\n",
    "plt.plot(df_predicted.seq_prediction, label = 'FF prediction')\n",
    "plt.plot(df_predicted.lstm_prediction, label = 'LSTM prediction')\n",
    "plt.grid()\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "backup['df_predicted'] = df_predicted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "backup.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lstm_accuracy=np.sqrt(metrics.mean_squared_error(df_predicted.google_45d, df_predicted.lstm_prediction))\n",
    "print('Accuracy of feedforward model is {} \\nAccuracy of LSTM model is {}'.format(round(seq_accuracy,2), round(lstm_accuracy)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Run time for feedforward model is {} seconds \\nRun time for LSTM model is {} seconds'.format(time_seq,time_lstm))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
